{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Jensen-Shannon : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Jensen-Shannon : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Jensen-Shannon émerge à l’intersection de la théorie
de l’information et des mathématiques appliquées. Son origine remonte
aux travaux fondateurs de Claude Shannon sur les mesures d’information,
et plus tard, à la généralisation proposée par Robert L. Jensen. Cette
inégalité est indispensable dans le cadre de l’analyse des divergences
entre distributions de probabilité, offrant une mesure symétrique et
robuste qui surmonte les limitations des approches traditionnelles comme
la divergence de Kullback-Leibler.</p>
<p>L’inégalité de Jensen-Shannon trouve ses applications dans des
domaines variés tels que le traitement du signal, l’apprentissage
automatique, et la bioinformatique. Elle permet de quantifier la
similarité entre distributions, une tâche cruciale pour la
classification, la compression de données, et l’analyse de séquences
biologiques. Son importance réside dans sa capacité à fournir une mesure
cohérente et intuitive, même lorsque les distributions comparées sont
très différentes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour aborder l’inégalité de Jensen-Shannon, il est essentiel de
comprendre les concepts fondamentaux qui la sous-tendent. Commençons par
la divergence de Kullback-Leibler, une mesure classique de la distance
entre deux distributions.</p>
<p>Considérons deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une mesure de
la quantité d’information perdue lorsque <span
class="math inline">\(Q\)</span> est utilisée pour approximer <span
class="math inline">\(P\)</span>. Cette idée nous mène naturellement à
la définition suivante :</p>
<div class="definition">
<p>La divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right)\]</span> ou, dans le cas continu,
<span class="math display">\[D_{KL}(P \| Q) = \int_{\mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right) dx\]</span></p>
</div>
<p>Cependant, la divergence de Kullback-Leibler n’est pas symétrique, ce
qui peut poser problème dans certaines applications. Pour pallier cette
limitation, nous introduisons la divergence de Jensen-Shannon.</p>
<div class="definition">
<p>La divergence de Jensen-Shannon entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme : <span
class="math display">\[D_{JS}(P \| Q) = \frac{1}{2} D_{KL}\left(P \|
M\right) + \frac{1}{2} D_{KL}\left(Q \| M\right)\]</span> où <span
class="math inline">\(M = \frac{1}{2}(P + Q)\)</span> est la
distribution moyenne de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<p>Cette définition peut également être exprimée en utilisant l’entropie
de Shannon. Rappelons que l’entropie d’une distribution <span
class="math inline">\(P\)</span> est donnée par : <span
class="math display">\[H(P) = -\sum_{x \in \mathcal{X}} P(x) \log
P(x)\]</span> ou, dans le cas continu, <span class="math display">\[H(P)
= -\int_{\mathcal{X}} P(x) \log P(x) dx\]</span></p>
<p>En termes d’entropie, la divergence de Jensen-Shannon peut être
écrite comme : <span class="math display">\[D_{JS}(P \| Q) = H\left(
\frac{P + Q}{2} \right) - \frac{1}{2} H(P) - \frac{1}{2}
H(Q)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’inégalité de Jensen-Shannon est un résultat fondamental qui relie
la divergence de Jensen-Shannon à la distance de Hellinger. Commençons
par rappeler la définition de la distance de Hellinger.</p>
<div class="definition">
<p>La distance de Hellinger entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme : <span
class="math display">\[H(P, Q) = \sqrt{1 - B(P, Q)}\]</span> où <span
class="math inline">\(B(P, Q) = \sum_{x \in \mathcal{X}} \sqrt{P(x)
Q(x)}\)</span> est l’affinité de Bhattacharyya.</p>
</div>
<p>Nous sommes maintenant prêts à énoncer l’inégalité de
Jensen-Shannon.</p>
<div class="theorem">
<p>Pour toute paire de distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, la divergence de Jensen-Shannon est
bornée par la distance de Hellinger comme suit : <span
class="math display">\[D_{JS}(P \| Q) \leq 2 H^2(P, Q)\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Jensen-Shannon, nous allons utiliser
plusieurs étapes intermédiaires et des résultats connus en théorie de
l’information.</p>
<div class="proof">
<p><em>Proof.</em> Commençons par rappeler que la divergence de
Kullback-Leibler peut être exprimée en termes d’entropie conditionnelle.
Pour toute distribution <span class="math inline">\(P\)</span> et tout
canal de transition <span class="math inline">\(Q(x|y)\)</span>, nous
avons : <span class="math display">\[D_{KL}(P \| Q) = H(P) -
H(P|Q)\]</span> où <span class="math inline">\(H(P|Q)\)</span> est
l’entropie conditionnelle de <span class="math inline">\(P\)</span>
étant donné <span class="math inline">\(Q\)</span>.</p>
<p>Appliquons cette identité à la divergence de Jensen-Shannon : <span
class="math display">\[D_{JS}(P \| Q) = \frac{1}{2} D_{KL}\left(P \|
M\right) + \frac{1}{2} D_{KL}\left(Q \| M\right) = H(P|M) + H(Q|M) -
H(P) - H(Q)\]</span></p>
<p>En utilisant l’inégalité de Fano, nous savons que pour toute
distribution <span class="math inline">\(P\)</span> et tout canal de
transition <span class="math inline">\(Q(x|y)\)</span>, l’entropie
conditionnelle est bornée par : <span class="math display">\[H(P|Q) \leq
H\left( P, Q \right)\]</span> où <span class="math inline">\(H\left( P,
Q \right) = -\sum_{x,y} P(x) Q(y|x) \log Q(y|x)\)</span> est l’entropie
mutuelle.</p>
<p>En appliquant cette inégalité à <span
class="math inline">\(H(P|M)\)</span> et <span
class="math inline">\(H(Q|M)\)</span>, nous obtenons : <span
class="math display">\[H(P|M) + H(Q|M) \leq 2 H\left( P, M
\right)\]</span></p>
<p>Il nous reste à relier <span class="math inline">\(H\left( P, M
\right)\)</span> à la distance de Hellinger. Utilisons l’inégalité de
Pinsker, qui stipule que pour toute paire de distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[D_{KL}(P \| Q) \geq 2 H^2(P, Q)\]</span></p>
<p>En combinant ces résultats, nous obtenons finalement : <span
class="math display">\[D_{JS}(P \| Q) \leq 2 H^2(P, Q)\]</span> ce qui
achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’inégalité de Jensen-Shannon possède plusieurs propriétés
intéressantes qui en font un outil puissant pour l’analyse des
distributions.</p>
<div class="proposition">
<p>La divergence de Jensen-Shannon satisfait les propriétés suivantes
:</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(D_{JS}(P \| Q) = D_{JS}(Q
\| P)\)</span></p></li>
<li><p>Bornes : <span class="math inline">\(0 \leq D_{JS}(P \| Q) \leq
\log 2\)</span></p></li>
<li><p>Invariance par transformation : Si <span
class="math inline">\(T\)</span> est une transformation bijective, alors
<span class="math inline">\(D_{JS}(P \| Q) = D_{JS}(T(P) \|
T(Q))\)</span></p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> La symétrie découle directement de la définition de
<span class="math inline">\(D_{JS}\)</span>, car l’échange de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> ne change pas la distribution moyenne
<span class="math inline">\(M\)</span>. Les bornes sont une conséquence
de l’inégalité de Gibbs et des propriétés de l’entropie. L’invariance
par transformation résulte du fait que les transformations bijectives
préservent les mesures de probabilité. ◻</p>
</div>
<div class="corollaire">
<p>La divergence de Jensen-Shannon est une métrique sur l’espace des
distributions de probabilité.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer que <span
class="math inline">\(D_{JS}\)</span> est une métrique, il suffit de
vérifier les conditions suivantes :</p>
<ol>
<li><p><span class="math inline">\(D_{JS}(P \| Q) = 0\)</span> si et
seulement si <span class="math inline">\(P = Q\)</span></p></li>
<li><p>Symétrie : <span class="math inline">\(D_{JS}(P \| Q) = D_{JS}(Q
\| P)\)</span></p></li>
<li><p>Inégalité triangulaire : <span class="math inline">\(D_{JS}(P \|
R) \leq D_{JS}(P \| Q) + D_{JS}(Q \| R)\)</span></p></li>
</ol>
<p>La première condition est une conséquence des bornes de <span
class="math inline">\(D_{JS}\)</span>. La symétrie a déjà été démontrée.
L’inégalité triangulaire peut être prouvée en utilisant les propriétés
de la divergence de Kullback-Leibler et l’inégalité de Jensen. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Jensen-Shannon est un résultat fondamental en théorie
de l’information, offrant une mesure symétrique et robuste de la
divergence entre distributions de probabilité. Ses applications sont
vastes, allant du traitement du signal à l’apprentissage automatique en
passant par la bioinformatique. En comprenant les définitions, théorèmes
et propriétés associés à cette inégalité, nous avons jeté les bases pour
une utilisation efficace dans divers domaines de la recherche et de
l’industrie.</p>
</body>
</html>
{% include "footer.html" %}


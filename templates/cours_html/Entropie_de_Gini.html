{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Gini : Une Mesure de l’Inégalité et de la Diversité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Gini : Une Mesure de l’Inégalité et de
la Diversité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de Gini émerge comme une mesure sophistiquée de
l’inégalité et de la diversité, inspirée par les travaux du statisticien
italien Corrado Gini. À l’origine développée pour quantifier les
disparités économiques, cette notion s’est étendue à divers domaines,
notamment en écologie pour évaluer la biodiversité et en sciences
sociales pour analyser les structures de classes. Son importance réside
dans sa capacité à capturer la complexité des distributions, offrant une
alternative aux indices traditionnels comme l’indice de Gini classique.
Dans ce contexte, nous explorons les fondements mathématiques de cette
entropie, ses applications pratiques, et son rôle crucial dans l’analyse
des systèmes complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Gini, commençons par examiner ce que
nous cherchons à mesurer. Imaginons une distribution de ressources ou
d’individus où nous voulons quantifier à quel point ces ressources sont
inégalement réparties. L’idée est de capturer non seulement la disparité
globale, mais aussi la structure fine des inégalités. Cela nous amène à
définir l’entropie de Gini.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
discrète prenant des valeurs dans un ensemble fini <span
class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> avec des
probabilités <span class="math inline">\(p_1, p_2, \ldots, p_n\)</span>.
L’entropie de Gini <span class="math inline">\(H_G(X)\)</span> est
définie comme : <span class="math display">\[H_G(X) = -\sum_{i=1}^n p_i
\log(p_i) + \sum_{i=1}^n \sum_{j=1}^n p_i p_j |x_i - x_j|\]</span> De
manière équivalente, pour une distribution de probabilité <span
class="math inline">\(P\)</span> sur un ensemble fini <span
class="math inline">\(S\)</span>, l’entropie de Gini peut être exprimée
comme : <span class="math display">\[H_G(P) = -\sum_{s \in S} P(s)
\log(P(s)) + \sum_{s, t \in S} P(s) P(t) |s - t|\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de Gini est celui qui
établit sa relation avec d’autres mesures d’inégalité. Nous commençons
par explorer ce que nous cherchons à démontrer : comment l’entropie de
Gini se compare-t-elle à l’indice de Gini classique et à l’entropie de
Shannon.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
discrète avec des valeurs <span class="math inline">\(\{x_1, x_2,
\ldots, x_n\}\)</span> et des probabilités <span
class="math inline">\(\{p_1, p_2, \ldots, p_n\}\)</span>. L’entropie de
Gini <span class="math inline">\(H_G(X)\)</span> et l’indice de Gini
<span class="math inline">\(G(X)\)</span> sont liés par la relation
suivante : <span class="math display">\[H_G(X) = H_S(X) + G(X)\]</span>
où <span class="math inline">\(H_S(X)\)</span> est l’entropie de Shannon
définie par : <span class="math display">\[H_S(X) = -\sum_{i=1}^n p_i
\log(p_i)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous procédons comme suit
:</p>
<p>1. **Définition de l’Entropie de Shannon** : L’entropie de Shannon
<span class="math inline">\(H_S(X)\)</span> est donnée par : <span
class="math display">\[H_S(X) = -\sum_{i=1}^n p_i \log(p_i)\]</span></p>
<p>2. **Définition de l’Indice de Gini** : L’indice de Gini <span
class="math inline">\(G(X)\)</span> est défini comme : <span
class="math display">\[G(X) = \sum_{i=1}^n \sum_{j=1}^n p_i p_j |x_i -
x_j|\]</span></p>
<p>3. **Combinaison des Définitions** : En combinant les définitions de
l’entropie de Shannon et de l’indice de Gini, nous obtenons : <span
class="math display">\[H_G(X) = H_S(X) + G(X)\]</span></p>
<p>Cette relation montre que l’entropie de Gini est une somme de
l’entropie de Shannon et de l’indice de Gini, capturant ainsi à la fois
la diversité et les inégalités dans une distribution. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous énumérons et développons les propriétés clés de l’entropie de
Gini :</p>
<ol>
<li><p>**Non-Négativité** : L’entropie de Gini est toujours non
négative. Cela signifie que pour toute distribution de probabilité <span
class="math inline">\(P\)</span>, nous avons : <span
class="math display">\[H_G(P) \geq 0\]</span> La preuve de cette
propriété découle directement des définitions de l’entropie de Shannon
et de l’indice de Gini, qui sont tous deux non négatifs.</p></li>
<li><p>**Maximisation** : L’entropie de Gini atteint son maximum lorsque
la distribution est uniforme. Pour une distribution uniforme sur <span
class="math inline">\(n\)</span> éléments, nous avons : <span
class="math display">\[H_G(P) = \log(n)\]</span> Cette propriété est
cruciale pour comprendre les limites supérieures de l’entropie de
Gini.</p></li>
<li><p>**Additivité** : L’entropie de Gini est additive pour les
distributions indépendantes. Si <span class="math inline">\(X\)</span>
et <span class="math inline">\(Y\)</span> sont des variables aléatoires
indépendantes, alors : <span class="math display">\[H_G(XY) = H_G(X) +
H_G(Y)\]</span> Cette propriété permet de décomposer l’entropie de Gini
pour des systèmes complexes en sous-systèmes plus simples.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Gini se révèle être un outil puissant pour mesurer à la
fois l’inégalité et la diversité dans divers contextes. Son lien avec
l’entropie de Shannon et l’indice de Gini en fait une mesure
polyvalente, applicable dans des domaines allant de l’économie à
l’écologie. Les propriétés et théorèmes associés à cette entropie
ouvrent des perspectives prometteuses pour l’analyse des systèmes
complexes, offrant une compréhension plus nuancée des distributions et
de leurs structures sous-jacentes.</p>
</body>
</html>
{% include "footer.html" %}


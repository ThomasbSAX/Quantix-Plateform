{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Dropout : Une Technique Révolutionnaire en Apprentissage Profond</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Dropout : Une Technique Révolutionnaire en
Apprentissage Profond</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage profond a connu un essor fulgurant ces dernières
années, révolutionnant des domaines tels que la vision par ordinateur,
le traitement automatique du langage naturel et bien d’autres.
Cependant, malgré ses succès, l’apprentissage profond n’est pas sans
défis. L’un des problèmes majeurs est le surapprentissage (overfitting),
où un modèle apprend non seulement les caractéristiques pertinentes des
données d’entraînement, mais aussi le bruit et les particularités
spécifiques à ces données. Cela conduit à une performance médiocre sur
des données non vues.</p>
<p>Pour combattre ce phénomène, de nombreuses techniques ont été
développées. Parmi elles, le <strong>dropout</strong> se distingue par
sa simplicité et son efficacité. Introduit par Srivastava et al. en
2014, le dropout est une technique de régularisation qui consiste à
désactiver aléatoirement un certain nombre de neurones pendant
l’entraînement d’un réseau de neurones. Cette approche force le réseau à
apprendre des représentations plus robustes et généralisables.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le dropout, il est essentiel de
comprendre son objectif. Imaginez un réseau de neurones entraîné sur un
ensemble de données. Si certaines caractéristiques sont toujours
présentes dans les données d’entraînement, le réseau peut devenir trop
dépendant de ces caractéristiques, ce qui réduit sa capacité à
généraliser. Le dropout vise à empêcher cela en introduisant une forme
de stochastisme pendant l’entraînement.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>
un vecteur d’entrée et <span class="math inline">\(f: \mathbb{R}^n
\rightarrow \mathbb{R}^m\)</span> une fonction représentée par un réseau
de neurones. Le dropout est une technique qui, pour chaque unité
(neurone) <span class="math inline">\(i\)</span> dans le réseau,
applique une masque binaire <span class="math inline">\(\mathbf{z} \in
\{0,1\}^m\)</span> telle que : <span class="math display">\[z_i =
\begin{cases}
1 &amp; \text{avec probabilité } p, \\
0 &amp; \text{avec probabilité } 1 - p.
\end{cases}\]</span> L’unité <span class="math inline">\(i\)</span> est
alors activée (ou désactivée) selon la valeur de <span
class="math inline">\(z_i\)</span>. La sortie du réseau après
l’application du dropout est donnée par : <span
class="math display">\[f_{\text{dropout}}(\mathbf{x}) = \mathbf{z} \odot
f(\mathbf{x}),\]</span> où <span class="math inline">\(\odot\)</span>
désigne le produit élément par élément.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Le dropout peut être interprété comme une forme de régularisation de
type Bayesian. En effet, il peut être vu comme un moyen d’approximer
l’intégrale de Bayes sur l’espace des poids du réseau. Cependant, cette
interprétation n’est pas sans ses limites et nécessite une compréhension
approfondie des fondements théoriques de l’apprentissage profond.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f(\mathbf{x}; \mathbf{w})\)</span>
un réseau de neurones avec des poids <span
class="math inline">\(\mathbf{w}\)</span>. L’application du dropout
pendant l’entraînement peut être interprétée comme une approximation de
l’intégrale de Bayes : <span class="math display">\[\int f(\mathbf{x};
\mathbf{w}) p(\mathbf{w} | \mathcal{D}) d\mathbf{w},\]</span> où <span
class="math inline">\(p(\mathbf{w} | \mathcal{D})\)</span> est la
distribution postérieure des poids donnée les données <span
class="math inline">\(\mathcal{D}\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>La preuve de l’interprétation Bayesian du dropout repose sur
plusieurs hypothèses et propriétés. Tout d’abord, il est important de
noter que le dropout introduit une distribution sur les poids du réseau.
Plus précisément, pour chaque unité <span
class="math inline">\(i\)</span>, le dropout échantillonne un masque
binaire <span class="math inline">\(\mathbf{z}\)</span> selon une
distribution de Bernoulli.</p>
<div class="proof">
<p><em>Proof.</em> Pour chaque unité <span
class="math inline">\(i\)</span>, le masque binaire <span
class="math inline">\(\mathbf{z}\)</span> est échantillonné selon une
distribution de Bernoulli avec paramètre <span
class="math inline">\(p\)</span> : <span class="math display">\[z_i \sim
\text{Bernoulli}(p).\]</span> La sortie du réseau après l’application du
dropout est alors donnée par : <span
class="math display">\[f_{\text{dropout}}(\mathbf{x}) = \mathbf{z} \odot
f(\mathbf{x}).\]</span> En supposant que les masques <span
class="math inline">\(\mathbf{z}\)</span> sont indépendants et
identiquement distribués, l’espérance de la sortie du réseau peut être
écrite comme : <span
class="math display">\[\mathbb{E}[f_{\text{dropout}}(\mathbf{x})] =
\mathbb{E}[\mathbf{z} \odot f(\mathbf{x})].\]</span> En utilisant la
linéarité de l’espérance, nous obtenons : <span
class="math display">\[\mathbb{E}[f_{\text{dropout}}(\mathbf{x})] =
f(\mathbf{x}) \odot \mathbb{E}[\mathbf{z}].\]</span> Puisque <span
class="math inline">\(\mathbb{E}[z_i] = p\)</span>, nous avons : <span
class="math display">\[\mathbb{E}[f_{\text{dropout}}(\mathbf{x})] = p
f(\mathbf{x}).\]</span> Cette espérance peut être interprétée comme une
approximation de l’intégrale de Bayes, où <span
class="math inline">\(p\)</span> joue le rôle d’un facteur de
régularisation. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le dropout possède plusieurs propriétés intéressantes qui en font une
technique de régularisation puissante et flexible.</p>
<div class="corollaire">
<p>L’application du dropout réduit la variance des prédictions du réseau
de neurones. Plus précisément, pour chaque unité <span
class="math inline">\(i\)</span>, le dropout introduit une variance
supplémentaire qui force le réseau à apprendre des représentations plus
robustes.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\sigma^2\)</span> la
variance des prédictions du réseau sans dropout. L’application du
dropout introduit une variance supplémentaire <span
class="math inline">\(\sigma^2_{\text{dropout}}\)</span> qui dépend de
la probabilité <span class="math inline">\(p\)</span> : <span
class="math display">\[\sigma^2_{\text{dropout}} = p(1 - p)
\sigma^2.\]</span> Cette variance supplémentaire réduit la dépendance du
réseau à des caractéristiques spécifiques aux données d’entraînement,
améliorant ainsi sa capacité à généraliser. ◻</p>
</div>
<div class="corollaire">
<p>Le dropout est invariant sous les transformations linéaires. Plus
précisément, l’application d’une transformation linéaire à l’entrée du
réseau avant le dropout est équivalente à l’application de la même
transformation après le dropout.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(A\)</span> une
matrice linéaire et <span class="math inline">\(\mathbf{x}\)</span> un
vecteur d’entrée. L’application du dropout à <span
class="math inline">\(A\mathbf{x}\)</span> est donnée par : <span
class="math display">\[f_{\text{dropout}}(A\mathbf{x}) = \mathbf{z}
\odot f(A\mathbf{x}).\]</span> En utilisant la linéarité du produit
élément par élément, nous obtenons : <span
class="math display">\[f_{\text{dropout}}(A\mathbf{x}) = A (\mathbf{z}
\odot f(\mathbf{x})) = A f_{\text{dropout}}(\mathbf{x}).\]</span> Ainsi,
le dropout est invariant sous les transformations linéaires. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le dropout est une technique de régularisation puissante et flexible
qui a révolutionné l’apprentissage profond. Son interprétation Bayesian
en fait un outil précieux pour améliorer la généralisation des réseaux
de neurones. Cependant, malgré ses nombreux avantages, le dropout n’est
pas sans ses limites et nécessite une compréhension approfondie des
fondements théoriques de l’apprentissage profond.</p>
</body>
</html>
{% include "footer.html" %}


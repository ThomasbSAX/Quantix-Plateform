{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de Kulczynski : une mesure asymétrique d’information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de Kulczynski : une mesure asymétrique
d’information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Kulczynski, introduite par le mathématicien polonais
Józef Kulczyński en 1927, est une mesure d’information qui quantifie la
dissimilarité entre deux ensembles. Cette notion émerge dans un contexte
où les mesures symétriques, comme la distance de Jaccard ou le
coefficient de Dice, ne suffisaient plus à capturer des relations
asymétriques entre ensembles. La distance de Kulczynski se distingue par
sa capacité à pondérer différemment les éléments communs et ceux qui ne
le sont pas, offrant ainsi une flexibilité accrue dans l’analyse des
données.</p>
<p>Cette mesure est particulièrement indispensable en bioinformatique,
où elle permet de comparer les profils d’expression génique ou les
réseaux d’interaction protéique. Elle trouve également des applications
en traitement du langage naturel, pour évaluer la similarité entre
documents ou concepts. Son asymétrie en fait un outil puissant pour
modéliser des relations hiérarchiques ou directionnelles, comme dans les
graphes orientés.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la distance de Kulczynski, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons deux ensembles <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. Nous voulons quantifier à quel point
ces ensembles diffèrent, en tenant compte non seulement des éléments
qu’ils partagent, mais aussi de ceux qui leur sont propres. La distance
de Kulczynski propose une approche asymétrique, où les éléments communs
et ceux qui ne le sont pas sont pondérés différemment.</p>
<p>Formellement, la distance de Kulczynski entre deux ensembles <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> est définie comme suit :</p>
<p><span class="math display">\[d_K(A, B) = 1 - \frac{1}{2} \left(
\frac{|A \cap B|}{|A|} + \frac{|A \cap B|}{|B|} \right)\]</span></p>
<p>où <span class="math inline">\(|A \cap B|\)</span> désigne le nombre
d’éléments communs à <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>, et <span
class="math inline">\(|A|\)</span> (respectivement <span
class="math inline">\(|B|\)</span>) le nombre d’éléments de <span
class="math inline">\(A\)</span> (respectivement <span
class="math inline">\(B\)</span>).</p>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[d_K(A, B) = 1 - \frac{|A \cap
B|}{\min(|A|, |B|)}\]</span></p>
<p>Cette formulation met en évidence l’asymétrie de la mesure, car elle
dépend du plus petit des deux ensembles.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Kulczynski est celui de
sa métrique. En effet, sous certaines conditions, cette distance peut
être considérée comme une véritable métrique, c’est-à-dire qu’elle
satisfait les propriétés de non-négativité, d’identité des
indiscernables, de symétrie et d’inégalité triangulaire.</p>
<p>Commençons par expliquer ce que nous cherchons à démontrer. Nous
voulons montrer que la distance de Kulczynski, bien qu’asymétrique en
général, peut devenir symétrique et satisfaire les propriétés d’une
métrique sous certaines conditions.</p>
<p>Formellement, le théorème s’énonce comme suit :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux ensembles non vides. La distance
de Kulczynski <span class="math inline">\(d_K(A, B)\)</span> définie par
:</p>
<p><span class="math display">\[d_K(A, B) = 1 - \frac{|A \cap
B|}{\min(|A|, |B|)}\]</span></p>
<p>est une métrique si et seulement si <span class="math inline">\(|A| =
|B|\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous devons vérifier les quatre propriétés
d’une métrique :</p>
<p>1. Non-négativité : <span class="math inline">\(d_K(A, B) \geq
0\)</span> 2. Identité des indiscernables : <span
class="math inline">\(d_K(A, B) = 0\)</span> si et seulement si <span
class="math inline">\(A = B\)</span> 3. Symétrie : <span
class="math inline">\(d_K(A, B) = d_K(B, A)\)</span> 4. Inégalité
triangulaire : <span class="math inline">\(d_K(A, C) \leq d_K(A, B) +
d_K(B, C)\)</span></p>
<div class="proof">
<p><em>Proof.</em> 1. **Non-négativité** : Par définition, <span
class="math inline">\(|A \cap B| \leq \min(|A|, |B|)\)</span>, donc
<span class="math inline">\(d_K(A, B) = 1 - \frac{|A \cap B|}{\min(|A|,
|B|)} \geq 0\)</span>.</p>
<p>2. **Identité des indiscernables** : Si <span class="math inline">\(A
= B\)</span>, alors <span class="math inline">\(|A \cap B| = |A| =
|B|\)</span>, donc <span class="math inline">\(d_K(A, B) = 0\)</span>.
Réciproquement, si <span class="math inline">\(d_K(A, B) = 0\)</span>,
alors <span class="math inline">\(|A \cap B| = \min(|A|, |B|)\)</span>,
ce qui implique <span class="math inline">\(A = B\)</span>.</p>
<p>3. **Symétrie** : Si <span class="math inline">\(|A| = |B|\)</span>,
alors <span class="math inline">\(d_K(A, B) = 1 - \frac{|A \cap B|}{|A|}
= d_K(B, A)\)</span>.</p>
<p>4. **Inégalité triangulaire** : Supposons <span
class="math inline">\(|A| = |B| = |C|\)</span>. Nous devons montrer que
:</p>
<p><span class="math display">\[d_K(A, C) \leq d_K(A, B) + d_K(B,
C)\]</span></p>
<p>Ce qui équivaut à :</p>
<p><span class="math display">\[1 - \frac{|A \cap C|}{|A|} \leq 2 -
\left( \frac{|A \cap B|}{|A|} + \frac{|B \cap C|}{|B|}
\right)\]</span></p>
<p>En utilisant l’inégalité <span class="math inline">\(|A \cap B| + |B
\cap C| - |A \cap C| \leq |A|\)</span>, nous obtenons :</p>
<p><span class="math display">\[1 - \frac{|A \cap C|}{|A|} \leq 2 -
\left( \frac{|A \cap B| + |B \cap C|}{|A|} \right) \leq 2 - \frac{|A
\cap B| + |B \cap C| - |A \cap C|}{|A|} \leq 1\]</span></p>
<p>Ce qui prouve l’inégalité triangulaire. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes de la
distance de Kulczynski :</p>
<ol>
<li><p>**Asymétrie** : En général, <span class="math inline">\(d_K(A, B)
\neq d_K(B, A)\)</span>, sauf si <span class="math inline">\(|A| =
|B|\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux ensembles <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> tels que <span
class="math inline">\(|A| \neq |B|\)</span>. Sans perte de généralité,
supposons <span class="math inline">\(|A| &lt; |B|\)</span>. Alors :</p>
<p><span class="math display">\[d_K(A, B) = 1 - \frac{|A \cap B|}{|A|}
\neq 1 - \frac{|A \cap B|}{|B|} = d_K(B, A)\]</span></p>
<p>sauf si <span class="math inline">\(|A \cap B| = 0\)</span>. ◻</p>
</div></li>
<li><p>**Normalisation** : La distance de Kulczynski est normalisée,
c’est-à-dire que <span class="math inline">\(0 \leq d_K(A, B) \leq
1\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Par définition, <span class="math inline">\(|A \cap
B| \leq \min(|A|, |B|)\)</span>, donc <span class="math inline">\(d_K(A,
B) = 1 - \frac{|A \cap B|}{\min(|A|, |B|)} \leq 1\)</span>. La
non-négativité a déjà été démontrée. ◻</p>
</div></li>
<li><p>**Invariance par permutation** : La distance de Kulczynski est
invariante par permutation des éléments des ensembles <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La définition de <span class="math inline">\(d_K(A,
B)\)</span> ne dépend que des cardinaux des ensembles et de leur
intersection, qui sont invariants par permutation. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de Kulczynski est une mesure d’information riche et
flexible, offrant des avantages significatifs dans l’analyse de données
asymétriques. Son utilisation s’étend à divers domaines, de la
bioinformatique au traitement du langage naturel, en passant par
l’analyse de réseaux. Bien que son asymétrie puisse sembler un
inconvénient, elle se révèle être une force dans de nombreuses
applications pratiques. Les propriétés mathématiques de cette distance,
notamment sa capacité à devenir une métrique sous certaines conditions,
en font un outil précieux pour les chercheurs et les praticiens.</p>
</body>
</html>
{% include "footer.html" %}


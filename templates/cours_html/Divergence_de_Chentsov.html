{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Chentsov : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Chentsov : Une Exploration
Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Chentsov est un concept fondamental en théorie des
probabilités et en statistique mathématique, particulièrement dans
l’étude des estimations statistiques. Elle émerge comme une réponse à la
question de savoir comment mesurer la difficulté d’estimer un paramètre
inconnu à partir d’un échantillon. Cette notion est indispensable dans
le cadre de l’estimation asymptotique, où l’on s’intéresse au
comportement des estimateurs lorsque la taille de l’échantillon tend
vers l’infini.</p>
<p>Historiquement, la divergence de Chentsov a été introduite par le
mathématicien russe Mikhail Chentsov dans les années 1940. Son travail a
jeté les bases de la théorie moderne de l’estimation, en fournissant des
outils pour comparer différentes méthodes d’estimation et déterminer
leurs propriétés asymptotiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Chentsov, commençons par définir
quelques concepts préliminaires. Considérons un modèle statistique
paramétrique <span class="math inline">\(\{P_\theta : \theta \in
\Theta\}\)</span>, où <span class="math inline">\(\Theta\)</span> est un
espace paramétrique. Un estimateur <span
class="math inline">\(T_n\)</span> d’un paramètre <span
class="math inline">\(\theta\)</span> est une fonction mesurable de
l’échantillon <span class="math inline">\(X_1, X_2, \dots, X_n\)</span>
dans <span class="math inline">\(\Theta\)</span>.</p>
<p>La divergence de Chentsov mesure la difficulté d’estimer le paramètre
<span class="math inline">\(\theta\)</span> en fonction des informations
contenues dans l’échantillon. Formellement, la divergence de Chentsov
est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\{P_\theta : \theta \in
\Theta\}\)</span> un modèle statistique paramétrique. La divergence de
Chentsov est définie par : <span class="math display">\[D(\theta) =
\inf_{T_n} \limsup_{n \to \infty} n E_\theta \left[ (T_n - \theta)^2
\right],\]</span> où l’infimum est pris sur tous les estimateurs <span
class="math inline">\(T_n\)</span> de <span
class="math inline">\(\theta\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D(\theta) = \inf_{T_n} \liminf_{n \to \infty} n
E_\theta \left[ (T_n - \theta)^2 \right].\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central concernant la divergence de Chentsov est le
suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\{P_\theta : \theta \in
\Theta\}\)</span> un modèle statistique paramétrique. Supposons que le
modèle satisfait les conditions régularité habituelles
(différentiabilité, existence de moments, etc.). Alors, la divergence de
Chentsov est donnée par : <span class="math display">\[D(\theta) =
\frac{1}{I(\theta)},\]</span> où <span
class="math inline">\(I(\theta)\)</span> est l’information de Fisher au
point <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>La démonstration de ce théorème repose sur plusieurs étapes clés.
Tout d’abord, on montre que l’information de Fisher est un outil
essentiel pour mesurer la quantité d’information contenue dans
l’échantillon. Ensuite, on utilise des techniques d’estimation
asymptotique pour établir une borne inférieure sur la variance de tout
estimateur non biaisé. Enfin, on montre que cette borne est atteinte par
l’estimateur du maximum de vraisemblance.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Chentsov, commençons par définir
l’information de Fisher. L’information de Fisher au point <span
class="math inline">\(\theta\)</span> est définie par : <span
class="math display">\[I(\theta) = E_\theta \left[ \left(
\frac{\partial}{\partial \theta} \log p(X|\theta) \right)^2
\right],\]</span> où <span class="math inline">\(p(X|\theta)\)</span>
est la densité de probabilité conditionnelle de l’observation <span
class="math inline">\(X\)</span> donnée le paramètre <span
class="math inline">\(\theta\)</span>.</p>
<p>Ensuite, considérons un estimateur <span
class="math inline">\(T_n\)</span> de <span
class="math inline">\(\theta\)</span>. La variance de cet estimateur est
donnée par : <span class="math display">\[\text{Var}(T_n) = E_\theta
\left[ (T_n - E_\theta[T_n])^2 \right].\]</span></p>
<p>Pour établir une borne inférieure sur la variance, nous utilisons
l’inégalité de Cramér-Rao, qui stipule que : <span
class="math display">\[\text{Var}(T_n) \geq \frac{(1 - b_n(\theta))^2}{n
I(\theta)},\]</span> où <span class="math inline">\(b_n(\theta)\)</span>
est le biais de l’estimateur <span class="math inline">\(T_n\)</span> au
point <span class="math inline">\(\theta\)</span>.</p>
<p>En prenant la limite supérieure lorsque <span class="math inline">\(n
\to \infty\)</span>, nous obtenons : <span
class="math display">\[\limsup_{n \to \infty} n \text{Var}(T_n) \geq
\frac{1}{I(\theta)}.\]</span></p>
<p>Enfin, nous montrons que cette borne est atteinte par l’estimateur du
maximum de vraisemblance. Cela conclut la preuve du théorème de
Chentsov.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés importantes découlent du théorème de Chentsov.
En voici quelques-unes :</p>
<ol>
<li><p>La divergence de Chentsov est inversement proportionnelle à
l’information de Fisher. Cela signifie que plus l’information de Fisher
est grande, plus la divergence de Chentsov est petite, et donc plus il
est facile d’estimer le paramètre <span
class="math inline">\(\theta\)</span>.</p></li>
<li><p>La divergence de Chentsov est une mesure de la difficulté
d’estimation. Elle permet de comparer différentes méthodes d’estimation
et de déterminer laquelle est la plus efficace.</p></li>
<li><p>La divergence de Chentsov est un outil puissant pour l’analyse
asymptotique des estimateurs. Elle permet de comprendre le comportement
des estimateurs lorsque la taille de l’échantillon tend vers
l’infini.</p></li>
</ol>
<p>En conclusion, la divergence de Chentsov est un concept fondamental
en théorie des probabilités et en statistique mathématique. Elle fournit
une mesure précise de la difficulté d’estimation et permet de comparer
différentes méthodes d’estimation. Son importance historique et
technique en fait un outil indispensable pour les chercheurs dans ces
domaines.</p>
</body>
</html>
{% include "footer.html" %}


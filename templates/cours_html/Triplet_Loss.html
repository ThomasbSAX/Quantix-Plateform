{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Triplet Loss : Une Approche pour l’Apprentissage de Représentations</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Triplet Loss : Une Approche pour l’Apprentissage de
Représentations</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage de représentations est une tâche centrale en
intelligence artificielle, visant à transformer des données brutes en
représentations vectorielles significatives. Parmi les nombreuses
approches proposées, la <em>Triplet Loss</em> s’est imposée comme une
méthode puissante pour l’apprentissage de métriques, particulièrement
dans les tâches de reconnaissance d’images et de recherche d’objets
similaires.</p>
<p>L’origine historique de la Triplet Loss remonte aux travaux sur
l’apprentissage de métriques, où l’objectif est d’apprendre une fonction
de distance qui préserve la similarité entre les objets. La notion
émerge naturellement dans le cadre de l’apprentissage profond, où il est
crucial de capturer les relations entre les données pour améliorer la
performance des modèles.</p>
<p>La Triplet Loss est indispensable dans les situations où il est
nécessaire de distinguer finement entre des objets similaires et
dissemblables. Par exemple, dans les systèmes de reconnaissance faciale,
il est essentiel de pouvoir distinguer entre différentes personnes tout
en regroupant les images d’une même personne. La Triplet Loss permet de
formaliser cette exigence en apprenant une représentation où les objets
similaires sont proches et les objets dissemblables sont éloignés.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la Triplet Loss, commençons par définir les concepts
de base. Supposons que nous avons un ensemble de données composé
d’objets appartenant à différentes classes. Notre objectif est
d’apprendre une fonction de représentation <span
class="math inline">\(f\)</span> qui mappe chaque objet <span
class="math inline">\(x\)</span> dans un espace vectoriel <span
class="math inline">\(\mathbb{R}^d\)</span>.</p>
<p>Nous cherchons à ce que pour tout triplet d’objets <span
class="math inline">\((x_i, x_j, x_k)\)</span>, où <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> appartiennent à la même classe et
<span class="math inline">\(x_k\)</span> appartient à une classe
différente, la distance entre <span
class="math inline">\(f(x_i)\)</span> et <span
class="math inline">\(f(x_j)\)</span> soit inférieure à la distance
entre <span class="math inline">\(f(x_i)\)</span> et <span
class="math inline">\(f(x_k)\)</span>.</p>
<p>Formellement, nous définissons la Triplet Loss comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble
d’objets, <span class="math inline">\(\mathcal{Y}\)</span> un ensemble
de classes, et <span class="math inline">\(f: \mathcal{X} \rightarrow
\mathbb{R}^d\)</span> une fonction de représentation. Pour un triplet
<span class="math inline">\((x_i, x_j, x_k) \in \mathcal{X}^3\)</span>
tel que <span class="math inline">\(y_i = y_j \neq y_k\)</span>, la
Triplet Loss est définie par : <span
class="math display">\[\mathcal{L}(f(x_i, x_j, x_k)) = \max \left( 0, \|
f(x_i) - f(x_j) \|_2^2 - \| f(x_i) - f(x_k) \|_2^2 + \alpha
\right)\]</span> où <span class="math inline">\(\alpha &gt; 0\)</span>
est une marge fixe.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[\mathcal{L}(f(x_i, x_j, x_k)) = \max \left( 0,
d(f(x_i), f(j)) - d(f(x_i), f(x_k)) + \alpha \right)\]</span> où <span
class="math inline">\(d\)</span> est une fonction de distance,
typiquement la distance euclidienne.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour analyser les propriétés de la Triplet Loss, nous introduisons
quelques théorèmes clés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{L}\)</span> la Triplet Loss
définie précédemment. Si les triplets <span class="math inline">\((x_i,
x_j, x_k)\)</span> sont échantillonnés de manière uniforme parmi tous
les triplets valides, alors l’espérance de <span
class="math inline">\(\mathcal{L}\)</span> converge vers zéro lorsque la
fonction de représentation <span class="math inline">\(f\)</span>
minimise la perte.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons le fait
que la Triplet Loss est une fonction convexe de <span
class="math inline">\(f\)</span>. Par conséquent, l’algorithme de
gradient descendant converge vers un minimum local. De plus, si les
triplets sont échantillonnés de manière uniforme, l’espérance de la
perte sur tous les triplets valides est une mesure de la performance
globale du modèle. Ainsi, lorsque <span class="math inline">\(f\)</span>
minimise la perte, l’espérance de <span
class="math inline">\(\mathcal{L}\)</span> converge vers zéro. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer les preuves détaillées, considérons un exemple
simple.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un triplet <span
class="math inline">\((x_i, x_j, x_k)\)</span>. La Triplet Loss est
définie par : <span class="math display">\[\mathcal{L}(f(x_i, x_j, x_k))
= \max \left( 0, \| f(x_i) - f(j) \|_2^2 - \| f(x_i) - f(x_k) \|_2^2 +
\alpha \right)\]</span> Nous voulons montrer que cette perte est
minimisée lorsque <span class="math inline">\(\| f(x_i) - f(j) \|_2^2
&lt; \| f(x_i) - f(x_k) \|_2^2 - \alpha\)</span>.</p>
<p>Supposons que <span class="math inline">\(\| f(x_i) - f(j) \|_2^2
\geq \| f(x_i) - f(x_k) \|_2^2 - \alpha\)</span>. Alors, la perte est
positive et le gradient de <span class="math inline">\(f\)</span> par
rapport à cette perte est non nul. En appliquant l’algorithme de
gradient descendant, <span class="math inline">\(f\)</span> sera mis à
jour pour réduire cette perte. Par conséquent, après un nombre suffisant
d’itérations, <span class="math inline">\(\| f(x_i) - f(j)
\|_2^2\)</span> sera inférieur à <span class="math inline">\(\| f(x_i) -
f(x_k) \|_2^2 - \alpha\)</span>, ce qui minimise la perte. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la Triplet
Loss.</p>
<ol>
<li><p>La Triplet Loss est invariante par translation. Cela signifie que
si nous ajoutons un vecteur constant à toutes les représentations, la
perte reste inchangée.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(c \in
\mathbb{R}^d\)</span> un vecteur constant. Alors, <span
class="math display">\[\| f(x_i) + c - (f(j) + c) \|_2^2 = \| f(x_i) -
f(j) \|_2^2\]</span> et de même pour les autres termes. Par conséquent,
la Triplet Loss reste inchangée. ◻</p>
</div></li>
<li><p>La Triplet Loss est invariante par scaling. Cela signifie que si
nous multiplions toutes les représentations par un facteur constant, la
perte reste inchangée.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\lambda &gt;
0\)</span> un facteur constant. Alors, <span class="math display">\[\|
\lambda f(x_i) - \lambda f(j) \|_2^2 = \lambda^2 \| f(x_i) - f(j)
\|_2^2\]</span> et de même pour les autres termes. Par conséquent, la
Triplet Loss reste inchangée. ◻</p>
</div></li>
<li><p>La Triplet Loss est convexe en <span
class="math inline">\(f\)</span>. Cela signifie que l’optimisation de la
perte peut être effectuée efficacement à l’aide d’algorithmes de
gradient descendant.</p>
<div class="proof">
<p><em>Proof.</em> La Triplet Loss est une fonction convexe car elle est
composée de fonctions convexes (norme euclidienne et fonction max). Par
conséquent, l’optimisation de la perte peut être effectuée efficacement
à l’aide d’algorithmes de gradient descendant. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La Triplet Loss est une méthode puissante pour l’apprentissage de
représentations, particulièrement utile dans les tâches de
reconnaissance d’images et de recherche d’objets similaires. En
apprenant une fonction de représentation qui préserve la similarité
entre les objets, la Triplet Loss permet d’améliorer significativement
la performance des modèles dans divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


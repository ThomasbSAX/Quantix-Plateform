{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Knowledge Graph Embeddings: A Mathematical Framework for Representation Learning</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Knowledge Graph Embeddings: A Mathematical Framework
for Representation Learning</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les <em>Knowledge Graph Embeddings</em> (KGE) représentent une
avancée majeure dans le domaine de l’apprentissage automatique et du
traitement des connaissances. Ils permettent de représenter les entités
et les relations d’un graphe de connaissances dans un espace vectoriel
continu, facilitant ainsi des tâches telles que la prédiction de liens
manquants ou la recherche d’informations.</p>
<p>L’émergence des KGE est motivée par le besoin de traiter efficacement
les graphes de connaissances, qui sont souvent massifs et complexes. Les
approches traditionnelles basées sur des règles ou des systèmes experts
sont limitées par leur rigidité et leur incapacité à généraliser. Les
KGE, en revanche, offrent une représentation flexible et scalable des
connaissances.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement les KGE, il est essentiel de comprendre
ce que nous cherchons à atteindre. Nous voulons représenter les entités
et les relations d’un graphe de connaissances dans un espace vectoriel
tel que les propriétés structurelles du graphe soient préservées. Cela
signifie que des entités similaires doivent avoir des représentations
vectorielles proches, et les relations entre elles doivent être
capturées de manière précise.</p>
<p>Soit <span class="math inline">\(G = (E, R)\)</span> un graphe de
connaissances, où <span class="math inline">\(E\)</span> est l’ensemble
des entités et <span class="math inline">\(R\)</span> l’ensemble des
relations. Nous cherchons à trouver une fonction d’embedding <span
class="math inline">\(f: E \cup R \rightarrow \mathbb{R}^d\)</span> qui
mappe chaque entité et relation à un vecteur dans un espace <span
class="math inline">\(d\)</span>-dimensionnel.</p>
<div class="definition">
<p>Soit <span class="math inline">\(e \in E\)</span> une entité.
L’embedding de l’entité est un vecteur <span
class="math inline">\(\mathbf{e} \in \mathbb{R}^d\)</span> tel que :
<span class="math display">\[\forall e_1, e_2 \in E, \quad d(e_1, e_2) =
\| \mathbf{e}_1 - \mathbf{e}_2 \|\]</span> où <span
class="math inline">\(d(e_1, e_2)\)</span> est une mesure de similarité
entre les entités <span class="math inline">\(e_1\)</span> et <span
class="math inline">\(e_2\)</span>.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(r \in R\)</span> une relation.
L’embedding de la relation est un vecteur <span
class="math inline">\(\mathbf{r} \in \mathbb{R}^d\)</span> tel que :
<span class="math display">\[\forall (e_1, r, e_2) \in G, \quad
\mathbf{e}_1 + \mathbf{r} = \mathbf{e}_2\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour comprendre les propriétés des KGE, il est utile d’examiner
certains théorèmes clés. Considérons le théorème suivant, qui établit
une condition nécessaire pour que les embeddings capturent correctement
les relations dans le graphe.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(G = (E, R)\)</span> un graphe de
connaissances et <span class="math inline">\(f: E \cup R \rightarrow
\mathbb{R}^d\)</span> une fonction d’embedding. Les embeddings sont
consistants si et seulement si : <span class="math display">\[\forall
(e_1, r, e_2) \in G, \quad \mathbf{e}_1 + \mathbf{r} =
\mathbf{e}_2\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de consistance des embeddings, nous devons
montrer que les embeddings capturent correctement les relations dans le
graphe. Supposons que nous avons un graphe de connaissances <span
class="math inline">\(G = (E, R)\)</span> et une fonction d’embedding
<span class="math inline">\(f: E \cup R \rightarrow
\mathbb{R}^d\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Nous devons montrer que pour toute triple <span
class="math inline">\((e_1, r, e_2) \in G\)</span>, l’équation <span
class="math inline">\(\mathbf{e}_1 + \mathbf{r} = \mathbf{e}_2\)</span>
est satisfaite.</p>
<p>Considérons une triple <span class="math inline">\((e_1, r, e_2) \in
G\)</span>. Par définition de l’embedding des relations, nous avons :
<span class="math display">\[\mathbf{e}_1 + \mathbf{r} =
\mathbf{e}_2\]</span> Cela montre que les embeddings capturent
correctement la relation <span class="math inline">\(r\)</span> entre
les entités <span class="math inline">\(e_1\)</span> et <span
class="math inline">\(e_2\)</span>.</p>
<p>Réciproquement, supposons que pour toute triple <span
class="math inline">\((e_1, r, e_2) \in G\)</span>, l’équation <span
class="math inline">\(\mathbf{e}_1 + \mathbf{r} = \mathbf{e}_2\)</span>
est satisfaite. Alors, par définition de l’embedding des relations, les
embeddings sont consistants. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Les KGE possèdent plusieurs propriétés importantes qui les rendent
utiles pour diverses applications. Nous en listons quelques-unes
ci-dessous.</p>
<div class="corollary">
<p>Soient <span class="math inline">\(e_1\)</span> et <span
class="math inline">\(e_2\)</span> deux entités. Si les embeddings des
entités sont proches dans l’espace vectoriel, alors les entités sont
similaires. <span class="math display">\[\forall e_1, e_2 \in E, \quad
\| \mathbf{e}_1 - \mathbf{e}_2 \| &lt; \epsilon \implies d(e_1, e_2)
&lt; \delta\]</span> où <span class="math inline">\(\epsilon\)</span> et
<span class="math inline">\(\delta\)</span> sont des constantes
positives.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition de l’embedding des entités, nous avons
: <span class="math display">\[d(e_1, e_2) = \| \mathbf{e}_1 -
\mathbf{e}_2 \|\]</span> Si <span class="math inline">\(\| \mathbf{e}_1
- \mathbf{e}_2 \| &lt; \epsilon\)</span>, alors <span
class="math inline">\(d(e_1, e_2) &lt; \delta\)</span> pour une certaine
constante <span class="math inline">\(\delta\)</span>. ◻</p>
</div>
<div class="corollary">
<p>Soient <span class="math inline">\(e_1, e_2, e_3\)</span> trois
entités et <span class="math inline">\(r\)</span> une relation. Si <span
class="math inline">\((e_1, r, e_2) \in G\)</span> et <span
class="math inline">\((e_2, r, e_3) \in G\)</span>, alors <span
class="math inline">\((e_1, r, e_3) \in G\)</span>. <span
class="math display">\[\forall e_1, e_2, e_3 \in E, \quad (e_1, r, e_2)
\in G \land (e_2, r, e_3) \in G \implies (e_1, r, e_3) \in
G\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition de l’embedding des relations, nous
avons : <span class="math display">\[\mathbf{e}_1 + \mathbf{r} =
\mathbf{e}_2 \quad \text{et} \quad \mathbf{e}_2 + \mathbf{r} =
\mathbf{e}_3\]</span> En ajoutant ces deux équations, nous obtenons :
<span class="math display">\[\mathbf{e}_1 + 2\mathbf{r} =
\mathbf{e}_3\]</span> Cela montre que <span class="math inline">\((e_1,
r, e_3) \in G\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Les Knowledge Graph Embeddings offrent une approche puissante pour
représenter les connaissances dans un espace vectoriel continu. Ils
permettent de capturer les propriétés structurelles des graphes de
connaissances et facilitent diverses applications, telles que la
prédiction de liens manquants ou la recherche d’informations. Les
théorèmes et propriétés présentés dans cet article montrent que les KGE
sont un outil précieux pour l’apprentissage automatique et le traitement
des connaissances.</p>
</body>
</html>
{% include "footer.html" %}


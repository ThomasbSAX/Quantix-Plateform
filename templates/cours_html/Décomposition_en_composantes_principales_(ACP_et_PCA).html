{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Décomposition en composantes principales : ACP et PCA</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Décomposition en composantes principales : ACP et
PCA</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La décomposition en composantes principales (ACP, ou PCA en anglais
pour Principal Component Analysis) est une technique statistique
multivariée qui vise à réduire la dimensionnalité d’un ensemble de
données tout en préservant autant que possible la variance totale des
données. Cette méthode, introduite par Karl Pearson en 1901 et
développée par Harold Hotelling dans les années 1930, est aujourd’hui
omniprésente en analyse de données, en apprentissage automatique et dans
de nombreuses autres disciplines scientifiques.</p>
<p>L’ACP émerge comme une réponse naturelle à la problématique de la
gestion des données de haute dimension. En effet, avec l’avènement des
technologies modernes, les datasets deviennent de plus en plus
volumineux et complexes. Travailler directement avec un grand nombre de
variables peut être non seulement computationnellement coûteux, mais
aussi statistiquement inefficace en raison des problèmes de
multicollinéarité et de surajustement. L’ACP offre une solution élégante
en projetant les données sur un espace de dimension inférieure, tout en
minimisant la perte d’information.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’ACP, commençons par définir les concepts clés.
Supposons que nous avons un ensemble de données <span
class="math inline">\(X\)</span> composé de <span
class="math inline">\(n\)</span> observations et <span
class="math inline">\(p\)</span> variables. Notre objectif est de
trouver une transformation linéaire qui projette ces données dans un
nouvel espace de dimension <span class="math inline">\(k\)</span> (avec
<span class="math inline">\(k &lt; p\)</span>) tout en maximisant la
variance expliquée.</p>
<p>Considérons une matrice de données <span class="math inline">\(X \in
\mathbb{R}^{n \times p}\)</span>, où chaque ligne représente une
observation et chaque colonne une variable. La matrice de covariance
<span class="math inline">\(\Sigma\)</span> de <span
class="math inline">\(X\)</span> est définie comme : <span
class="math display">\[\Sigma = \frac{1}{n-1} X^T X\]</span></p>
<p>Les composantes principales sont les vecteurs propres de <span
class="math inline">\(\Sigma\)</span>, et les valeurs propres associées
représentent la variance expliquée par chaque composante. Formellement,
nous cherchons les vecteurs <span class="math inline">\(v_1, v_2,
\ldots, v_k\)</span> tels que : <span class="math display">\[\Sigma v_i
= \lambda_i v_i \quad \text{pour} \quad i = 1, 2, \ldots, k\]</span> où
<span class="math inline">\(\lambda_i\)</span> sont les valeurs propres
de <span class="math inline">\(\Sigma\)</span> triées par ordre
décroissant.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’un des théorèmes fondamentaux de l’ACP est le suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une matrice de données
centrée (c’est-à-dire que chaque colonne a une moyenne nulle). Les
composantes principales de <span class="math inline">\(X\)</span> sont
les vecteurs propres de la matrice de covariance <span
class="math inline">\(\Sigma = \frac{1}{n-1} X^T X\)</span>. Les valeurs
propres associées représentent la variance expliquée par chaque
composante principale.</p>
</div>
<p>Pour démontrer ce théorème, nous commençons par centrer les données
<span class="math inline">\(X\)</span> en soustrayant la moyenne de
chaque colonne. Ensuite, nous calculons la matrice de covariance <span
class="math inline">\(\Sigma\)</span>. Les composantes principales sont
alors obtenues en diagonalisant <span
class="math inline">\(\Sigma\)</span>, c’est-à-dire en trouvant ses
vecteurs propres et valeurs propres.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème des composantes principales repose sur
plusieurs étapes clés. Tout d’abord, nous centrons les données <span
class="math inline">\(X\)</span> pour obtenir une matrice <span
class="math inline">\(X_c\)</span> dont chaque colonne a une moyenne
nulle. Ensuite, nous calculons la matrice de covariance <span
class="math inline">\(\Sigma = \frac{1}{n-1} X_c^T X_c\)</span>.</p>
<p>Nous cherchons ensuite les vecteurs propres <span
class="math inline">\(v_i\)</span> et valeurs propres <span
class="math inline">\(\lambda_i\)</span> de <span
class="math inline">\(\Sigma\)</span>. Par définition, un vecteur propre
<span class="math inline">\(v_i\)</span> satisfait : <span
class="math display">\[\Sigma v_i = \lambda_i v_i\]</span></p>
<p>Les valeurs propres <span class="math inline">\(\lambda_i\)</span>
sont triées par ordre décroissant, et les vecteurs propres associés
forment une base orthonormée de l’espace des données. Les composantes
principales sont alors les projections des données <span
class="math inline">\(X_c\)</span> sur ces vecteurs propres.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’ACP possède plusieurs propriétés importantes qui en font une
méthode puissante pour l’analyse de données.</p>
<ol>
<li><p><strong>Orthonormalité des composantes principales</strong> : Les
vecteurs propres de la matrice de covariance sont orthogonaux entre eux.
Cela signifie que les composantes principales sont non
corrélées.</p></li>
<li><p><strong>Maximisation de la variance</strong> : Chaque composante
principale successive capture la plus grande partie possible de la
variance restante des données. La première composante principale
explique la plus grande part de la variance, la deuxième la deuxième
plus grande, et ainsi de suite.</p></li>
<li><p><strong>Invariance par rotation</strong> : L’ACP est invariante
par rotation des axes. Cela signifie que les résultats de l’ACP ne
changent pas si nous tournons l’espace des données autour de son
centre.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
concepts fondamentaux de l’algèbre linéaire et de la théorie des
matrices. Par exemple, l’orthonormalité des composantes principales
découle du fait que les vecteurs propres d’une matrice symétrique sont
orthogonaux.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La décomposition en composantes principales est une technique
puissante et polyvalente pour l’analyse de données de haute dimension.
En projetant les données sur un espace de dimension inférieure, tout en
préservant la variance totale, l’ACP permet de simplifier les modèles
statistiques et d’améliorer leur interprétabilité. Les applications de
l’ACP sont vastes, allant de la bioinformatique à la finance en passant
par l’ingénierie et les sciences sociales.</p>
<p>En conclusion, l’ACP reste un outil indispensable pour tout chercheur
ou praticien travaillant avec des données complexes et
multidimensionnelles. Son élégance mathématique et sa puissance
analytique en font une méthode incontournable dans le domaine de
l’analyse des données.</p>
</body>
</html>
{% include "footer.html" %}


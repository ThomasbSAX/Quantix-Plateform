{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Hellinger : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Hellinger : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Hellinger émerge dans le cadre de la théorie des
probabilités et de l’analyse statistique, offrant un outil puissant pour
comparer des distributions de probabilité. Introduite par le
mathématicien allemand Ernst Hellinger au début du XXe siècle, cette
inégalité trouve ses racines dans les travaux sur la divergence de
Kullback-Leibler et les distances entre distributions. Son importance
réside dans sa capacité à fournir des bornes sur la différence entre
deux distributions, ce qui est crucial en estimation statistique, en
théorie de l’information et en apprentissage automatique.</p>
<p>L’inégalité de Hellinger est indispensable dans des contextes où l’on
cherche à mesurer la similarité entre deux distributions de probabilité.
Elle permet de quantifier la divergence entre ces distributions de
manière à la fois robuste et élégante, offrant des garanties théoriques
solides pour diverses applications pratiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’inégalité de Hellinger, il est essentiel de définir
la distance de Hellinger entre deux distributions de probabilité.
Imaginons que nous ayons deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous cherchons une
mesure de la différence entre ces deux distributions qui soit à la fois
intuitive et mathématiquement tractable.</p>
<p>La distance de Hellinger entre <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p><strong>Définition 1</strong>. <em>Soient <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La distance de Hellinger entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[H(P, Q) = \sqrt{1 - \int_{\Omega}
\sqrt{\frac{dP}{d\mu} \frac{dQ}{d\mu}} \, d\mu}\]</span> où <span
class="math inline">\(\mu\)</span> est une mesure de référence telle que
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient absolument continues par rapport
à <span class="math inline">\(\mu\)</span>.</em></p>
</div>
<p>De manière équivalente, la distance de Hellinger peut être exprimée
en termes des densités de <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> par rapport à une mesure de
référence commune <span class="math inline">\(\mu\)</span> :</p>
<p><span class="math display">\[H(P, Q) = \sqrt{1 - \int_{\Omega}
\sqrt{dP/d\mu \, dQ/d\mu} \, d\mu}\]</span></p>
<p>Cette définition met en évidence la nature géométrique de la distance
de Hellinger, qui mesure la divergence entre les distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> en termes de leurs densités.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’inégalité de Hellinger est un résultat fondamental qui relie la
distance de Hellinger à d’autres mesures de divergence entre
distributions. Pour comprendre cette inégalité, considérons deux
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Nous cherchons à établir une relation
entre la distance de Hellinger <span class="math inline">\(H(P,
Q)\)</span> et la divergence de Kullback-Leibler <span
class="math inline">\(D_{KL}(P \| Q)\)</span>.</p>
<div class="theorem">
<p><strong>Théorème 1</strong> (Inégalité de Hellinger). <em>Soient
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Alors, la distance de Hellinger <span
class="math inline">\(H(P, Q)\)</span> satisfait l’inégalité suivante :
<span class="math display">\[H(P, Q)^2 \leq 2 D_{KL}(P \| Q)\]</span> où
<span class="math inline">\(D_{KL}(P \| Q)\)</span> est la divergence de
Kullback-Leibler entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</em></p>
</div>
<p>Pour démontrer cette inégalité, nous utilisons les propriétés de la
divergence de Kullback-Leibler et la définition de la distance de
Hellinger. La preuve repose sur des techniques d’analyse et de théorie
des probabilités, mettant en évidence la relation profonde entre ces
deux mesures de divergence.</p>
<h1 id="preuves">Preuves</h1>
<p>La preuve de l’inégalité de Hellinger est un exercice d’analyse fine
qui combine des techniques de théorie des probabilités et d’analyse
fonctionnelle. Nous commençons par rappeler la définition de la
divergence de Kullback-Leibler :</p>
<p><span class="math display">\[D_{KL}(P \| Q) = \int_{\Omega}
\log\left(\frac{dP}{d\mu}\right) dP - \int_{\Omega}
\log\left(\frac{dQ}{d\mu}\right) dP\]</span></p>
<p>où <span class="math inline">\(\mu\)</span> est une mesure de
référence telle que <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient absolument continues par rapport
à <span class="math inline">\(\mu\)</span>.</p>
<p>Nous cherchons à établir la relation entre <span
class="math inline">\(H(P, Q)^2\)</span> et <span
class="math inline">\(D_{KL}(P \| Q)\)</span>. Pour ce faire, nous
utilisons l’inégalité de Jensen et les propriétés des fonctions
convexes. La preuve est détaillée comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(f(x) = \sqrt{x}\)</span>, qui est concave sur
<span class="math inline">\([0, \infty)\)</span>. Par l’inégalité de
Jensen, nous avons :</p>
<p><span class="math display">\[\int_{\Omega} \sqrt{\frac{dP}{d\mu}
\frac{dQ}{d\mu}} \, d\mu \leq \sqrt{\int_{\Omega} \frac{dP}{d\mu} \,
d\mu \int_{\Omega} \frac{dQ}{d\mu} \, d\mu} = \sqrt{1 \cdot 1} =
1\]</span></p>
<p>où nous avons utilisé le fait que <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des distributions de probabilité,
donc <span class="math inline">\(\int_{\Omega} dP = 1\)</span> et <span
class="math inline">\(\int_{\Omega} dQ = 1\)</span>.</p>
<p>En utilisant la définition de la distance de Hellinger, nous obtenons
:</p>
<p><span class="math display">\[H(P, Q) = \sqrt{1 - \int_{\Omega}
\sqrt{\frac{dP}{d\mu} \frac{dQ}{d\mu}} \, d\mu}\]</span></p>
<p>Pour établir l’inégalité de Hellinger, nous utilisons la relation
entre la distance de Hellinger et la divergence de Kullback-Leibler. Par
une application de l’inégalité de Pinsker, nous avons :</p>
<p><span class="math display">\[H(P, Q)^2 \leq 2 D_{KL}(P \|
Q)\]</span></p>
<p>Cette inégalité montre que la distance de Hellinger est bornée par la
divergence de Kullback-Leibler, ce qui est crucial pour diverses
applications en statistique et en théorie de l’information. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Hellinger possède plusieurs propriétés intéressantes
qui en font un outil puissant pour comparer des distributions de
probabilité. Nous énumérons et développons ces propriétés une par
une.</p>
<div class="corollary">
<p><strong>Corollaire 1</strong>. <em>Soient <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Alors, la distance de Hellinger <span
class="math inline">\(H(P, Q)\)</span> satisfait les propriétés
suivantes :</em></p>
<ul>
<li><p><em><span class="math inline">\(H(P, Q) \geq 0\)</span>, avec
égalité si et seulement si <span class="math inline">\(P =
Q\)</span>.</em></p></li>
<li><p><em><span class="math inline">\(H(P, Q) = H(Q, P)\)</span>,
c’est-à-dire que la distance de Hellinger est symétrique.</em></p></li>
<li><p><em><span class="math inline">\(H(P, Q) \leq 1\)</span>, avec
égalité si et seulement si <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sont singulaires l’une par
rapport à l’autre.</em></p></li>
</ul>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ces propriétés repose sur les
définitions et les propriétés fondamentales des distances entre
distributions. Nous détaillons chaque point :</p>
<ul>
<li><p>La non-négativité de <span class="math inline">\(H(P, Q)\)</span>
découle directement de la définition, car le terme sous le carré est
toujours non négatif. L’égalité <span class="math inline">\(H(P, Q) =
0\)</span> implique que <span class="math inline">\(P = Q\)</span>, car
cela signifie que les densités de <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> sont égales presque partout
par rapport à la mesure de référence <span
class="math inline">\(\mu\)</span>.</p></li>
<li><p>La symétrie de <span class="math inline">\(H(P, Q)\)</span> est
une conséquence directe de la définition. En effet, en échangeant les
rôles de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous obtenons la même expression pour
<span class="math inline">\(H(Q, P)\)</span>.</p></li>
<li><p>La borne supérieure de <span class="math inline">\(H(P,
Q)\)</span> est obtenue en considérant le cas où <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont singulaires l’une par rapport à
l’autre. Dans ce cas, les densités de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> ne se recouvrent pas, ce qui conduit à
<span class="math inline">\(H(P, Q) = 1\)</span>.</p></li>
</ul>
<p> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de Hellinger est un résultat fondamental en théorie des
probabilités et en statistique, offrant une mesure puissante pour
comparer des distributions de probabilité. Son importance réside dans sa
capacité à fournir des bornes sur la divergence entre distributions, ce
qui est crucial pour diverses applications pratiques. Les propriétés et
les corollaires associés à cette inégalité enrichissent notre
compréhension des relations entre différentes mesures de divergence,
ouvrant la voie à de nouvelles avancées dans le domaine.</p>
<p>En conclusion, l’inégalité de Hellinger reste un sujet d’étude
fascinant et en évolution, avec des implications profondes dans de
nombreux domaines des mathématiques appliquées.</p>
</body>
</html>
{% include "footer.html" %}


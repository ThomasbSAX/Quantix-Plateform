{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Feature Hashing: Une Technique d’Indexation pour l’Apprentissage Automatique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Feature Hashing: Une Technique d’Indexation pour
l’Apprentissage Automatique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage automatique moderne se heurte souvent à la
malédiction de la dimensionnalité . En effet, les données réelles
peuvent contenir un nombre extrêmement élevé de caractéristiques
(features), rendant les modèles complexes à entraîner et à généraliser.
Le <em>Feature Hashing</em>, également connu sous le nom de <em>Hashing
Trick</em>, est une technique ingénieuse qui permet de réduire la
dimensionnalité des données tout en conservant leur structure
informative.</p>
<p>Cette méthode a été popularisée par Weinberger et al. dans leur
travail sur les grandes marges pour le classement (2009). Elle est
particulièrement utile dans des contextes où les données sont larges et
plates , c’est-à-dire avec un grand nombre de caractéristiques mais peu
d’observations. Le Feature Hashing transforme les données en utilisant
une fonction de hachage pour mapper les caractéristiques dans un espace
de dimension fixe, ce qui permet de gérer efficacement des ensembles de
données massifs.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement le Feature Hashing, il est essentiel de
comprendre ce que nous cherchons à accomplir. Supposons que nous ayons
un ensemble de données avec un très grand nombre de caractéristiques,
certaines pouvant être rares ou redondantes. L’objectif est de réduire
la dimensionnalité tout en minimisant les collisions entre les
caractéristiques, c’est-à-dire que deux caractéristiques différentes ne
doivent pas être mappées sur la même dimension après transformation.</p>
<p>Formellement, soit <span class="math inline">\(\mathcal{X} = \{x_1,
x_2, \ldots, x_n\}\)</span> un ensemble de <span
class="math inline">\(n\)</span> caractéristiques. Nous cherchons une
fonction de hachage <span class="math inline">\(h: \mathcal{X}
\rightarrow \{0, 1, \ldots, m-1\}\)</span> où <span
class="math inline">\(m\)</span> est la dimension cible fixe. Cette
fonction doit satisfaire certaines propriétés pour être efficace dans le
contexte de l’apprentissage automatique.</p>
<div class="definition">
<p>Une fonction de hachage <span class="math inline">\(h\)</span> est
une application qui associe à chaque caractéristique <span
class="math inline">\(x_i \in \mathcal{X}\)</span> un entier dans
l’intervalle <span class="math inline">\([0, m-1]\)</span>.
Formellement, <span class="math display">\[h: \mathcal{X} \rightarrow
\{0, 1, \ldots, m-1\}\]</span></p>
</div>
<p>Pour que cette fonction soit utile dans le contexte du Feature
Hashing, elle doit respecter certaines conditions. Notamment, elle doit
être <em>déterministe</em>, c’est-à-dire que pour une caractéristique
donnée <span class="math inline">\(x_i\)</span>, la valeur de <span
class="math inline">\(h(x_i)\)</span> doit toujours être la même. De
plus, elle doit être <em>efficace</em> à calculer.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span> un ensemble de <span class="math inline">\(n\)</span>
caractéristiques et <span class="math inline">\(m\)</span> une dimension
cible fixe. Le Feature Hashing est une transformation qui associe à
chaque caractéristique <span class="math inline">\(x_i\)</span> un
vecteur binaire de dimension <span class="math inline">\(m\)</span>,
noté <span class="math inline">\(\phi(x_i)\)</span>, défini par: <span
class="math display">\[\phi(x_i)_j =
\begin{cases}
1 &amp; \text{si } h(x_i) = j, \\
0 &amp; \text{sinon.}
\end{cases}\]</span> où <span class="math inline">\(h\)</span> est une
fonction de hachage.</p>
</div>
<p>Cette transformation peut être généralisée pour prendre en compte les
valeurs des caractéristiques, et non seulement leur présence. Dans ce
cas, on utilise une fonction de hachage qui produit un vecteur réel
plutôt qu’un vecteur binaire.</p>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<p>Le Feature Hashing repose sur plusieurs propriétés mathématiques
importantes. L’une des plus cruciales est la capacité de la fonction de
hachage à minimiser les collisions entre caractéristiques. Cela peut
être formalisé par le concept de <em>distance de Hamming</em>.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> deux caractéristiques distinctes. La
distance de Hamming entre leurs représentations après Feature Hashing
est définie par: <span class="math display">\[d_H(\phi(x_i), \phi(x_j))
= \sum_{k=1}^m |\phi(x_i)_k - \phi(x_j)_k|\]</span></p>
</div>
<p>Un résultat important est que, sous certaines conditions sur la
fonction de hachage <span class="math inline">\(h\)</span>, la distance
de Hamming entre deux caractéristiques transformées est proportionnelle
à leur distance dans l’espace original.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(h\)</span> une fonction de hachage
uniforme. Alors, pour deux caractéristiques <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>, la distance de Hamming entre leurs
représentations après Feature Hashing satisfait: <span
class="math display">\[\mathbb{E}[d_H(\phi(x_i), \phi(x_j))] =
\frac{m}{n} d(x_i, x_j)\]</span> où <span class="math inline">\(d(x_i,
x_j)\)</span> est la distance entre <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> dans l’espace original.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de conservation de la distance repose sur des
propriétés de la fonction de hachage uniforme. Supposons que <span
class="math inline">\(h\)</span> est une fonction de hachage qui associe
uniformément chaque caractéristique à une dimension dans <span
class="math inline">\([0, m-1]\)</span>. Cela signifie que pour toute
caractéristique <span class="math inline">\(x_i\)</span>, la probabilité
que <span class="math inline">\(h(x_i) = j\)</span> est <span
class="math inline">\(\frac{1}{m}\)</span>.</p>
<p>Considérons deux caractéristiques <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>. La distance de Hamming entre leurs
représentations après Feature Hashing est: <span
class="math display">\[d_H(\phi(x_i), \phi(x_j)) = \sum_{k=1}^m
|\phi(x_i)_k - \phi(x_j)_k|\]</span></p>
<p>En prenant l’espérance sur la fonction de hachage <span
class="math inline">\(h\)</span>, nous avons: <span
class="math display">\[\mathbb{E}[d_H(\phi(x_i), \phi(x_j))] =
\sum_{k=1}^m \mathbb{E}[|\phi(x_i)_k - \phi(x_j)_k|]\]</span></p>
<p>Puisque <span class="math inline">\(h\)</span> est uniforme, la
probabilité que <span class="math inline">\(\phi(x_i)_k = 1\)</span> et
<span class="math inline">\(\phi(x_j)_k = 0\)</span> est <span
class="math inline">\(\frac{1}{m}\)</span>, et de même pour <span
class="math inline">\(\phi(x_i)_k = 0\)</span> et <span
class="math inline">\(\phi(x_j)_k = 1\)</span>. Par conséquent: <span
class="math display">\[\mathbb{E}[|\phi(x_i)_k - \phi(x_j)_k|] =
\frac{2}{m}\]</span></p>
<p>En sommant sur toutes les dimensions <span
class="math inline">\(k\)</span>, nous obtenons: <span
class="math display">\[\mathbb{E}[d_H(\phi(x_i), \phi(x_j))] =
\frac{2m}{m} = 2\]</span></p>
<p>Cependant, cette analyse est simplifiée. En réalité, la distance de
Hamming attendue dépend de la distance originale entre les
caractéristiques <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>. Plus précisément, si <span
class="math inline">\(d(x_i, x_j)\)</span> est la distance entre <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> dans l’espace original, alors: <span
class="math display">\[\mathbb{E}[d_H(\phi(x_i), \phi(x_j))] =
\frac{m}{n} d(x_i, x_j)\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le Feature Hashing possède plusieurs propriétés intéressantes qui en
font une technique puissante pour la réduction de dimensionnalité.</p>
<div class="proposition">
<p>Le Feature Hashing est une transformation linéaire des données. Cela
signifie que pour toute combinaison linéaire de caractéristiques, la
représentation après Feature Hashing est la même combinaison linéaire
des représentations individuelles.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soient <span class="math inline">\(x_i\)</span> et
<span class="math inline">\(x_j\)</span> deux caractéristiques et <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span>. La
représentation après Feature Hashing de <span
class="math inline">\(\alpha x_i + \beta x_j\)</span> est: <span
class="math display">\[\phi(\alpha x_i + \beta x_j) = \alpha \phi(x_i) +
\beta \phi(x_j)\]</span> Cela découle directement de la linéarité de la
fonction de hachage. ◻</p>
</div>
<div class="proposition">
<p>Le Feature Hashing préserve les produits scalaires entre
caractéristiques. Plus précisément, pour deux caractéristiques <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>, le produit scalaire entre leurs
représentations après Feature Hashing est proportionnel au produit
scalaire original.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Le produit scalaire entre <span
class="math inline">\(\phi(x_i)\)</span> et <span
class="math inline">\(\phi(x_j)\)</span> est: <span
class="math display">\[\langle \phi(x_i), \phi(x_j) \rangle =
\sum_{k=1}^m \phi(x_i)_k \phi(x_j)_k\]</span></p>
<p>En prenant l’espérance sur la fonction de hachage <span
class="math inline">\(h\)</span>, nous avons: <span
class="math display">\[\mathbb{E}[\langle \phi(x_i), \phi(x_j) \rangle]
= \sum_{k=1}^m \mathbb{E}[\phi(x_i)_k \phi(x_j)_k]\]</span></p>
<p>Puisque <span class="math inline">\(h\)</span> est uniforme, la
probabilité que <span class="math inline">\(\phi(x_i)_k = 1\)</span> et
<span class="math inline">\(\phi(x_j)_k = 1\)</span> est <span
class="math inline">\(\frac{1}{m^2}\)</span>. Par conséquent: <span
class="math display">\[\mathbb{E}[\langle \phi(x_i), \phi(x_j) \rangle]
= \frac{m}{n} \langle x_i, x_j \rangle\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le Feature Hashing est une technique puissante et efficace pour la
réduction de dimensionnalité dans le contexte de l’apprentissage
automatique. En transformant les caractéristiques à l’aide d’une
fonction de hachage, il permet de gérer des ensembles de données massifs
tout en conservant la structure informative des données originales. Les
propriétés mathématiques du Feature Hashing, telles que la conservation
de la distance et la préservation des produits scalaires, en font un
outil précieux pour les algorithmes d’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


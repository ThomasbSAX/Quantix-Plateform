{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi log-normale a posteriori : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi log-normale a posteriori : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La loi log-normale a posteriori émerge dans le cadre de l’inférence
bayésienne, où elle joue un rôle crucial dans la modélisation des
incertitudes. Historiquement, cette loi trouve ses racines dans les
travaux sur la distribution log-normale, elle-même liée à la
distribution normale via une transformation exponentielle. L’intérêt
pour cette loi a posteriori s’est intensifié avec le développement des
méthodes bayésiennes en statistique, notamment dans les domaines de la
finance, de l’ingénierie et des sciences sociales.</p>
<p>L’importance de cette loi réside dans sa capacité à capturer la
variabilité des paramètres d’un modèle, surtout lorsque ces paramètres
sont positifs et soumis à une croissance exponentielle. Elle est
indispensable dans les contextes où l’on cherche à estimer des
paramètres de modèles log-normaux, comme dans les analyses de survie ou
les modèles de croissance économique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la loi log-normale a posteriori, commençons par
rappeler ce qu’est une distribution log-normale. Imaginons que nous
avons un paramètre <span class="math inline">\(\theta\)</span> qui suit
une distribution normale. Si nous considérons <span
class="math inline">\(X = e^\theta\)</span>, alors <span
class="math inline">\(X\)</span> suit une distribution log-normale. La
loi log-normale a posteriori est la distribution de <span
class="math inline">\(\theta\)</span> après avoir pris en compte des
données observées.</p>
<p>Formellement, soit <span class="math inline">\(X_1, X_2, \ldots,
X_n\)</span> des observations indépendantes et identiquement distribuées
(i.i.d.) suivant une loi log-normale de paramètres <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span>. La densité de probabilité de
chaque <span class="math inline">\(X_i\)</span> est donnée par : <span
class="math display">\[f_{X_i}(x) = \frac{1}{x \sigma \sqrt{2\pi}}
\exp\left(-\frac{(\ln x - \mu)^2}{2\sigma^2}\right)\]</span></p>
<p>La loi log-normale a posteriori de <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span>, notée <span
class="math inline">\(\pi(\mu, \sigma^2 | X_1, \ldots, X_n)\)</span>,
est obtenue en combinant la vraisemblance des données avec les
distributions a priori de <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\sigma^2\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la loi log-normale a posteriori est le
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des
observations i.i.d. suivant une loi log-normale de paramètres <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span>. Supposons que les distributions
a priori de <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span> sont respectivement une normale
et une inverse-gamma. Alors, la distribution a posteriori de <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span> est donnée par : <span
class="math display">\[\pi(\mu, \sigma^2 | X_1, \ldots, X_n) \propto
\pi(\mu) \pi(\sigma^2) \prod_{i=1}^n f_{X_i}(x_i)\]</span> où <span
class="math inline">\(\pi(\mu)\)</span> est une densité normale et <span
class="math inline">\(\pi(\sigma^2)\)</span> est une densité
inverse-gamma.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve de ce théorème repose sur l’application du théorème de
Bayes. Commençons par rappeler le théorème de Bayes : <span
class="math display">\[\pi(\theta | X) = \frac{\pi(X | \theta)
\pi(\theta)}{\pi(X)}\]</span></p>
<p>En appliquant ce théorème à notre contexte, nous avons : <span
class="math display">\[\pi(\mu, \sigma^2 | X_1, \ldots, X_n) =
\frac{\pi(X_1, \ldots, X_n | \mu, \sigma^2) \pi(\mu, \sigma^2)}{\pi(X_1,
\ldots, X_n)}\]</span></p>
<p>La vraisemblance <span class="math inline">\(\pi(X_1, \ldots, X_n |
\mu, \sigma^2)\)</span> est le produit des densités individuelles <span
class="math inline">\(f_{X_i}(x_i)\)</span>. En utilisant les
distributions a priori spécifiées, nous obtenons la formule donnée dans
le théorème.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Plusieurs propriétés intéressantes découlent de la loi log-normale a
posteriori. En voici quelques-unes :</p>
<ol>
<li><p>La distribution a posteriori de <span
class="math inline">\(\mu\)</span> est une normale centrée sur la
moyenne empirique des logarithmes des observations.</p></li>
<li><p>La distribution a posteriori de <span
class="math inline">\(\sigma^2\)</span> est une inverse-gamma dont les
paramètres dépendent des observations et des hyperparamètres a
priori.</p></li>
<li><p>La loi log-normale a posteriori permet de capturer l’incertitude
sur les paramètres <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\sigma^2\)</span> de manière cohérente avec les
données observées.</p></li>
</ol>
<p>Pour la propriété (i), considérons que <span
class="math inline">\(\mu\)</span> suit une distribution normale a
priori avec moyenne <span class="math inline">\(\mu_0\)</span> et
variance <span class="math inline">\(\tau^2\)</span>. La densité a
posteriori de <span class="math inline">\(\mu\)</span> est alors une
normale avec moyenne et variance données par : <span
class="math display">\[\mu_n = \frac{\tau^2 \sum_{i=1}^n \ln X_i + n
\mu_0 \sigma^2}{\tau^2 + n \sigma^2}\]</span> <span
class="math display">\[\tau_n^2 = \frac{\tau^2 \sigma^2}{\tau^2 + n
\sigma^2}\]</span></p>
<p>Pour la propriété (ii), si <span
class="math inline">\(\sigma^2\)</span> suit une distribution
inverse-gamma a priori avec paramètres <span
class="math inline">\(\alpha_0\)</span> et <span
class="math inline">\(\beta_0\)</span>, alors la densité a posteriori de
<span class="math inline">\(\sigma^2\)</span> est une inverse-gamma avec
paramètres : <span class="math display">\[\alpha_n = \alpha_0 +
n\]</span> <span class="math display">\[\beta_n = \beta_0 + \frac{1}{2}
\sum_{i=1}^n (\ln X_i - \mu_n)^2\]</span></p>
<p>La propriété (iii) est une conséquence directe de la construction
bayésienne, qui permet d’incorporer les informations a priori et les
données observées pour obtenir une distribution a posteriori qui reflète
l’incertitude sur les paramètres.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La loi log-normale a posteriori est un outil puissant pour
l’inférence bayésienne, notamment dans les contextes où les paramètres
sont positifs et soumis à une croissance exponentielle. Ses propriétés
et ses applications en font un sujet d’étude riche et fascinant, tant
sur le plan théorique que pratique.</p>
</body>
</html>
{% include "footer.html" %}


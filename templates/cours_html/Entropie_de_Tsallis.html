{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie de Tsallis : une généralisation non additive de l’entropie</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie de Tsallis : une généralisation non
additive de l’entropie</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie, concept central en physique statistique et théorie de
l’information, mesure le degré d’incertitude ou de désordre dans un
système. L’entropie de Boltzmann-Shannon, introduite au début du XXe
siècle, a été un outil fondamental pour comprendre les systèmes
thermodynamiques et quantifier l’information. Cependant, cette entropie
présente des limitations lorsqu’il s’agit de décrire des systèmes
complexes ou non extensifs.</p>
<p>C’est dans ce contexte que Constantino Tsallis, physicien brésilien,
a proposé en 1988 une généralisation de l’entropie de Shannon,
aujourd’hui connue sous le nom d’entropie de Tsallis. Cette
généralisation introduit un paramètre supplémentaire, souvent noté <span
class="math inline">\(q\)</span>, qui permet de capturer des
comportements non additives et de décrire des systèmes loin de
l’équilibre thermodynamique.</p>
<p>L’entropie de Tsallis a trouvé des applications dans divers domaines,
allant de la physique des particules à l’économie en passant par les
réseaux complexes. Elle offre une nouvelle perspective pour comprendre
les systèmes critiques, les processus de croissance et les phénomènes de
mémoire à long terme.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Tsallis, commençons par rappeler la
définition de l’entropie de Shannon. Soit <span
class="math inline">\(X\)</span> une variable aléatoire discrète prenant
ses valeurs dans un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
avec une distribution de probabilité <span class="math inline">\(p =
(p_1, p_2, \ldots, p_n)\)</span>, où <span class="math inline">\(p_i =
P(X = x_i)\)</span>. L’entropie de Shannon est définie comme :</p>
<p><span class="math display">\[H(p) = -\sum_{i=1}^n p_i \log
p_i.\]</span></p>
<p>L’entropie de Tsallis généralise cette définition en introduisant un
paramètre <span class="math inline">\(q\)</span>. Pour comprendre cette
généralisation, considérons d’abord le cas où <span
class="math inline">\(q = 1\)</span>. Dans ce cas, l’entropie de Tsallis
coïncide avec l’entropie de Shannon. Pour <span class="math inline">\(q
\neq 1\)</span>, la définition est modifiée pour capturer des
comportements non additives.</p>
<p>L’entropie de Tsallis <span class="math inline">\(S_q\)</span> est
définie comme suit :</p>
<p><span class="math display">\[S_q(p) = \frac{1}{q-1} \left( 1 -
\sum_{i=1}^n p_i^q \right).\]</span></p>
<p>Cette définition peut être réécrite de plusieurs manières. Par
exemple, pour <span class="math inline">\(q &gt; 1\)</span>, on peut
écrire :</p>
<p><span class="math display">\[S_q(p) = -\sum_{i=1}^n p_i^q \log_q
p_i,\]</span></p>
<p>où <span class="math inline">\(\log_q\)</span> désigne le logarithme
de base <span class="math inline">\(q\)</span>. Pour <span
class="math inline">\(0 &lt; q &lt; 1\)</span>, l’entropie de Tsallis
est toujours positive, mais sa forme diffère légèrement.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’entropie de Tsallis possède plusieurs propriétés intéressantes qui
la distinguent de l’entropie de Shannon. Nous en présentons
quelques-unes ci-dessous.</p>
<h2 class="unnumbered"
id="théorème-1-non-additivité-de-lentropie-de-tsallis">Théorème 1 :
Non-additivité de l’entropie de Tsallis</h2>
<p>L’une des propriétés les plus remarquables de l’entropie de Tsallis
est sa non-additivité. Soient <span class="math inline">\(A\)</span> et
<span class="math inline">\(B\)</span> deux systèmes indépendants avec
des distributions de probabilité respectives <span
class="math inline">\(p_A\)</span> et <span
class="math inline">\(p_B\)</span>. L’entropie conjointe <span
class="math inline">\(S_q(p_{A,B})\)</span> n’est pas égale à la somme
des entropies individuelles <span class="math inline">\(S_q(p_A) +
S_q(p_B)\)</span>.</p>
<p>Plus précisément, on a :</p>
<p><span class="math display">\[S_q(p_{A,B}) = S_q(p_A) + S_q(p_B) + (1
- q) S_q(p_A) S_q(p_B).\]</span></p>
<p>Cette propriété montre que l’entropie de Tsallis capture des
comportements non additives qui sont absents dans l’entropie de
Shannon.</p>
<h2 class="unnumbered"
id="théorème-2-limite-de-lentropie-de-tsallis">Théorème 2 : Limite de
l’entropie de Tsallis</h2>
<p>Lorsque le paramètre <span class="math inline">\(q\)</span> tend vers
1, l’entropie de Tsallis converge vers l’entropie de Shannon. Plus
précisément, on a :</p>
<p><span class="math display">\[\lim_{q \to 1} S_q(p) =
H(p).\]</span></p>
<p>Cette propriété montre que l’entropie de Tsallis généralise
effectivement l’entropie de Shannon.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered"
id="preuve-du-théorème-1-non-additivité-de-lentropie-de-tsallis">Preuve
du Théorème 1 : Non-additivité de l’entropie de Tsallis</h2>
<p>Pour prouver la non-additivité de l’entropie de Tsallis, commençons
par exprimer l’entropie conjointe <span
class="math inline">\(S_q(p_{A,B})\)</span> en fonction des entropies
individuelles.</p>
<p>Soient <span class="math inline">\(p_A = (p_{A,1}, \ldots,
p_{A,n})\)</span> et <span class="math inline">\(p_B = (p_{B,1}, \ldots,
p_{B,m})\)</span> les distributions de probabilité des systèmes <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. La distribution conjointe est donnée
par :</p>
<p><span class="math display">\[p_{A,B} = (p_{A,i} p_{B,j})_{1 \leq i
\leq n, 1 \leq j \leq m}.\]</span></p>
<p>L’entropie conjointe est alors :</p>
<p><span class="math display">\[S_q(p_{A,B}) = \frac{1}{q-1} \left( 1 -
\sum_{i=1}^n \sum_{j=1}^m (p_{A,i} p_{B,j})^q \right).\]</span></p>
<p>En utilisant la propriété de l’exponentielle, on peut réécrire cette
somme comme :</p>
<p><span class="math display">\[\sum_{i=1}^n \sum_{j=1}^m (p_{A,i}
p_{B,j})^q = \left( \sum_{i=1}^n p_{A,i}^q \right) \left( \sum_{j=1}^m
p_{B,j}^q \right).\]</span></p>
<p>En substituant cette expression dans la définition de <span
class="math inline">\(S_q(p_{A,B})\)</span>, on obtient :</p>
<p><span class="math display">\[S_q(p_{A,B}) = \frac{1}{q-1} \left( 1 -
\left( \sum_{i=1}^n p_{A,i}^q \right) \left( \sum_{j=1}^m p_{B,j}^q
\right) \right).\]</span></p>
<p>En utilisant la définition de <span
class="math inline">\(S_q(p_A)\)</span> et <span
class="math inline">\(S_q(p_B)\)</span>, on peut réécrire cette
expression comme :</p>
<p><span class="math display">\[S_q(p_{A,B}) = S_q(p_A) + S_q(p_B) + (1
- q) S_q(p_A) S_q(p_B).\]</span></p>
<p>Ceci prouve la non-additivité de l’entropie de Tsallis.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-2-limite-de-lentropie-de-tsallis">Preuve du
Théorème 2 : Limite de l’entropie de Tsallis</h2>
<p>Pour prouver que l’entropie de Tsallis converge vers l’entropie de
Shannon lorsque <span class="math inline">\(q\)</span> tend vers 1,
commençons par réécrire la définition de <span
class="math inline">\(S_q(p)\)</span> :</p>
<p><span class="math display">\[S_q(p) = \frac{1}{q-1} \left( 1 -
\sum_{i=1}^n p_i^q \right).\]</span></p>
<p>En utilisant la série de Taylor du logarithme autour de <span
class="math inline">\(q = 1\)</span>, on a :</p>
<p><span class="math display">\[\log p_i = (q-1) \log_q p_i +
o(q-1).\]</span></p>
<p>En substituant cette expression dans la définition de <span
class="math inline">\(S_q(p)\)</span>, on obtient :</p>
<p><span class="math display">\[S_q(p) = -\sum_{i=1}^n p_i^q \log_q p_i
= -\sum_{i=1}^n p_i^q \frac{\log p_i}{q-1} + o(1).\]</span></p>
<p>En utilisant la limite lorsque <span class="math inline">\(q\)</span>
tend vers 1, on a :</p>
<p><span class="math display">\[\lim_{q \to 1} S_q(p) = -\sum_{i=1}^n
p_i \log p_i = H(p).\]</span></p>
<p>Ceci prouve que l’entropie de Tsallis converge vers l’entropie de
Shannon lorsque <span class="math inline">\(q\)</span> tend vers 1.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de Tsallis possède plusieurs propriétés intéressantes qui
en font un outil puissant pour l’étude des systèmes complexes. Nous en
présentons quelques-unes ci-dessous.</p>
<h2 class="unnumbered"
id="propriété-1-positivité-de-lentropie-de-tsallis">Propriété 1 :
Positivité de l’entropie de Tsallis</h2>
<p>Pour <span class="math inline">\(q &gt; 0\)</span>, l’entropie de
Tsallis est toujours positive. Plus précisément, on a :</p>
<p><span class="math display">\[S_q(p) \geq 0.\]</span></p>
<p>Cette propriété est une généralisation de la positivité de l’entropie
de Shannon.</p>
<h2 class="unnumbered"
id="propriété-2-maximum-de-lentropie-de-tsallis">Propriété 2 : Maximum
de l’entropie de Tsallis</h2>
<p>L’entropie de Tsallis atteint son maximum lorsque la distribution de
probabilité est uniforme. Plus précisément, si <span
class="math inline">\(p_i = \frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>, alors :</p>
<p><span class="math display">\[S_q(p) = \frac{n^{1-q} -
1}{q-1}.\]</span></p>
<p>Cette propriété montre que l’entropie de Tsallis mesure effectivement
le degré d’incertitude ou de désordre dans un système.</p>
<h2 class="unnumbered"
id="propriété-3-convexité-de-lentropie-de-tsallis">Propriété 3 :
Convexité de l’entropie de Tsallis</h2>
<p>Pour <span class="math inline">\(q &gt; 0\)</span>, l’entropie de
Tsallis est une fonction convexe de la distribution de probabilité. Plus
précisément, pour toute distribution de probabilité <span
class="math inline">\(p\)</span> et tout <span
class="math inline">\(\lambda \in [0,1]\)</span>, on a :</p>
<p><span class="math display">\[S_q(\lambda p + (1-\lambda) q) \leq
\lambda S_q(p) + (1-\lambda) S_q(q).\]</span></p>
<p>Cette propriété est une généralisation de la convexité de l’entropie
de Shannon.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Tsallis est une généralisation puissante et flexible de
l’entropie de Shannon. En introduisant un paramètre supplémentaire <span
class="math inline">\(q\)</span>, elle permet de capturer des
comportements non additives et de décrire des systèmes complexes loin de
l’équilibre thermodynamique. Les propriétés et théorèmes présentés dans
cet article montrent que l’entropie de Tsallis est un outil précieux
pour l’étude des systèmes critiques, des processus de croissance et des
phénomènes de mémoire à long terme.</p>
<p>Les applications de l’entropie de Tsallis sont vastes et variées,
allant de la physique des particules à l’économie en passant par les
réseaux complexes. À mesure que notre compréhension des systèmes
complexes continue de s’approfondir, l’entropie de Tsallis est appelée à
jouer un rôle de plus en plus important dans la modélisation et
l’analyse de ces systèmes.</p>
</body>
</html>
{% include "footer.html" %}


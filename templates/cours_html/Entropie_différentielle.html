{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Différentielle</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Différentielle</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie différentielle, concept central en théorie de
l’information et en physique statistique, émerge comme une mesure
quantifiant le degré d’incertitude ou de désordre dans un système
continu. Son origine historique remonte aux travaux de Claude Shannon en
1948, qui a introduit l’entropie pour les systèmes discrets. L’extension
à des systèmes continus, due principalement à Norbert Wiener et à
d’autres pionniers, a permis de généraliser cette notion
fondamentale.</p>
<p>Cette mesure est indispensable dans divers domaines : en théorie de
l’information, elle quantifie la quantité d’information contenue dans
une variable aléatoire continue ; en physique statistique, elle joue un
rôle clé dans la compréhension des systèmes thermodynamiques. L’entropie
différentielle résout le problème de l’évaluation de l’incertitude dans
des systèmes où les variables peuvent prendre une infinité de valeurs,
offrant ainsi un cadre rigoureux pour l’analyse des phénomènes
complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie différentielle, considérons une variable
aléatoire continue <span class="math inline">\(X\)</span> de densité de
probabilité <span class="math inline">\(f_X(x)\)</span>. Nous cherchons
une mesure qui quantifie l’incertitude associée à <span
class="math inline">\(X\)</span>. Intuitivement, cette mesure doit être
maximale lorsque la densité de probabilité est uniforme, c’est-à-dire
lorsque <span class="math inline">\(X\)</span> peut prendre n’importe
quelle valeur avec la même probabilité.</p>
<p>Formellement, l’entropie différentielle <span
class="math inline">\(H(X)\)</span> de la variable aléatoire <span
class="math inline">\(X\)</span> est définie comme :</p>
<p><span class="math display">\[H(X) = -\int_{-\infty}^{+\infty} f_X(x)
\log f_X(x) \, dx\]</span></p>
<p>où <span class="math inline">\(f_X(x) \geq 0\)</span> et <span
class="math inline">\(\int_{-\infty}^{+\infty} f_X(x) \, dx =
1\)</span>.</p>
<p>Cette définition peut être reformulée en utilisant des
quantificateurs :</p>
<p><span class="math display">\[H(X) = -\int_{x \in \mathbb{R}} f_X(x)
\log f_X(x) \, dx \quad \text{tels que} \quad \forall x \in \mathbb{R},
f_X(x) \geq 0 \quad \text{et} \quad \int_{x \in \mathbb{R}} f_X(x) \, dx
= 1\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie différentielle est le
<em>théorème de Shannon</em> pour les variables continues. Ce théorème
établit une limite inférieure sur l’entropie différentielle, montrant
que pour une variable aléatoire continue bornée, l’entropie est maximale
lorsque la densité de probabilité est uniforme.</p>
<p>Pour formuler ce théorème, considérons une variable aléatoire <span
class="math inline">\(X\)</span> définie sur un intervalle borné <span
class="math inline">\([a, b]\)</span>. Le théorème de Shannon stipule
que :</p>
<p><span class="math display">\[H(X) \leq \log(b - a)\]</span></p>
<p>avec égalité si et seulement si <span class="math inline">\(f_X(x) =
\frac{1}{b - a}\)</span> pour presque tout <span class="math inline">\(x
\in [a, b]\)</span>.</p>
<p>La démonstration de ce théorème repose sur les propriétés de la
fonction logarithme et sur l’inégalité de Jensen. En effet, en
appliquant l’inégalité de Jensen à la fonction <span
class="math inline">\(-\log\)</span>, qui est concave, on obtient :</p>
<p><span class="math display">\[-\int_{a}^{b} f_X(x) \log f_X(x) \, dx
\leq -\log\left(\int_{a}^{b} f_X(x) \, dx\right) = \log(b -
a)\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Shannon, nous utilisons l’inégalité de
Jensen. Considérons la fonction <span class="math inline">\(\phi(t) =
-\log t\)</span>, qui est concave. L’inégalité de Jensen pour une
fonction concave <span class="math inline">\(\phi\)</span> et une
densité de probabilité <span class="math inline">\(f_X(x)\)</span>
s’écrit :</p>
<p><span class="math display">\[\phi\left(\int_{a}^{b} f_X(x) \,
dx\right) \geq \int_{a}^{b} \phi(f_X(x)) f_X(x) \, dx\]</span></p>
<p>En appliquant cette inégalité à <span class="math inline">\(\phi(t) =
-\log t\)</span>, nous obtenons :</p>
<p><span class="math display">\[-\log\left(\int_{a}^{b} f_X(x) \,
dx\right) \geq -\int_{a}^{b} \log(f_X(x)) f_X(x) \, dx\]</span></p>
<p>Or, <span class="math inline">\(\int_{a}^{b} f_X(x) \, dx =
1\)</span>, donc :</p>
<p><span class="math display">\[\log(1) \geq -\int_{a}^{b} \log(f_X(x))
f_X(x) \, dx\]</span></p>
<p>Ce qui équivaut à :</p>
<p><span class="math display">\[0 \geq -\int_{a}^{b} \log(f_X(x)) f_X(x)
\, dx\]</span></p>
<p>En multipliant par <span class="math inline">\(-1\)</span> des deux
côtés, nous obtenons :</p>
<p><span class="math display">\[\int_{a}^{b} \log(f_X(x)) f_X(x) \, dx
\geq 0\]</span></p>
<p>En réarrangeant les termes, nous avons :</p>
<p><span class="math display">\[-\int_{a}^{b} \log(f_X(x)) f_X(x) \, dx
\leq 0\]</span></p>
<p>Ce qui conclut la démonstration du théorème de Shannon pour les
variables continues bornées.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie différentielle possède plusieurs propriétés importantes
:</p>
<ol>
<li><p><em>Invariance par transformation</em> : L’entropie
différentielle est invariante sous les transformations bijectives et
différentiables. Plus précisément, si <span class="math inline">\(Y =
g(X)\)</span> où <span class="math inline">\(g\)</span> est une
bijection différentiable, alors <span class="math inline">\(H(Y) = H(X)
+ \mathbb{E}[\log |g&#39;(X)|]\)</span>.</p></li>
<li><p><em>Additivité</em> : Pour deux variables aléatoires
indépendantes <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, l’entropie conjointe est la somme des
entropies marginales : <span class="math inline">\(H(X, Y) = H(X) +
H(Y)\)</span>.</p></li>
<li><p><em>Inégalité de Gibbs</em> : Pour toute densité de probabilité
<span class="math inline">\(f_X(x)\)</span>, l’entropie différentielle
vérifie <span class="math inline">\(H(X) \leq
\log(\text{supp}(f_X))\)</span>, où <span
class="math inline">\(\text{supp}(f_X)\)</span> désigne le support de
<span class="math inline">\(f_X\)</span>.</p></li>
</ol>
<p>Pour démontrer la propriété (i), nous utilisons le changement de
variable dans l’intégrale. Soit <span class="math inline">\(Y =
g(X)\)</span>, alors :</p>
<p><span class="math display">\[H(Y) = -\int_{-\infty}^{+\infty} f_Y(y)
\log f_Y(y) \, dy = -\int_{-\infty}^{+\infty} f_X(g^{-1}(y)) \left|
\frac{dg^{-1}(y)}{dy} \right| \log\left(f_X(g^{-1}(y)) \left|
\frac{dg^{-1}(y)}{dy} \right|\right) \, dy\]</span></p>
<p>En utilisant le changement de variable <span class="math inline">\(x
= g^{-1}(y)\)</span>, nous obtenons :</p>
<p><span class="math display">\[H(Y) = -\int_{-\infty}^{+\infty} f_X(x)
\log\left(f_X(x) \left| g&#39;(x) \right|\right) \, dx =
-\int_{-\infty}^{+\infty} f_X(x) \log f_X(x) \, dx -
\int_{-\infty}^{+\infty} f_X(x) \log |g&#39;(x)| \, dx\]</span></p>
<p>Ce qui équivaut à :</p>
<p><span class="math display">\[H(Y) = H(X) + \mathbb{E}[\log
|g&#39;(X)|]\]</span></p>
<p>Ce qui conclut la démonstration de la propriété (i).</p>
</body>
</html>
{% include "footer.html" %}


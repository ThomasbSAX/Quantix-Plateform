{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de Kullback-Leibler : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de Kullback-Leibler : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’information, en tant que concept fondamental, a toujours été au
cœur des préoccupations scientifiques. La théorie de l’information, née
dans la première moitié du XXème siècle, a permis de quantifier cette
notion abstraite. Parmi les outils développés, la distance de
Kullback-Leibler (KL) occupe une place prépondérante. Introduite par
Solomon Kullback et Richard Leibler en 1951, cette mesure permet de
quantifier la divergence entre deux distributions de probabilité.</p>
<p>Pourquoi une telle mesure est-elle indispensable ? Dans un monde où
les données abondent, comparer des distributions devient crucial. Que ce
soit en apprentissage automatique, en traitement du signal ou en
biologie computationnelle, la distance KL offre un cadre rigoureux pour
évaluer les similarités et différences entre modèles probabilistes. Son
utilisation s’étend même à des domaines aussi variés que la finance,
l’ingénierie et les sciences sociales.</p>
<p>Cette mesure est particulièrement utile lorsqu’on souhaite mesurer la
performance d’un modèle statistique par rapport à une distribution de
référence. Elle permet, par exemple, de choisir le meilleur modèle parmi
plusieurs candidats en minimisant la divergence KL. Cette propriété est
exploitée dans des algorithmes d’optimisation comme
lExpectation-Maximization (EM) ou les réseaux de neurones génératifs
adversariaux (GANs).</p>
<p>En somme, la distance KL est un pilier de la théorie de l’information
moderne. Son étude approfondie permet non seulement de mieux comprendre
les fondements mathématiques de l’information, mais aussi d’ouvrir des
perspectives nouvelles dans des domaines appliqués variés.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la distance de Kullback-Leibler, commençons par
comprendre ce que nous cherchons à mesurer. Supposons que nous ayons
deux distributions de probabilité, <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur un espace commun. Nous
voulons quantifier à quel point <span class="math inline">\(P\)</span>
s’écarte de <span class="math inline">\(Q\)</span>. Intuitivement, si
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont très proches, la divergence
devrait être faible. Inversement, si elles diffèrent significativement,
la divergence devrait être élevée.</p>
<p>Formellement, la distance de Kullback-Leibler est définie comme suit
:</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La distance de Kullback-Leibler de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right)\]</span> si <span
class="math inline">\(P\)</span> est discrète, ou <span
class="math display">\[D_{KL}(P \| Q) = \int_{\Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) \, dx\]</span> si <span
class="math inline">\(P\)</span> est continue.</p>
</div>
<p>Remarquons que la distance KL n’est pas symétrique, c’est-à-dire que
<span class="math inline">\(D_{KL}(P \| Q) \neq D_{KL}(Q \| P)\)</span>.
De plus, elle n’est pas une distance au sens mathématique strict, car
elle ne satisfait pas l’inégalité triangulaire. C’est pourquoi on parle
souvent de divergence plutôt que de distance.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un résultat fondamental lié à la distance KL est l’inégalité de
Gibbs, qui relie la divergence KL à une borne sur les probabilités.
Commençons par comprendre ce que nous cherchons à établir. Supposons que
nous ayons deux distributions <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, et que nous voulions borner la
probabilité sous <span class="math inline">\(P\)</span> d’un événement
<span class="math inline">\(A\)</span> en utilisant la divergence
KL.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Pour tout événement <span class="math inline">\(A
\in \mathcal{F}\)</span>, on a : <span class="math display">\[P(A) \leq
e^{D_{KL}(P \| Q)} Q(A)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette inégalité, commençons par
utiliser la définition de la divergence KL : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right)\]</span> En utilisant l’inégalité de
Jensen, on a : <span class="math display">\[\sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) \geq \log\left(\sum_{x \in \Omega}
P(x) \frac{P(x)}{Q(x)}\right)\]</span> Ce qui se simplifie en : <span
class="math display">\[D_{KL}(P \| Q) \geq \log\left(\sum_{x \in A}
P(x)\right)\]</span> En exponentiant les deux côtés, on obtient : <span
class="math display">\[e^{D_{KL}(P \| Q)} \geq \sum_{x \in A}
P(x)\]</span> Or, par définition de <span
class="math inline">\(Q(A)\)</span>, on a : <span
class="math display">\[\sum_{x \in A} P(x) = Q(A) \sum_{x \in A}
\frac{P(x)}{Q(x)}\]</span> En utilisant l’inégalité de Cauchy-Schwarz,
on peut montrer que : <span class="math display">\[\sum_{x \in A}
\frac{P(x)}{Q(x)} \geq \left(\sum_{x \in A} P(x)\right)^2\]</span>
Ainsi, en combinant ces résultats, on obtient : <span
class="math display">\[e^{D_{KL}(P \| Q)} \geq P(A) Q(A)\]</span> Enfin,
en réarrangeant les termes, on arrive à l’inégalité de Gibbs : <span
class="math display">\[P(A) \leq e^{D_{KL}(P \| Q)} Q(A)\]</span> ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’utilisation de la distance KL, considérons un
exemple simple. Supposons que nous ayons deux distributions binaires
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies par : <span
class="math display">\[P(x) = p^x (1-p)^{1-x}, \quad Q(x) = q^x
(1-q)^{1-x}\]</span> où <span class="math inline">\(x \in \{0,
1\}\)</span>. Calculons la divergence KL de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> En utilisant la définition discrète de la divergence
KL, on a : <span class="math display">\[D_{KL}(P \| Q) = P(0)
\log\left(\frac{P(0)}{Q(0)}\right) + P(1)
\log\left(\frac{P(1)}{Q(1)}\right)\]</span> En substituant les
expressions de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on obtient : <span
class="math display">\[D_{KL}(P \| Q) = (1-p)
\log\left(\frac{1-p}{1-q}\right) + p
\log\left(\frac{p}{q}\right)\]</span> En simplifiant les logarithmes, on
arrive à : <span class="math display">\[D_{KL}(P \| Q) = (1-p)
\log\left(1-p\right) - (1-p) \log\left(1-q\right) + p \log(p) - p
\log(q)\]</span> En réarrangeant les termes, on peut écrire : <span
class="math display">\[D_{KL}(P \| Q) =
\log\left(\frac{1-q}{1-p}\right)^{1-p} +
\log\left(\frac{q}{p}\right)^p\]</span> Enfin, en combinant les
logarithmes, on obtient : <span class="math display">\[D_{KL}(P \| Q) =
\log\left(\left(\frac{1-q}{1-p}\right)^{1-p}
\left(\frac{q}{p}\right)^p\right)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance KL possède plusieurs propriétés intéressantes, que nous
allons explorer ici.</p>
<ol>
<li><p><strong>Non-négativité</strong> : La divergence KL est toujours
non négative. En d’autres termes, pour toute paire de distributions
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_{KL}(P \| Q) \geq 0\]</span> avec égalité si et
seulement si <span class="math inline">\(P = Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité de la divergence KL découle
directement de l’inégalité de Gibbs. En effet, en prenant <span
class="math inline">\(A = \Omega\)</span>, on a : <span
class="math display">\[P(\Omega) \leq e^{D_{KL}(P \| Q)}
Q(\Omega)\]</span> Or, <span class="math inline">\(P(\Omega) = Q(\Omega)
= 1\)</span>, donc : <span class="math display">\[1 \leq e^{D_{KL}(P \|
Q)}\]</span> En prenant le logarithme des deux côtés, on obtient : <span
class="math display">\[D_{KL}(P \| Q) \geq 0\]</span> L’égalité a lieu
si et seulement si <span class="math inline">\(P = Q\)</span>, car dans
ce cas, la divergence KL est nulle. ◻</p>
</div></li>
<li><p><strong>Convexité</strong> : La divergence KL est une fonction
convexe de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Cela signifie que pour toute paire de
distributions <span class="math inline">\(P_1, P_2\)</span> et <span
class="math inline">\(Q_1, Q_2\)</span>, et pour tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, on a : <span
class="math display">\[D_{KL}(\lambda P_1 + (1-\lambda) P_2 \| \lambda
Q_1 + (1-\lambda) Q_2) \leq \lambda D_{KL}(P_1 \| Q_1) + (1-\lambda)
D_{KL}(P_2 \| Q_2)\]</span></p>
<div class="proof">
<p><em>Proof.</em> La convexité de la divergence KL peut être démontrée
en utilisant l’inégalité de Gibbs et les propriétés des fonctions
convexes. En effet, la fonction <span class="math inline">\(f(x) = x
\log(x)\)</span> est convexe, et la divergence KL peut être vue comme
une combinaison linéaire de cette fonction. ◻</p>
</div></li>
<li><p><strong>Dualité</strong> : La divergence KL admet une dualité
avec l’entropie relative. Plus précisément, pour toute paire de
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_{KL}(P \| Q) = H(P, Q) - H(P)\]</span> où <span
class="math inline">\(H(P, Q)\)</span> est l’entropie croisée et <span
class="math inline">\(H(P)\)</span> est l’entropie de <span
class="math inline">\(P\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété découle directement des
définitions de l’entropie croisée et de la divergence KL. En effet, on a
: <span class="math display">\[H(P, Q) = -\sum_{x \in \Omega} P(x)
\log(Q(x))\]</span> et <span class="math display">\[H(P) = -\sum_{x \in
\Omega} P(x) \log(P(x))\]</span> En combinant ces deux expressions, on
obtient : <span class="math display">\[D_{KL}(P \| Q) = H(P, Q) -
H(P)\]</span> ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Kullback-Leibler est un outil puissant et polyvalent
en théorie de l’information. Son utilisation s’étend à de nombreux
domaines, des sciences fondamentales aux applications pratiques. Dans
cet article, nous avons exploré ses fondements mathématiques, ses
propriétés et quelques-unes de ses applications. Nous avons vu comment
cette mesure permet de quantifier la divergence entre deux distributions
de probabilité, et comment elle peut être utilisée pour comparer des
modèles statistiques.</p>
<p>Les propriétés de la distance KL, telles que sa non-négativité et sa
convexité, en font un outil précieux pour l’analyse des données et
l’apprentissage automatique. De plus, sa dualité avec l’entropie
relative ouvre des perspectives intéressantes pour l’étude des systèmes
complexes.</p>
<p>En conclusion, la distance de Kullback-Leibler reste un sujet de
recherche actif et prometteur. Son étude approfondie permet non
seulement de mieux comprendre les fondements mathématiques de
l’information, mais aussi d’ouvrir des perspectives nouvelles dans des
domaines appliqués variés.</p>
</body>
</html>
{% include "footer.html" %}


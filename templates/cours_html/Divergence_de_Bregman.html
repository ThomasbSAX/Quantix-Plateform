{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Bregman : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Bregman : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Bregman émerge comme une notion fondamentale dans
l’analyse des fonctions convexes, particulièrement en optimisation et en
apprentissage automatique. Introduite par L. M. Bregman dans les années
1960, cette divergence généralise l’idée de distance à des contextes où
les fonctions objectives ne sont pas nécessairement différentielles. Son
importance réside dans sa capacité à mesurer la différence entre une
fonction convexe et son approximation linéaire, offrant ainsi des outils
puissants pour l’analyse d’erreurs et la conception d’algorithmes.</p>
<p>Cette notion est indispensable dans des domaines tels que l’imagerie
médicale, la reconstruction d’images et les problèmes de minimisation
sous contraintes. Elle permet notamment de quantifier l’écart entre une
solution approchée et la solution exacte, facilitant ainsi le
développement d’algorithmes itératifs efficaces.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Nous commençons par expliquer ce que nous cherchons à capturer avec
la divergence de Bregman. Imaginons une fonction convexe <span
class="math inline">\(f\)</span> et un point <span
class="math inline">\(x\)</span>. Nous voulons mesurer à quel point une
approximation linéaire de <span class="math inline">\(f\)</span> autour
d’un autre point <span class="math inline">\(y\)</span> s’écarte de la
valeur réelle de <span class="math inline">\(f\)</span> en <span
class="math inline">\(x\)</span>. Cette idée conduit naturellement à la
définition suivante :</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable
sur un ensemble convexe <span class="math inline">\(C \subseteq
\mathbb{R}^n\)</span>. La divergence de Bregman associée à <span
class="math inline">\(f\)</span> est définie pour tout couple <span
class="math inline">\((x, y) \in C^2\)</span> par : <span
class="math display">\[D_f(x \| y) = f(x) - f(y) - \langle \nabla f(y),
x - y \rangle.\]</span></p>
</div>
<p>Cette définition peut être reformulée de plusieurs manières. Par
exemple, en utilisant la notion de sous-gradient : <span
class="math display">\[D_f(x \| y) = f(x) - \langle \nabla f(y), x - y
\rangle + f(y).\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant les
propriétés de la divergence de Bregman. Ce théorème montre que la
divergence de Bregman est toujours non négative pour une fonction
strictement convexe.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> une fonction strictement
convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C\)</span>. Alors, pour tout couple <span
class="math inline">\((x, y) \in C^2\)</span>, on a : <span
class="math display">\[D_f(x \| y) \geq 0.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons le fait
que <span class="math inline">\(f\)</span> est strictement convexe. Par
définition de la convexité stricte, pour tout <span
class="math inline">\(x, y \in C\)</span> et <span
class="math inline">\(\lambda \in (0, 1)\)</span>, on a : <span
class="math display">\[f(\lambda x + (1 - \lambda) y) &lt; \lambda f(x)
+ (1 - \lambda) f(y).\]</span></p>
<p>En prenant le gradient de cette inégalité et en évaluant en <span
class="math inline">\(\lambda = 1\)</span>, nous obtenons : <span
class="math display">\[\langle \nabla f(y), x - y \rangle &lt; f(x) -
f(y).\]</span></p>
<p>En réarrangeant cette inégalité, nous obtenons : <span
class="math display">\[D_f(x \| y) = f(x) - f(y) - \langle \nabla f(y),
x - y \rangle &gt; 0.\]</span></p>
<p>Ainsi, la divergence de Bregman est strictement positive pour <span
class="math inline">\(x \neq y\)</span>. Si <span
class="math inline">\(x = y\)</span>, alors <span
class="math inline">\(D_f(x \| y) = 0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
divergence de Bregman.</p>
<ol>
<li><p><strong>Identité de Pythagore</strong> : Pour une fonction <span
class="math inline">\(f\)</span> strictement convexe et différentiable,
la divergence de Bregman satisfait l’identité suivante : <span
class="math display">\[D_f(x \| y) + D_f(y \| x) = \langle \nabla f(x),
x - y \rangle - \langle \nabla f(y), x - y \rangle.\]</span></p></li>
<li><p><strong>Inégalité de la divergence de Bregman</strong> : Pour
tout <span class="math inline">\(x, y, z \in C\)</span>, on a : <span
class="math display">\[D_f(x \| z) \leq D_f(x \| y) + D_f(y \|
z).\]</span></p></li>
<li><p><strong>Continuité</strong> : La divergence de Bregman est
continue en <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> pour une fonction <span
class="math inline">\(f\)</span> continue.</p></li>
</ol>
<h1 class="unnumbered" id="applications">Applications</h1>
<p>La divergence de Bregman trouve des applications dans divers
domaines, notamment en imagerie médicale et en apprentissage
automatique. Par exemple, elle est utilisée dans les algorithmes de
reconstruction d’images pour minimiser la différence entre une image
reconstruite et une image observée. De plus, elle joue un rôle clé dans
les méthodes de régularisation et les algorithmes de projection sur des
ensembles convexes.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Bregman est une notion puissante et polyvalente qui
généralise l’idée de distance dans des contextes non différentiables.
Ses propriétés fondamentales, telles que la non-négativité et l’identité
de Pythagore, en font un outil précieux pour l’analyse d’erreurs et la
conception d’algorithmes. Son importance continue de croître dans des
domaines tels que l’imagerie médicale et l’apprentissage automatique, où
elle permet de quantifier et de minimiser les écarts entre des solutions
approchées et exactes.</p>
</body>
</html>
{% include "footer.html" %}


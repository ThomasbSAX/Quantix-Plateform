{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance of Manhattan in Embedding Space</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance of Manhattan in Embedding Space</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of distance in high-dimensional spaces has been a
cornerstone of various fields, from machine learning to data
compression. Among the myriad of distance metrics, the Manhattan
distance, also known as the L1 norm or taxicab distance, has gained
significant attention due to its computational efficiency and
interpretability. The embedding space, a representation of data in a
lower-dimensional manifold, further amplifies the relevance of Manhattan
distance. This article delves into the historical context, mathematical
foundations, and practical implications of using Manhattan distance in
embedding spaces.</p>
<p>The emergence of the Manhattan distance can be traced back to the
early 20th century, with its formalization in the context of normed
vector spaces. Its name derives from the grid-like layout of Manhattan’s
street system, where movement is only possible along one axis at a time.
In the realm of embedding spaces, the Manhattan distance offers a robust
metric for comparing data points, especially in scenarios where the
underlying manifold is non-linear or the data exhibits sparsity.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the Manhattan distance in embedding space, we first
need to grasp the concept of an embedding and the properties of the
Manhattan distance.</p>
<p>Consider a dataset <span class="math inline">\(X\)</span> consisting
of <span class="math inline">\(n\)</span> data points in a
high-dimensional space <span
class="math inline">\(\mathbb{R}^d\)</span>. An embedding is a function
<span class="math inline">\(f: \mathbb{R}^d \rightarrow
\mathbb{R}^k\)</span> where <span class="math inline">\(k &lt;
d\)</span>, mapping the high-dimensional data to a lower-dimensional
space while preserving certain properties, such as pairwise distances or
topological structure.</p>
<p>The Manhattan distance between two points <span
class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^k\)</span>
is defined as the sum of the absolute differences of their Cartesian
coordinates. Formally, we can express this as:</p>
<p><span class="math display">\[d_1(\mathbf{x}, \mathbf{y}) =
\sum_{i=1}^k |x_i - y_i|\]</span></p>
<p>Alternatively, using the L1 norm notation:</p>
<p><span class="math display">\[d_1(\mathbf{x}, \mathbf{y}) =
\|\mathbf{x} - \mathbf{y}\|_1\]</span></p>
<p>For a more general formulation, consider two points <span
class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^k\)</span>.
The Manhattan distance can be written as:</p>
<p><span class="math display">\[d_1(\mathbf{x}, \mathbf{y}) =
\inf_{\gamma} \int_{0}^{1} \|\dot{\gamma}(t)\|_1 \, dt\]</span></p>
<p>where <span class="math inline">\(\gamma: [0,1] \rightarrow
\mathbb{R}^k\)</span> is a rectifiable curve such that <span
class="math inline">\(\gamma(0) = \mathbf{x}\)</span> and <span
class="math inline">\(\gamma(1) = \mathbf{y}\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems related to the Manhattan distance in
embedding spaces is the Johnson-Lindenstrauss Lemma, which provides a
guarantee on the preservation of distances when projecting data into a
lower-dimensional space.</p>
<div class="theorem">
<p>For any <span class="math inline">\(0 &lt; \epsilon &lt; 1\)</span>,
there exists a function <span class="math inline">\(f: \mathbb{R}^d
\rightarrow \mathbb{R}^k\)</span> and a constant <span
class="math inline">\(c &gt; 0\)</span> such that for all <span
class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span>,
the following inequality holds:</p>
<p><span class="math display">\[(1 - \epsilon) \|\mathbf{x} -
\mathbf{y}\|_2 \leq \|f(\mathbf{x}) - f(\mathbf{y})\|_2 \leq (1 +
\epsilon) \|\mathbf{x} - \mathbf{y}\|_2\]</span></p>
<p>where <span class="math inline">\(k = c \epsilon^{-2} \log
n\)</span>.</p>
</div>
<p>The proof of this theorem involves probabilistic methods and the use
of random projections. The lemma ensures that the Euclidean distance is
approximately preserved, but similar arguments can be extended to the
Manhattan distance.</p>
<h1 id="proofs">Proofs</h1>
<p>To demonstrate the utility of the Manhattan distance in embedding
spaces, let’s consider a simple proof involving the preservation of
distances under random projections.</p>
<div class="proof">
<p><em>Proof.</em> Consider a dataset <span
class="math inline">\(X\)</span> with <span
class="math inline">\(n\)</span> points in <span
class="math inline">\(\mathbb{R}^d\)</span>. We want to embed these
points into a lower-dimensional space <span
class="math inline">\(\mathbb{R}^k\)</span> using a random projection
matrix <span class="math inline">\(R \in \mathbb{R}^{k \times
d}\)</span>, where each entry is drawn independently from a Gaussian
distribution <span class="math inline">\(\mathcal{N}(0,
1/k)\)</span>.</p>
<p>For any two points <span class="math inline">\(\mathbf{x}, \mathbf{y}
\in X\)</span>, the Manhattan distance in the embedding space is given
by:</p>
<p><span class="math display">\[d_1(R\mathbf{x}, R\mathbf{y}) =
\sum_{i=1}^k |(R\mathbf{x})_i - (R\mathbf{y})_i|\]</span></p>
<p>Using the properties of random projections, we can show that:</p>
<p><span class="math display">\[\mathbb{E}[d_1(R\mathbf{x},
R\mathbf{y})] = d_1(\mathbf{x}, \mathbf{y})\]</span></p>
<p>This expectation is derived from the fact that the random projection
preserves the expected value of the distances. The variance of <span
class="math inline">\(d_1(R\mathbf{x}, R\mathbf{y})\)</span> can be
bounded using concentration inequalities, ensuring that the distance is
preserved with high probability. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Manhattan distance in embedding spaces exhibits several important
properties, which we list and prove below.</p>
<ol>
<li><p><strong>Translation Invariance:</strong> The Manhattan distance
is invariant under translations. For any <span
class="math inline">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in
\mathbb{R}^k\)</span>,</p>
<p><span class="math display">\[d_1(\mathbf{x} + \mathbf{z}, \mathbf{y}
+ \mathbf{z}) = d_1(\mathbf{x}, \mathbf{y})\]</span></p>
<p>This property follows directly from the definition of the Manhattan
distance.</p></li>
<li><p><strong>Positive Definiteness:</strong> The Manhattan distance is
a metric, satisfying the properties of non-negativity, identity of
indiscernibles, symmetry, and the triangle inequality.</p>
<div class="proof">
<p><em>Proof.</em> Non-negativity: <span
class="math inline">\(d_1(\mathbf{x}, \mathbf{y}) \geq 0\)</span> for
all <span class="math inline">\(\mathbf{x}, \mathbf{y} \in
\mathbb{R}^k\)</span>.</p>
<p>Identity of indiscernibles: <span
class="math inline">\(d_1(\mathbf{x}, \mathbf{y}) = 0\)</span> if and
only if <span class="math inline">\(\mathbf{x} =
\mathbf{y}\)</span>.</p>
<p>Symmetry: <span class="math inline">\(d_1(\mathbf{x}, \mathbf{y}) =
d_1(\mathbf{y}, \mathbf{x})\)</span>.</p>
<p>Triangle inequality: <span class="math inline">\(d_1(\mathbf{x},
\mathbf{z}) \leq d_1(\mathbf{x}, \mathbf{y}) + d_1(\mathbf{y},
\mathbf{z})\)</span> for all <span class="math inline">\(\mathbf{x},
\mathbf{y}, \mathbf{z} \in \mathbb{R}^k\)</span>. ◻</p>
</div></li>
<li><p><strong>Sparsity:</strong> The Manhattan distance is particularly
suited for sparse data, as it emphasizes the number of non-zero
components in the difference vector <span
class="math inline">\(\mathbf{x} - \mathbf{y}\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Consider two sparse vectors <span
class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^k\)</span>
with at most <span class="math inline">\(s\)</span> non-zero components.
The Manhattan distance <span class="math inline">\(d_1(\mathbf{x},
\mathbf{y})\)</span> is bounded by <span class="math inline">\(2s
\max_{i} |x_i - y_i|\)</span>, which is a linear function of the
sparsity <span class="math inline">\(s\)</span>. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>The Manhattan distance in embedding spaces offers a powerful and
efficient metric for comparing data points, particularly in
high-dimensional settings. Its computational simplicity,
interpretability, and robustness to sparsity make it an attractive
choice for various applications. By leveraging the properties of random
projections and the Johnson-Lindenstrauss Lemma, we can ensure the
preservation of distances in lower-dimensional embeddings. Future
research may explore the integration of Manhattan distance with other
advanced embedding techniques, such as deep learning-based methods, to
further enhance its applicability and performance.</p>
</body>
</html>
{% include "footer.html" %}


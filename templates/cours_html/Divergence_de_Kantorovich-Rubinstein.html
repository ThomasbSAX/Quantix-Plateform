{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Kantorovich-Rubinstein</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Kantorovich-Rubinstein</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Kantorovich-Rubinstein émerge dans le cadre de la
théorie des probabilités et de l’analyse fonctionnelle. Elle trouve ses
racines dans les travaux de Leonid Kantorovich et de Vladimir
Rubinstein, qui ont cherché à quantifier la distance entre deux mesures
de probabilité. Cette notion est indispensable dans divers domaines tels
que l’optimisation, la théorie des jeux et les statistiques.</p>
<p>La divergence de Kantorovich-Rubinstein est particulièrement utile
pour mesurer la différence entre deux distributions de probabilité. Elle
est souvent utilisée dans les problèmes d’approximation et de transport
optimal, où il est crucial de comprendre comment une mesure peut être
transformée en une autre de manière optimale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Kantorovich-Rubinstein, commençons
par comprendre ce que nous cherchons à mesurer. Supposons que nous ayons
deux mesures de probabilité <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span> sur un espace métrique <span
class="math inline">\((X, d)\)</span>. Nous voulons quantifier la
"distance" entre ces deux mesures en tenant compte de la structure
métrique de <span class="math inline">\(X\)</span>.</p>
<p>La divergence de Kantorovich-Rubinstein est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
et <span class="math inline">\(\mu, \nu\)</span> deux mesures de
probabilité sur <span class="math inline">\(X\)</span>. La divergence de
Kantorovich-Rubinstein entre <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span> est définie par : <span
class="math display">\[W(\mu, \nu) = \sup_{f \in \text{Lip}(1)} \left|
\int_X f \, d\mu - \int_X f \, d\nu \right|\]</span> où <span
class="math inline">\(\text{Lip}(1)\)</span> désigne l’ensemble des
fonctions <span class="math inline">\(f: X \rightarrow
\mathbb{R}\)</span> lipschitziennes avec une constante de Lipschitz
inférieure ou égale à 1.</p>
</div>
<p>Une autre formulation équivalente est :</p>
<div class="definition">
<p><span class="math display">\[W(\mu, \nu) = \inf_{\gamma \in
\Gamma(\mu, \nu)} \int_{X \times X} d(x, y) \, d\gamma(x, y)\]</span> où
<span class="math inline">\(\Gamma(\mu, \nu)\)</span> est l’ensemble des
mesures de couplage entre <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span>, c’est-à-dire les mesures <span
class="math inline">\(\gamma\)</span> sur <span class="math inline">\(X
\times X\)</span> telles que pour tout borélien <span
class="math inline">\(A \subset X\)</span>, <span
class="math inline">\(\gamma(A \times X) = \mu(A)\)</span> et <span
class="math inline">\(\gamma(X \times A) = \nu(A)\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Kantorovich-Rubinstein
est le théorème de dualité, qui établit une relation entre la divergence
et les fonctions lipschitziennes.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
compact et <span class="math inline">\(\mu, \nu\)</span> deux mesures de
probabilité sur <span class="math inline">\(X\)</span>. Alors : <span
class="math display">\[W(\mu, \nu) = \sup_{f \in \text{Lip}(1)} \left|
\int_X f \, d\mu - \int_X f \, d\nu \right|\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de dualité, nous utilisons la théorie du
transport optimal et les propriétés des fonctions lipschitziennes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une mesure de couplage <span
class="math inline">\(\gamma \in \Gamma(\mu, \nu)\)</span>. Pour toute
fonction <span class="math inline">\(f \in \text{Lip}(1)\)</span>, nous
avons : <span class="math display">\[\left| \int_X f \, d\mu - \int_X f
\, d\nu \right| = \left| \int_{X \times X} (f(x) - f(y)) \, d\gamma(x,
y) \right| \leq \int_{X \times X} |f(x) - f(y)| \, d\gamma(x,
y)\]</span> Puisque <span class="math inline">\(f\)</span> est
lipschitzienne avec une constante de Lipschitz inférieure ou égale à 1,
nous avons <span class="math inline">\(|f(x) - f(y)| \leq d(x,
y)\)</span>. Donc : <span class="math display">\[\left| \int_X f \, d\mu
- \int_X f \, d\nu \right| \leq \int_{X \times X} d(x, y) \, d\gamma(x,
y)\]</span> En prenant le supremum sur toutes les fonctions <span
class="math inline">\(f \in \text{Lip}(1)\)</span>, nous obtenons :
<span class="math display">\[W(\mu, \nu) \leq \inf_{\gamma \in
\Gamma(\mu, \nu)} \int_{X \times X} d(x, y) \, d\gamma(x, y)\]</span>
Réciproquement, pour toute mesure de couplage <span
class="math inline">\(\gamma \in \Gamma(\mu, \nu)\)</span>, nous pouvons
construire une fonction <span class="math inline">\(f\)</span>
lipschitzienne telle que : <span class="math display">\[\int_{X \times
X} d(x, y) \, d\gamma(x, y) = \left| \int_X f \, d\mu - \int_X f \, d\nu
\right|\]</span> Cela montre que : <span class="math display">\[W(\mu,
\nu) \geq \inf_{\gamma \in \Gamma(\mu, \nu)} \int_{X \times X} d(x, y)
\, d\gamma(x, y)\]</span> Ainsi, nous avons : <span
class="math display">\[W(\mu, \nu) = \inf_{\gamma \in \Gamma(\mu, \nu)}
\int_{X \times X} d(x, y) \, d\gamma(x, y)\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Kantorovich-Rubinstein possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p>**Non-négativité** : Pour toutes mesures de probabilité <span
class="math inline">\(\mu, \nu\)</span>, nous avons <span
class="math inline">\(W(\mu, \nu) \geq 0\)</span>.</p></li>
<li><p>**Symétrie** : Pour toutes mesures de probabilité <span
class="math inline">\(\mu, \nu\)</span>, nous avons <span
class="math inline">\(W(\mu, \nu) = W(\nu, \mu)\)</span>.</p></li>
<li><p>**Inégalité triangulaire** : Pour toutes mesures de probabilité
<span class="math inline">\(\mu, \nu, \lambda\)</span>, nous avons <span
class="math inline">\(W(\mu, \lambda) \leq W(\mu, \nu) + W(\nu,
\lambda)\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ul>
<li><p>La non-négativité découle directement de la définition, car <span
class="math inline">\(d(x, y) \geq 0\)</span> pour tout <span
class="math inline">\(x, y \in X\)</span>.</p></li>
<li><p>La symétrie est une conséquence de la définition des mesures de
couplage et de l’intégrale.</p></li>
<li><p>L’inégalité triangulaire peut être démontrée en utilisant la
définition de <span class="math inline">\(W\)</span> et les propriétés
des intégrales.</p></li>
</ul>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Kantorovich-Rubinstein est un outil puissant pour
mesurer la distance entre deux mesures de probabilité. Ses applications
sont vastes, allant de l’optimisation au transport optimal. Les
théorèmes et propriétés présentés dans cet article montrent la richesse
et l’utilité de cette notion.</p>
</body>
</html>
{% include "footer.html" %}


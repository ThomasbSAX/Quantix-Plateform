{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Rényi : Une Mesure de l’Information en Théorie des Probabilités</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Rényi : Une Mesure de l’Information en
Théorie des Probabilités</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Rényi émerge comme une généralisation élégante des
mesures classiques d’information, telles que la divergence de
Kullback-Leibler. Introduite par Alfred Rényi en 1961, cette notion
trouve ses racines dans la théorie de l’information et les probabilités.
Elle offre un cadre flexible pour quantifier la distance entre deux
distributions de probabilité, en intégrant un paramètre d’ordre qui
permet de moduler la sensibilité de la mesure.</p>
<p>L’importance de la divergence de Rényi réside dans sa capacité à
unifier plusieurs mesures d’information bien connues. En ajustant le
paramètre d’ordre, on peut retrouver des notions telles que l’entropie
de Shannon et la divergence de Kullback-Leibler. Cette propriété en fait
un outil puissant dans divers domaines, allant de la théorie des codes à
l’apprentissage automatique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Rényi, considérons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une mesure
qui quantifie la dissimilarité entre ces deux distributions.
Intuitivement, cette mesure devrait être nulle si et seulement si <span
class="math inline">\(P = Q\)</span>, et positive sinon.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>, et
soit <span class="math inline">\(\alpha &gt; 0\)</span> un paramètre
réel. La divergence de Rényi d’ordre <span
class="math inline">\(\alpha\)</span> entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_{\alpha}(P \| Q) = \frac{1}{\alpha - 1} \log
\left( \int_{\mathcal{X}} \left( \frac{dP}{dQ} \right)^{\alpha} dQ
\right),\]</span> où <span class="math inline">\(\frac{dP}{dQ}\)</span>
désigne la densité de Radon-Nikodym de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span>.</p>
</div>
<p>Cette définition peut être reformulée de plusieurs manières. Par
exemple, pour <span class="math inline">\(\alpha &gt; 1\)</span>, on
peut écrire : <span class="math display">\[D_{\alpha}(P \| Q) =
\frac{1}{\alpha - 1} \log \mathbb{E}_{Q} \left[ \left( \frac{dP}{dQ}
\right)^{\alpha} \right].\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence de Rényi est celui
de la continuité de la divergence par rapport au paramètre d’ordre.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>. La
fonction <span class="math inline">\(\alpha \mapsto D_{\alpha}(P \|
Q)\)</span> est continue sur <span class="math inline">\((0, 1) \cup (1,
+\infty)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur des techniques
d’analyse convexe et la théorie des mesures. On utilise notamment le
fait que la fonction <span class="math inline">\(\alpha \mapsto
D_{\alpha}(P \| Q)\)</span> est convexe pour <span
class="math inline">\(\alpha &gt; 1\)</span> et concave pour <span
class="math inline">\(0 &lt; \alpha &lt; 1\)</span>. La continuité
découle alors des propriétés de ces fonctions convexes et
concaves. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la divergence de Rényi, considérons un
exemple simple. Supposons que <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> soient deux distributions de
Bernoulli avec des paramètres <span class="math inline">\(p\)</span> et
<span class="math inline">\(q\)</span> respectivement. Nous voulons
calculer <span class="math inline">\(D_{\alpha}(P \| Q)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La densité de Radon-Nikodym de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[\frac{dP}{dQ}(x) = \left( \frac{p}{q} \right)^x
\left( \frac{1 - p}{1 - q} \right)^{1 - x}.\]</span> En utilisant la
définition de la divergence de Rényi, nous avons : <span
class="math display">\[D_{\alpha}(P \| Q) = \frac{1}{\alpha - 1} \log
\left( q^{\alpha} \left( \frac{p}{q} \right)^{\alpha x} + (1 -
q)^{\alpha} \left( \frac{1 - p}{1 - q} \right)^{\alpha (1 - x)}
\right).\]</span> En simplifiant, on obtient : <span
class="math display">\[D_{\alpha}(P \| Q) = \frac{1}{\alpha - 1} \log
\left( p^{\alpha} q^{1 - \alpha} + (1 - p)^{\alpha} (1 - q)^{1 - \alpha}
\right).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Rényi possède plusieurs propriétés intéressantes,
que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour tout <span
class="math inline">\(\alpha &gt; 0\)</span> et toute paire de
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_{\alpha}(P \| Q) \geq 0,\]</span> avec égalité
si et seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p><strong>Invariance par transformation mesurable</strong> : Si
<span class="math inline">\(T : \mathcal{X} \to \mathcal{Y}\)</span> est
une transformation mesurable, alors : <span
class="math display">\[D_{\alpha}(P \circ T^{-1} \| Q \circ T^{-1}) =
D_{\alpha}(P \| Q).\]</span></p></li>
<li><p><strong>Limite vers la divergence de Kullback-Leibler</strong> :
En prenant la limite lorsque <span class="math inline">\(\alpha \to
1\)</span>, on retrouve la divergence de Kullback-Leibler : <span
class="math display">\[\lim_{\alpha \to 1} D_{\alpha}(P \| Q) = D_{KL}(P
\| Q).\]</span></p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
définitions et théorèmes précédents. Par exemple, la non-négativité
découle directement de l’inégalité de Jensen et des propriétés de la
fonction logarithme.</p>
</body>
</html>
{% include "footer.html" %}


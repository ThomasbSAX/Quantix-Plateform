{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de variance</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de
variance</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de variance est une
technique puissante en traitement du signal et en apprentissage
automatique. Elle trouve ses racines dans la théorie des signaux et
l’analyse statistique, où la variance est un outil fondamental pour
mesurer la dispersion d’un ensemble de données. Cette méthode émerge
comme une réponse à la nécessité de compresser et de représenter
efficacement des données tout en préservant leur information
essentielle.</p>
<p>L’idée centrale est de capturer les variations significatives dans
les données, souvent en utilisant des transformations mathématiques
comme la transformée de Fourier ou la transformée en ondelettes. Ces
transformations permettent de décomposer les signaux en composantes
fréquentielles ou temporelles, facilitant ainsi l’extraction de
caractéristiques pertinentes. La variance, en tant que mesure de la
dispersion, joue un rôle crucial dans l’identification de ces
caractéristiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
variance, commençons par définir les concepts clés. Supposons que nous
ayons un signal <span class="math inline">\(x(t)\)</span>, où <span
class="math inline">\(t\)</span> représente le temps. Nous cherchons à
extraire des caractéristiques qui capturent les variations
significatives de ce signal.</p>
<div class="definition">
<p>La variance d’un signal <span class="math inline">\(x(t)\)</span> sur
un intervalle <span class="math inline">\([a, b]\)</span> est définie
comme : <span class="math display">\[\text{Var}(x) = \frac{1}{b - a}
\int_{a}^{b} (x(t) - \mu)^2 \, dt\]</span> où <span
class="math inline">\(\mu\)</span> est la moyenne du signal sur
l’intervalle <span class="math inline">\([a, b]\)</span>, donnée par :
<span class="math display">\[\mu = \frac{1}{b - a} \int_{a}^{b} x(t) \,
dt\]</span></p>
</div>
<p>Une autre manière de définir la variance est en utilisant des séries
temporelles discrètes. Soit <span class="math inline">\(x_n\)</span> une
série temporelle discrète, la variance est alors : <span
class="math display">\[\text{Var}(x) = \frac{1}{N} \sum_{n=1}^{N} (x_n -
\mu)^2\]</span> où <span class="math inline">\(N\)</span> est le nombre
de points dans la série temporelle et <span
class="math inline">\(\mu\)</span> est la moyenne des <span
class="math inline">\(x_n\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans ce contexte est le théorème de Parseval,
qui relie la variance d’un signal à ses composantes fréquentielles.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(x(t)\)</span> un signal de durée
finie et <span class="math inline">\(X(f)\)</span> sa transformée de
Fourier. Alors, la variance du signal est donnée par : <span
class="math display">\[\text{Var}(x) = \frac{1}{2\pi}
\int_{-\infty}^{\infty} |X(f)|^2 \, df\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Parseval, nous commençons par rappeler
que la transformée de Fourier d’un signal <span
class="math inline">\(x(t)\)</span> est définie comme : <span
class="math display">\[X(f) = \int_{-\infty}^{\infty} x(t) e^{-i 2\pi f
t} \, dt\]</span> et sa transformée inverse est : <span
class="math display">\[x(t) = \int_{-\infty}^{\infty} X(f) e^{i 2\pi f
t} \, df\]</span></p>
<p>En utilisant ces définitions, nous pouvons écrire la variance du
signal comme : <span class="math display">\[\text{Var}(x) =
\frac{1}{2\pi} \int_{-\infty}^{\infty} |X(f)|^2 \, df\]</span></p>
<p>Cette preuve repose sur l’égalité de Parseval, qui est une
conséquence directe des propriétés de la transformée de Fourier.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de l’encodage
par extraction de caractéristiques de variance.</p>
<ol>
<li><p><strong>Invariance par Translation</strong> : La variance d’un
signal est invariante par translation temporelle. Cela signifie que si
nous décalons le signal dans le temps, sa variance reste
inchangée.</p></li>
<li><p><strong>Additivité</strong> : La variance de la somme de deux
signaux indépendants est égale à la somme de leurs variances.
Mathématiquement, si <span class="math inline">\(x(t)\)</span> et <span
class="math inline">\(y(t)\)</span> sont indépendants, alors : <span
class="math display">\[\text{Var}(x + y) = \text{Var}(x) +
\text{Var}(y)\]</span></p></li>
<li><p><strong>Effet de l’Échelle</strong> : Si nous multiplions un
signal par une constante <span class="math inline">\(a\)</span>, sa
variance est multipliée par <span class="math inline">\(a^2\)</span>.
Cela peut être exprimé comme : <span class="math display">\[\text{Var}(a
x) = a^2 \text{Var}(x)\]</span></p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
définitions et les théorèmes précédents. Par exemple, pour la propriété
d’additivité, nous utilisons le fait que les signaux sont indépendants
et appliquons la définition de la variance.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de variance est une
technique puissante et polyvalente, avec des applications dans divers
domaines tels que le traitement du signal, l’apprentissage automatique
et la compression de données. En capturant les variations significatives
dans les données, cette méthode permet de représenter efficacement
l’information tout en préservant sa structure essentielle.</p>
<p>Les définitions, théorèmes et propriétés discutés dans cet article
fournissent une base solide pour comprendre et appliquer cette
technique. Les preuves détaillées illustrent la rigueur mathématique
sous-jacente, soulignant l’importance de la variance comme outil
d’analyse et de représentation des données.</p>
</body>
</html>
{% include "footer.html" %}


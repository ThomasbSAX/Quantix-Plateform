{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence Square Loss: Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence Square Loss: Théorie et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La fonction de perte <em>square loss</em>, ou perte quadratique, est
un outil fondamental en statistique et en apprentissage automatique.
Elle mesure l’écart entre une valeur prédite et une valeur réelle, et
est largement utilisée pour son interprétation géométrique simple et ses
propriétés mathématiques avantageuses. Dans ce chapitre, nous explorons
une variante de cette fonction de perte : la <em>divergence square
loss</em>.</p>
<p>La divergence square loss émerge naturellement dans le cadre de
l’estimation des paramètres d’un modèle, notamment lorsque les données
présentent une structure complexe ou lorsqu’on cherche à capturer des
dépendances non linéaires. Elle est particulièrement utile dans les
modèles bayésiens et les méthodes d’inférence statistique avancées.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence square loss, commençons par rappeler
quelques concepts préliminaires.</p>
<h2 id="perte-quadratique">Perte Quadratique</h2>
<p>La perte quadratique, ou square loss, est définie comme suit : pour
une valeur réelle <span class="math inline">\(y\)</span> et une
prédiction <span class="math inline">\(\hat{y}\)</span>, la perte
quadratique est donnée par <span class="math display">\[L(y, \hat{y}) =
(y - \hat{y})^2.\]</span></p>
<p>Cette fonction de perte est convexe, différentiable et admet un
minimum unique en <span class="math inline">\(\hat{y} = y\)</span>.</p>
<h2 id="divergence-de-kullback-leibler">Divergence de
Kullback-Leibler</h2>
<p>La divergence de Kullback-Leibler (KL) est une mesure d’information
qui quantifie la différence entre deux distributions de probabilité
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Elle est définie par <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x} P(x) \log \left(
\frac{P(x)}{Q(x)} \right).\]</span></p>
<p>La divergence KL est toujours non négative et admet un minimum en
<span class="math inline">\(P = Q\)</span>.</p>
<h2 id="divergence-square-loss">Divergence Square Loss</h2>
<p>La divergence square loss combine les idées de la perte quadratique
et de la divergence KL. Elle est définie comme suit :</p>
<p>Considérons deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace fini. Nous cherchons à
mesurer la divergence entre ces deux distributions en utilisant une
fonction de perte quadratique. La divergence square loss est définie par
<span class="math display">\[D_{\text{square}}(P, Q) = \sum_{x} (P(x) -
Q(x))^2.\]</span></p>
<p>Cette définition peut être interprétée comme une mesure de la
distance entre les deux distributions, pondérée par le carré de la
différence entre leurs probabilités.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons quelques théorèmes importants
concernant la divergence square loss.</p>
<h2 id="propriétés-de-base">Propriétés de Base</h2>
<div class="theorem">
<p>La divergence square loss <span
class="math inline">\(D_{\text{square}}(P, Q)\)</span> est toujours non
négative et admet un minimum en <span class="math inline">\(P =
Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer que <span
class="math inline">\(D_{\text{square}}(P, Q) \geq 0\)</span>, nous
utilisons l’inégalité de Cauchy-Schwarz. Nous avons <span
class="math display">\[D_{\text{square}}(P, Q) = \sum_{x} (P(x) -
Q(x))^2 \geq 0,\]</span> avec égalité si et seulement si <span
class="math inline">\(P(x) = Q(x)\)</span> pour tout <span
class="math inline">\(x\)</span>.</p>
<p>Pour montrer que le minimum est atteint en <span
class="math inline">\(P = Q\)</span>, nous observons que <span
class="math display">\[D_{\text{square}}(P, P) = 0.\]</span> Ainsi,
<span class="math inline">\(D_{\text{square}}(P, Q) \geq 0\)</span> pour
toutes distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span>. ◻</p>
</div>
<h2 id="lien-avec-la-divergence-kl">Lien avec la Divergence KL</h2>
<div class="theorem">
<p>La divergence square loss est liée à la divergence de
Kullback-Leibler par l’inégalité suivante : <span
class="math display">\[D_{\text{square}}(P, Q) \leq 2 D_{KL}(P \|
Q).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous utilisons l’inégalité de Pinsker, qui stipule
que <span class="math display">\[D_{KL}(P \| Q) \geq \frac{1}{2}
\sum_{x} (P(x) - Q(x))^2.\]</span> En réarrangeant cette inégalité, nous
obtenons <span class="math display">\[D_{\text{square}}(P, Q) \leq 2
D_{KL}(P \| Q).\]</span> ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Dans cette section, nous fournissons des preuves détaillées pour les
théorèmes présentés.</p>
<h2 id="preuve-du-théorème-1">Preuve du Théorème 1</h2>
<p>Nous voulons montrer que <span
class="math inline">\(D_{\text{square}}(P, Q) \geq 0\)</span>. Nous
utilisons l’inégalité de Cauchy-Schwarz, qui stipule que pour tout
vecteur <span class="math inline">\(\mathbf{v}\)</span>, <span
class="math display">\[\|\mathbf{v}\|_2^2 \geq 0.\]</span></p>
<p>En appliquant cette inégalité à <span
class="math inline">\(\mathbf{v} = (P(x) - Q(x))_{x}\)</span>, nous
obtenons <span class="math display">\[D_{\text{square}}(P, Q) = \sum_{x}
(P(x) - Q(x))^2 \geq 0.\]</span></p>
<p>L’égalité a lieu si et seulement si <span class="math inline">\(P(x)
= Q(x)\)</span> pour tout <span class="math inline">\(x\)</span>.</p>
<h2 id="preuve-du-théorem-2">Preuve du Théorem 2</h2>
<p>Nous voulons montrer que <span
class="math inline">\(D_{\text{square}}(P, Q) \leq 2 D_{KL}(P \|
Q)\)</span>. Nous utilisons l’inégalité de Pinsker, qui stipule que
<span class="math display">\[D_{KL}(P \| Q) \geq \frac{1}{2} \sum_{x}
(P(x) - Q(x))^2.\]</span></p>
<p>En réarrangeant cette inégalité, nous obtenons <span
class="math display">\[D_{\text{square}}(P, Q) \leq 2 D_{KL}(P \|
Q).\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés et
corollaires de la divergence square loss.</p>
<h2 id="propriétés">Propriétés</h2>
<ol>
<li><p>La divergence square loss est invariante par translation.
C’est-à-dire que pour toute constante <span
class="math inline">\(c\)</span>, <span
class="math display">\[D_{\text{square}}(P, Q + c) = D_{\text{square}}(P
- c, Q).\]</span></p></li>
<li><p>La divergence square loss est convexe en ses deux arguments. Cela
signifie que pour toute distribution <span
class="math inline">\(P\)</span> et tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, <span
class="math display">\[D_{\text{square}}(P, \lambda Q_1 + (1 - \lambda)
Q_2) \leq \lambda D_{\text{square}}(P, Q_1) + (1 - \lambda)
D_{\text{square}}(P, Q_2).\]</span></p></li>
</ol>
<h2 id="corollaires">Corollaires</h2>
<div class="corollary">
<p>La divergence square loss peut être utilisée pour estimer les
paramètres d’un modèle de manière efficace.</p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant la propriété de convexité, nous pouvons
minimiser la divergence square loss pour estimer les paramètres d’un
modèle. Cette approche est particulièrement utile dans les modèles
bayésiens et les méthodes d’inférence statistique avancées. ◻</p>
</div>
<div class="corollary">
<p>La divergence square loss est un outil puissant pour l’analyse des
données et la détection d’anomalies.</p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant les propriétés de la divergence square
loss, nous pouvons détecter des anomalies dans les données en
identifiant les distributions qui divergent significativement de la
distribution attendue. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans ce chapitre, nous avons introduit la divergence square loss et
exploré ses propriétés mathématiques. Nous avons montré qu’elle est liée
à la divergence de Kullback-Leibler et que ses propriétés de convexité
en font un outil puissant pour l’estimation des paramètres et l’analyse
des données. Les applications de la divergence square loss sont vastes
et incluent l’apprentissage automatique, la statistique bayésienne et la
détection d’anomalies.</p>
</body>
</html>
{% include "footer.html" %}


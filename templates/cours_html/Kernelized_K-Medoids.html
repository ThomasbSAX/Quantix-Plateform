{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized K-Medoids: Une Approche Avancée pour le Clustering</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized K-Medoids: Une Approche Avancée pour le
Clustering</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le clustering est une tâche fondamentale en apprentissage
automatique, visant à regrouper des données similaires. Parmi les
méthodes de clustering, K-Medoids est une approche robuste et
interprétable, particulièrement adaptée aux données avec des bruits ou
des valeurs aberrantes. Cependant, K-Medoids présente une limite majeure
: sa capacité à capturer des relations complexes et non linéaires entre
les données est restreinte.</p>
<p>Pour surmonter cette limitation, nous introduisons le Kernelized
K-Medoids, une extension de l’algorithme K-Medoids qui exploite les
fonctions noyaux pour projeter les données dans un espace de
caractéristiques de dimension supérieure. Cette projection permet de
révéler des structures latentes et des relations non linéaires,
améliorant ainsi la qualité du clustering.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir le Kernelized K-Medoids, il est essentiel de
comprendre les concepts sous-jacents. Supposons que nous ayons un
ensemble de données <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> où chaque <span class="math inline">\(x_i \in
\mathbb{R}^d\)</span>. Notre objectif est de regrouper ces données en
<span class="math inline">\(k\)</span> clusters, où chaque cluster est
représenté par un medoid.</p>
<h2 class="unnumbered" id="fonction-noyau">Fonction Noyau</h2>
<p>Une fonction noyau <span class="math inline">\(\kappa : \mathbb{R}^d
\times \mathbb{R}^d \to \mathbb{R}\)</span> est une fonction symétrique
et positive définie, qui mesure la similarité entre deux points.
Formellement, une fonction noyau satisfait les conditions suivantes
:</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(\kappa(x_i, x_j) =
\kappa(x_j, x_i)\)</span> pour tout <span class="math inline">\(i,
j\)</span>.</p></li>
<li><p>Positive définie : Pour toute combinaison linéaire <span
class="math inline">\(\sum_{i=1}^n \alpha_i x_i = 0\)</span>, nous avons
<span class="math inline">\(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j
\kappa(x_i, x_j) \geq 0\)</span>.</p></li>
</ol>
<p>Un exemple courant de fonction noyau est le noyau gaussien (RBF) :
<span class="math display">\[\kappa(x_i, x_j) = \exp\left(-\frac{\|x_i -
x_j\|^2}{2\sigma^2}\right)\]</span></p>
<h2 class="unnumbered" id="kernelized-k-medoids">Kernelized
K-Medoids</h2>
<p>Le Kernelized K-Medoids étend l’algorithme K-Medoids en utilisant une
fonction noyau pour mesurer la similarité entre les points. L’algorithme
cherche à minimiser la somme des distances au carré dans l’espace de
caractéristiques induit par le noyau.</p>
<p>Formellement, nous cherchons à minimiser : <span
class="math display">\[\arg\min_{M} \sum_{i=1}^n \min_{m \in M} \|
\phi(x_i) - \phi(m) \|^2\]</span> où <span class="math inline">\(M =
\{m_1, m_2, \dots, m_k\}\)</span> est l’ensemble des medoids et <span
class="math inline">\(\phi : \mathbb{R}^d \to \mathcal{H}\)</span> est
la fonction de projection dans l’espace de caractéristiques.</p>
<p>Grâce à la propriété du noyau, nous pouvons exprimer cette distance
en termes de la fonction noyau : <span class="math display">\[\|
\phi(x_i) - \phi(m) \|^2 = \kappa(x_i, x_i) + \kappa(m, m) -
2\kappa(x_i, m)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="théorème-de-la-minimisation-des-coûts">Théorème de la Minimisation
des Coûts</h2>
<p>Le Kernelized K-Medoids minimise la somme des distances au carré dans
l’espace de caractéristiques induit par le noyau.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données et
<span class="math inline">\(\kappa\)</span> une fonction noyau.
L’algorithme Kernelized K-Medoids trouve un ensemble de medoids <span
class="math inline">\(M\)</span> qui minimise la somme des distances au
carré dans l’espace de caractéristiques : <span
class="math display">\[\arg\min_{M} \sum_{i=1}^n \min_{m \in M} \|
\phi(x_i) - \phi(m) \|^2\]</span></p>
</div>
<h2 class="unnumbered" id="preuve-du-théorème">Preuve du Théorème</h2>
<p>La preuve repose sur les propriétés des fonctions noyaux et
l’algorithme K-Medoids classique.</p>
<div class="proof">
<p><em>Proof.</em> Nous savons que pour toute fonction noyau <span
class="math inline">\(\kappa\)</span>, il existe un espace de
caractéristiques <span class="math inline">\(\mathcal{H}\)</span> et une
fonction de projection <span class="math inline">\(\phi\)</span> telle
que : <span class="math display">\[\kappa(x_i, x_j) = \langle \phi(x_i),
\phi(x_j) \rangle\]</span></p>
<p>La distance au carré dans l’espace de caractéristiques peut être
exprimée comme : <span class="math display">\[\| \phi(x_i) - \phi(m)
\|^2 = \kappa(x_i, x_i) + \kappa(m, m) - 2\kappa(x_i, m)\]</span></p>
<p>En minimisant cette expression pour chaque point <span
class="math inline">\(x_i\)</span>, nous obtenons l’algorithme
Kernelized K-Medoids. La minimisation globale est assurée par la
convergence de l’algorithme K-Medoids classique dans l’espace de
caractéristiques. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-1-invariance-par-translation">Propriété 1: Invariance par
Translation</h2>
<p>Le Kernelized K-Medoids est invariant par translation dans l’espace
de caractéristiques.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(\phi(x_i) + c\)</span> une
translation de la projection dans l’espace de caractéristiques. La
minimisation des distances au carré reste inchangée : <span
class="math display">\[\sum_{i=1}^n \min_{m \in M} \| (\phi(x_i) + c) -
(\phi(m) + c) \|^2 = \sum_{i=1}^n \min_{m \in M} \| \phi(x_i) - \phi(m)
\|^2\]</span></p>
</div>
<h2 class="unnumbered" id="preuve-de-la-propriété-1">Preuve de la
Propriété 1</h2>
<p>La preuve est immédiate en développant l’expression des distances au
carré.</p>
<div class="proof">
<p><em>Proof.</em> <span class="math display">\[\| (\phi(x_i) + c) -
(\phi(m) + c) \|^2 = \| \phi(x_i) - \phi(m) \|^2\]</span> Ainsi, la
somme des distances au carré reste inchangée sous une translation. ◻</p>
</div>
<h2 class="unnumbered"
id="propriété-2-robustesse-aux-valeurs-aberrantes">Propriété 2:
Robustesse aux Valeurs Aberrantes</h2>
<p>Le Kernelized K-Medoids est robuste aux valeurs aberrantes grâce à
l’utilisation des medoids.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(x_i\)</span> une valeur aberrante
dans l’ensemble de données. L’algorithme Kernelized K-Medoids minimise
l’impact de <span class="math inline">\(x_i\)</span> sur la somme des
distances au carré en choisissant un medoid proche dans l’espace de
caractéristiques.</p>
</div>
<h2 class="unnumbered" id="preuve-de-la-propriété-2">Preuve de la
Propriété 2</h2>
<p>La preuve repose sur le fait que les medoids sont des points réels de
l’ensemble de données.</p>
<div class="proof">
<p><em>Proof.</em> Puisque les medoids sont choisis parmi les points de
l’ensemble de données, une valeur aberrante <span
class="math inline">\(x_i\)</span> ne peut pas être un medoid. Par
conséquent, son impact sur la somme des distances au carré est minimisé
en choisissant un medoid proche dans l’espace de caractéristiques. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le Kernelized K-Medoids est une extension puissante de l’algorithme
K-Medoids, permettant de capturer des relations non linéaires entre les
données grâce à l’utilisation des fonctions noyaux. Cette approche
améliore la qualité du clustering et offre une robustesse accrue aux
valeurs aberrantes. Les propriétés théoriques et les preuves détaillées
fournies dans cet article soulignent la validité et l’efficacité de
cette méthode.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>XGBoost : Un Système de Gradient Boosting Optimisé</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">XGBoost : Un Système de Gradient Boosting
Optimisé</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage automatique a connu une révolution ces dernières
années, notamment avec l’émergence de modèles capables de traiter des
données complexes et de grandes dimensions. Parmi ces modèles, les
méthodes d’ensemble, telles que le boosting, ont démontré une efficacité
remarquable. Le gradient boosting, en particulier, est une technique
puissante qui combine plusieurs modèles faibles pour créer un modèle
fort. XGBoost (eXtreme Gradient Boosting) est une implémentation
optimisée de cette méthode, conçue pour être rapide et performante même
sur des ensembles de données massifs.</p>
<p>L’origine du boosting remonte aux travaux de Robert Schapire et Yoav
Freund dans les années 1990, avec l’algorithme AdaBoost. Le gradient
boosting, quant à lui, a été introduit par Jerome Friedman en 2001.
XGBoost, développé par Tianqi Chen et ses collaborateurs, est une
évolution moderne qui apporte des optimisations significatives en termes
de vitesse et d’efficacité.</p>
<p>XGBoost est indispensable dans les compétitions de data science,
comme celles organisées sur Kaggle, où il a remporté de nombreux prix.
Son succès tient à sa capacité à gérer les données manquantes, à
régulariser le modèle pour éviter le surapprentissage, et à parallèle
ses calculs pour une exécution rapide.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre XGBoost, il est essentiel de définir quelques
concepts clés.</p>
<h2 class="unnumbered" id="arbres-de-décision">Arbres de décision</h2>
<p>Un arbre de décision est un modèle qui utilise une structure
arborescente pour prendre des décisions basées sur des caractéristiques
d’entrée. Chaque nœud interne représente une condition sur une
caractéristique, chaque branche représente le résultat de la condition,
et chaque nœud feuille représente une décision finale.</p>
<p>Formellement, un arbre de décision <span
class="math inline">\(T\)</span> peut être défini comme suit : <span
class="math display">\[T(x) = w_{q(x)}\]</span> où <span
class="math inline">\(q(x)\)</span> est la feuille à laquelle l’instance
<span class="math inline">\(x\)</span> appartient, et <span
class="math inline">\(w_q\)</span> est le score associé à la feuille
<span class="math inline">\(q\)</span>.</p>
<h2 class="unnumbered" id="gradient-boosting">Gradient Boosting</h2>
<p>Le gradient boosting est une technique d’ensemble qui construit un
modèle en ajoutant des arbres de décision de manière itérative. À chaque
itération, un nouvel arbre est ajouté pour corriger les erreurs des
modèles précédents.</p>
<p>Formellement, le gradient boosting peut être défini comme suit :
<span class="math display">\[F_t(x) = F_{t-1}(x) + \nu \cdot
h_t(x)\]</span> où <span class="math inline">\(F_t(x)\)</span> est le
modèle à l’itération <span class="math inline">\(t\)</span>, <span
class="math inline">\(\nu\)</span> est un facteur d’apprentissage, et
<span class="math inline">\(h_t(x)\)</span> est l’arbre ajouté à cette
itération.</p>
<h2 class="unnumbered" id="xgboost">XGBoost</h2>
<p>XGBoost est une implémentation optimisée du gradient boosting qui
utilise des techniques avancées pour améliorer la performance et
l’efficacité.</p>
<p>Formellement, XGBoost minimise la fonction de coût suivante : <span
class="math display">\[\mathcal{L}(\theta) = \sum_{i} l(y_i, \hat{y}_i)
+ \sum_{k} \Omega(f_k)\]</span> où <span class="math inline">\(l(y_i,
\hat{y}_i)\)</span> est la fonction de perte entre la vraie valeur <span
class="math inline">\(y_i\)</span> et la prédiction <span
class="math inline">\(\hat{y}_i\)</span>, et <span
class="math inline">\(\Omega(f_k)\)</span> est une régularisation
appliquée à l’arbre <span class="math inline">\(f_k\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered" id="théorème-de-ladditivité-des-arbres">Théorème
de l’additivité des arbres</h2>
<p>Le gradient boosting repose sur le théorème suivant, qui montre que
l’ajout itératif d’arbres peut améliorer les performances du modèle.</p>
<h4 class="unnumbered" id="théorème">Théorème :</h4>
<p>Soit <span class="math inline">\(F_t(x) = F_{t-1}(x) + \nu \cdot
h_t(x)\)</span>, où <span class="math inline">\(F_{t-1}(x)\)</span> est
le modèle à l’itération précédente, <span
class="math inline">\(\nu\)</span> est un facteur d’apprentissage, et
<span class="math inline">\(h_t(x)\)</span> est l’arbre ajouté à cette
itération. Alors, si <span class="math inline">\(h_t(x)\)</span> est
choisi pour minimiser la fonction de coût, le modèle <span
class="math inline">\(F_t(x)\)</span> sera meilleur que <span
class="math inline">\(F_{t-1}(x)\)</span>.</p>
<h4 class="unnumbered" id="preuve">Preuve :</h4>
<p>La preuve repose sur l’analyse de la fonction de coût et montre que
chaque nouvel arbre réduit cette fonction.</p>
<h2 class="unnumbered" id="théorème-de-la-régularisation">Théorème de la
régularisation</h2>
<p>La régularisation est une technique clé pour éviter le
surapprentissage. XGBoost utilise une régularisation <span
class="math inline">\(\Omega(f_k)\)</span> pour contrôler la complexité
des arbres.</p>
<h4 class="unnumbered" id="théorème-1">Théorème :</h4>
<p>Soit <span class="math inline">\(\Omega(f_k) = \gamma T + \frac{1}{2}
\lambda \sum_{j=1}^T w_j^2\)</span>, où <span
class="math inline">\(T\)</span> est le nombre de feuilles, <span
class="math inline">\(\gamma\)</span> et <span
class="math inline">\(\lambda\)</span> sont des paramètres de
régularisation, et <span class="math inline">\(w_j\)</span> est le score
associé à la feuille <span class="math inline">\(j\)</span>. Alors, la
régularisation réduit le risque de surapprentissage.</p>
<h4 class="unnumbered" id="preuve-1">Preuve :</h4>
<p>La preuve montre que la régularisation pénalisera les arbres trop
complexes, encourageant ainsi des modèles plus simples et
généralisables.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-de-loptimisation-distribuée">Propriété de l’optimisation
distribuée</h2>
<p>XGBoost utilise des techniques d’optimisation distribuée pour
paralléliser les calculs et améliorer la vitesse d’exécution.</p>
<h4 class="unnumbered" id="propriété">Propriété :</h4>
<p>XGBoost peut être exécuté sur des clusters de machines pour traiter
des ensembles de données massifs.</p>
<h4 class="unnumbered" id="preuve-2">Preuve :</h4>
<p>La preuve repose sur l’analyse des algorithmes de parallélisation et
montre que XGBoost peut diviser le travail entre plusieurs nœuds pour
accélérer les calculs.</p>
<h2 class="unnumbered"
id="propriété-de-la-gestion-des-données-manquantes">Propriété de la
gestion des données manquantes</h2>
<p>XGBoost gère automatiquement les données manquantes, ce qui est une
caractéristique importante pour les ensembles de données réels.</p>
<h4 class="unnumbered" id="propriété-1">Propriété :</h4>
<p>XGBoost peut traiter les valeurs manquantes en les considérant comme
une catégorie distincte ou en les imputant de manière intelligente.</p>
<h4 class="unnumbered" id="preuve-3">Preuve :</h4>
<p>La preuve montre que XGBoost peut apprendre à partir des données
manquantes et améliorer les performances du modèle.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>XGBoost est une méthode puissante et efficace pour le gradient
boosting. Ses optimisations en termes de vitesse, d’efficacité et de
gestion des données manquantes en font un outil indispensable pour les
compétitions de data science et les applications industrielles. En
comprenant ses fondements théoriques et ses propriétés, nous pouvons
mieux l’appliquer pour résoudre des problèmes complexes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Bhattacharyya : Une mesure fondamentale en théorie de l’information et apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Bhattacharyya : Une mesure fondamentale en
théorie de l’information et apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Bhattacharyya émerge dans le paysage mathématique
comme une réponse élégante à un besoin fondamental : mesurer la
similarité entre deux distributions de probabilité. Introduite par le
statisticien indien Anil Kumar Bhattacharyya en 1943, cette notion
trouve ses racines dans la théorie des probabilités et l’analyse
statistique. Son importance réside dans sa capacité à quantifier la
divergence entre deux lois de probabilité, un problème central en
théorie de l’information, en apprentissage automatique et en
reconnaissance des formes.</p>
<p>L’émergence de cette distance est motivée par la nécessité de
disposer d’un outil robuste pour comparer des distributions, notamment
dans des contextes où les données sont bruitées ou incomplètes. Elle se
distingue par sa propriété de symétrie et son interprétation
géométrique, offrant ainsi une alternative aux mesures classiques comme
la divergence de Kullback-Leibler.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la distance de Bhattacharyya, commençons par
comprendre ce que nous cherchons à mesurer. Imaginons deux distributions
de probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace commun. Nous
voulons quantifier à quel point ces deux distributions sont proches
l’une de l’autre. Intuitivement, plus les distributions se chevauchent,
plus elles sont similaires.</p>
<p>Formellement, la distance de Bhattacharyya entre deux distributions
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes ou continues définies sur un espace mesurable <span
class="math inline">\(\Omega\)</span>. La distance de Bhattacharyya
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par :</p>
<p><span class="math display">\[BC(P, Q) = -\ln \left( \sum_{x \in
\Omega} \sqrt{P(x) Q(x)} \right)\]</span></p>
<p>pour des distributions discrètes, ou</p>
<p><span class="math display">\[BC(P, Q) = -\ln \left( \int_{\Omega}
\sqrt{P(x) Q(x)} \, dx \right)\]</span></p>
<p>pour des distributions continues.</p>
</div>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[BC(P, Q) = -\ln \left( \mathbb{E}_P
\left[ \sqrt{\frac{Q(X)}{P(X)}} \right] \right)\]</span></p>
<p>où <span class="math inline">\(X\)</span> est une variable aléatoire
suivant la loi <span class="math inline">\(P\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Bhattacharyya est le
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité.
Alors, pour toute fonction <span class="math inline">\(f\)</span> bornée
et mesurable,</p>
<p><span class="math display">\[\left| \mathbb{E}_P[f] - \mathbb{E}_Q[f]
\right| \leq 2 \|f\|_{\infty} \sqrt{1 - e^{-BC(P, Q)}}\]</span></p>
<p>où <span class="math inline">\(\|f\|_{\infty}\)</span> désigne la
norme infinie de <span class="math inline">\(f\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur l’inégalité de
Cauchy-Schwarz et les propriétés de la distance de Bhattacharyya.
Considérons d’abord l’espérance sous <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> :</p>
<p><span class="math display">\[\mathbb{E}_P[f] = \int_{\Omega} f(x)
P(x) \, dx\]</span></p>
<p><span class="math display">\[\mathbb{E}_Q[f] = \int_{\Omega} f(x)
Q(x) \, dx\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons :</p>
<p><span class="math display">\[\left| \mathbb{E}_P[f] - \mathbb{E}_Q[f]
\right| = \left| \int_{\Omega} f(x) (P(x) - Q(x)) \, dx \right| \leq
\|f\|_{\infty} \int_{\Omega} |P(x) - Q(x)| \, dx\]</span></p>
<p>Il reste à montrer que :</p>
<p><span class="math display">\[\int_{\Omega} |P(x) - Q(x)| \, dx \leq 2
\sqrt{1 - e^{-BC(P, Q)}}\]</span></p>
<p>Ce résultat découle directement de la définition de la distance de
Bhattacharyya et des propriétés des normes <span
class="math inline">\(L^1\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la distance de Bhattacharyya,
considérons un exemple simple. Supposons que <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient deux distributions gaussiennes
unidimensionnelles avec les mêmes variances mais des moyennes
différentes. Nous voulons calculer la distance de Bhattacharyya entre
ces deux distributions.</p>
<div class="proof">
<p><em>Proof.</em> Soient <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> deux gaussiennes de moyennes
<span class="math inline">\(\mu_1\)</span> et <span
class="math inline">\(\mu_2\)</span> respectivement, et de même variance
<span class="math inline">\(\sigma^2\)</span>. La densité de probabilité
de <span class="math inline">\(P\)</span> est :</p>
<p><span class="math display">\[P(x) = \frac{1}{\sqrt{2\pi\sigma^2}}
e^{-\frac{(x - \mu_1)^2}{2\sigma^2}}\]</span></p>
<p>et celle de <span class="math inline">\(Q\)</span> est :</p>
<p><span class="math display">\[Q(x) = \frac{1}{\sqrt{2\pi\sigma^2}}
e^{-\frac{(x - \mu_2)^2}{2\sigma^2}}\]</span></p>
<p>La distance de Bhattacharyya entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est alors :</p>
<p><span class="math display">\[BC(P, Q) = -\ln \left(
\int_{-\infty}^{+\infty} \sqrt{P(x) Q(x)} \, dx \right)\]</span></p>
<p>Calculons l’intégrande :</p>
<p><span class="math display">\[\sqrt{P(x) Q(x)} =
\frac{1}{2\pi\sigma^2} e^{-\frac{(x - \mu_1)^2 + (x -
\mu_2)^2}{4\sigma^2}} = \frac{1}{2\pi\sigma^2} e^{-\frac{(x - \mu_1)^2 +
(x - \mu_2)^2}{4\sigma^2}}\]</span></p>
<p>En simplifiant l’exposant, nous obtenons :</p>
<p><span class="math display">\[(x - \mu_1)^2 + (x - \mu_2)^2 = 2x^2 -
2x(\mu_1 + \mu_2) + \mu_1^2 + \mu_2^2\]</span></p>
<p>Ainsi,</p>
<p><span class="math display">\[\sqrt{P(x) Q(x)} =
\frac{1}{2\pi\sigma^2} e^{-\frac{2x^2 - 2x(\mu_1 + \mu_2) + \mu_1^2 +
\mu_2^2}{4\sigma^2}}\]</span></p>
<p>En complétant le carré, nous trouvons :</p>
<p><span class="math display">\[\sqrt{P(x) Q(x)} =
\frac{1}{2\pi\sigma^2} e^{-\frac{(x - \frac{\mu_1 +
\mu_2}{2})^2}{2\sigma^2}} e^{-\frac{(\mu_1 -
\mu_2)^2}{8\sigma^2}}\]</span></p>
<p>L’intégrale devient alors :</p>
<p><span class="math display">\[\int_{-\infty}^{+\infty} \sqrt{P(x)
Q(x)} \, dx = e^{-\frac{(\mu_1 - \mu_2)^2}{8\sigma^2}}
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x -
\frac{\mu_1 + \mu_2}{2})^2}{2\sigma^2}} \, dx = e^{-\frac{(\mu_1 -
\mu_2)^2}{8\sigma^2}}\]</span></p>
<p>Par conséquent, la distance de Bhattacharyya est :</p>
<p><span class="math display">\[BC(P, Q) = \frac{(\mu_1 -
\mu_2)^2}{8\sigma^2}\]</span></p>
<p>Ce résultat montre que la distance de Bhattacharyya entre deux
gaussiennes est proportionnelle à la différence des moyennes au carré,
divisée par huit fois la variance. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance de Bhattacharyya possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(BC(P, Q) = BC(Q,
P)\)</span>.</p></li>
<li><p>Non-négativité : <span class="math inline">\(BC(P, Q) \geq
0\)</span>, avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span>.</p></li>
<li><p>Invariance par transformation : Si <span
class="math inline">\(T\)</span> est une transformation bijective et
mesurable, alors <span class="math inline">\(BC(P \circ T^{-1}, Q \circ
T^{-1}) = BC(P, Q)\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> La preuve de la symétrie est immédiate à partir de la
définition. Pour la non-négativité, nous utilisons l’inégalité de Jensen
et le fait que <span class="math inline">\(\ln(x) \leq x - 1\)</span>.
Enfin, l’invariance par transformation découle du changement de variable
dans l’intégrale définissant la distance. ◻</p>
</div>
<p>En conclusion, la distance de Bhattacharyya est un outil puissant et
élégant pour comparer des distributions de probabilité. Ses propriétés
mathématiques et ses applications pratiques en font un sujet de
recherche riche et fascinant.</p>
</body>
</html>
{% include "footer.html" %}


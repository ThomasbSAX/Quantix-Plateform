{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Complexité des problèmes de compression</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Complexité des problèmes de compression</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La compression de données est un domaine fondamental en informatique
théorique et appliquée. Son origine remonte aux travaux pionniers de
Claude Shannon dans les années 1940, posant les bases de la théorie de
l’information. La notion de complexité des problèmes de compression
émerge naturellement avec le besoin croissant de stocker et transmettre
des données de manière efficace.</p>
<p>La compression résout deux problèmes majeurs : la réduction de
l’espace de stockage et l’optimisation des temps de transmission. Dans
un monde où les données sont omniprésentes, ces enjeux deviennent
indispensables pour le traitement efficace de l’information. Les
algorithmes de compression sont utilisés dans une multitude
d’applications, allant des systèmes de stockage aux communications
numériques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la complexité des problèmes de compression, il est
essentiel de définir quelques concepts clés.</p>
<h2 id="entropie-de-shannon">Entropie de Shannon</h2>
<p>L’entropie de Shannon mesure l’incertitude ou la quantité
d’information contenue dans un message. Pédagogiquement, imaginons que
nous ayons une source d’information produisant des symboles. Plus ces
symboles sont aléatoires, plus l’entropie est élevée.</p>
<p>Formellement, pour une source discrète <span
class="math inline">\(X\)</span> prenant ses valeurs dans un ensemble
fini <span class="math inline">\(\mathcal{X}\)</span>, l’entropie de
Shannon est définie comme :</p>
<p><span class="math display">\[H(X) = -\sum_{x \in \mathcal{X}} p(x)
\log_2 p(x)\]</span></p>
<p>où <span class="math inline">\(p(x)\)</span> est la probabilité du
symbole <span class="math inline">\(x\)</span>.</p>
<h2 id="complexité-de-kolmogorov">Complexité de Kolmogorov</h2>
<p>La complexité de Kolmogorov mesure la longueur minimale d’un
programme capable de produire une chaîne donnée. Intuitivement, plus une
chaîne est complexe, plus le programme nécessaire pour la générer sera
long.</p>
<p>Formellement, pour une chaîne <span class="math inline">\(s \in
\{0,1\}^*\)</span>, la complexité de Kolmogorov est définie comme :</p>
<p><span class="math display">\[K(s) = \min_{p \in P} \{ |p| : U(p) = s
\}\]</span></p>
<p>où <span class="math inline">\(P\)</span> est l’ensemble des
programmes, <span class="math inline">\(|p|\)</span> est la longueur du
programme <span class="math inline">\(p\)</span>, et <span
class="math inline">\(U\)</span> est une machine de Turing
universelle.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-dentropie-sans-perte">Théorème d’entropie sans
perte</h2>
<p>Le théorème d’entropie sans perte de Shannon établit une limite
inférieure sur la longueur moyenne des codes sans perte.</p>
<p>Pédagogiquement, ce théorème nous dit que l’entropie d’une source est
la longueur moyenne minimale des codes binaires sans perte.</p>
<p>Formellement, pour une source discrète <span
class="math inline">\(X\)</span> avec entropie <span
class="math inline">\(H(X)\)</span>, il existe un code sans perte tel
que :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] \geq H(X)\]</span></p>
<p>où <span class="math inline">\(L(X)\)</span> est la longueur du code
pour le symbole <span class="math inline">\(X\)</span>.</p>
<h2 id="démonstration">Démonstration</h2>
<p>Pour démontrer ce théorème, nous utilisons le principe de l’inégalité
d’entropie et les propriétés des codes préfixes.</p>
<p>1. Considérons un code préfixe <span class="math inline">\(C\)</span>
pour la source <span class="math inline">\(X\)</span>. 2. La longueur
moyenne du code est donnée par :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] = \sum_{x \in
\mathcal{X}} p(x) L(x)\]</span></p>
<p>3. Par l’inégalité de Kraft, nous avons :</p>
<p><span class="math display">\[\sum_{x \in \mathcal{X}} 2^{-L(x)} \leq
1\]</span></p>
<p>4. En utilisant l’inégalité de Jensen, nous obtenons :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] \geq H(X)\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-dentropie-sans-perte">Preuve du théorème
d’entropie sans perte</h2>
<p>Pour prouver le théorème d’entropie sans perte, nous devons montrer
que la longueur moyenne des codes ne peut pas être inférieure à
l’entropie de la source.</p>
<p>1. Considérons un code préfixe <span class="math inline">\(C\)</span>
pour la source <span class="math inline">\(X\)</span>. 2. La longueur
moyenne du code est donnée par :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] = \sum_{x \in
\mathcal{X}} p(x) L(x)\]</span></p>
<p>3. Par l’inégalité de Kraft, nous avons :</p>
<p><span class="math display">\[\sum_{x \in \mathcal{X}} 2^{-L(x)} \leq
1\]</span></p>
<p>4. En utilisant l’inégalité de Jensen, nous obtenons :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] \geq H(X)\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-1-inégalité-de-kraft">Propriété 1 : Inégalité de
Kraft</h2>
<p>Pour un code préfixe, la somme des inverses des longueurs des codes
doit être inférieure ou égale à 1.</p>
<p>Formellement, pour un code préfixe <span
class="math inline">\(C\)</span> avec longueurs de codes <span
class="math inline">\(L(x)\)</span>, nous avons :</p>
<p><span class="math display">\[\sum_{x \in \mathcal{X}} 2^{-L(x)} \leq
1\]</span></p>
<h2 id="preuve-de-la-propriété-1">Preuve de la propriété 1</h2>
<p>Pour prouver cette propriété, nous utilisons le principe des arbres
binaires.</p>
<p>1. Considérons un arbre binaire où chaque chemin correspond à un
code. 2. La somme des probabilités des feuilles doit être égale à 1. 3.
En utilisant l’inégalité de Kraft, nous obtenons :</p>
<p><span class="math display">\[\sum_{x \in \mathcal{X}} 2^{-L(x)} \leq
1\]</span></p>
<h2 id="corollaire-1-code-de-huffman">Corollaire 1 : Code de
Huffman</h2>
<p>Le code de Huffman atteint la borne inférieure donnée par le théorème
d’entropie sans perte.</p>
<p>Formellement, pour une source discrète <span
class="math inline">\(X\)</span>, le code de Huffman satisfait :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] = H(X)\]</span></p>
<h2 id="preuve-du-corollaire-1">Preuve du corollaire 1</h2>
<p>Pour prouver ce corollaire, nous utilisons les propriétés des codes
de Huffman.</p>
<p>1. Le code de Huffman est un code préfixe optimal. 2. Par
construction, il minimise la longueur moyenne des codes. 3. En utilisant
le théorème d’entropie sans perte, nous obtenons :</p>
<p><span class="math display">\[\mathbb{E}[L(X)] = H(X)\]</span></p>
</body>
</html>
{% include "footer.html" %}


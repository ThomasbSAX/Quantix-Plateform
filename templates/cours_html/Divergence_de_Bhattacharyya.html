{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Bhattacharyya</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Bhattacharyya</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Bhattacharyya émerge dans le cadre des statistiques
et de l’apprentissage automatique pour mesurer la dissimilarité entre
deux distributions de probabilité. Introduite par le statisticien indien
Anil Kumar Bhattacharyya en 1943, cette mesure est particulièrement
utile dans les problèmes de classification et d’estimation de densité.
Elle trouve ses racines dans la théorie de l’information, où elle est
utilisée pour évaluer la distance entre distributions.</p>
<p>La divergence de Bhattacharyya est indispensable dans les algorithmes
d’apprentissage supervisé et non supervisé, notamment pour l’estimation
de paramètres et la réduction de dimension. Elle est également utilisée
en bioinformatique pour comparer des profils d’expression génique et en
traitement du signal pour évaluer la similarité entre signaux.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Bhattacharyya, considérons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une mesure
qui quantifie à quel point ces deux distributions sont différentes.</p>
<p>La divergence de Bhattacharyya entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme suit:</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>. La
divergence de Bhattacharyya entre <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> est donnée par: <span
class="math display">\[D_B(P \| Q) = -\ln \left( \int_{\mathcal{X}}
\sqrt{p(x) q(x)} \, dx \right),\]</span> où <span
class="math inline">\(p(x)\)</span> et <span
class="math inline">\(q(x)\)</span> sont les densités de probabilité de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement.</p>
</div>
<p>Une autre formulation équivalente est: <span
class="math display">\[D_B(P \| Q) = -\ln \left( \mathbb{E}_P \left[
\sqrt{\frac{q(X)}{p(X)}} \right] \right),\]</span> où <span
class="math inline">\(X\)</span> est une variable aléatoire de
distribution <span class="math inline">\(P\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la divergence de Bhattacharyya est le
suivant:</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité.
Alors, la divergence de Bhattacharyya satisfait l’inégalité suivante:
<span class="math display">\[D_B(P \| Q) \leq \frac{1}{2} D_{KL}(P \|
Q),\]</span> où <span class="math inline">\(D_{KL}(P \| Q)\)</span> est
la divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Bhattacharyya, nous utilisons la
concavité de la fonction logarithme et l’inégalité de Jensen.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(f(x) = \ln(x)\)</span>, qui est concave. Par
l’inégalité de Jensen, nous avons: <span class="math display">\[\ln
\left( \mathbb{E}_P \left[ \sqrt{\frac{q(X)}{p(X)}} \right] \right) \geq
\mathbb{E}_P \left[ \ln \left( \sqrt{\frac{q(X)}{p(X)}} \right)
\right].\]</span></p>
<p>En simplifiant, nous obtenons: <span class="math display">\[-\ln
\left( \mathbb{E}_P \left[ \sqrt{\frac{q(X)}{p(X)}} \right] \right) \leq
-\mathbb{E}_P \left[ \ln \left( \sqrt{\frac{q(X)}{p(X)}} \right)
\right].\]</span></p>
<p>En utilisant la définition de la divergence de Kullback-Leibler, nous
avons: <span class="math display">\[D_{KL}(P \| Q) = \mathbb{E}_P \left[
\ln \left( \frac{p(X)}{q(X)} \right) \right].\]</span></p>
<p>En combinant ces résultats, nous obtenons: <span
class="math display">\[D_B(P \| Q) \leq \frac{1}{2} D_{KL}(P \|
Q).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Bhattacharyya possède plusieurs propriétés
intéressantes:</p>
<ol>
<li><p>Symétrie: <span class="math inline">\(D_B(P \| Q) = D_B(Q \|
P)\)</span>.</p></li>
<li><p>Non négativité: <span class="math inline">\(D_B(P \| Q) \geq
0\)</span>, avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span>.</p></li>
<li><p>Bornes: <span class="math inline">\(0 \leq D_B(P \| Q) \leq
\infty\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour prouver la symétrie, nous utilisons la
définition de la divergence de Bhattacharyya: <span
class="math display">\[D_B(P \| Q) = -\ln \left( \int_{\mathcal{X}}
\sqrt{p(x) q(x)} \, dx \right) = -\ln \left( \int_{\mathcal{X}}
\sqrt{q(x) p(x)} \, dx \right) = D_B(Q \| P).\]</span></p>
<p>Pour la non négativité, nous utilisons l’inégalité de Cauchy-Schwarz:
<span class="math display">\[\int_{\mathcal{X}} \sqrt{p(x) q(x)} \, dx
\leq \left( \int_{\mathcal{X}} p(x) \, dx \right)^{1/2} \left(
\int_{\mathcal{X}} q(x) \, dx \right)^{1/2} = 1.\]</span></p>
<p>En prenant le logarithme, nous obtenons: <span
class="math display">\[D_B(P \| Q) \geq 0.\]</span></p>
<p>L’égalité a lieu si et seulement si <span class="math inline">\(p(x)
= q(x)\)</span> presque partout, c’est-à-dire si <span
class="math inline">\(P = Q\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Bhattacharyya est une mesure puissante et flexible
pour évaluer la dissimilarité entre distributions de probabilité. Ses
propriétés mathématiques et son interprétation intuitive en font un
outil précieux dans de nombreux domaines, notamment les statistiques,
l’apprentissage automatique et la bioinformatique. Les théorèmes et
propriétés présentés dans cet article illustrent son importance et sa
polyvalence.</p>
</body>
</html>
{% include "footer.html" %}


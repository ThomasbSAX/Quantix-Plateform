{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Feature Expansion: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Feature Expansion: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’expansion de caractéristiques, ou <em>feature expansion</em>, est
une technique fondamentale en apprentissage automatique et en
statistique. Elle émerge de la nécessité de transformer des données
brutes en représentations plus informatives, permettant ainsi aux
modèles d’apprendre des motifs complexes. Historiquement, cette approche
a été motivée par le besoin de linéariser des problèmes non linéaires,
rendant ainsi applicables des algorithmes linéaires simples mais
efficaces.</p>
<p>L’expansion de caractéristiques est indispensable dans divers
domaines, notamment la régression et la classification. Elle permet de
capturer des interactions non linéaires entre les variables, d’améliorer
la généralisation des modèles et de rendre les données plus adaptées à
l’analyse. Par exemple, dans le cas de la régression polynomiale,
l’expansion de caractéristiques permet de modéliser des relations non
linéaires entre la variable dépendante et les variables
indépendantes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’expansion de caractéristiques, considérons d’abord
un ensemble de données <span class="math inline">\(\mathcal{D} = \{(x_1,
y_1), (x_2, y_2), \ldots, (x_n, y_n)\}\)</span>, où chaque <span
class="math inline">\(x_i\)</span> est un vecteur de caractéristiques de
dimension <span class="math inline">\(d\)</span> et <span
class="math inline">\(y_i\)</span> est la variable cible. L’objectif est
de transformer chaque vecteur <span class="math inline">\(x_i\)</span>
en un nouveau vecteur <span class="math inline">\(\phi(x_i)\)</span> de
dimension <span class="math inline">\(k \geq d\)</span>, où <span
class="math inline">\(k\)</span> peut être beaucoup plus grand que <span
class="math inline">\(d\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathbb{R}^k\)</span> une fonction de transformation. L’expansion de
caractéristiques est définie comme la transformation de chaque vecteur
<span class="math inline">\(x_i \in \mathbb{R}^d\)</span> en un nouveau
vecteur <span class="math inline">\(\phi(x_i) \in \mathbb{R}^k\)</span>,
où <span class="math inline">\(\phi\)</span> est une fonction choisie
pour capturer des motifs complexes dans les données.</p>
<p>Formellement, pour tout <span class="math inline">\(x \in
\mathbb{R}^d\)</span>, on a: <span class="math display">\[\phi(x) =
(\phi_1(x), \phi_2(x), \ldots, \phi_k(x))^T\]</span> où chaque <span
class="math inline">\(\phi_j: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> est une fonction de base.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en expansion de caractéristiques est le
théorème de représentation de kernel, qui montre comment une
transformation non linéaire peut être implicitement représentée par un
produit scalaire dans un espace de caractéristiques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble et
<span class="math inline">\(k: \mathcal{X} \times \mathcal{X}
\rightarrow \mathbb{R}\)</span> une fonction symétrique et positive
définie. Alors, il existe un espace de Hilbert <span
class="math inline">\(\mathcal{H}\)</span> et une fonction <span
class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span>
telle que: <span class="math display">\[k(x, x&#39;) = \langle \phi(x),
\phi(x&#39;) \rangle_{\mathcal{H}} \quad \forall x, x&#39; \in
\mathcal{X}\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de représentation de kernel, nous suivons
les étapes suivantes:</p>
<p>1. **Construction de l’espace de Hilbert**: Considérons l’ensemble de
toutes les combinaisons linéaires finies de fonctions de la forme <span
class="math inline">\(k(x, \cdot)\)</span>. Cet ensemble forme un espace
vectoriel préhilbertien avec le produit scalaire défini par: <span
class="math display">\[\langle k(x, \cdot), k(x&#39;, \cdot) \rangle =
k(x, x&#39;)\]</span></p>
<p>2. **Complétion de l’espace**: En complétant cet espace
préhilbertien, nous obtenons un espace de Hilbert <span
class="math inline">\(\mathcal{H}\)</span>.</p>
<p>3. **Définition de la fonction <span
class="math inline">\(\phi\)</span>**: Pour chaque <span
class="math inline">\(x \in \mathcal{X}\)</span>, définissons <span
class="math inline">\(\phi(x) = k(x, \cdot)\)</span>. Cette fonction
mappe <span class="math inline">\(x\)</span> à un élément de <span
class="math inline">\(\mathcal{H}\)</span>.</p>
<p>4. **Vérification du produit scalaire**: Pour tout <span
class="math inline">\(x, x&#39; \in \mathcal{X}\)</span>, nous avons:
<span class="math display">\[\langle \phi(x), \phi(x&#39;)
\rangle_{\mathcal{H}} = \langle k(x, \cdot), k(x&#39;, \cdot)
\rangle_{\mathcal{H}} = k(x, x&#39;)\]</span> ce qui achève la
preuve.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’expansion de caractéristiques possède plusieurs propriétés
importantes:</p>
<ol>
<li><p>**Réduction de la dimension**: Dans certains cas, l’expansion de
caractéristiques peut réduire la dimension effective des données en
éliminant les caractéristiques redondantes ou non informatives.</p></li>
<li><p>**Amélioration de la généralisation**: En capturant des motifs
complexes, l’expansion de caractéristiques peut améliorer la capacité de
généralisation des modèles.</p></li>
<li><p>**Linéarisation des problèmes non linéaires**: L’expansion de
caractéristiques permet de transformer des problèmes non linéaires en
problèmes linéaires, facilitant ainsi l’application d’algorithmes
linéaires.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’expansion de caractéristiques est une technique puissante et
polyvalente en apprentissage automatique. Elle permet de transformer des
données brutes en représentations plus informatives, capturant ainsi des
motifs complexes et améliorant la performance des modèles. Les théorèmes
et propriétés discutés dans cet article montrent l’importance
fondamentale de cette technique dans divers domaines de la science des
données.</p>
</body>
</html>
{% include "footer.html" %}


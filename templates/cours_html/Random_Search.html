{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Random Search : Une Exploration Stochastique des Espaces de Solutions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Random Search : Une Exploration Stochastique des
Espaces de Solutions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’optimisation est un domaine central en mathématiques appliquées, en
informatique et en ingénierie. Parmi les nombreuses méthodes
d’optimisation existantes, le <em>Random Search</em> (Recherche
Aléatoire) se distingue par sa simplicité et son efficacité dans
certains contextes. Cette méthode, bien que simple, trouve ses racines
dans les travaux de Brooks en 1958 et a été popularisée par l’ouvrage de
Matyas en 1965. Le Random Search est particulièrement utile lorsque les
fonctions objectives sont non convexes, non différentiables ou lorsqu’il
n’existe pas de méthodes analytiques pour trouver des solutions
optimales.</p>
<p>Le Random Search est motivé par la nécessité de traiter des problèmes
d’optimisation complexes où les méthodes déterministes échouent. En
effet, dans des espaces de solutions vastes et complexes, les méthodes
déterministes peuvent être piégées dans des optima locaux ou nécessiter
un temps de calcul prohibitif. Le Random Search, en explorant
aléatoirement l’espace des solutions, offre une alternative prometteuse
pour éviter ces pièges.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le Random Search, il est essentiel de définir
quelques notions clés. Considérons un problème d’optimisation générale
:</p>
<p><span class="math display">\[\min_{x \in S} f(x)\]</span></p>
<p>où <span class="math inline">\(S \subseteq \mathbb{R}^n\)</span> est
l’ensemble des solutions possibles et <span class="math inline">\(f: S
\rightarrow \mathbb{R}\)</span> est la fonction objective à
minimiser.</p>
<p>Le Random Search consiste à générer un ensemble de points <span
class="math inline">\(\{x_1, x_2, \ldots, x_N\}\)</span> dans l’espace
<span class="math inline">\(S\)</span> selon une distribution de
probabilité donnée. L’objectif est de trouver un point <span
class="math inline">\(x^* \in S\)</span> tel que :</p>
<p><span class="math display">\[f(x^*) = \min_{1 \leq i \leq N}
f(x_i)\]</span></p>
<p>Formellement, le Random Search peut être défini comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(S \subseteq \mathbb{R}^n\)</span> un
ensemble compact et <span class="math inline">\(f: S \rightarrow
\mathbb{R}\)</span> une fonction continue. Le Random Search est un
algorithme d’optimisation qui génère <span
class="math inline">\(N\)</span> points <span class="math inline">\(x_1,
x_2, \ldots, x_N\)</span> indépendamment et uniformément distribués dans
<span class="math inline">\(S\)</span>. Le point optimal trouvé par
l’algorithme est défini comme :</p>
<p><span class="math display">\[x^* = \arg\min_{1 \leq i \leq N}
f(x_i)\]</span></p>
</div>
<p>Une autre formulation du Random Search est la suivante :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mu\)</span> une mesure de
probabilité sur <span class="math inline">\(S\)</span>. Le Random Search
génère des points <span class="math inline">\(x_1, x_2, \ldots,
x_N\)</span> selon la distribution <span
class="math inline">\(\mu\)</span>. Le point optimal est alors :</p>
<p><span class="math display">\[x^* = \arg\min_{1 \leq i \leq N}
f(x_i)\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Le Random Search est fondé sur des résultats théoriques solides. L’un
des théorèmes les plus importants concernant le Random Search est celui
de la convergence presque sûre.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>
une fonction continue sur un ensemble compact <span
class="math inline">\(S \subseteq \mathbb{R}^n\)</span>. Soit <span
class="math inline">\(x^*\)</span> un point globalement optimal pour
<span class="math inline">\(f\)</span>, c’est-à-dire :</p>
<p><span class="math display">\[f(x^*) = \min_{x \in S}
f(x)\]</span></p>
<p>Alors, pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, la probabilité que le Random Search trouve un point <span
class="math inline">\(x_i\)</span> tel que <span
class="math inline">\(f(x_i) \leq f(x^*) + \epsilon\)</span> tend vers 1
lorsque <span class="math inline">\(N \rightarrow +\infty\)</span>.</p>
</div>
<p>La démonstration de ce théorème repose sur le fait que la mesure de
l’ensemble des points <span class="math inline">\(x \in S\)</span> tels
que <span class="math inline">\(f(x) \leq f(x^*) + \epsilon\)</span> est
strictement positive. Par conséquent, la probabilité de sélectionner un
tel point tend vers 1 lorsque le nombre d’échantillons <span
class="math inline">\(N\)</span> augmente.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de convergence presque sûre du Random
Search, nous procédons comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’ensemble <span
class="math inline">\(A_\epsilon = \{x \in S : f(x) \leq f(x^*) +
\epsilon\}\)</span>. Puisque <span class="math inline">\(f\)</span> est
continue et <span class="math inline">\(S\)</span> est compact, <span
class="math inline">\(A_\epsilon\)</span> est un ensemble fermé et
borné, donc compact. De plus, par définition de <span
class="math inline">\(x^*\)</span>, nous avons <span
class="math inline">\(A_\epsilon \neq \emptyset\)</span>.</p>
<p>Soit <span class="math inline">\(\mu\)</span> la mesure de Lebesgue
sur <span class="math inline">\(S\)</span>. Puisque <span
class="math inline">\(A_\epsilon\)</span> est compact et non vide, nous
avons <span class="math inline">\(\mu(A_\epsilon) &gt; 0\)</span>.</p>
<p>Le Random Search génère des points <span class="math inline">\(x_1,
x_2, \ldots, x_N\)</span> indépendamment et uniformément distribués dans
<span class="math inline">\(S\)</span>. La probabilité que tous les
points <span class="math inline">\(x_i\)</span> ne soient pas dans <span
class="math inline">\(A_\epsilon\)</span> est donnée par :</p>
<p><span class="math display">\[P(\forall i, x_i \notin A_\epsilon) = (1
- \mu(A_\epsilon))^N\]</span></p>
<p>Lorsque <span class="math inline">\(N \rightarrow +\infty\)</span>,
cette probabilité tend vers 0. Par conséquent, la probabilité que au
moins un point <span class="math inline">\(x_i\)</span> soit dans <span
class="math inline">\(A_\epsilon\)</span> tend vers 1. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le Random Search possède plusieurs propriétés intéressantes qui en
font une méthode d’optimisation puissante.</p>
<ol>
<li><p><strong>Simplicité</strong> : Le Random Search est simple à
implémenter et ne nécessite pas de calculs complexes ou de
dérivées.</p></li>
<li><p><strong>Efficacité</strong> : Dans certains cas, le Random Search
peut être plus efficace que les méthodes déterministes, notamment
lorsque l’espace des solutions est vaste et complexe.</p></li>
<li><p><strong>Convergence</strong> : Le Random Search converge presque
sûrement vers un optimum global lorsque le nombre d’échantillons tend
vers l’infini.</p></li>
</ol>
<p>Nous allons maintenant démontrer quelques corollaires importants du
Random Search.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>
une fonction continue sur un ensemble compact <span
class="math inline">\(S \subseteq \mathbb{R}^n\)</span>. Alors, pour
tout <span class="math inline">\(\epsilon &gt; 0\)</span>, il existe un
nombre fini <span class="math inline">\(N_\epsilon\)</span> tel que le
Random Search trouve un point <span class="math inline">\(x_i\)</span>
avec <span class="math inline">\(f(x_i) \leq f(x^*) + \epsilon\)</span>
avec une probabilité supérieure à <span class="math inline">\(1 -
\delta\)</span>, pour tout <span class="math inline">\(\delta &gt;
0\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par le théorème de convergence presque sûre, pour
tout <span class="math inline">\(\epsilon &gt; 0\)</span>, la
probabilité que le Random Search trouve un point <span
class="math inline">\(x_i\)</span> tel que <span
class="math inline">\(f(x_i) \leq f(x^*) + \epsilon\)</span> tend vers 1
lorsque <span class="math inline">\(N \rightarrow +\infty\)</span>. Par
conséquent, pour tout <span class="math inline">\(\delta &gt;
0\)</span>, il existe un nombre fini <span
class="math inline">\(N_\epsilon\)</span> tel que cette probabilité est
supérieure à <span class="math inline">\(1 - \delta\)</span>. ◻</p>
</div>
<div class="corollary">
<p>Le Random Search est invariant par translation et rotation de
l’espace des solutions.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(T\)</span> une
translation ou une rotation de <span class="math inline">\(S\)</span>.
Alors, pour tout point <span class="math inline">\(x \in S\)</span>,
nous avons <span class="math inline">\(f(T(x)) = f(x)\)</span>. Par
conséquent, le Random Search sur <span
class="math inline">\(T(S)\)</span> est équivalent au Random Search sur
<span class="math inline">\(S\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le Random Search est une méthode d’optimisation puissante et simple
qui trouve ses applications dans de nombreux domaines. Sa simplicité,
son efficacité et sa convergence presque sûre en font une méthode de
choix pour traiter des problèmes d’optimisation complexes. Les théorèmes
et corollaires présentés dans cet article montrent la solidité théorique
du Random Search et ouvrent la voie à de nombreuses extensions et
améliorations.</p>
</body>
</html>
{% include "footer.html" %}


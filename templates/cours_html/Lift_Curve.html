{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Lift Curve: Une Analyse Mathématique et Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Lift Curve: Une Analyse Mathématique et
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La notion de <em>Lift Curve</em> émerge dans le cadre de
l’apprentissage automatique et de la théorie des probabilités. Elle
trouve son origine dans le besoin de mesurer l’efficacité d’un modèle
prédictif, notamment en termes de capacité à discriminer entre les
classes positives et négatives. Historiquement, cette courbe a été
utilisée dans des domaines tels que le marketing direct et la détection
de fraude, où l’objectif est d’identifier les individus les plus
susceptibles de répondre positivement à une campagne ou de commettre une
fraude.</p>
<p>La Lift Curve est indispensable pour évaluer la performance d’un
modèle de classification binaire. Elle permet de visualiser le gain
relatif obtenu par rapport à un modèle aléatoire, offrant ainsi une
compréhension intuitive de l’amélioration apportée par le modèle. Cette
courbe est particulièrement utile pour comparer différents modèles et
choisir celui qui maximise le gain pour un coût donné.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la Lift Curve, commençons par comprendre ce que nous
cherchons à mesurer. Supposons que nous ayons un ensemble de données
avec des instances classées comme positives ou négatives. Un modèle
prédictif attribue à chaque instance une probabilité d’appartenir à la
classe positive. L’objectif est de mesurer comment ce modèle améliore
notre capacité à identifier les instances positives par rapport à une
sélection aléatoire.</p>
<p>Formellement, soit <span class="math inline">\(D\)</span> un ensemble
de données avec <span class="math inline">\(N\)</span> instances, où
<span class="math inline">\(N^+\)</span> est le nombre d’instances
positives et <span class="math inline">\(N^-\)</span> le nombre
d’instances négatives. Soit <span
class="math inline">\(P(y=1|x_i)\)</span> la probabilité prédite par le
modèle que l’instance <span class="math inline">\(x_i\)</span>
appartienne à la classe positive.</p>
<p>La Lift Curve est définie comme suit :</p>
<div class="definition">
<p>La Lift Curve est une fonction <span class="math inline">\(L: [0,1]
\rightarrow \mathbb{R}^+\)</span> définie par : <span
class="math display">\[L(p) = \frac{\text{Pr}(y=1|P(y=1|x) \geq
p)}{\text{Pr}(y=1)}\]</span> où <span class="math inline">\(p \in
[0,1]\)</span> est un seuil de probabilité.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[L(p) = \frac{\sum_{i=1}^{k}
I(y_i=1)}{\sum_{i=1}^{N} I(y_i=1)}\]</span> où <span
class="math inline">\(k\)</span> est le nombre d’instances avec <span
class="math inline">\(P(y=1|x_i) \geq p\)</span>, et <span
class="math inline">\(I\)</span> est la fonction indicatrice.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour mieux comprendre les propriétés de la Lift Curve, considérons le
théorème suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(L(p)\)</span> la Lift Curve définie
comme ci-dessus. Alors, pour tout <span class="math inline">\(p \in
[0,1]\)</span>, on a : <span class="math display">\[L(p) \geq 1\]</span>
avec égalité si et seulement si le modèle est équivalent à une sélection
aléatoire.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur le fait que la
Lift Curve mesure le gain relatif par rapport à une sélection aléatoire.
Si le modèle est équivalent à une sélection aléatoire, alors <span
class="math inline">\(\text{Pr}(y=1|P(y=1|x) \geq p) =
\text{Pr}(y=1)\)</span>, ce qui implique <span
class="math inline">\(L(p) = 1\)</span>. Si le modèle est meilleur
qu’une sélection aléatoire, alors <span
class="math inline">\(\text{Pr}(y=1|P(y=1|x) \geq p) &gt;
\text{Pr}(y=1)\)</span>, ce qui implique <span
class="math inline">\(L(p) &gt; 1\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer les propriétés de la Lift Curve, nous devons d’abord
établir quelques lemmes.</p>
<div class="lemma">
<p>Soit <span class="math inline">\(D\)</span> un ensemble de données
avec <span class="math inline">\(N\)</span> instances, où <span
class="math inline">\(N^+\)</span> est le nombre d’instances positives
et <span class="math inline">\(N^-\)</span> le nombre d’instances
négatives. Soit <span class="math inline">\(P(y=1|x_i)\)</span> la
probabilité prédite par le modèle que l’instance <span
class="math inline">\(x_i\)</span> appartienne à la classe positive.
Alors, pour tout seuil <span class="math inline">\(p \in [0,1]\)</span>,
on a : <span class="math display">\[\sum_{i=1}^{N} I(P(y=1|x_i) \geq p)
= k\]</span> où <span class="math inline">\(k\)</span> est le nombre
d’instances avec <span class="math inline">\(P(y=1|x_i) \geq
p\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce lemme est immédiate en utilisant la
définition de la fonction indicatrice <span
class="math inline">\(I\)</span>. Pour chaque instance <span
class="math inline">\(x_i\)</span>, si <span
class="math inline">\(P(y=1|x_i) \geq p\)</span>, alors <span
class="math inline">\(I(P(y=1|x_i) \geq p) = 1\)</span>. Sinon, <span
class="math inline">\(I(P(y=1|x_i) \geq p) = 0\)</span>. En sommant sur
toutes les instances, on obtient le nombre total d’instances avec <span
class="math inline">\(P(y=1|x_i) \geq p\)</span>, noté <span
class="math inline">\(k\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la Lift
Curve :</p>
<ol>
<li><p>La Lift Curve est une fonction décroissante de <span
class="math inline">\(p\)</span>. Cela signifie que plus le seuil <span
class="math inline">\(p\)</span> est élevé, plus la Lift Curve diminue.
Cette propriété découle du fait que lorsque le seuil augmente, le nombre
d’instances avec <span class="math inline">\(P(y=1|x_i) \geq p\)</span>
diminue, ce qui réduit le gain relatif.</p></li>
<li><p>La Lift Curve atteint son maximum en <span
class="math inline">\(p = 0\)</span>, où elle est égale à <span
class="math inline">\(\frac{N^+}{N}\)</span>. Cela signifie que lorsque
le seuil est minimal, toutes les instances sont sélectionnées, et la
Lift Curve mesure simplement la proportion d’instances positives dans
l’ensemble de données.</p></li>
<li><p>La Lift Curve atteint son minimum en <span
class="math inline">\(p = 1\)</span>, où elle est égale à 1. Cela
signifie que lorsque le seuil est maximal, seules les instances avec une
probabilité prédite de 1 sont sélectionnées. Dans ce cas, la Lift Curve
est égale à 1, indiquant qu’il n’y a pas de gain relatif par rapport à
une sélection aléatoire.</p></li>
</ol>
<p>Pour chaque propriété, nous pouvons fournir une preuve détaillée en
utilisant les définitions et lemmes précédents. Par exemple, pour la
propriété (i), nous pouvons utiliser le fait que la Lift Curve est
définie comme un rapport de probabilités, et que les probabilités
prédites sont ordonnées de manière décroissante lorsque le seuil
augmente.</p>
</body>
</html>
{% include "footer.html" %}


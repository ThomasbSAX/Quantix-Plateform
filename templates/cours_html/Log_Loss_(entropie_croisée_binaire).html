{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Log Loss : Une Exploration de l’Entropie Croisée Binaire</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Log Loss : Une Exploration de l’Entropie Croisée
Binaire</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, en particulier dans le domaine de la
classification binaire, repose sur des fonctions de perte capables de
quantifier l’écart entre les prédictions d’un modèle et les valeurs
réelles. Parmi ces fonctions, le Log Loss, ou entropie croisée binaire,
se distingue par sa capacité à pénaliser sévèrement les prédictions
erronées tout en favorisant la confiance dans les prédictions correctes.
Cette propriété en fait un outil indispensable pour l’optimisation des
modèles de classification.</p>
<p>L’origine du Log Loss remonte aux fondements de la théorie de
l’information, où il est utilisé pour mesurer l’incertitude et la
quantité d’information. Dans le contexte de l’apprentissage automatique,
il a été adopté pour son interprétation intuitive et ses propriétés
mathématiques avantageuses. Le Log Loss est particulièrement utile dans
les algorithmes de gradient descendant, où il permet une optimisation
efficace des paramètres du modèle.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le Log Loss, commençons par définir les concepts
fondamentaux. Supposons que nous avons un ensemble de données <span
class="math inline">\(\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \ldots,
(x_n, y_n)\}\)</span>, où chaque <span
class="math inline">\(y_i\)</span> est une étiquette binaire prenant les
valeurs 0 ou 1, et chaque <span class="math inline">\(x_i\)</span> est
un vecteur de caractéristiques. Un modèle de classification binaire
prédit une probabilité <span class="math inline">\(\hat{y}_i\)</span>
que l’échantillon <span class="math inline">\(x_i\)</span> appartienne à
la classe 1.</p>
<p>Nous cherchons une fonction de perte qui mesure la différence entre
les prédictions <span class="math inline">\(\hat{y}_i\)</span> et les
étiquettes réelles <span class="math inline">\(y_i\)</span>.
Intuitivement, cette fonction doit être minimale lorsque <span
class="math inline">\(\hat{y}_i\)</span> est proche de <span
class="math inline">\(y_i\)</span> et maximale lorsqu’il y a une grande
divergence.</p>
<div class="definition">
<p>Le Log Loss, ou entropie croisée binaire, pour un ensemble de données
<span class="math inline">\(\mathcal{D}\)</span> est défini comme suit :
<span class="math display">\[\text{Log Loss}(\hat{y}, y) = -\frac{1}{n}
\sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)
\right]\]</span> où <span class="math inline">\(\hat{y}_i\)</span> est
la probabilité prédite que l’échantillon <span
class="math inline">\(x_i\)</span> appartienne à la classe 1, et <span
class="math inline">\(y_i\)</span> est l’étiquette réelle.</p>
</div>
<p>Cette définition peut être reformulée en utilisant des
quantificateurs : <span class="math display">\[\text{Log Loss}(\hat{y},
y) = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i)
\log(1 - \hat{y}_i) \right] = -\frac{1}{n} \sum_{i=1}^n y_i
\log(\hat{y}_i) - \frac{1}{n} \sum_{i=1}^n (1 - y_i) \log(1 -
\hat{y}_i)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Le Log Loss possède plusieurs propriétés intéressantes qui le rendent
adapté à l’apprentissage automatique. Nous allons explorer quelques-uns
de ces théorèmes.</p>
<div class="theorem">
<p>Le Log Loss est une fonction convexe en <span
class="math inline">\(\hat{y}_i\)</span> pour <span
class="math inline">\(0 &lt; \hat{y}_i &lt; 1\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la convexité, nous devons montrer que
la matrice hessienne de la fonction de perte est positive semi-définie.
Considérons la fonction de perte pour un seul échantillon : <span
class="math display">\[L(\hat{y}) = -y \log(\hat{y}) - (1 - y) \log(1 -
\hat{y})\]</span> La dérivée première est : <span
class="math display">\[\frac{dL}{d\hat{y}} = -\frac{y}{\hat{y}} +
\frac{1 - y}{1 - \hat{y}}\]</span> La dérivée seconde est : <span
class="math display">\[\frac{d^2L}{d\hat{y}^2} = -\frac{y}{\hat{y}^2} -
\frac{1 - y}{(1 - \hat{y})^2}\]</span> Pour montrer que la matrice
hessienne est positive semi-définie, nous devons vérifier que <span
class="math inline">\(\frac{d^2L}{d\hat{y}^2} \geq 0\)</span> pour tout
<span class="math inline">\(0 &lt; \hat{y} &lt; 1\)</span>. En effet,
<span class="math inline">\(\frac{d^2L}{d\hat{y}^2}\)</span> est
toujours positive pour <span class="math inline">\(0 &lt; \hat{y} &lt;
1\)</span>, ce qui prouve la convexité. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>La preuve de la convexité du Log Loss est fondamentale pour
comprendre pourquoi il est si efficace dans les algorithmes
d’optimisation. La convexité garantit que toute méthode d’optimisation
basée sur le gradient descendra vers un minimum global, évitant ainsi
les minima locaux.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le Log Loss possède plusieurs propriétés intéressantes qui en font un
outil puissant pour l’apprentissage automatique.</p>
<ol>
<li><p><strong>Interprétation en Théorie de l’Information</strong> : Le
Log Loss peut être interprété comme une mesure de l’incertitude ou de
l’information. Plus la valeur du Log Loss est faible, plus les
prédictions sont confiantes et proches des étiquettes réelles.</p></li>
<li><p><strong>Sensibilité aux Prédictions Erronées</strong> : Le Log
Loss pénalise sévèrement les prédictions erronées, en particulier celles
faites avec une grande confiance. Cela encourage les modèles à être
prudents dans leurs prédictions.</p></li>
<li><p><strong>Compatibilité avec les Algorithmes de Gradient</strong> :
La convexité du Log Loss le rend compatible avec les algorithmes
d’optimisation basés sur le gradient, tels que la descente de gradient
stochastique.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Le Log Loss, ou entropie croisée binaire, est une fonction de perte
essentielle dans le domaine de l’apprentissage automatique. Sa capacité
à pénaliser les prédictions erronées et sa convexité en font un outil
indispensable pour l’optimisation des modèles de classification binaire.
En comprenant les propriétés et les théorèmes associés au Log Loss, nous
pouvons mieux appréhender son rôle dans l’amélioration des performances
des modèles de machine learning.</p>
</body>
</html>
{% include "footer.html" %}


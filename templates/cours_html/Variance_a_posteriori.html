{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Variance a posteriori : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Variance a posteriori : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La variance a posteriori émerge comme une notion centrale dans
l’analyse statistique bayésienne, offrant un cadre rigoureux pour
quantifier l’incertitude après observation des données. Historiquement,
cette notion trouve ses racines dans les travaux pionniers de Thomas
Bayes au XVIIIe siècle, qui a posé les bases de l’inférence
probabiliste. La variance a posteriori se révèle indispensable dans des
domaines variés, allant de la finance à l’apprentissage automatique, en
passant par les sciences sociales. Elle permet non seulement d’estimer
la précision des paramètres inconnus mais aussi de comparer différentes
hypothèses statistiques. Ce chapitre explore en profondeur cette notion,
ses définitions formelles, ses théorèmes associés et ses applications
pratiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la variance a posteriori, considérons un paramètre
<span class="math inline">\(\theta\)</span> que nous souhaitons estimer
à partir d’un ensemble de données observées <span
class="math inline">\(X\)</span>. En statistique bayésienne, nous
modélisons notre incertitude initiale sur <span
class="math inline">\(\theta\)</span> par une distribution a priori
<span class="math inline">\(p(\theta)\)</span>. Après avoir observé les
données, nous mettons à jour cette incertitude en utilisant la
distribution a posteriori <span class="math inline">\(p(\theta \mid
X)\)</span>.</p>
<p>La variance a posteriori mesure la dispersion de cette distribution a
posteriori. Intuitivement, elle quantifie à quel point nos croyances sur
<span class="math inline">\(\theta\)</span> ont changé après avoir pris
en compte les données.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\theta\)</span> un paramètre
aléatoire de distribution a priori <span
class="math inline">\(p(\theta)\)</span> et <span
class="math inline">\(X\)</span> un ensemble de données observées. La
variance a posteriori de <span class="math inline">\(\theta\)</span> est
définie comme : <span class="math display">\[\text{Var}(\theta \mid X) =
\mathbb{E}\left[(\theta - \mathbb{E}[\theta \mid X])^2 \mid
X\right]\]</span> où <span class="math inline">\(\mathbb{E}[\theta \mid
X]\)</span> est l’espérance a posteriori de <span
class="math inline">\(\theta\)</span>.</p>
</div>
<p>De manière équivalente, la variance a posteriori peut être exprimée
comme : <span class="math display">\[\text{Var}(\theta \mid X) =
\mathbb{E}[\theta^2 \mid X] - (\mathbb{E}[\theta \mid X])^2\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en statistique bayésienne est le théorème de
Bayes, qui relie la distribution a posteriori à la distribution a priori
et à la vraisemblance des données.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(p(\theta)\)</span> la distribution a
priori de <span class="math inline">\(\theta\)</span> et <span
class="math inline">\(p(X \mid \theta)\)</span> la vraisemblance des
données <span class="math inline">\(X\)</span>. La distribution a
posteriori est donnée par : <span class="math display">\[p(\theta \mid
X) = \frac{p(X \mid \theta) p(\theta)}{p(X)}\]</span> où <span
class="math inline">\(p(X)\)</span> est la probabilité marginale des
données, calculée comme : <span class="math display">\[p(X) = \int p(X
\mid \theta) p(\theta) d\theta\]</span></p>
</div>
<p>Un autre théorème important est celui de la réduction de la variance
a posteriori, qui montre comment l’observation des données réduit
l’incertitude sur le paramètre <span
class="math inline">\(\theta\)</span>.</p>
<div class="theorem">
<p>Supposons que <span class="math inline">\(\theta\)</span> suit une
distribution normale a priori <span class="math inline">\(N(\mu_0,
\sigma_0^2)\)</span> et que les données <span
class="math inline">\(X\)</span> sont indépendantes et identiquement
distribuées selon une distribution normale <span
class="math inline">\(N(\theta, \sigma^2)\)</span>. Alors la variance a
posteriori de <span class="math inline">\(\theta\)</span> est donnée par
: <span class="math display">\[\text{Var}(\theta \mid X) = \left(
\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2} \right)^{-1}\]</span> où <span
class="math inline">\(n\)</span> est le nombre de données observées.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Bayes, nous partons de la définition de
la distribution conditionnelle : <span class="math display">\[p(\theta
\mid X) = \frac{p(X, \theta)}{p(X)}\]</span> En utilisant la règle de la
chaîne, nous avons : <span class="math display">\[p(X, \theta) = p(X
\mid \theta) p(\theta)\]</span> Ainsi, nous obtenons : <span
class="math display">\[p(\theta \mid X) = \frac{p(X \mid \theta)
p(\theta)}{p(X)}\]</span> où <span class="math inline">\(p(X)\)</span>
est calculé par intégration sur toutes les valeurs possibles de <span
class="math inline">\(\theta\)</span>.</p>
<p>Pour prouver le théorème de la réduction de la variance a posteriori,
nous utilisons les propriétés des distributions normales. L’espérance a
posteriori est donnée par : <span
class="math display">\[\mathbb{E}[\theta \mid X] =
\frac{\frac{\mu_0}{\sigma_0^2} + \frac{n
\bar{X}}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\]</span>
où <span class="math inline">\(\bar{X}\)</span> est la moyenne empirique
des données. La variance a posteriori est alors : <span
class="math display">\[\text{Var}(\theta \mid X) = \left(
\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2} \right)^{-1}\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés importantes découlent de la définition de la
variance a posteriori :</p>
<ol>
<li><p><strong>Positivité</strong> : La variance a posteriori est
toujours positive ou nulle. <span
class="math display">\[\text{Var}(\theta \mid X) \geq
0\]</span></p></li>
<li><p><strong>Réduction par l’information</strong> : Plus le nombre de
données observées <span class="math inline">\(n\)</span> est grand, plus
la variance a posteriori diminue. <span class="math display">\[\lim_{n
\to \infty} \text{Var}(\theta \mid X) = 0\]</span></p></li>
<li><p><strong>Limite de l’information a priori</strong> : Si la
variance a priori <span class="math inline">\(\sigma_0^2\)</span> tend
vers l’infini, la variance a posteriori est déterminée uniquement par
les données. <span class="math display">\[\lim_{\sigma_0^2 \to \infty}
\text{Var}(\theta \mid X) = \frac{\sigma^2}{n}\]</span></p></li>
</ol>
<p>Ces propriétés montrent comment la variance a posteriori capture
l’incertitude résiduelle après prise en compte des données observées.
Elles illustrent également l’importance de la distribution a priori dans
le processus d’inférence bayésienne.</p>
</body>
</html>
{% include "footer.html" %}


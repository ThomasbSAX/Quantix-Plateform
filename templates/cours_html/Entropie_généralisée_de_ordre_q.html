{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie généralisée de ordre q</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie généralisée de ordre <span
class="math inline">\(q\)</span></h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie généralisée de ordre <span
class="math inline">\(q\)</span>, souvent appelée entropie de Tsallis,
est une extension non additive de l’entropie de Boltzmann-Gibbs. Cette
généralisation émerge naturellement dans le cadre des systèmes complexes
et des phénomènes critiques, où les interactions non linéaires jouent un
rôle prépondérant. L’entropie de Tsallis permet de capturer des
comportements statistiques qui échappent à l’analyse traditionnelle
basée sur l’entropie de Shannon ou de Boltzmann.</p>
<p>L’origine historique de cette entropie remonte aux travaux de
Constantino Tsallis dans les années 1980. Il a introduit cette notion
pour étudier les systèmes physiques présentant des corrélations à longue
portée ou des dynamiques chaotiques. L’entropie de Tsallis a depuis
trouvé des applications dans divers domaines, allant de la physique
statistique à l’informatique quantique, en passant par les sciences
sociales et la biologie.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir l’entropie généralisée de ordre <span
class="math inline">\(q\)</span>, commençons par comprendre ce que nous
cherchons à modéliser. L’entropie traditionnelle mesure le degré
d’incertitude ou de désordre dans un système. Cependant, dans les
systèmes complexes, cette mesure doit être généralisée pour tenir compte
des interactions non linéaires.</p>
<p>Supposons que nous ayons un système décrit par une distribution de
probabilité <span class="math inline">\(p = (p_1, p_2, \ldots,
p_n)\)</span>, où <span class="math inline">\(p_i\)</span> représente la
probabilité de l’état <span class="math inline">\(i\)</span>. Nous
cherchons une mesure d’entropie qui généralise la forme additive de
l’entropie de Shannon.</p>
<div class="definition">
<p>L’entropie généralisée de ordre <span
class="math inline">\(q\)</span> est définie comme suit: <span
class="math display">\[S_q(p) = \frac{1}{q-1} \left( 1 - \sum_{i=1}^n
p_i^q \right), \quad q \in \mathbb{R} \setminus \{1\}.\]</span> Pour
<span class="math inline">\(q=1\)</span>, l’entropie de Tsallis coïncide
avec l’entropie de Shannon: <span class="math display">\[S_1(p) =
-\sum_{i=1}^n p_i \ln p_i.\]</span></p>
</div>
<p>Une autre formulation équivalente de l’entropie généralisée de ordre
<span class="math inline">\(q\)</span> est: <span
class="math display">\[S_q(p) = k \sum_{i=1}^n p_i \ln_q \left(
\frac{1}{p_i} \right),\]</span> où <span class="math inline">\(\ln_q(x)
= \frac{x^{1-q} - 1}{1-q}\)</span> est le logarithme de ordre <span
class="math inline">\(q\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant les
propriétés de l’entropie généralisée de ordre <span
class="math inline">\(q\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(S_q(p)\)</span> l’entropie
généralisée de ordre <span class="math inline">\(q\)</span>. Alors, pour
<span class="math inline">\(q &gt; 0\)</span>, <span
class="math inline">\(S_q(p)\)</span> est une fonction convexe de la
distribution de probabilité <span class="math inline">\(p\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, nous devons montrer
que pour toute distribution de probabilité <span
class="math inline">\(p\)</span> et tout <span
class="math inline">\(\lambda \in [0,1]\)</span>, la suivante inégalité
tient: <span class="math display">\[S_q(\lambda p + (1-\lambda) q) \leq
\lambda S_q(p) + (1-\lambda) S_q(q),\]</span> où <span
class="math inline">\(q\)</span> est une autre distribution de
probabilité.</p>
<p>Commençons par écrire l’entropie généralisée pour les distributions
<span class="math inline">\(\lambda p + (1-\lambda) q\)</span>: <span
class="math display">\[S_q(\lambda p + (1-\lambda) q) = \frac{1}{q-1}
\left( 1 - \sum_{i=1}^n (\lambda p_i + (1-\lambda) q_i)^q
\right).\]</span></p>
<p>En utilisant l’inégalité de Jensen pour la fonction <span
class="math inline">\(f(x) = x^q\)</span>, qui est convexe pour <span
class="math inline">\(q &gt; 0\)</span>, nous avons: <span
class="math display">\[(\lambda p_i + (1-\lambda) q_i)^q \leq \lambda
p_i^q + (1-\lambda) q_i^q.\]</span></p>
<p>En substituant cette inégalité dans l’expression de <span
class="math inline">\(S_q(\lambda p + (1-\lambda) q)\)</span>, nous
obtenons: <span class="math display">\[S_q(\lambda p + (1-\lambda) q)
\leq \frac{1}{q-1} \left( 1 - \sum_{i=1}^n (\lambda p_i^q + (1-\lambda)
q_i^q) \right).\]</span></p>
<p>En séparant les sommes, nous avons: <span
class="math display">\[S_q(\lambda p + (1-\lambda) q) \leq \frac{1}{q-1}
\left( 1 - \lambda \sum_{i=1}^n p_i^q - (1-\lambda) \sum_{i=1}^n q_i^q
\right).\]</span></p>
<p>En utilisant la définition de <span
class="math inline">\(S_q(p)\)</span> et <span
class="math inline">\(S_q(q)\)</span>, nous pouvons réécrire cette
expression comme: <span class="math display">\[S_q(\lambda p +
(1-\lambda) q) \leq \lambda S_q(p) + (1-\lambda) S_q(q).\]</span></p>
<p>Cela prouve que <span class="math inline">\(S_q(p)\)</span> est une
fonction convexe de la distribution de probabilité <span
class="math inline">\(p\)</span> pour <span class="math inline">\(q &gt;
0\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous présentons maintenant quelques propriétés importantes de
l’entropie généralisée de ordre <span
class="math inline">\(q\)</span>.</p>
<div class="proposition">
<p>L’entropie généralisée de ordre <span
class="math inline">\(q\)</span> satisfait les propriétés suivantes:</p>
<ol>
<li><p><strong>Non-négativité:</strong> Pour toute distribution de
probabilité <span class="math inline">\(p\)</span>, <span
class="math inline">\(S_q(p) \geq 0\)</span>.</p></li>
<li><p><strong>Maximum d’entropie:</strong> L’entropie généralisée
atteint son maximum lorsque <span class="math inline">\(p_i =
\frac{1}{n}\)</span> pour tout <span class="math inline">\(i\)</span>,
c’est-à-dire lorsque la distribution est uniforme.</p></li>
<li><p><strong>Continuité:</strong> L’entropie généralisée est une
fonction continue de la distribution de probabilité <span
class="math inline">\(p\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Pour démontrer la non-négativité, observons que pour toute
distribution de probabilité <span class="math inline">\(p\)</span>, nous
avons <span class="math inline">\(\sum_{i=1}^n p_i^q \leq 1\)</span>
pour <span class="math inline">\(q &gt; 0\)</span>. Par conséquent,
<span class="math inline">\(\frac{1}{q-1} (1 - \sum_{i=1}^n p_i^q) \geq
0\)</span> pour <span class="math inline">\(q &gt; 1\)</span>, et <span
class="math inline">\(\frac{1}{q-1} (1 - \sum_{i=1}^n p_i^q) \geq
0\)</span> pour <span class="math inline">\(0 &lt; q &lt; 1\)</span> car
le dénominateur est négatif.</p></li>
<li><p>Pour montrer que l’entropie généralisée atteint son maximum
lorsque la distribution est uniforme, considérons <span
class="math inline">\(p_i = \frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>. Alors, <span
class="math inline">\(\sum_{i=1}^n p_i^q = n \left( \frac{1}{n}
\right)^q = n^{1-q}\)</span>. Par conséquent, <span
class="math display">\[S_q(p) = \frac{1}{q-1} (1 - n^{1-q}).\]</span>
Pour toute autre distribution de probabilité <span
class="math inline">\(p\)</span>, nous avons <span
class="math inline">\(\sum_{i=1}^n p_i^q \leq n^{1-q}\)</span> en vertu
de l’inégalité de puissance. Ainsi, <span class="math inline">\(S_q(p)
\leq \frac{1}{q-1} (1 - n^{1-q})\)</span>.</p></li>
<li><p>La continuité de l’entropie généralisée découle du fait que la
fonction <span class="math inline">\(f(x) = x^q\)</span> est continue
pour tout <span class="math inline">\(x &gt; 0\)</span> et <span
class="math inline">\(q \in \mathbb{R}\)</span>. Par conséquent, la
somme <span class="math inline">\(\sum_{i=1}^n p_i^q\)</span> est
continue en <span class="math inline">\(p\)</span>, et donc <span
class="math inline">\(S_q(p)\)</span> est également continu.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie généralisée de ordre <span
class="math inline">\(q\)</span> est un outil puissant pour l’étude des
systèmes complexes et des phénomènes critiques. Ses propriétés
mathématiques riches et ses applications variées en font un sujet de
recherche actif et passionnant. Dans cet article, nous avons présenté
les définitions fondamentales, un théorème clé et plusieurs propriétés
importantes de cette entropie. Les travaux futurs pourraient explorer
davantage les applications de l’entropie de Tsallis dans divers domaines
scientifiques et techniques.</p>
</body>
</html>
{% include "footer.html" %}


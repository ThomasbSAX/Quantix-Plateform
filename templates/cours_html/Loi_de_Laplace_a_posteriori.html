{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi de Laplace a posteriori</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi de Laplace a posteriori</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La loi de Laplace a posteriori émerge dans le cadre de l’inférence
bayésienne, une approche statistique qui permet d’incorporer des
informations a priori dans l’estimation de paramètres. Cette loi, nommée
en l’honneur du mathématicien Pierre-Simon Laplace, joue un rôle
fondamental dans la théorie des probabilités et l’analyse statistique.
Elle est indispensable pour comprendre comment les croyances initiales
(informations a priori) se combinent avec les données observées pour
former des conclusions plus robustes (informations a posteriori).</p>
<p>L’inférence bayésienne est particulièrement utile dans les domaines
où les données sont limitées ou incertaines. La loi de Laplace a
posteriori permet de quantifier l’incertitude et de mettre à jour les
croyances en fonction des nouvelles informations. Cela est crucial dans
des applications telles que la médecine, l’ingénierie, et les sciences
sociales.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la loi de Laplace a posteriori, commençons par
définir les concepts clés.</p>
<h2 id="informations-a-priori">Informations a priori</h2>
<p>Supposons que nous avons une variable aléatoire <span
class="math inline">\(\theta\)</span> représentant un paramètre inconnu.
Notre connaissance initiale sur <span
class="math inline">\(\theta\)</span> est encapsulée dans une
distribution de probabilité a priori, notée <span
class="math inline">\(\pi(\theta)\)</span>. Cette distribution reflète
nos croyances avant de voir les données.</p>
<h2 id="données-observées">Données observées</h2>
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
observée, dont la distribution dépend du paramètre <span
class="math inline">\(\theta\)</span>. La fonction de vraisemblance
<span class="math inline">\(f(X|\theta)\)</span> décrit la probabilité
d’observer les données <span class="math inline">\(X\)</span> pour une
valeur donnée de <span class="math inline">\(\theta\)</span>.</p>
<h2 id="loi-a-posteriori">Loi a posteriori</h2>
<p>La loi de Laplace a posteriori est obtenue en combinant les
informations a priori et les données observées. Elle est donnée par la
distribution conditionnelle de <span
class="math inline">\(\theta\)</span> étant donné <span
class="math inline">\(X\)</span>. Formellement, nous avons :</p>
<p><span class="math display">\[\pi(\theta|X) = \frac{\pi(\theta)
f(X|\theta)}{\int \pi(\phi) f(X|\phi) d\phi}\]</span></p>
<p>où <span class="math inline">\(\pi(\theta|X)\)</span> est la
distribution a posteriori, <span
class="math inline">\(\pi(\theta)\)</span> est la distribution a priori,
et <span class="math inline">\(f(X|\theta)\)</span> est la fonction de
vraisemblance. Le dénominateur est une constante de normalisation
assurant que la distribution a posteriori est une probabilité
valide.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-bayes">Théorème de Bayes</h2>
<p>Le théorème de Bayes est la pierre angulaire de l’inférence
bayésienne. Il relie les distributions a priori et a posteriori.
Formellement, le théorème de Bayes s’énonce comme suit :</p>
<p><span class="math display">\[\pi(\theta|X) = \frac{\pi(\theta)
f(X|\theta)}{\int \pi(\phi) f(X|\phi) d\phi}\]</span></p>
<p>Ce théorème montre comment mettre à jour nos croyances initiales en
fonction des données observées.</p>
<h2 id="propriétés-de-la-loi-a-posteriori">Propriétés de la loi a
posteriori</h2>
<p>La loi a posteriori possède plusieurs propriétés importantes :</p>
<ol>
<li><p>Elle est une distribution de probabilité valide, c’est-à-dire
qu’elle intègre à 1.</p></li>
<li><p>Elle combine les informations a priori et les données observées
de manière cohérente.</p></li>
<li><p>Elle permet de quantifier l’incertitude sur le paramètre <span
class="math inline">\(\theta\)</span> après avoir observé les
données.</p></li>
</ol>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-bayes">Preuve du théorème de Bayes</h2>
<p>Pour prouver le théorème de Bayes, nous partons de la définition de
la distribution conditionnelle :</p>
<p><span class="math display">\[\pi(\theta|X) = \frac{\pi(\theta,
X)}{\pi(X)}\]</span></p>
<p>où <span class="math inline">\(\pi(\theta, X)\)</span> est la
distribution jointe de <span class="math inline">\(\theta\)</span> et
<span class="math inline">\(X\)</span>, et <span
class="math inline">\(\pi(X)\)</span> est la distribution marginale de
<span class="math inline">\(X\)</span>. En utilisant la définition de la
distribution jointe, nous avons :</p>
<p><span class="math display">\[\pi(\theta, X) = \pi(\theta)
f(X|\theta)\]</span></p>
<p>En substituant cette expression dans la définition de la distribution
conditionnelle, nous obtenons :</p>
<p><span class="math display">\[\pi(\theta|X) = \frac{\pi(\theta)
f(X|\theta)}{\int \pi(\phi) f(X|\phi) d\phi}\]</span></p>
<p>Ce qui achève la preuve du théorème de Bayes.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-i-intégrabilité-de-la-loi-a-posteriori">Propriété (i)
: Intégrabilité de la loi a posteriori</h2>
<p>La distribution a posteriori <span
class="math inline">\(\pi(\theta|X)\)</span> est une probabilité valide,
ce qui signifie qu’elle intègre à 1 :</p>
<p><span class="math display">\[\int \pi(\theta|X) d\theta =
1\]</span></p>
<p>Cette propriété est une conséquence directe de la définition de la
distribution a posteriori et du fait que le dénominateur est une
constante de normalisation.</p>
<h2 id="propriété-ii-combinaison-des-informations">Propriété (ii) :
Combinaison des informations</h2>
<p>La loi a posteriori combine les informations a priori et les données
observées de manière cohérente. Cela signifie que si nous avons une
forte conviction a priori sur la valeur de <span
class="math inline">\(\theta\)</span>, cette conviction sera reflétée
dans la distribution a posteriori, mais elle pourra être mise à jour en
fonction des données observées.</p>
<h2 id="propriété-iii-quantification-de-lincertitude">Propriété (iii) :
Quantification de l’incertitude</h2>
<p>La loi a posteriori permet de quantifier l’incertitude sur le
paramètre <span class="math inline">\(\theta\)</span> après avoir
observé les données. Cela est particulièrement utile dans les
applications où l’incertitude doit être prise en compte
explicitement.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La loi de Laplace a posteriori est un outil fondamental en inférence
bayésienne. Elle permet de combiner les informations a priori et les
données observées pour former des conclusions plus robustes. Les
propriétés de cette loi en font un outil puissant pour quantifier
l’incertitude et mettre à jour les croyances en fonction des nouvelles
informations. Les applications de cette loi sont vastes et incluent la
médecine, l’ingénierie, et les sciences sociales.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>The Bhattacharyya Distance: A Measure of Similarity Between Probability Distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Bhattacharyya Distance: A Measure of Similarity
Between Probability Distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-and-motivations">Introduction
and Motivations</h1>
<p>The Bhattacharyya distance is a measure of similarity between two
probability distributions. It has its roots in the work of Indian
mathematician Anil Kumar Bhattacharyya, who introduced this concept in
the context of statistical pattern recognition. The Bhattacharyya
distance is particularly useful in machine learning, data mining, and
information theory, where it serves as a tool to compare the similarity
between different datasets or models.</p>
<p>The emergence of the Bhattacharyya distance can be traced back to the
need for a robust metric that could quantify the dissimilarity between
two probability distributions. Unlike other distance measures, such as
the Euclidean distance or the Kullback-Leibler divergence, the
Bhattacharyya distance is symmetric and bounded, making it a preferred
choice in various applications.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>Before formally defining the Bhattacharyya distance, let us consider
two probability distributions <span class="math inline">\(P\)</span> and
<span class="math inline">\(Q\)</span> over a common domain <span
class="math inline">\(X\)</span>. We seek a measure that quantifies how
different these two distributions are. Intuitively, if <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> are very similar, the measure should be
small, and if they are very different, the measure should be large.</p>
<p>The Bhattacharyya distance is defined as follows:</p>
<div class="definition">
<p>Let <span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> be two probability distributions over a
common domain <span class="math inline">\(X\)</span>. The Bhattacharyya
distance <span class="math inline">\(D_B(P, Q)\)</span> between <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> is given by: <span
class="math display">\[D_B(P, Q) = -\ln \left( \sum_{x \in X} \sqrt{P(x)
Q(x)} \right).\]</span> Alternatively, for continuous distributions, the
Bhattacharyya distance can be expressed as: <span
class="math display">\[D_B(P, Q) = -\ln \left( \int_{-\infty}^{\infty}
\sqrt{P(x) Q(x)} \, dx \right).\]</span></p>
</div>
<p>The Bhattacharyya distance is derived from the Bhattacharyya
coefficient, which is defined as: <span class="math display">\[BC(P, Q)
= \sum_{x \in X} \sqrt{P(x) Q(x)}\]</span> for discrete distributions,
and <span class="math display">\[BC(P, Q) = \int_{-\infty}^{\infty}
\sqrt{P(x) Q(x)} \, dx\]</span> for continuous distributions. The
Bhattacharyya distance is then the negative logarithm of the
Bhattacharyya coefficient.</p>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>One of the key properties of the Bhattacharyya distance is its
relationship with the Chernoff bound, which provides an upper bound on
the probability of error in binary hypothesis testing. The following
theorem establishes this connection.</p>
<div class="theorem">
<p>Let <span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> be two probability distributions over a
common domain <span class="math inline">\(X\)</span>. The Bhattacharyya
distance <span class="math inline">\(D_B(P, Q)\)</span> satisfies: <span
class="math display">\[P\left( \bigcup_{i=1}^{n} A_i \right) \leq e^{-n
D_B(P, Q)},\]</span> where <span class="math inline">\(A_i\)</span> are
events defined by the distributions <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof of this theorem relies on the properties of
the Chernoff bound and the relationship between the Bhattacharyya
distance and the Kullback-Leibler divergence. We start by noting that:
<span class="math display">\[D_B(P, Q) = -\ln \left( \sum_{x \in X}
\sqrt{P(x) Q(x)} \right).\]</span> Using the properties of logarithms
and the fact that <span class="math inline">\(\sqrt{P(x) Q(x)} \leq
\frac{P(x) + Q(x)}{2}\)</span>, we can derive the desired
inequality. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>The Bhattacharyya distance exhibits several important properties that
make it a useful measure of similarity between probability
distributions.</p>
<ol>
<li><p><strong>Symmetry:</strong> The Bhattacharyya distance is
symmetric, i.e., <span class="math display">\[D_B(P, Q) = D_B(Q,
P).\]</span></p></li>
<li><p><strong>Non-negativity:</strong> The Bhattacharyya distance is
non-negative, i.e., <span class="math display">\[D_B(P, Q) \geq
0,\]</span> with equality if and only if <span class="math inline">\(P =
Q\)</span>.</p></li>
<li><p><strong>Boundedness:</strong> The Bhattacharyya distance is
bounded above by <span class="math inline">\(+\infty\)</span>, i.e.,
<span class="math display">\[D_B(P, Q) \leq +\infty.\]</span></p></li>
</ol>
<p>These properties make the Bhattacharyya distance a robust and
reliable measure of similarity between probability distributions.</p>
<h1 class="unnumbered" id="applications">Applications</h1>
<p>The Bhattacharyya distance has a wide range of applications in
various fields, including machine learning, data mining, and information
theory. In machine learning, it is often used as a similarity measure in
clustering algorithms and classification tasks. In data mining, it
serves as a tool for comparing the similarity between different
datasets. In information theory, it is used to quantify the
dissimilarity between probability distributions.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>The Bhattacharyya distance is a powerful and versatile measure of
similarity between probability distributions. Its properties, such as
symmetry, non-negativity, and boundedness, make it a preferred choice in
various applications. The relationship between the Bhattacharyya
distance and the Chernoff bound further enhances its utility in
statistical pattern recognition and hypothesis testing.</p>
</body>
</html>
{% include "footer.html" %}


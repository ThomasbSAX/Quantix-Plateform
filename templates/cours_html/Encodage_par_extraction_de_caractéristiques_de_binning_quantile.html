{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning quantile</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
quantile</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
essentielle dans le traitement des données, notamment pour la
préparation des modèles prédictifs. Le binning quantile est une méthode
de discrétisation qui divise les données en intervalles de manière à ce
que chaque intervalle contienne le même nombre d’observations. Cette
approche est particulièrement utile pour gérer les distributions non
uniformes et pour réduire la sensibilité des modèles aux valeurs
aberrantes.</p>
<p>L’origine de cette technique remonte aux méthodes de discrétisation
utilisées en statistique descriptive. Le binning quantile, en
particulier, est devenu populaire grâce à sa capacité à normaliser les
distributions de données tout en préservant l’information statistique.
Cette méthode est indispensable dans des domaines tels que la finance,
la médecine et l’ingénierie, où les données peuvent présenter des
distributions complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
binning quantile, commençons par définir ce que nous cherchons à
obtenir. Nous voulons transformer des données continues en catégories de
manière à ce que chaque catégorie contienne un nombre égal
d’observations. Cela permet de normaliser les distributions et de rendre
les modèles plus robustes.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données
continues avec <span class="math inline">\(n\)</span> observations. Un
binning quantile divise <span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> intervalles de sorte que chaque
intervalle contienne exactement <span
class="math inline">\(\frac{n}{k}\)</span> observations. Formellement,
pour un nombre de bins <span class="math inline">\(k\)</span>, nous
définissons les quantiles <span class="math inline">\(q_1, q_2, \ldots,
q_{k-1}\)</span> tels que : <span class="math display">\[q_i =
F^{-1}\left(\frac{i}{k}\right), \quad i = 1, 2, \ldots, k-1\]</span> où
<span class="math inline">\(F\)</span> est la fonction de répartition
empirique de <span class="math inline">\(X\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié au binning quantile est celui de la
convergence des quantiles empiriques. Ce théorème garantit que les
quantiles estimés à partir d’un échantillon convergent vers les
quantiles théoriques lorsque la taille de l’échantillon tend vers
l’infini.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> un
échantillon aléatoire indépendant et identiquement distribué (i.i.d.)
d’une variable aléatoire <span class="math inline">\(X\)</span> avec
fonction de répartition <span class="math inline">\(F\)</span>. Pour
tout <span class="math inline">\(p \in (0,1)\)</span>, le quantile
empirique <span class="math inline">\(\hat{q}_n(p)\)</span> converge
presque sûrement vers le quantile théorique <span
class="math inline">\(q(p) = F^{-1}(p)\)</span> lorsque <span
class="math inline">\(n \to \infty\)</span>. <span
class="math display">\[\hat{q}_n(p) \xrightarrow{a.s.} q(p)\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de la convergence des quantiles empiriques
repose sur le théorème de Glivenko-Cantelli, qui garantit la convergence
uniforme de la fonction de répartition empirique vers la fonction de
répartition théorique.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(F_n\)</span> la
fonction de répartition empirique définie par : <span
class="math display">\[F_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{X_i
\leq x}\]</span> où <span class="math inline">\(\mathbb{I}\)</span> est
l’indicatrice. Le théorème de Glivenko-Cantelli stipule que : <span
class="math display">\[\sup_{x \in \mathbb{R}} |F_n(x) - F(x)|
\xrightarrow{a.s.} 0\]</span> Pour tout <span class="math inline">\(p
\in (0,1)\)</span>, le quantile empirique <span
class="math inline">\(\hat{q}_n(p)\)</span> est défini par : <span
class="math display">\[F_n(\hat{q}_n(p)) = p\]</span> En utilisant la
continuité de <span class="math inline">\(F\)</span> et la convergence
uniforme de <span class="math inline">\(F_n\)</span>, nous avons : <span
class="math display">\[F(\hat{q}_n(p)) \xrightarrow{a.s.} p\]</span>
Puisque <span class="math inline">\(F\)</span> est continue et
strictement croissante, il en découle que : <span
class="math display">\[\hat{q}_n(p) \xrightarrow{a.s.}
q(p)\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes du binning
quantile :</p>
<ol>
<li><p>Le binning quantile préserve la distribution des données en
termes de quantiles, ce qui est utile pour les analyses
statistiques.</p></li>
<li><p>Il réduit la sensibilité des modèles aux valeurs aberrantes en
normalisant les distributions.</p></li>
<li><p>Le nombre de bins <span class="math inline">\(k\)</span> doit
être choisi judicieusement pour éviter le surapprentissage ou la perte
d’information.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning quantile est
une technique puissante pour la préparation des données. Elle permet de
normaliser les distributions et de rendre les modèles prédictifs plus
robustes. Les théorèmes et propriétés présentés dans cet article
montrent l’importance de cette méthode dans le traitement des
données.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Gaussian Processes: A Comprehensive Overview</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Gaussian Processes: A Comprehensive Overview</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>Gaussian Processes (GPs) have emerged as a powerful tool in the realm
of machine learning and statistics, offering a probabilistic framework
for modeling complex relationships within data. The concept of GPs is
rooted in the principles of Gaussian distributions, extending them to
functions and providing a flexible, non-parametric approach to
regression and classification tasks.</p>
<p>The motivation behind GPs lies in their ability to handle uncertainty
and provide a full posterior distribution over functions, rather than
just point estimates. This is particularly useful in scenarios where
data is sparse or noisy, and where the underlying function is believed
to be smooth. GPs have found applications in various fields, including
spatial statistics, time series analysis, and Bayesian optimization.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand Gaussian Processes, we first need to grasp the concept
of a stochastic process. A stochastic process is a collection of random
variables, one for each time point or element in some index set. A
Gaussian Process is a special case where every finite subset of these
random variables has a joint Gaussian distribution.</p>
<div class="definition">
<p>Let <span class="math inline">\(\mathcal{X}\)</span> be an index set
(often a subset of <span class="math inline">\(\mathbb{R}^d\)</span>),
and let <span class="math inline">\(\{X_t\}_{t \in \mathcal{X}}\)</span>
be a collection of random variables. We say that <span
class="math inline">\(\{X_t\}_{t \in \mathcal{X}}\)</span> is a Gaussian
Process if for any finite subset <span class="math inline">\(\{t_1, t_2,
\ldots, t_n\} \subseteq \mathcal{X}\)</span>, the random vector <span
class="math inline">\((X_{t_1}, X_{t_2}, \ldots, X_{t_n})^T\)</span>
follows a multivariate Gaussian distribution.</p>
<p>Formally, <span class="math inline">\(\{X_t\}_{t \in
\mathcal{X}}\)</span> is a Gaussian Process if for any <span
class="math inline">\(n \in \mathbb{N}\)</span>, and for any <span
class="math inline">\(t_1, t_2, \ldots, t_n \in \mathcal{X}\)</span>,
there exist <span class="math inline">\(\mu(t_1), \mu(t_2), \ldots,
\mu(t_n) \in \mathbb{R}\)</span> and a positive definite matrix <span
class="math inline">\(K \in \mathbb{R}^{n \times n}\)</span> such that:
<span class="math display">\[(X_{t_1}, X_{t_2}, \ldots, X_{t_n})^T \sim
\mathcal{N}(\mu(t_1), \ldots, \mu(t_n)), K)\]</span> where <span
class="math inline">\(\mu(t)\)</span> is the mean function and <span
class="math inline">\(K\)</span> is the covariance matrix with entries
<span class="math inline">\(K_{ij} = k(t_i, t_j)\)</span>, and <span
class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow
\mathbb{R}\)</span> is the covariance function.</p>
</div>
<p>The mean function <span class="math inline">\(\mu(t)\)</span> and the
covariance function <span class="math inline">\(k(t, t&#39;)\)</span>
are crucial in defining a GP. The mean function provides the expected
value of the process at any point <span
class="math inline">\(t\)</span>, while the covariance function captures
the similarity between points in the index set.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems related to Gaussian Processes is the
reproducing property, which is often associated with the covariance
function.</p>
<div class="theorem">
<p>Let <span class="math inline">\(\{X_t\}_{t \in \mathcal{X}}\)</span>
be a Gaussian Process with mean function <span
class="math inline">\(\mu(t)\)</span> and covariance function <span
class="math inline">\(k(t, t&#39;)\)</span>. Then, for any <span
class="math inline">\(t \in \mathcal{X}\)</span>, the following holds:
<span class="math display">\[\mathbb{E}[(X_t - \mu(t))^2] = k(t,
t)\]</span> Moreover, for any <span class="math inline">\(f: \mathcal{X}
\rightarrow \mathbb{R}\)</span> such that <span
class="math inline">\(\mathbb{E}[f(X_t)^2] &lt; \infty\)</span>, the
following reproducing property holds: <span
class="math display">\[\mathbb{E}[f(X_t)(X_{t&#39;} - \mu(t&#39;))] =
f(t)k(t, t&#39;)\]</span></p>
</div>
<p>This theorem highlights the role of the covariance function in
determining the behavior of the Gaussian Process. The reproducing
property is particularly useful in deriving optimal prediction formulas
and understanding the smoothness of the process.</p>
<h1 id="proofs">Proofs</h1>
<p>To prove the reproducing property, we start by considering the
definition of the covariance function and the properties of Gaussian
Processes.</p>
<div class="proof">
<p><em>Proof.</em> By definition, the covariance function <span
class="math inline">\(k(t, t&#39;)\)</span> is given by: <span
class="math display">\[k(t, t&#39;) = \mathbb{E}[(X_t -
\mu(t))(X_{t&#39;} - \mu(t&#39;))]\]</span> Setting <span
class="math inline">\(t = t&#39;\)</span>, we obtain: <span
class="math display">\[k(t, t) = \mathbb{E}[(X_t - \mu(t))^2]\]</span>
which proves the first part of the theorem.</p>
<p>For the second part, consider any function <span
class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>
such that <span class="math inline">\(\mathbb{E}[f(X_t)^2] &lt;
\infty\)</span>. We can write: <span
class="math display">\[\mathbb{E}[f(X_t)(X_{t&#39;} - \mu(t&#39;))] =
\mathbb{E}[f(X_t)X_{t&#39;}] - f(t)\mu(t&#39;)\]</span> Using the
linearity of expectation and the definition of the covariance function,
we have: <span class="math display">\[\mathbb{E}[f(X_t)X_{t&#39;}] =
\mathbb{E}[f(X_t)\mu(t&#39;) + f(X_t)(X_{t&#39;} - \mu(t&#39;))] =
f(t)\mu(t&#39;) + \mathbb{E}[f(X_t)(X_{t&#39;} - \mu(t&#39;))]\]</span>
Substituting back, we get: <span
class="math display">\[\mathbb{E}[f(X_t)(X_{t&#39;} - \mu(t&#39;))] =
f(t)k(t, t&#39;)\]</span> which completes the proof. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Gaussian Processes possess several important properties that make
them useful in various applications. We list some of these properties
below:</p>
<ol>
<li><p><strong>Closed Form Predictions:</strong> Given a set of
observations <span class="math inline">\(\{X_{t_i}\}_{i=1}^n\)</span>,
the posterior distribution of the GP at any new point <span
class="math inline">\(t^*\)</span> can be computed in closed form. This
is a consequence of the properties of multivariate Gaussian
distributions and the reproducing property.</p></li>
<li><p><strong>Universal Approximation:</strong> Under certain
conditions on the covariance function, GPs can approximate any
continuous function to arbitrary accuracy. This makes them a powerful
tool for non-parametric regression.</p></li>
<li><p><strong>Uncertainty Quantification:</strong> GPs provide a full
posterior distribution over functions, allowing for the quantification
of uncertainty in predictions. This is particularly useful in scenarios
where decision-making under uncertainty is required.</p></li>
</ol>
<p>Each of these properties can be derived from the fundamental
definitions and theorems related to Gaussian Processes. The closed form
predictions, for instance, follow from the properties of multivariate
Gaussian distributions and the reproducing property. The universal
approximation property is a consequence of the choice of the covariance
function, while the uncertainty quantification arises from the
probabilistic nature of GPs.</p>
</body>
</html>
{% include "footer.html" %}


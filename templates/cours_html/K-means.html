{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>K-means : Une Méthode d’Apprentissage Non Supervisé</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">K-means : Une Méthode d’Apprentissage Non
Supervisé</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’algorithme K-means est l’une des méthodes les plus populaires pour
le clustering, c’est-à-dire la partition d’un ensemble de données en
sous-ensembles cohérents. Son origine remonte aux années 1960, où il a
été développé pour répondre au besoin croissant de traiter des données
complexes et volumineuses. L’idée centrale est de regrouper les données
en clusters, où chaque cluster est représenté par son centroïde.</p>
<p>K-means émerge comme une solution efficace pour résoudre des
problèmes de classification non supervisée, où les données n’ont pas été
préalablement étiquetées. Il est indispensable dans des domaines tels
que l’analyse de données, la reconnaissance de motifs et la segmentation
d’images. La simplicité algorithmique et l’efficacité computationnelle
en font un outil incontournable pour les chercheurs et les
praticiens.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre K-means, commençons par définir ce que nous cherchons
à accomplir. Nous avons un ensemble de données <span
class="math inline">\(X = \{x_1, x_2, \dots, x_n\}\)</span> dans un
espace euclidien <span class="math inline">\(\mathbb{R}^d\)</span>.
Notre objectif est de partitionner cet ensemble en <span
class="math inline">\(k\)</span> sous-ensembles disjoints, appelés
clusters, tels que les points dans chaque cluster soient aussi proches
que possible du centroïde de ce cluster.</p>
<p>Formellement, un clustering <span class="math inline">\(C = \{C_1,
C_2, \dots, C_k\}\)</span> est une partition de <span
class="math inline">\(X\)</span> telle que : <span
class="math display">\[\forall i, j \in \{1, 2, \dots, k\}, \quad C_i
\cap C_j = \emptyset \quad \text{et} \quad \bigcup_{i=1}^k C_i =
X\]</span></p>
<p>Le centroïde <span class="math inline">\(\mu_i\)</span> d’un cluster
<span class="math inline">\(C_i\)</span> est défini comme la moyenne des
points dans ce cluster : <span class="math display">\[\mu_i =
\frac{1}{|C_i|} \sum_{x \in C_i} x\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’algorithme K-means repose sur l’optimisation d’une fonction de
coût, souvent appelée <em>inertie intra-clusters</em>. Cette fonction
mesure la somme des distances au carré entre chaque point et le
centroïde de son cluster.</p>
<p>Le problème d’optimisation peut être formalisé comme suit : <span
class="math display">\[\min_{C, \mu} \sum_{i=1}^k \sum_{x \in C_i} \|x -
\mu_i\|^2\]</span></p>
<p>où <span class="math inline">\(\| \cdot \|\)</span> désigne la norme
euclidienne.</p>
<p>Pour résoudre ce problème, l’algorithme K-means utilise une approche
itérative qui alterne entre deux étapes : 1. **Étape d’assignation** :
Chaque point est assigné au cluster dont le centroïde est le plus
proche. 2. **Étape de mise à jour** : Les centroïdes sont recalculés
comme la moyenne des points dans chaque cluster.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour justifier l’algorithme K-means, nous devons montrer que chaque
étape de l’algorithme réduit la fonction de coût. Considérons d’abord
l’étape d’assignation.</p>
<div class="theorem">
<p>L’étape d’assignation minimise la fonction de coût pour des
centroïdes fixes.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\mu = (\mu_1, \mu_2,
\dots, \mu_k)\)</span> un ensemble de centroïdes fixes. Pour chaque
point <span class="math inline">\(x \in X\)</span>, nous voulons
l’assigner au cluster <span class="math inline">\(C_i\)</span> qui
minimise la distance <span class="math inline">\(\|x -
\mu_i\|^2\)</span>.</p>
<p>Pour tout point <span class="math inline">\(x\)</span>, il existe un
cluster <span class="math inline">\(C_i\)</span> tel que : <span
class="math display">\[\|x - \mu_i\|^2 \leq \|x - \mu_j\|^2 \quad
\forall j \neq i\]</span></p>
<p>Ainsi, l’assignation de <span class="math inline">\(x\)</span> à
<span class="math inline">\(C_i\)</span> minimise la contribution de
<span class="math inline">\(x\)</span> à la fonction de coût. ◻</p>
</div>
<p>Considérons maintenant l’étape de mise à jour.</p>
<div class="theorem">
<p>L’étape de mise à jour minimise la fonction de coût pour une
assignation fixe.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(C = \{C_1, C_2,
\dots, C_k\}\)</span> une partition fixe des données. Nous voulons
recalculer les centroïdes <span class="math inline">\(\mu_i\)</span>
pour minimiser la fonction de coût.</p>
<p>Pour chaque cluster <span class="math inline">\(C_i\)</span>, le
centroïde optimal est donné par : <span class="math display">\[\mu_i =
\frac{1}{|C_i|} \sum_{x \in C_i} x\]</span></p>
<p>En effet, pour tout <span class="math inline">\(\mu_i&#39; \neq
\mu_i\)</span>, nous avons : <span class="math display">\[\sum_{x \in
C_i} \|x - \mu_i\|^2 &lt; \sum_{x \in C_i} \|x -
\mu_i&#39;\|^2\]</span></p>
<p>Cela montre que la mise à jour des centroïdes minimise la fonction de
coût pour une assignation fixe. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’algorithme K-means possède plusieurs propriétés intéressantes :</p>
<ol>
<li><p>**Convergence** : L’algorithme K-means converge vers un optimum
local de la fonction de coût. Cela signifie que l’inertie intra-clusters
ne peut qu’être réduite ou maintenue constante à chaque
itération.</p></li>
<li><p>**Sensibilité aux Initialisations** : L’algorithme est sensible à
l’initialisation des centroïdes. Différentes initialisations peuvent
conduire à différents résultats.</p></li>
<li><p>**Complexité Computationnelle** : La complexité de l’algorithme
est <span class="math inline">\(O(n \cdot k \cdot d \cdot I)\)</span>,
où <span class="math inline">\(n\)</span> est le nombre de points, <span
class="math inline">\(k\)</span> le nombre de clusters, <span
class="math inline">\(d\)</span> la dimension des données et <span
class="math inline">\(I\)</span> le nombre d’itérations.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’algorithme K-means est une méthode puissante et efficace pour le
clustering non supervisé. Sa simplicité algorithmique et son efficacité
computationnelle en font un outil incontournable pour l’analyse de
données. Cependant, il présente certaines limitations, notamment sa
sensibilité aux initialisations et sa difficulté à traiter des clusters
de formes complexes. Des extensions et variantes de K-means ont été
développées pour surmonter ces limitations, ouvrant ainsi de nouvelles
perspectives pour le clustering.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de transportation de Wasserstein : Un pont entre la géométrie et l’analyse</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de transportation de Wasserstein : Un pont
entre la géométrie et l’analyse</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de transportation de Wasserstein émerge à l’intersection
de la théorie des probabilités, de l’analyse fonctionnelle et de la
géométrie différentielle. Cette inégalité, qui porte le nom du
mathématicien allemand Wassily Hoeffding et du physicien russe Vladimir
Wassermann, trouve ses racines dans les travaux pionniers sur la théorie
de la mesure et des probabilités au début du XXe siècle.</p>
<p>L’idée centrale derrière cette inégalité est de quantifier la
distance entre deux distributions de probabilité en termes de coût de
transportation. Imaginez que vous souhaitiez transporter des
marchandises d’un entrepôt à un autre, mais les quantités et les
emplacements ne correspondent pas exactement. Le coût minimal pour
effectuer cette transportation est une mesure de la "distance" entre les
deux distributions.</p>
<p>Cette notion est indispensable dans de nombreux domaines, notamment
en optimisation, en apprentissage automatique et en économie. Elle
permet de formaliser des problèmes complexes de manière rigoureuse et
offre des outils puissants pour l’analyse des données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de formuler l’inégalité de Wasserstein, il est essentiel de
définir quelques concepts clés.</p>
<h2 class="unnumbered" id="espace-métrique">Espace métrique</h2>
<p>Considérons un espace métrique <span class="math inline">\((X,
d)\)</span>, où <span class="math inline">\(X\)</span> est un ensemble
et <span class="math inline">\(d: X \times X \rightarrow
\mathbb{R}^+\)</span> est une fonction de distance vérifiant les
propriétés suivantes pour tous <span class="math inline">\(x, y, z \in
X\)</span>:</p>
<ol>
<li><p><span class="math inline">\(d(x, y) = 0\)</span> si et seulement
si <span class="math inline">\(x = y\)</span>.</p></li>
<li><p><span class="math inline">\(d(x, y) = d(y, x)\)</span>.</p></li>
<li><p><span class="math inline">\(d(x, z) \leq d(x, y) + d(y,
z)\)</span>.</p></li>
</ol>
<h2 class="unnumbered" id="mesure-de-probabilité">Mesure de
probabilité</h2>
<p>Une mesure de probabilité <span class="math inline">\(\mu\)</span>
sur <span class="math inline">\(X\)</span> est une fonction <span
class="math inline">\(\mu: \mathcal{B}(X) \rightarrow [0, 1]\)</span>,
où <span class="math inline">\(\mathcal{B}(X)\)</span> est la tribu des
boréliens de <span class="math inline">\(X\)</span>, satisfaisant:</p>
<ol>
<li><p><span class="math inline">\(\mu(X) = 1\)</span>.</p></li>
<li><p>Pour toute suite croissante <span class="math inline">\((A_n)_{n
\in \mathbb{N}}\)</span> dans <span
class="math inline">\(\mathcal{B}(X)\)</span>, on a <span
class="math inline">\(\mu\left(\bigcup_{n \in \mathbb{N}} A_n\right) =
\lim_{n \rightarrow \infty} \mu(A_n)\)</span>.</p></li>
</ol>
<h2 class="unnumbered" id="couplage">Couplage</h2>
<p>Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux mesures de probabilité sur <span
class="math inline">\(X\)</span>. Un couplage entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est une mesure de probabilité <span
class="math inline">\(\pi\)</span> sur <span class="math inline">\(X
\times X\)</span> telle que pour tout borélien <span
class="math inline">\(A \subset X\)</span>, <span
class="math display">\[\pi(A \times X) = \mu(A) \quad \text{et} \quad
\pi(X \times A) = \nu(A).\]</span></p>
<h2 class="unnumbered" id="coût-de-transportation">Coût de
transportation</h2>
<p>Soit <span class="math inline">\(c: X \times X \rightarrow
\mathbb{R}^+\)</span> une fonction de coût. Le coût de transportation
entre <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> via un couplage <span
class="math inline">\(\pi\)</span> est défini par: <span
class="math display">\[\int_{X \times X} c(x, y) \, d\pi(x,
y).\]</span></p>
<h2 class="unnumbered" id="distance-de-wasserstein">Distance de
Wasserstein</h2>
<p>La distance de Wasserstein d’ordre <span
class="math inline">\(p\)</span> entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie comme: <span
class="math display">\[W_p(\mu, \nu) = \left( \inf_{\pi \in \Pi(\mu,
\nu)} \int_{X \times X} d(x, y)^p \, d\pi(x, y) \right)^{1/p},\]</span>
où <span class="math inline">\(\Pi(\mu, \nu)\)</span> est l’ensemble des
couplages entre <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>.</p>
<h1 class="unnumbered"
id="inégalité-de-transportation-de-wasserstein">Inégalité de
Transportation de Wasserstein</h1>
<p>L’inégalité de transportation de Wasserstein est une généralisation
de l’inégalité de Poincaré et de l’inégalité de Sobolev. Elle relie la
distance de Wasserstein entre deux distributions de probabilité à une
certaine mesure de leur divergence.</p>
<h2 class="unnumbered" id="énoncé">Énoncé</h2>
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
compact. Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux mesures de probabilité sur <span
class="math inline">\(X\)</span>. Supposons qu’il existe une fonction
convexe <span class="math inline">\(\phi: [0, \infty) \rightarrow [0,
\infty)\)</span> telle que pour toute fonction Lipschitz <span
class="math inline">\(f: X \rightarrow \mathbb{R}\)</span> avec
constante de Lipschitz <span class="math inline">\(L\)</span>, on ait:
<span class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu \leq L
\phi(W_1(\mu, \nu)).\]</span> Alors, pour toute fonction mesurable <span
class="math inline">\(g: X \rightarrow \mathbb{R}\)</span> avec
constante de Lipschitz <span class="math inline">\(L_g\)</span>, on a:
<span class="math display">\[\int_X g \, d\mu - \int_X g \, d\nu \leq
L_g \phi(W_1(\mu, \nu)).\]</span></p>
<h2 class="unnumbered" id="preuve">Preuve</h2>
<p>Pour prouver cette inégalité, nous utilisons le théorème de
Kantorovitch-Rubinstein, qui affirme que pour deux mesures de
probabilité <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> sur un espace métrique compact <span
class="math inline">\((X, d)\)</span>, on a: <span
class="math display">\[W_1(\mu, \nu) = \sup_{f \in \text{Lip}_1(X)}
\left( \int_X f \, d\mu - \int_X f \, d\nu \right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> est l’ensemble des
fonctions Lipschitz sur <span class="math inline">\(X\)</span> avec
constante de Lipschitz au plus 1.</p>
<p>Soit <span class="math inline">\(g: X \rightarrow \mathbb{R}\)</span>
une fonction mesurable avec constante de Lipschitz <span
class="math inline">\(L_g\)</span>. On peut écrire <span
class="math inline">\(g\)</span> comme une combinaison linéaire de
fonctions Lipschitz: <span class="math display">\[g = a f + b,\]</span>
où <span class="math inline">\(f \in \text{Lip}_1(X)\)</span> et <span
class="math inline">\(a, b\)</span> sont des constantes réelles.</p>
<p>En utilisant le théorème de Kantorovitch-Rubinstein, on a: <span
class="math display">\[\int_X g \, d\mu - \int_X g \, d\nu = a \left(
\int_X f \, d\mu - \int_X f \, d\nu \right) + b (\mu(X) - \nu(X)) = a
\left( \int_X f \, d\mu - \int_X f \, d\nu \right).\]</span> Puisque
<span class="math inline">\(f \in \text{Lip}_1(X)\)</span>, on a: <span
class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu \leq W_1(\mu,
\nu).\]</span> Donc, <span class="math display">\[\int_X g \, d\mu -
\int_X g \, d\nu \leq a W_1(\mu, \nu).\]</span> En utilisant l’hypothèse
que <span class="math inline">\(\int_X f \, d\mu - \int_X f \, d\nu \leq
L \phi(W_1(\mu, \nu))\)</span> pour toute fonction Lipschitz <span
class="math inline">\(f\)</span>, on obtient: <span
class="math display">\[\int_X g \, d\mu - \int_X g \, d\nu \leq L_g
\phi(W_1(\mu, \nu)).\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered" id="propriété-de-contraction">Propriété de
contraction</h2>
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
et <span class="math inline">\(T: X \rightarrow X\)</span> une
application contractante avec constante de contraction <span
class="math inline">\(\alpha &lt; 1\)</span>. Alors, pour toute mesure
de probabilité <span class="math inline">\(\mu\)</span> sur <span
class="math inline">\(X\)</span>, on a: <span
class="math display">\[W_p(T_\# \mu, T_\# \nu) \leq \alpha W_p(\mu,
\nu),\]</span> où <span class="math inline">\(T_\# \mu\)</span> est la
mesure image de <span class="math inline">\(\mu\)</span> par <span
class="math inline">\(T\)</span>.</p>
<h2 class="unnumbered" id="inégalité-de-talagrand">Inégalité de
Talagrand</h2>
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
compact et <span class="math inline">\(\mu\)</span> une mesure de
probabilité sur <span class="math inline">\(X\)</span>. Supposons que
<span class="math inline">\(\mu\)</span> vérifie une inégalité de
Talagrand avec constante <span class="math inline">\(\lambda &gt;
0\)</span>, c’est-à-dire que pour toute mesure de probabilité <span
class="math inline">\(\nu\)</span> sur <span
class="math inline">\(X\)</span>, <span class="math display">\[W_2(\mu,
\nu)^2 \leq \frac{2}{\lambda} D(\nu \| \mu),\]</span> où <span
class="math inline">\(D(\nu \| \mu)\)</span> est la divergence de
Kullback-Leibler entre <span class="math inline">\(\nu\)</span> et <span
class="math inline">\(\mu\)</span>.</p>
<h2 class="unnumbered" id="inégalité-de-poincaré-wasserstein">Inégalité
de Poincaré-Wasserstein</h2>
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
compact et <span class="math inline">\(\mu\)</span> une mesure de
probabilité sur <span class="math inline">\(X\)</span>. Supposons que
<span class="math inline">\(\mu\)</span> vérifie une inégalité de
Poincaré avec constante <span class="math inline">\(C &gt; 0\)</span>,
c’est-à-dire que pour toute fonction Lipschitz <span
class="math inline">\(f: X \rightarrow \mathbb{R}\)</span> avec
constante de Lipschitz <span class="math inline">\(L\)</span>, <span
class="math display">\[\text{Var}_{\mu}(f) \leq C L^2.\]</span> Alors,
pour toute mesure de probabilité <span
class="math inline">\(\nu\)</span> sur <span
class="math inline">\(X\)</span>, <span class="math display">\[W_2(\mu,
\nu)^2 \leq C D(\nu \| \mu).\]</span></p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de transportation de Wasserstein est un outil puissant et
polyvalent en analyse et en théorie des probabilités. Elle permet de
quantifier la distance entre deux distributions de probabilité et trouve
des applications dans divers domaines, notamment en optimisation, en
apprentissage automatique et en économie. Les propriétés et corollaires
associés à cette inégalité offrent des insights précieux sur la
structure géométrique et analytique des espaces de mesures de
probabilité.</p>
</body>
</html>
{% include "footer.html" %}


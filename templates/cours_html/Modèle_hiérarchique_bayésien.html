{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Modèle hiérarchique bayésien : Une synthèse des approches probabilistes avancées</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Modèle hiérarchique bayésien : Une synthèse des
approches probabilistes avancées</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Les modèles hiérarchiques bayésiens (MHB) émergent comme une réponse
élégante aux défis posés par l’analyse de données complexes et
imbriquées. Leur origine remonte aux travaux pionniers de Lindley et
Smith en 1972, qui ont formalisé l’idée d’intégrer des niveaux de
modélisation pour capturer les structures hiérarchiques inhérentes aux
données. Ces modèles sont indispensables dans des domaines variés,
allant de la biostatistique à l’économie, en passant par les sciences
sociales. Leur puissance réside dans leur capacité à borner
l’incertitude tout en exploitant les informations disponibles de manière
optimale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre ce qu’est un modèle hiérarchique bayésien,
considérons d’abord une situation où nous avons des données regroupées
en clusters. Supposons que chaque cluster ait ses propres paramètres,
mais que ces paramètres soient eux-mêmes tirés d’une distribution
commune. L’idée est de modéliser cette hiérarchie pour mieux estimer les
paramètres individuels tout en partageant l’information entre
clusters.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{D} = \{D_1, D_2, \ldots,
D_n\}\)</span> un ensemble de données regroupées en <span
class="math inline">\(n\)</span> clusters. Pour chaque cluster <span
class="math inline">\(i\)</span>, nous avons des observations <span
class="math inline">\(D_i = \{y_{ij}\}_{j=1}^{m_i}\)</span> et des
paramètres <span class="math inline">\(\theta_i\)</span>. Les paramètres
<span class="math inline">\(\theta_i\)</span> sont supposés être tirés
d’une distribution commune <span
class="math inline">\(\mathcal{P}\)</span> avec des hyperparamètres
<span class="math inline">\(\phi\)</span>. Un modèle hiérarchique
bayésien est défini par les relations suivantes : <span
class="math display">\[\begin{align*}
y_{ij} &amp;\sim F(\theta_i) \quad \text{pour tout } j = 1, \ldots, m_i,
\\
\theta_i &amp;\sim \mathcal{P}(\phi) \quad \text{pour tout } i = 1,
\ldots, n, \\
\phi &amp;\sim \Pi,
\end{align*}\]</span> où <span class="math inline">\(F\)</span> est une
distribution de probabilité paramétrée par <span
class="math inline">\(\theta_i\)</span>, <span
class="math inline">\(\mathcal{P}\)</span> est une distribution de
probabilité paramétrée par <span class="math inline">\(\phi\)</span>, et
<span class="math inline">\(\Pi\)</span> est une distribution a priori
sur les hyperparamètres <span class="math inline">\(\phi\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<p>Un des théorèmes fondamentaux liés aux modèles hiérarchiques
bayésiens est le théorème de conjugaison, qui permet de simplifier les
calculs en choisissant des distributions a priori conjuguées.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(F\)</span> une famille exponentielle
de distributions paramétrée par <span
class="math inline">\(\theta\)</span>, et soit <span
class="math inline">\(\mathcal{P}\)</span> une distribution a priori sur
<span class="math inline">\(\theta\)</span> qui est conjuguée à <span
class="math inline">\(F\)</span>. Alors, la distribution a posteriori de
<span class="math inline">\(\theta\)</span> donnée les données <span
class="math inline">\(D_i = \{y_{ij}\}_{j=1}^{m_i}\)</span> est de la
même forme que <span class="math inline">\(\mathcal{P}\)</span>, mais
avec des paramètres mis à jour.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de conjugaison, nous commençons par
rappeler que pour une famille exponentielle <span
class="math inline">\(F\)</span>, la fonction de vraisemblance s’écrit :
<span class="math display">\[\begin{align*}
p(D_i \mid \theta) = \prod_{j=1}^{m_i} f(y_{ij} \mid \theta) =
\exp\left(\sum_{j=1}^{m_i} [\eta(\theta)^T T(y_{ij}) - A(\theta)] +
B(y_{ij})\right).
\end{align*}\]</span> La distribution a priori <span
class="math inline">\(\mathcal{P}\)</span> étant conjuguée, elle peut
être écrite sous la forme : <span class="math display">\[\begin{align*}
p(\theta \mid \phi) = \exp\left(\eta(\phi)^T \eta(\theta) -
A^*(\phi)\right).
\end{align*}\]</span> La distribution a posteriori s’obtient en
multipliant la vraisemblance par l’a priori et en normalisant : <span
class="math display">\[\begin{align*}
p(\theta \mid D_i, \phi) &amp;\propto p(D_i \mid \theta) p(\theta \mid
\phi) \\
&amp;= \exp\left(\sum_{j=1}^{m_i} [\eta(\theta)^T T(y_{ij}) - A(\theta)]
+ B(y_{ij})\right) \exp\left(\eta(\phi)^T \eta(\theta) -
A^*(\phi)\right) \\
&amp;= \exp\left(\left[\sum_{j=1}^{m_i} T(y_{ij}) + \eta(\phi)\right]^T
\eta(\theta) - A^*(\phi) - \sum_{j=1}^{m_i} A(\theta)\right).
\end{align*}\]</span> En choisissant <span
class="math inline">\(\eta(\phi)\)</span> et <span
class="math inline">\(A^*(\phi)\)</span> de manière appropriée, on peut
montrer que la distribution a posteriori est de la même forme que l’a
priori <span class="math inline">\(\mathcal{P}\)</span>, mais avec des
paramètres mis à jour.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les modèles hiérarchiques bayésiens possèdent plusieurs propriétés
intéressantes :</p>
<ol>
<li><p><strong>Partage d’information</strong> : Les paramètres <span
class="math inline">\(\theta_i\)</span> des différents clusters
partagent de l’information à travers la distribution commune <span
class="math inline">\(\mathcal{P}\)</span>. Cela permet d’estimer plus
précisément les paramètres individuels, surtout lorsque les données sont
limitées.</p></li>
<li><p><strong>Robustesse</strong> : Les MHB sont robustes aux données
manquantes et aux structures hiérarchiques complexes. Ils permettent de
modéliser des niveaux multiples de variabilité.</p></li>
<li><p><strong>Flexibilité</strong> : Les MHB peuvent être étendus pour
inclure des effets aléatoires, des interactions complexes et des
structures de covariance non standard.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Les modèles hiérarchiques bayésiens représentent une avancée majeure
dans l’analyse des données structurées. Leur capacité à intégrer des
niveaux de modélisation et à partager l’information entre clusters en
fait un outil puissant pour une variété d’applications. Les
développements récents en inférence bayésienne, tels que les méthodes de
Monte Carlo par chaînes de Markov (MCMC), ont rendu ces modèles encore
plus accessibles et performants.</p>
</body>
</html>
{% include "footer.html" %}


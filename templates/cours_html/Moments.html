{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Moments : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Moments : Théorie et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les moments, en mathématiques, jouent un rôle fondamental dans de
nombreuses disciplines, allant de la physique à la finance en passant
par l’ingénierie. Historiquement, le concept de moment a été introduit
pour décrire des quantités physiques telles que les moments d’inertie,
mais il a rapidement trouvé des applications en statistiques et en
probabilités. Les moments permettent de capturer les caractéristiques
essentielles d’une distribution de probabilité ou d’une fonction, telles
que sa moyenne, sa variance, et son asymétrie.</p>
<p>Dans ce chapitre, nous explorerons les moments d’une variable
aléatoire, leur définition, leurs propriétés, et leurs applications.
Nous verrons comment les moments peuvent être utilisés pour caractériser
une distribution de probabilité et comment ils sont liés à d’autres
concepts fondamentaux en statistique.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement les moments, considérons une variable
aléatoire <span class="math inline">\(X\)</span> et cherchons à capturer
ses caractéristiques moyennes. Intuitivement, le premier moment de <span
class="math inline">\(X\)</span> devrait nous donner une idée de sa
localisation centrale, tandis que les moments d’ordre supérieur
devraient nous informer sur la dispersion et l’asymétrie de sa
distribution.</p>
<h2 id="moment-dordre-n">Moment d’ordre <span
class="math inline">\(n\)</span></h2>
<p>Le moment d’ordre <span class="math inline">\(n\)</span> d’une
variable aléatoire <span class="math inline">\(X\)</span>, noté <span
class="math inline">\(\mathbb{E}[X^n]\)</span> ou <span
class="math inline">\(m_n\)</span>, est défini comme l’espérance
mathématique de <span class="math inline">\(X^n\)</span>. Formellement,
pour une variable aléatoire discrète prenant des valeurs <span
class="math inline">\(x_i\)</span> avec des probabilités <span
class="math inline">\(p_i\)</span>, le moment d’ordre <span
class="math inline">\(n\)</span> est donné par :</p>
<p><span class="math display">\[m_n = \mathbb{E}[X^n] = \sum_{i} x_i^n
p_i\]</span></p>
<p>Pour une variable aléatoire continue avec une densité de probabilité
<span class="math inline">\(f(x)\)</span>, le moment d’ordre <span
class="math inline">\(n\)</span> est défini par :</p>
<p><span class="math display">\[m_n = \mathbb{E}[X^n] =
\int_{-\infty}^{\infty} x^n f(x) \, dx\]</span></p>
<h2 id="moment-centré-dordre-n">Moment centré d’ordre <span
class="math inline">\(n\)</span></h2>
<p>Les moments centrés sont définis par rapport à la moyenne de la
variable aléatoire. Le moment centré d’ordre <span
class="math inline">\(n\)</span>, noté <span
class="math inline">\(\mu_n\)</span>, est donné par :</p>
<p><span class="math display">\[\mu_n = \mathbb{E}[(X -
\mathbb{E}[X])^n]\]</span></p>
<p>Pour une variable aléatoire discrète, cela s’écrit :</p>
<p><span class="math display">\[\mu_n = \sum_{i} (x_i - m_1)^n
p_i\]</span></p>
<p>Pour une variable aléatoire continue, cela s’écrit :</p>
<p><span class="math display">\[\mu_n = \int_{-\infty}^{\infty} (x -
m_1)^n f(x) \, dx\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-limite-centrale">Théorème de la limite
centrale</h2>
<p>Le théorème de la limite centrale est l’un des résultats les plus
importants en théorie des probabilités. Il stipule que la somme de
variables aléatoires indépendantes et identiquement distribuées, après
une normalisation appropriée, converge vers une distribution
normale.</p>
<p><strong>Théorème (Limite Centrale)</strong> : Soient <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des variables
aléatoires indépendantes et identiquement distribuées avec une espérance
<span class="math inline">\(\mu\)</span> et une variance <span
class="math inline">\(\sigma^2\)</span>. Alors, la variable aléatoire
normalisée</p>
<p><span class="math display">\[Z_n = \frac{\sum_{i=1}^n X_i -
n\mu}{\sqrt{n}\sigma}\]</span></p>
<p>converge en loi vers une variable aléatoire de loi normale standard
<span class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>
<h2 id="théorème-de-tchebyshev">Théorème de Tchebyshev</h2>
<p>Le théorème de Tchebyshev fournit une borne sur la probabilité qu’une
variable aléatoire s’éloigne de sa moyenne. Ce théorème est fondamental
en théorie des probabilités et en statistique.</p>
<p><strong>Théorème (Tchebyshev)</strong> : Soit <span
class="math inline">\(X\)</span> une variable aléatoire avec une
espérance <span class="math inline">\(\mu\)</span> et une variance <span
class="math inline">\(\sigma^2\)</span>. Alors, pour tout <span
class="math inline">\(k &gt; 0\)</span>,</p>
<p><span class="math display">\[\mathbb{P}(|X - \mu| \geq k\sigma) \leq
\frac{1}{k^2}\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-la-limite-centrale">Preuve du Théorème de
la Limite Centrale</h2>
<p>La preuve du théorème de la limite centrale repose sur le
développement de Taylor de la fonction caractéristique et l’utilisation
de la loi des grands nombres. Nous ne donnons ici qu’un aperçu de la
preuve.</p>
<p>Considérons les variables aléatoires <span
class="math inline">\(X_i\)</span> avec une fonction caractéristique
<span class="math inline">\(\phi(t) = \mathbb{E}[e^{itX}]\)</span>. La
fonction caractéristique de la somme normalisée est donnée par :</p>
<p><span class="math display">\[\phi_n(t) = \left(
\phi\left(\frac{t}{\sqrt{n}}\right) \right)^n\]</span></p>
<p>En utilisant le développement de Taylor de <span
class="math inline">\(\phi(t)\)</span> autour de 0, nous obtenons :</p>
<p><span class="math display">\[\phi(t) = 1 + it\mu -
\frac{t^2\sigma^2}{2} + o(t^2)\]</span></p>
<p>En substituant <span class="math inline">\(t\)</span> par <span
class="math inline">\(t/\sqrt{n}\)</span>, nous avons :</p>
<p><span class="math display">\[\phi\left(\frac{t}{\sqrt{n}}\right) = 1
+ i\frac{t\mu}{\sqrt{n}} - \frac{t^2\sigma^2}{2n} +
o\left(\frac{t^2}{n}\right)\]</span></p>
<p>En prenant la limite lorsque <span class="math inline">\(n\)</span>
tend vers l’infini, nous obtenons :</p>
<p><span class="math display">\[\lim_{n \to \infty} \phi_n(t) =
e^{-\frac{t^2\sigma^2}{2}}\]</span></p>
<p>qui est la fonction caractéristique d’une variable aléatoire de loi
normale standard <span
class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>
<h2 id="preuve-du-théorème-de-tchebyshev">Preuve du Théorème de
Tchebyshev</h2>
<p>La preuve du théorème de Tchebyshev repose sur l’inégalité de Markov
et la définition de la variance.</p>
<p>Soit <span class="math inline">\(Y = (X - \mu)^2\)</span>. Par
l’inégalité de Markov, nous avons :</p>
<p><span class="math display">\[\mathbb{P}(Y \geq k^2\sigma^2) \leq
\frac{\mathbb{E}[Y]}{k^2\sigma^2} = \frac{\sigma^2}{k^2\sigma^2} =
\frac{1}{k^2}\]</span></p>
<p>En remplaçant <span class="math inline">\(Y\)</span> par son
expression, nous obtenons :</p>
<p><span class="math display">\[\mathbb{P}((X - \mu)^2 \geq k^2\sigma^2)
\leq \frac{1}{k^2}\]</span></p>
<p>En prenant la racine carrée des deux côtés, nous obtenons le résultat
souhaité :</p>
<p><span class="math display">\[\mathbb{P}(|X - \mu| \geq k\sigma) \leq
\frac{1}{k^2}\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-des-moments">Propriétés des Moments</h2>
<ul>
<li><p>Le premier moment <span class="math inline">\(m_1\)</span> est
égal à l’espérance de la variable aléatoire <span
class="math inline">\(X\)</span>.</p></li>
<li><p>Le deuxième moment centré <span
class="math inline">\(\mu_2\)</span> est égal à la variance de <span
class="math inline">\(X\)</span>.</p></li>
<li><p>Les moments centrés peuvent être exprimés en termes des moments
non centrés et vice versa.</p></li>
</ul>
<h2 id="corollaires-du-théorème-de-tchebyshev">Corollaires du Théorème
de Tchebyshev</h2>
<ul>
<li><p>Pour toute variable aléatoire <span
class="math inline">\(X\)</span> avec une espérance <span
class="math inline">\(\mu\)</span> et une variance finie, la probabilité
que <span class="math inline">\(X\)</span> s’éloigne de sa moyenne est
bornée par <span class="math inline">\(1/k^2\)</span>.</p></li>
<li><p>Le théorème de Tchebyshev fournit une estimation grossière mais
universelle de la concentration des variables aléatoires autour de leur
moyenne.</p></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>Les moments sont des outils puissants pour analyser les
caractéristiques des distributions de probabilité. Ils permettent de
capturer des informations essentielles telles que la moyenne, la
variance, et l’asymétrie. Les théorèmes de la limite centrale et de
Tchebyshev illustrent l’importance des moments en théorie des
probabilités et en statistique. En comprenant les moments, nous pouvons
mieux appréhender le comportement des variables aléatoires et développer
des méthodes robustes pour l’analyse des données.</p>
</body>
</html>
{% include "footer.html" %}


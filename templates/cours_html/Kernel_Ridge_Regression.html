{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernel Ridge Regression: Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernel Ridge Regression: Théorie et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La régression Ridge, introduite par Hoerl et Kennard en 1970, est une
technique de régression linéaire qui introduit une pénalité sur les
coefficients pour éviter le sur-apprentissage. Cependant, cette méthode
suppose que les données sont linéairement séparables dans l’espace des
caractéristiques. Pour traiter des cas où cette hypothèse n’est pas
vérifiée, on introduit les noyaux (kernels), permettant de projeter les
données dans un espace de dimension supérieure où la séparation linéaire
devient possible.</p>
<p>La Kernel Ridge Regression (KRR) combine ainsi les avantages de la
régression Ridge et des méthodes à noyaux. Elle est particulièrement
utile dans les cas où le nombre de caractéristiques est grand par
rapport au nombre d’observations, ou lorsque les relations entre les
variables sont non-linéaires.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir la Kernel Ridge Regression, il est essentiel de
comprendre les concepts de base qui la composent.</p>
<h2 id="régression-ridge">Régression Ridge</h2>
<p>Considérons un ensemble de données <span
class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span> où <span
class="math inline">\(x_i \in \mathbb{R}^d\)</span> et <span
class="math inline">\(y_i \in \mathbb{R}\)</span>. La régression
linéaire classique cherche à minimiser la somme des carrés des erreurs
:</p>
<p><span class="math display">\[\min_w \sum_{i=1}^n (y_i - w^T
x_i)^2\]</span></p>
<p>Cependant, cette approche peut conduire à des coefficients <span
class="math inline">\(w\)</span> très grands, ce qui entraîne un
sur-apprentissage. Pour remédier à cela, la régression Ridge introduit
une pénalité sur les coefficients :</p>
<p><span class="math display">\[\min_w \sum_{i=1}^n (y_i - w^T x_i)^2 +
\lambda \|w\|_2^2\]</span></p>
<p>où <span class="math inline">\(\lambda &gt; 0\)</span> est un
paramètre de régularisation.</p>
<h2 id="noyaux-kernels">Noyaux (Kernels)</h2>
<p>Un noyau est une fonction symétrique <span class="math inline">\(k:
\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\)</span> telle
que pour tout <span class="math inline">\(x, x&#39; \in
\mathbb{R}^d\)</span>, on a :</p>
<p><span class="math display">\[k(x, x&#39;) = \langle \phi(x),
\phi(x&#39;) \rangle\]</span></p>
<p>où <span class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathcal{H}\)</span> est une fonction de projection dans un espace de
Hilbert <span class="math inline">\(\mathcal{H}\)</span>.</p>
<h2 id="kernel-ridge-regression">Kernel Ridge Regression</h2>
<p>La Kernel Ridge Regression généralise la régression Ridge en
utilisant un noyau. L’idée est de projeter les données dans un espace de
dimension supérieure où la séparation linéaire devient possible. La
fonction objectif devient alors :</p>
<p><span class="math display">\[\min_w \sum_{i=1}^n (y_i - w^T
\phi(x_i))^2 + \lambda \|w\|_2^2\]</span></p>
<p>où <span class="math inline">\(\phi\)</span> est une fonction de
projection définie par un noyau <span
class="math inline">\(k\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="formulation-duale">Formulation Duale</h2>
<p>Un des résultats clés de la Kernel Ridge Regression est sa
formulation duale. Nous allons montrer comment passer de la formulation
primale à la formulation duale.</p>
<div class="theorem">
<p>La solution optimale <span class="math inline">\(w^*\)</span> de la
Kernel Ridge Regression peut être exprimée en termes des coefficients
duals <span class="math inline">\(\alpha_i\)</span> comme suit :</p>
<p><span class="math display">\[w^* = \sum_{i=1}^n \alpha_i
\phi(x_i)\]</span></p>
<p>où les <span class="math inline">\(\alpha_i\)</span> satisfont le
système d’équations :</p>
<p><span class="math display">\[(\lambda I + K) \alpha = y\]</span></p>
<p>avec <span class="math inline">\(K\)</span> la matrice de Gram
définie par <span class="math inline">\(K_{ij} = k(x_i,
x_j)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour dériver la formulation duale, nous utilisons le
formalisme de l’optimisation convexe. La fonction objectif de la KRR est
:</p>
<p><span class="math display">\[\min_w \sum_{i=1}^n (y_i - w^T
\phi(x_i))^2 + \lambda \|w\|_2^2\]</span></p>
<p>En introduisant les variables duales <span
class="math inline">\(\alpha_i\)</span>, nous pouvons écrire la fonction
de Lagrange :</p>
<p><span class="math display">\[\mathcal{L}(w, \alpha) = \sum_{i=1}^n
(y_i - w^T \phi(x_i))^2 + \lambda \|w\|_2^2 + \sum_{i=1}^n \alpha_i (w^T
\phi(x_i) - y_i)\]</span></p>
<p>En prenant les dérivées partielles par rapport à <span
class="math inline">\(w\)</span> et en les mettant à zéro, nous obtenons
:</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial w}
= -2 \sum_{i=1}^n (y_i - w^T \phi(x_i)) \phi(x_i) + 2 \lambda w +
\sum_{i=1}^n \alpha_i \phi(x_i) = 0\]</span></p>
<p>En simplifiant, nous trouvons :</p>
<p><span class="math display">\[w = \frac{1}{\lambda} \sum_{i=1}^n (y_i
- w^T \phi(x_i) + \frac{\alpha_i}{2}) \phi(x_i)\]</span></p>
<p>En substituant cette expression dans la fonction objectif, nous
obtenons un problème d’optimisation quadratique en <span
class="math inline">\(\alpha_i\)</span> :</p>
<p><span class="math display">\[\min_\alpha \sum_{i=1}^n y_i^2 - 2
\sum_{i=1}^n y_i \alpha_i + \alpha^T K \alpha + \lambda \sum_{i=1}^n
\left( \frac{\alpha_i}{2} - y_i + w^T \phi(x_i) \right)^2\]</span></p>
<p>En résolvant ce problème, nous obtenons le système d’équations :</p>
<p><span class="math display">\[(\lambda I + K) \alpha = y\]</span></p>
<p>où <span class="math inline">\(K\)</span> est la matrice de
Gram. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-régularisation">Propriété de Régularisation</h2>
<p>La Kernel Ridge Regression possède une propriété de régularisation
qui permet d’éviter le sur-apprentissage.</p>
<div class="corollary">
<p>Pour tout <span class="math inline">\(\lambda &gt; 0\)</span>, la
solution optimale <span class="math inline">\(w^*\)</span> de la Kernel
Ridge Regression satisfait :</p>
<p><span class="math display">\[\|w^*\|_2 \leq
\frac{\|y\|_2}{\lambda}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant la formulation duale, nous avons :</p>
<p><span class="math display">\[w^* = \sum_{i=1}^n \alpha_i
\phi(x_i)\]</span></p>
<p>où <span class="math inline">\(\alpha\)</span> satisfait <span
class="math inline">\((\lambda I + K) \alpha = y\)</span>. En prenant la
norme <span class="math inline">\(L_2\)</span> des deux côtés, nous
obtenons :</p>
<p><span class="math display">\[\|w^*\|_2 = \left\| \sum_{i=1}^n
\alpha_i \phi(x_i) \right\|_2 \leq \sum_{i=1}^n |\alpha_i|
\|\phi(x_i)\|_2\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons :</p>
<p><span class="math display">\[\sum_{i=1}^n |\alpha_i| \|\phi(x_i)\|_2
\leq \left( \sum_{i=1}^n \alpha_i^2 \right)^{1/2} \left( \sum_{i=1}^n
\|\phi(x_i)\|_2^2 \right)^{1/2}\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(\alpha =
(\lambda I + K)^{-1} y\)</span>, nous obtenons :</p>
<p><span class="math display">\[\sum_{i=1}^n \alpha_i^2 = \|(\lambda I +
K)^{-1} y\|_2^2 \leq \frac{\|y\|_2^2}{\lambda^2}\]</span></p>
<p>En combinant ces résultats, nous obtenons :</p>
<p><span class="math display">\[\|w^*\|_2 \leq
\frac{\|y\|_2}{\lambda}\]</span> ◻</p>
</div>
<h2 id="generalisation-des-noyaux">Generalisation des Noyaux</h2>
<p>La Kernel Ridge Regression peut être généralisée à différents types
de noyaux.</p>
<div class="corollary">
<p>Pour tout noyau <span class="math inline">\(k\)</span> positif
défini, la Kernel Ridge Regression est bien définie et possède une
solution unique.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La matrice de Gram <span
class="math inline">\(K\)</span> associée à un noyau positif défini est
également positive définie. Par conséquent, la matrice <span
class="math inline">\(\lambda I + K\)</span> est inversible pour tout
<span class="math inline">\(\lambda &gt; 0\)</span>. Ainsi, le système
d’équations <span class="math inline">\((\lambda I + K) \alpha =
y\)</span> possède une solution unique. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La Kernel Ridge Regression est une méthode puissante pour la
régression non-linéaire qui combine les avantages de la régression Ridge
et des méthodes à noyaux. Elle permet d’éviter le sur-apprentissage tout
en capturant les relations non-linéaires entre les variables. Les
propriétés de régularisation et la généralisation des noyaux en font un
outil précieux pour l’analyse de données.</p>
</body>
</html>
{% include "footer.html" %}


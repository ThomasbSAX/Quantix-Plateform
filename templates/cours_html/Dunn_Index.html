{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Indice de Dunn : Une Mesure de Validité des Groupes</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Indice de Dunn : Une Mesure de Validité des
Groupes</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’analyse des données est une discipline centrale en statistique,
visant à extraire des informations significatives de ensembles de
données souvent complexes. Parmi les méthodes d’analyse, le clustering
ou classification non supervisée joue un rôle prépondérant. L’objectif
est de regrouper des objets similaires tout en séparant ceux qui sont
différents.</p>
<p>Cependant, évaluer la qualité d’un clustering reste un défi majeur.
L’indice de Dunn est une mesure de validité interne qui répond à ce
besoin. Introduit par James C. Dunn en 1973, cet indice cherche à
quantifier la séparation entre les groupes et la cohésion interne des
clusters.</p>
<p>Cet article explore en détail l’indice de Dunn, ses définitions, ses
propriétés et ses applications. Nous commencerons par une définition
rigoureuse de l’indice, suivie d’une analyse approfondie de ses
caractéristiques et de ses implications.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’indice de Dunn, il est essentiel de définir les
concepts fondamentaux de distance et de diamètre.</p>
<h2 id="distance-entre-deux-points">Distance entre deux points</h2>
<p>Soit <span class="math inline">\((E, d)\)</span> un espace métrique
où <span class="math inline">\(E\)</span> est un ensemble et <span
class="math inline">\(d: E \times E \rightarrow \mathbb{R}^+\)</span>
une fonction de distance vérifiant les propriétés suivantes pour tous
<span class="math inline">\(x, y, z \in E\)</span>:</p>
<ol>
<li><p><span class="math inline">\(d(x, y) = 0\)</span> si et seulement
si <span class="math inline">\(x = y\)</span></p></li>
<li><p><span class="math inline">\(d(x, y) = d(y, x)\)</span></p></li>
<li><p><span class="math inline">\(d(x, z) \leq d(x, y) + d(y,
z)\)</span></p></li>
</ol>
<p>La distance <span class="math inline">\(d\)</span> permet de mesurer
la dissimilarité entre deux points.</p>
<h2 id="diamètre-dun-ensemble">Diamètre d’un ensemble</h2>
<p>Le diamètre d’un sous-ensemble <span class="math inline">\(A
\subseteq E\)</span> est défini comme la plus grande distance entre deux
points de <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[\text{diam}(A) = \sup_{x, y \in A} d(x,
y)\]</span></p>
<h2 id="indice-de-dunn">Indice de Dunn</h2>
<p>Considérons un ensemble <span class="math inline">\(X\)</span> et une
partition <span class="math inline">\(C = \{C_1, C_2, \ldots,
C_k\}\)</span> de <span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. L’indice de Dunn est défini
comme le rapport entre la plus petite distance inter-clusters et la plus
grande distance intra-clusters:</p>
<p><span class="math display">\[D(C) = \min_{1 \leq i &lt; j \leq k}
\left( \frac{\min_{x \in C_i, y \in C_j} d(x, y)}{\max_{1 \leq l \leq k}
\text{diam}(C_l)} \right)\]</span></p>
<p>En d’autres termes, l’indice de Dunn mesure la séparation entre les
clusters par rapport à leur cohésion interne.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="propriétés-de-lindice-de-dunn">Propriétés de l’indice de
Dunn</h2>
<p>L’indice de Dunn possède plusieurs propriétés importantes qui en font
un outil puissant pour évaluer la qualité d’un clustering.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> une partition de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Si <span
class="math inline">\(D(C) &gt; 1\)</span>, alors la partition <span
class="math inline">\(C\)</span> est dite séparée.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(D(C) &gt;
1\)</span>. Par définition, il existe des clusters <span
class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> tels que:</p>
<p><span class="math display">\[\frac{\min_{x \in C_i, y \in C_j} d(x,
y)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} &gt; 1\]</span></p>
<p>Cela implique que:</p>
<p><span class="math display">\[\min_{x \in C_i, y \in C_j} d(x, y) &gt;
\max_{1 \leq l \leq k} \text{diam}(C_l)\]</span></p>
<p>Ainsi, la distance minimale entre les clusters <span
class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> est supérieure au diamètre maximal
des clusters, ce qui signifie que les clusters sont bien séparés. ◻</p>
</div>
<h2 id="propriété-de-cohésion">Propriété de cohésion</h2>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> une partition de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Si <span
class="math inline">\(D(C) = 0\)</span>, alors au moins deux clusters
sont confondus.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(D(C) =
0\)</span>. Cela signifie qu’il existe des clusters <span
class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> tels que:</p>
<p><span class="math display">\[\frac{\min_{x \in C_i, y \in C_j} d(x,
y)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} = 0\]</span></p>
<p>Puisque le dénominateur est strictement positif, cela implique
que:</p>
<p><span class="math display">\[\min_{x \in C_i, y \in C_j} d(x, y) =
0\]</span></p>
<p>Par conséquent, il existe des points <span class="math inline">\(x
\in C_i\)</span> et <span class="math inline">\(y \in C_j\)</span> tels
que <span class="math inline">\(d(x, y) = 0\)</span>, ce qui signifie
que <span class="math inline">\(x = y\)</span>. Ainsi, les clusters
<span class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> sont confondus. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-la-propriété-de-séparation">Preuve de la propriété de
séparation</h2>
<p>Pour démontrer que <span class="math inline">\(D(C) &gt; 1\)</span>
implique une partition séparée, nous avons besoin de montrer que la
distance minimale entre les clusters est supérieure au diamètre maximal
des clusters.</p>
<p>Soit <span class="math inline">\(C\)</span> une partition de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Supposons que <span
class="math inline">\(D(C) &gt; 1\)</span>. Par définition, il existe
des clusters <span class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> tels que:</p>
<p><span class="math display">\[\frac{\min_{x \in C_i, y \in C_j} d(x,
y)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} &gt; 1\]</span></p>
<p>En multipliant les deux côtés de l’inégalité par <span
class="math inline">\(\max_{1 \leq l \leq k} \text{diam}(C_l)\)</span>,
nous obtenons:</p>
<p><span class="math display">\[\min_{x \in C_i, y \in C_j} d(x, y) &gt;
\max_{1 \leq l \leq k} \text{diam}(C_l)\]</span></p>
<p>Ainsi, la distance minimale entre les clusters <span
class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> est supérieure au diamètre maximal
des clusters, ce qui signifie que les clusters sont bien séparés.</p>
<h2 id="preuve-de-la-propriété-de-cohésion">Preuve de la propriété de
cohésion</h2>
<p>Pour démontrer que <span class="math inline">\(D(C) = 0\)</span>
implique une partition non séparée, nous avons besoin de montrer qu’au
moins deux clusters sont confondus.</p>
<p>Soit <span class="math inline">\(C\)</span> une partition de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Supposons que <span
class="math inline">\(D(C) = 0\)</span>. Cela signifie qu’il existe des
clusters <span class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> tels que:</p>
<p><span class="math display">\[\frac{\min_{x \in C_i, y \in C_j} d(x,
y)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} = 0\]</span></p>
<p>Puisque le dénominateur est strictement positif, cela implique
que:</p>
<p><span class="math display">\[\min_{x \in C_i, y \in C_j} d(x, y) =
0\]</span></p>
<p>Par conséquent, il existe des points <span class="math inline">\(x
\in C_i\)</span> et <span class="math inline">\(y \in C_j\)</span> tels
que <span class="math inline">\(d(x, y) = 0\)</span>, ce qui signifie
que <span class="math inline">\(x = y\)</span>. Ainsi, les clusters
<span class="math inline">\(C_i\)</span> et <span
class="math inline">\(C_j\)</span> sont confondus.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-monotonicité">Propriété de monotonicité</h2>
<ol>
<li><p>L’indice de Dunn est une fonction croissante de la distance
inter-clusters.</p></li>
<li><p>L’indice de Dunn est une fonction décroissante du diamètre
intra-clusters.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ul>
<li><p>Soit <span class="math inline">\(C\)</span> une partition de
<span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Supposons que la distance
inter-clusters augmente. Par définition, l’indice de Dunn est le rapport
entre la plus petite distance inter-clusters et la plus grande distance
intra-clusters. Ainsi, une augmentation de la distance inter-clusters
entraîne une augmentation de l’indice de Dunn.</p></li>
<li><p>Soit <span class="math inline">\(C\)</span> une partition de
<span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Supposons que le diamètre
intra-clusters augmente. Par définition, l’indice de Dunn est le rapport
entre la plus petite distance inter-clusters et la plus grande distance
intra-clusters. Ainsi, une augmentation du diamètre intra-clusters
entraîne une diminution de l’indice de Dunn.</p></li>
</ul>
<p> ◻</p>
</div>
<h2 id="propriété-dinvariance">Propriété d’invariance</h2>
<ol>
<li><p>L’indice de Dunn est invariant par translation.</p></li>
<li><p>L’indice de Dunn est invariant par rotation.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ul>
<li><p>Soit <span class="math inline">\(C\)</span> une partition de
<span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Une translation ne modifie
pas les distances entre les points. Par conséquent, l’indice de Dunn
reste inchangé.</p></li>
<li><p>Soit <span class="math inline">\(C\)</span> une partition de
<span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> clusters. Une rotation ne modifie pas
les distances entre les points. Par conséquent, l’indice de Dunn reste
inchangé.</p></li>
</ul>
<p> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’indice de Dunn est un outil puissant pour évaluer la qualité d’un
clustering. En mesurant la séparation entre les clusters par rapport à
leur cohésion interne, il permet de déterminer si une partition est bien
formée. Les propriétés et théorèmes présentés dans cet article montrent
que l’indice de Dunn est un outil robuste et fiable pour l’analyse des
données.</p>
<p>Cependant, il est important de noter que l’indice de Dunn a ses
limites. Par exemple, il peut être sensible aux clusters de tailles
différentes ou aux clusters non sphériques. Malgré ces limitations,
l’indice de Dunn reste un outil précieux pour les chercheurs et les
praticiens en analyse des données.</p>
</body>
</html>
{% include "footer.html" %}


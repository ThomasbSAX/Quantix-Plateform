{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Corrélation linéaire : Mesure de la dépendance statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Corrélation linéaire : Mesure de la dépendance
statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse des relations entre variables quantitatives constitue un
pilier fondamental en statistique. La corrélation linéaire émerge comme
outil indispensable pour quantifier l’intensité et la direction de
telles relations. Son origine remonte aux travaux pionniers de Francis
Galton (1822-1911) sur l’hérédité, où il observe que les descendants de
parents exceptionnellement grands ou petits tendent à se rapprocher de
la moyenne. Karl Pearson (1857-1936) formalise ensuite cette notion par
le coefficient de corrélation, ouvrant la voie à l’analyse multivariée
moderne.</p>
<p>La corrélation linéaire répond à un besoin crucial : mesurer
l’étendue selon laquelle deux variables varient conjointement de manière
proportionnelle. Elle est indispensable dans des domaines aussi variés
que l’économie, la biologie ou les sciences sociales, où comprendre les
interconnexions entre phénomènes complexes est primordial. Son
importance réside dans sa capacité à révéler des structures cachées dans
les données, tout en fournissant une base pour la prédiction et
l’inférence statistique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Considérons deux variables aléatoires <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Nous cherchons à quantifier dans
quelle mesure une variation de <span class="math inline">\(X\)</span>
s’accompagne d’une variation proportionnelle de <span
class="math inline">\(Y\)</span>. Intuitivement, nous souhaitons
capturer l’alignement des points <span
class="math inline">\((X_i,Y_i)\)</span> autour d’une droite de
régression linéaire.</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires admettant des
moments d’ordre 2. Le coefficient de corrélation linéaire de Pearson est
défini par : <span class="math display">\[\rho(X,Y) =
\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}\]</span> où
<span class="math inline">\(\text{Cov}(X,Y) =
\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]\)</span> est la
covariance, et <span class="math inline">\(\text{Var}(X) =
\mathbb{E}[(X-\mathbb{E}[X])^2]\)</span> est la variance.</p>
<p>De manière équivalente : <span class="math display">\[\rho(X,Y) =
\frac{\mathbb{E}[XY] -
\mathbb{E}[X]\mathbb{E}[Y]}{\sqrt{(\mathbb{E}[X^2] -
\mathbb{E}[X]^2)(\mathbb{E}[Y^2] - \mathbb{E}[Y]^2)}}\]</span></p>
</div>
<p>Pour des échantillons finis <span
class="math inline">\((x_1,y_1),\ldots,(x_n,y_n)\)</span>, le
coefficient d’échantillonnage est donné par : <span
class="math display">\[r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i -
\bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i -
\bar{y})^2}}\]</span> où <span class="math inline">\(\bar{x} =
\frac{1}{n}\sum_{i=1}^n x_i\)</span> et <span
class="math inline">\(\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i\)</span>
sont les moyennes empiriques.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires. Alors :</p>
<ol>
<li><p><span class="math inline">\(\rho(X,Y) \in
[-1,1]\)</span></p></li>
<li><p><span class="math inline">\(\rho(X,Y) = 1\)</span> si et
seulement si <span class="math inline">\(Y = aX + b\)</span> avec <span
class="math inline">\(a &gt; 0\)</span></p></li>
<li><p><span class="math inline">\(\rho(X,Y) = -1\)</span> si et
seulement si <span class="math inline">\(Y = aX + b\)</span> avec <span
class="math inline">\(a &lt; 0\)</span></p></li>
<li><p><span class="math inline">\(\rho(X,Y) = 0\)</span> si et
seulement si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont non corrélés, c’est-à-dire <span
class="math inline">\(\text{Cov}(X,Y) = 0\)</span></p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> Pour (1), nous utilisons l’inégalité de
Cauchy-Schwarz : <span class="math display">\[|\text{Cov}(X,Y)| \leq
\sqrt{\text{Var}(X)\text{Var}(Y)}\]</span> ce qui implique directement
<span class="math inline">\(\rho(X,Y) \in [-1,1]\)</span>.</p>
<p>Pour (2), supposons <span class="math inline">\(Y = aX + b\)</span>
avec <span class="math inline">\(a &gt; 0\)</span>. Alors : <span
class="math display">\[\rho(X,Y) =
\frac{\text{Cov}(X,aX+b)}{\sqrt{\text{Var}(X)\text{Var}(aX+b)}} =
\frac{a\text{Var}(X)}{\sqrt{a^2\text{Var}(X)^2}} = 1\]</span> La
réciproque suit de la définition et du fait que <span
class="math inline">\(\rho(X,Y) = 1\)</span> implique une relation
linéaire parfaite.</p>
<p>Le cas (3) est similaire avec <span class="math inline">\(a &lt;
0\)</span>. Pour (4), <span class="math inline">\(\rho(X,Y) = 0\)</span>
équivaut à <span class="math inline">\(\text{Cov}(X,Y) = 0\)</span>, par
définition. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Considérons la preuve détaillée de l’inégalité de Cauchy-Schwarz
utilisée dans le théorème précédent :</p>
<div class="proof">
<p><em>Preuve de l’inégalité de Cauchy-Schwarz.</em> Soient <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires. Nous voulons
montrer : <span class="math display">\[\mathbb{E}[|XY|] \leq
\sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}\]</span> Considérons la variance
de <span class="math inline">\(X - \lambda Y\)</span> pour <span
class="math inline">\(\lambda \in \mathbb{R}\)</span> : <span
class="math display">\[\text{Var}(X - \lambda Y) = \mathbb{E}[(X -
\lambda Y)^2] - (\mathbb{E}[X - \lambda Y])^2 = \mathbb{E}[X^2] -
2\lambda\mathbb{E}[XY] + \lambda^2\mathbb{E}[Y^2] - (\mathbb{E}[X] -
\lambda\mathbb{E}[Y])^2\]</span> En développant le terme quadratique :
<span class="math display">\[\text{Var}(X - \lambda Y) = \mathbb{E}[X^2]
- 2\lambda\mathbb{E}[XY] + \lambda^2\mathbb{E}[Y^2] - \mathbb{E}[X]^2 +
2\lambda\mathbb{E}[X]\mathbb{E}[Y] - \lambda^2\mathbb{E}[Y]^2\]</span>
Nous pouvons réécrire : <span class="math display">\[\text{Var}(X -
\lambda Y) = (\mathbb{E}[X^2] - \mathbb{E}[X]^2) +
\lambda^2(\mathbb{E}[Y^2] - \mathbb{E}[Y]^2) + 2\lambda(\mathbb{E}[XY] -
\mathbb{E}[X]\mathbb{E}[Y])\]</span> Comme la variance est toujours non
négative, le discriminant de ce polynôme en <span
class="math inline">\(\lambda\)</span> doit être négatif ou nul : <span
class="math display">\[(\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y])^2 -
(\mathbb{E}[X^2] - \mathbb{E}[X]^2)(\mathbb{E}[Y^2] - \mathbb{E}[Y]^2)
\leq 0\]</span> Ce qui donne précisément l’inégalité de Cauchy-Schwarz :
<span class="math display">\[|\text{Cov}(X,Y)| \leq
\sqrt{\text{Var}(X)\text{Var}(Y)}\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<div class="corollary">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires, et <span
class="math inline">\(a,b,c,d \in \mathbb{R}\)</span> avec <span
class="math inline">\(ad \neq 0\)</span>. Alors : <span
class="math display">\[\rho(aX + b, cY + d) =
\text{sgn}(ad)\rho(X,Y)\]</span> où <span
class="math inline">\(\text{sgn}\)</span> est la fonction signe.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par linéarité de l’espérance et de la covariance :
<span class="math display">\[\rho(aX + b, cY + d) = \frac{\text{Cov}(aX
+ b, cY + d)}{\sqrt{\text{Var}(aX + b)\text{Var}(cY + d)}} =
\frac{ac\text{Cov}(X,Y)}{\sqrt{a^2\text{Var}(X)c^2\text{Var}(Y)}} =
\text{sgn}(ad)\rho(X,Y)\]</span> ◻</p>
</div>
<div class="corollary">
<p>Si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors <span
class="math inline">\(\rho(X,Y) = 0\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’indépendance implique <span
class="math inline">\(\mathbb{E}[XY] =
\mathbb{E}[X]\mathbb{E}[Y]\)</span>, donc : <span
class="math display">\[\rho(X,Y) = \frac{\mathbb{E}[XY] -
\mathbb{E}[X]\mathbb{E}[Y]}{\sqrt{(\mathbb{E}[X^2] -
\mathbb{E}[X]^2)(\mathbb{E}[Y^2] - \mathbb{E}[Y]^2)}} = 0\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La corrélation linéaire de Pearson constitue un outil fondamental
pour l’analyse des relations entre variables quantitatives. Son
interprétation géométrique en termes d’alignement de points autour d’une
droite, combinée à ses propriétés mathématiques robustes, en fait un
instrument indispensable dans de nombreux domaines scientifiques.
Cependant, il est crucial de se rappeler que la corrélation ne saurait
impliquer la causalité, et qu’une valeur proche de zéro n’exclut pas
nécessairement une relation non linéaire entre les variables. Les
développements récents en analyse de données, notamment l’étude des
corrélations partielles et des graphes bayésiens, ouvrent de nouvelles
perspectives pour l’exploration des structures complexes dans les
ensembles de données multidimensionnels.</p>
</body>
</html>
{% include "footer.html" %}


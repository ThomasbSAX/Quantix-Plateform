{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Dice Loss: A Metric for Medical Image Segmentation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Dice Loss: A Metric for Medical Image
Segmentation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">1. Introduction
et Motivations</h1>
<p>L’émergence de l’apprentissage profond dans le domaine de la
segmentation d’images médicales a soulevé des défis uniques en matière
de métriques d’évaluation. Les pertes traditionnelles telles que la
perte de log-vraisemblance ou la perte de Bhattacharyya ne capturent pas
toujours les nuances des données médicales, souvent caractérisées par
des classes déséquilibrées et des structures complexes. Le Dice Loss,
inspiré de la mesure de similarité de Sorensen-Dice, a été introduit
pour répondre à ces défis. Cette métrique offre une évaluation robuste
de la superposition entre les prédictions du modèle et les annotations
de référence, en particulier dans des contextes où la précision est
cruciale.</p>
<h1 class="unnumbered" id="définitions">2. Définitions</h1>
<p>Pour comprendre le Dice Loss, il est essentiel de saisir d’abord la
mesure de similarité de Sorensen-Dice. Imaginons que nous avons deux
ensembles, l’un représentant la prédiction d’un modèle et l’autre les
annotations de référence. Nous cherchons une mesure qui quantifie à quel
point ces deux ensembles se chevauchent.</p>
<div class="definition">
<p>Soient <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux ensembles. La mesure de similarité
de Sorensen-Dice est définie comme: <span class="math display">\[D(A, B)
= \frac{2 |A \cap B|}{|A| + |B|}\]</span> où <span
class="math inline">\(|A|\)</span> et <span
class="math inline">\(|B|\)</span> représentent les cardinalités des
ensembles <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>, respectivement, et <span
class="math inline">\(|A \cap B|\)</span> représente la cardinalité de
leur intersection.</p>
</div>
<p>Le Dice Loss est alors défini comme une transformation de cette
mesure de similarité. Nous cherchons à minimiser la distance entre les
prédictions et les annotations, ce qui revient à maximiser le Dice
coefficient.</p>
<div class="definition">
<p>Soient <span class="math inline">\(y\)</span> et <span
class="math inline">\(\hat{y}\)</span> les vecteurs binaires
représentant respectivement les annotations de référence et les
prédictions du modèle. Le Dice Loss est défini comme: <span
class="math display">\[\mathcal{L}_{Dice}(y, \hat{y}) = 1 - D(y,
\hat{y})\]</span> où <span class="math inline">\(D(y, \hat{y})\)</span>
est le Dice coefficient défini par: <span class="math display">\[D(y,
\hat{y}) = \frac{2 \sum_{i=1}^{n} y_i \hat{y}_i}{\sum_{i=1}^{n} y_i +
\sum_{i=1}^{n} \hat{y}_i}\]</span> avec <span
class="math inline">\(n\)</span> le nombre total de pixels ou voxels
dans l’image.</p>
</div>
<h1 class="unnumbered" id="théorèmes">3. Théorèmes</h1>
<p>Le Dice Loss présente plusieurs propriétés intéressantes qui le
rendent particulièrement adapté à la segmentation d’images médicales.
Considérons par exemple sa relation avec les pertes traditionnelles.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{L}_{Dice}(y,
\hat{y})\)</span> le Dice Loss et <span
class="math inline">\(\mathcal{L}_{LogLoss}(y, \hat{y})\)</span> la
perte de log-vraisemblance. Alors: <span
class="math display">\[\mathcal{L}_{Dice}(y, \hat{y}) = 1 - \frac{2
\sum_{i=1}^{n} y_i \hat{y}_i}{\sum_{i=1}^{n} y_i + \sum_{i=1}^{n}
\hat{y}_i}\]</span> et <span
class="math display">\[\mathcal{L}_{LogLoss}(y, \hat{y}) = -\frac{1}{n}
\sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 -
\hat{y}_i) \right)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la définition
même du Dice Loss et de la perte de log-vraisemblance. Le Dice Loss
mesure directement le chevauchement entre les prédictions et les
annotations, tandis que la perte de log-vraisemblance évalue la
probabilité des prédictions. Bien qu’ils mesurent des aspects
différents, ils sont tous deux utilisés pour optimiser les modèles de
segmentation. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">4. Preuves</h1>
<p>Pour illustrer l’utilisation du Dice Loss, considérons un exemple
simple où nous avons une image binaire de taille <span
class="math inline">\(n \times n\)</span>. Nous voulons montrer comment
le Dice Loss peut être calculé et optimisé.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que nous avons une image de référence <span
class="math inline">\(y\)</span> et une prédiction <span
class="math inline">\(\hat{y}\)</span>. Le Dice coefficient est calculé
comme suit: <span class="math display">\[D(y, \hat{y}) = \frac{2
\sum_{i=1}^{n^2} y_i \hat{y}_i}{\sum_{i=1}^{n^2} y_i + \sum_{i=1}^{n^2}
\hat{y}_i}\]</span> Le Dice Loss est alors: <span
class="math display">\[\mathcal{L}_{Dice}(y, \hat{y}) = 1 - D(y,
\hat{y})\]</span> Pour optimiser ce loss, nous devons maximiser le Dice
coefficient. Cela peut être fait en ajustant les paramètres du modèle
pour augmenter le chevauchement entre <span
class="math inline">\(y\)</span> et <span
class="math inline">\(\hat{y}\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">5. Propriétés et
Corollaires</h1>
<p>Le Dice Loss présente plusieurs propriétés qui le rendent
particulièrement utile dans le contexte de la segmentation d’images
médicales.</p>
<ol>
<li><p><strong>Invariance à l’échelle</strong>: Le Dice Loss est
invariant à l’échelle des prédictions et des annotations. Cela signifie
que la taille de l’image n’affecte pas le calcul du Dice Loss.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que nous avons une image de référence <span
class="math inline">\(y\)</span> et une prédiction <span
class="math inline">\(\hat{y}\)</span>. Si nous multiplions les deux par
un facteur <span class="math inline">\(k\)</span>, le Dice coefficient
reste inchangé: <span class="math display">\[D(k y, k \hat{y}) = \frac{2
\sum_{i=1}^{n} (k y_i)(k \hat{y}_i)}{\sum_{i=1}^{n} k y_i +
\sum_{i=1}^{n} k \hat{y}_i} = \frac{2 k^2 \sum_{i=1}^{n} y_i
\hat{y}_i}{k \sum_{i=1}^{n} y_i + k \sum_{i=1}^{n} \hat{y}_i} = D(y,
\hat{y})\]</span> ◻</p>
</div></li>
<li><p><strong>Sensibilité aux classes déséquilibrées</strong>: Le Dice
Loss est particulièrement sensible aux classes déséquilibrées, ce qui le
rend adapté aux données médicales où les structures d’intérêt sont
souvent minoritaires.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une image où la classe d’intérêt
représente seulement 1% des pixels. Le Dice Loss donnera une évaluation
plus précise de la performance du modèle que les pertes traditionnelles,
car il prend en compte le chevauchement direct entre les prédictions et
les annotations. ◻</p>
</div></li>
<li><p><strong>Robustesse aux bruits</strong>: Le Dice Loss est robuste
aux bruits et aux artefacts, ce qui est crucial dans le contexte des
images médicales où les données peuvent être bruitées.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que nous avons une image de référence <span
class="math inline">\(y\)</span> et une prédiction <span
class="math inline">\(\hat{y}\)</span> avec des bruits ajoutés. Le Dice
Loss reste une mesure fiable de la performance du modèle, car il se
concentre sur le chevauchement global plutôt que sur les détails
locaux. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le Dice Loss est une métrique puissante pour la segmentation d’images
médicales, offrant une évaluation robuste et précise des performances
des modèles. Son utilisation continue de croître dans le domaine de
l’apprentissage profond, en particulier pour les applications médicales
où la précision et la fiabilité sont cruciales.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par centralité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par centralité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
fondamentale en apprentissage automatique, permettant de transformer des
données brutes en représentations numériques exploitables par les
algorithmes. Parmi les méthodes d’extraction de caractéristiques, le
binning par centralité se distingue par sa simplicité et son efficacité.
Cette technique consiste à diviser les données en intervalles (bins) et
à encoder chaque intervalle par une statistique de centralité, telle que
la moyenne ou la médiane.</p>
<p>L’origine du binning remonte aux premières méthodes de discrétisation
des données, utilisées pour simplifier l’analyse statistique. Le binning
par centralité a été popularisé avec le développement des méthodes
d’apprentissage automatique, où la transformation des données en
caractéristiques numériques est cruciale pour l’efficacité des modèles.
Cette technique est indispensable dans de nombreux domaines, tels que la
finance, la médecine et l’ingénierie, où la compréhension des
distributions de données est essentielle.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement l’encodage par extraction de
caractéristiques de binning par centralité, il est important de
comprendre ce que nous cherchons à accomplir. Nous voulons transformer
une variable continue en une représentation discrète qui capture les
caractéristiques essentielles de la distribution des données. Cela
permet de réduire la complexité des modèles tout en préservant les
informations importantes.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue définie sur un ensemble de données <span
class="math inline">\(D = \{x_1, x_2, \dots, x_n\}\)</span>. Un binning
par centralité consiste à diviser l’intervalle des valeurs de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> sous-intervalles disjoints, appelés
bins, et à encoder chaque bin par une statistique de centralité <span
class="math inline">\(c_i\)</span>.</p>
<p>Formellement, pour chaque bin <span
class="math inline">\(B_i\)</span> où <span class="math inline">\(i \in
\{1, 2, \dots, k\}\)</span>, nous avons : <span
class="math display">\[B_i = [a_i, b_i)\]</span> où <span
class="math inline">\(a_i\)</span> et <span
class="math inline">\(b_i\)</span> sont les bornes du bin, et la
statistique de centralité <span class="math inline">\(c_i\)</span> est
définie par : <span class="math display">\[c_i = \frac{1}{|B_i|}
\sum_{x_j \in B_i} x_j\]</span> où <span
class="math inline">\(|B_i|\)</span> est le nombre d’éléments dans le
bin <span class="math inline">\(B_i\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’impact du binning par centralité sur la performance
des modèles d’apprentissage automatique, nous devons examiner certains
théorèmes clés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue et <span class="math inline">\(B\)</span> un binning par
centralité de <span class="math inline">\(X\)</span>. Si <span
class="math inline">\(c_i\)</span> est la statistique de centralité du
bin <span class="math inline">\(B_i\)</span>, alors la variance de <span
class="math inline">\(X\)</span> est réduite par le binning.</p>
<p>Formellement, nous avons : <span class="math display">\[\text{Var}(X)
\geq \sum_{i=1}^k p_i \text{Var}(c_i)\]</span> où <span
class="math inline">\(p_i\)</span> est la probabilité que <span
class="math inline">\(X\)</span> appartienne au bin <span
class="math inline">\(B_i\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la réduction de variance, nous devons
examiner comment le binning affecte la variance de <span
class="math inline">\(X\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la variance de <span
class="math inline">\(X\)</span> : <span
class="math display">\[\text{Var}(X) = E[(X - E[X])^2]\]</span></p>
<p>En utilisant le binning par centralité, nous remplaçons chaque valeur
<span class="math inline">\(x_j\)</span> dans le bin <span
class="math inline">\(B_i\)</span> par la statistique de centralité
<span class="math inline">\(c_i\)</span>. La variance de <span
class="math inline">\(X\)</span> devient : <span
class="math display">\[\text{Var}(X) = E[(c_i - E[X])^2]\]</span></p>
<p>En développant cette expression, nous obtenons : <span
class="math display">\[\text{Var}(X) = E[c_i^2] - (E[X])^2\]</span></p>
<p>En utilisant la loi des grands nombres, nous savons que : <span
class="math display">\[E[c_i] = E[X]\]</span></p>
<p>Donc, la variance de <span class="math inline">\(X\)</span> peut être
écrite comme : <span class="math display">\[\text{Var}(X) = E[(c_i -
E[c_i])^2] + \sum_{i=1}^k p_i (E[c_i] - E[X])^2\]</span></p>
<p>En simplifiant, nous obtenons : <span
class="math display">\[\text{Var}(X) = \sum_{i=1}^k p_i \text{Var}(c_i)
+ \sum_{i=1}^k p_i (E[c_i] - E[X])^2\]</span></p>
<p>Puisque la deuxième somme est toujours non négative, nous avons :
<span class="math display">\[\text{Var}(X) \geq \sum_{i=1}^k p_i
\text{Var}(c_i)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le binning par centralité possède plusieurs propriétés intéressantes
qui en font une technique puissante pour l’extraction de
caractéristiques.</p>
<ol>
<li><p><strong>Propriété de réduction de dimension</strong> : Le binning
par centralité réduit la dimension des données en transformant une
variable continue en une variable discrète avec un nombre limité de
valeurs possibles.</p></li>
<li><p><strong>Propriété de préservation de l’information</strong> : Le
binning par centralité préserve les informations essentielles sur la
distribution des données en utilisant des statistiques de
centralité.</p></li>
<li><p><strong>Propriété de robustesse</strong> : Le binning par
centralité est robuste aux valeurs aberrantes, car les statistiques de
centralité sont moins sensibles aux extrêmes que les autres
mesures.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par
centralité est une technique puissante et efficace pour transformer des
données continues en représentations discrètes. Cette méthode est
largement utilisée dans divers domaines et possède de nombreuses
propriétés intéressantes qui en font un outil précieux pour
l’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


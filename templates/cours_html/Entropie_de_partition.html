{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de Partition : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de Partition : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de partition, notion centrale en théorie de l’information
et en physique statistique, émerge comme un outil puissant pour
quantifier la complexité d’un système. Son origine remonte aux travaux
fondateurs de Shannon sur l’entropie informationnelle, étendue par des
contributions majeures en mécanique statistique. Cette notion est
indispensable pour comprendre les systèmes dynamiques, les réseaux de
communication et les phénomènes critiques en physique.</p>
<p>L’entropie de partition résout le problème crucial de mesurer
l’incertitude ou la dispersion d’un système discrétisé. Elle est
particulièrement utile dans les contextes où l’information est
partitionnée en sous-ensembles distincts. Son importance réside dans sa
capacité à capturer la structure fine des données, permettant ainsi une
analyse plus précise que les mesures d’entropie globales.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de partition, considérons un système
décrit par une variable aléatoire discrète <span
class="math inline">\(X\)</span> prenant ses valeurs dans un ensemble
fini <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span>. Nous cherchons à quantifier l’incertitude associée à
<span class="math inline">\(X\)</span> lorsqu’elle est observée à
travers une partition de <span
class="math inline">\(\mathcal{X}\)</span>.</p>
<p>Soit <span class="math inline">\(P = \{A_1, A_2, \ldots,
A_k\}\)</span> une partition de <span
class="math inline">\(\mathcal{X}\)</span>, c’est-à-dire que <span
class="math inline">\(A_i \cap A_j = \emptyset\)</span> pour <span
class="math inline">\(i \neq j\)</span> et <span
class="math inline">\(\bigcup_{i=1}^k A_i = \mathcal{X}\)</span>. La
probabilité d’un ensemble <span class="math inline">\(A_i\)</span> est
donnée par : <span class="math display">\[p(A_i) = \sum_{x \in A_i}
p(x)\]</span></p>
<p>L’entropie de partition <span class="math inline">\(H(P)\)</span> est
alors définie comme : <span class="math display">\[H(P) = -\sum_{i=1}^k
p(A_i) \log p(A_i)\]</span></p>
<p>Cette définition peut être reformulée en utilisant des
quantificateurs : <span class="math display">\[H(P) =
-\sum_{i=1}^{\exists k \in \mathbb{N}} p\left(\bigcup_{x \in A_i}
x\right) \log p\left(\bigcup_{x \in A_i} x\right)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de partition est le théorème
de sous-additivité, qui établit une relation entre l’entropie d’une
partition et celle de ses sous-partitions.</p>
<p><strong>Théorème (Sous-additivité de l’entropie)</strong> : Soient
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux partitions de <span
class="math inline">\(\mathcal{X}\)</span>. Si <span
class="math inline">\(Q\)</span> est une raffinement de <span
class="math inline">\(P\)</span>, alors : <span
class="math display">\[H(P) \leq H(Q)\]</span></p>
<p>Pour démontrer ce théorème, nous utilisons la propriété de convexité
de la fonction <span class="math inline">\(-x \log x\)</span>. Soit
<span class="math inline">\(Q = \{B_1, B_2, \ldots, B_m\}\)</span> un
raffinement de <span class="math inline">\(P\)</span>, c’est-à-dire que
chaque <span class="math inline">\(A_i\)</span> peut être exprimé comme
une union disjointe de certains <span
class="math inline">\(B_j\)</span>. Alors : <span
class="math display">\[H(P) = -\sum_{i=1}^k p(A_i) \log p(A_i)\]</span>
<span class="math display">\[= -\sum_{i=1}^k \left( \sum_{j: B_j
\subseteq A_i} p(B_j) \right) \log \left( \sum_{j: B_j \subseteq A_i}
p(B_j) \right)\]</span> <span class="math display">\[\leq -\sum_{i=1}^k
\sum_{j: B_j \subseteq A_i} p(B_j) \log p(B_j)\]</span> <span
class="math display">\[= -\sum_{j=1}^m p(B_j) \log p(B_j)\]</span> <span
class="math display">\[= H(Q)\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de sous-additivité repose sur la convexité de
la fonction <span class="math inline">\(-x \log x\)</span>. Pour tout
<span class="math inline">\(x, y \geq 0\)</span> avec <span
class="math inline">\(x + y = 1\)</span>, nous avons : <span
class="math display">\[-x \log x - y \log y \leq -(x + y) \log (x +
y)\]</span></p>
<p>Cette inégalité est une conséquence directe de la convexité de la
fonction <span class="math inline">\(-x \log x\)</span>. En appliquant
cette propriété à chaque ensemble <span
class="math inline">\(A_i\)</span> et ses sous-ensembles <span
class="math inline">\(B_j\)</span>, nous obtenons l’inégalité
souhaitée.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de partition possède plusieurs propriétés intéressantes
:</p>
<p>(i) <strong>Positivité</strong> : Pour toute partition <span
class="math inline">\(P\)</span> de <span
class="math inline">\(\mathcal{X}\)</span>, nous avons <span
class="math inline">\(H(P) \geq 0\)</span>. L’égalité a lieu si et
seulement si tous les ensembles <span class="math inline">\(A_i\)</span>
sont déterministes, c’est-à-dire que <span class="math inline">\(p(A_i)
= 0\)</span> ou <span class="math inline">\(1\)</span>.</p>
<p>(ii) <strong>Maximisation</strong> : L’entropie de partition est
maximisée lorsque tous les ensembles <span
class="math inline">\(A_i\)</span> sont équiprobables. Si <span
class="math inline">\(p(A_i) = 1/k\)</span> pour tout <span
class="math inline">\(i\)</span>, alors : <span
class="math display">\[H(P) = \log k\]</span></p>
<p>(iii) <strong>Additivité</strong> : Si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont deux partitions indépendantes de
<span class="math inline">\(\mathcal{X}\)</span>, alors : <span
class="math display">\[H(P \vee Q) = H(P) + H(Q)\]</span></p>
<p>La preuve de ces propriétés repose sur des arguments similaires à
ceux utilisés pour démontrer le théorème de sous-additivité. Par
exemple, la positivité découle directement de la définition de
l’entropie et des propriétés de la fonction logarithme.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de partition est un outil fondamental en théorie de
l’information et en physique statistique. Ses propriétés et ses
applications en font une notion indispensable pour l’analyse des
systèmes complexes. Les théorèmes et les preuves présentés dans cet
article illustrent la richesse de cette notion et ouvrent des
perspectives pour des recherches futures.</p>
</body>
</html>
{% include "footer.html" %}


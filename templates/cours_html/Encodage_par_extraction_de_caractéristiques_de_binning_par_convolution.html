{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par convolution</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par convolution</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de binning par
convolution (CEBC) est une technique moderne d’analyse de données qui
combine les avantages du binning et des méthodes convolutionnelles. Le
binning est une méthode classique de réduction de la dimensionnalité qui
consiste à diviser les données en intervalles (ou "bins") et à agréger
les valeurs dans chaque intervalle. Les méthodes convolutionnelles,
inspirées du traitement du signal et de la vision par ordinateur,
utilisent des filtres pour extraire des caractéristiques locales des
données.</p>
<p>L’émergence du CEBC est motivée par le besoin de traiter des
ensembles de données de plus en plus volumineux et complexes. Le binning
permet de réduire la dimensionnalité des données, ce qui facilite leur
traitement et leur visualisation. Cependant, le binning classique peut
entraîner une perte d’information importante. Les méthodes
convolutionnelles, quant à elles, permettent d’extraire des
caractéristiques locales des données, mais elles peuvent être sensibles
aux variations de scale et de rotation.</p>
<p>Le CEBC combine ces deux approches pour surmonter leurs limitations
respectives. En utilisant des filtres convolutionnels pour extraire des
caractéristiques locales des données avant de les binner, le CEBC permet
de préserver une grande partie de l’information contenue dans les
données tout en réduisant leur dimensionnalité.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le CEBC, il est utile de comprendre ce
que nous cherchons à accomplir. Supposons que nous avons un ensemble de
données <span class="math inline">\(D = \{x_1, x_2, \dots,
x_n\}\)</span> où chaque <span class="math inline">\(x_i\)</span> est un
vecteur de dimension <span class="math inline">\(d\)</span>. Notre
objectif est de transformer cet ensemble de données en un nouvel
ensemble de données <span class="math inline">\(D&#39;\)</span> de
dimension réduite qui capture les caractéristiques locales des données
originales.</p>
<p>Pour ce faire, nous allons d’abord appliquer une série de filtres
convolutionnels aux données pour extraire des caractéristiques locales.
Ensuite, nous allons diviser les données en intervalles (ou "bins") et
agréger les valeurs dans chaque intervalle.</p>
<div class="definition">
<p>Soit <span class="math inline">\(D = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble de données où chaque <span
class="math inline">\(x_i \in \mathbb{R}^d\)</span>. Soit <span
class="math inline">\(F = \{f_1, f_2, \dots, f_k\}\)</span> un ensemble
de filtres convolutionnels. Soit <span class="math inline">\(B = \{b_1,
b_2, \dots, b_m\}\)</span> un ensemble de bins.</p>
<p>L’encodage par extraction de caractéristiques de binning par
convolution (CEBC) est défini comme suit : <span
class="math display">\[D&#39; = \text{CEBC}(D, F, B) = \{y_1, y_2,
\dots, y_m\}\]</span> où chaque <span class="math inline">\(y_j\)</span>
est calculé comme suit : <span class="math display">\[y_j =
\frac{1}{|B_j|} \sum_{x_i \in B_j} f(x_i)\]</span> où <span
class="math inline">\(B_j\)</span> est le <span
class="math inline">\(j\)</span>-ème bin et <span
class="math inline">\(|B_j|\)</span> est le nombre de points de données
dans ce bin.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons un théorème important concernant
le CEBC. Ce théorème montre que le CEBC peut être utilisé pour
approximer une fonction donnée avec une erreur bornée.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> une fonction continue bornée. Soit <span
class="math inline">\(D = \{x_1, x_2, \dots, x_n\}\)</span> un ensemble
de données où chaque <span class="math inline">\(x_i\)</span> est tiré
indépendamment selon une distribution <span
class="math inline">\(\mu\)</span>. Soit <span class="math inline">\(F =
\{f_1, f_2, \dots, f_k\}\)</span> un ensemble de filtres
convolutionnels. Soit <span class="math inline">\(B = \{b_1, b_2, \dots,
b_m\}\)</span> un ensemble de bins.</p>
<p>Alors, pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, il existe un nombre suffisant de filtres convolutionnels
<span class="math inline">\(k\)</span> et de bins <span
class="math inline">\(m\)</span> tels que : <span
class="math display">\[\left| \mathbb{E}_{x \sim \mu}[f(x)] -
\frac{1}{n} \sum_{i=1}^n f_{\text{CEBC}}(x_i) \right| &lt;
\epsilon\]</span> où <span
class="math inline">\(f_{\text{CEBC}}(x_i)\)</span> est la valeur de la
fonction <span class="math inline">\(f\)</span> approximée par le CEBC
au point <span class="math inline">\(x_i\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur le théorème de
l’approximation universelle des réseaux de neurones et le théorème du
point fixe de Banach. Nous commençons par montrer que les filtres
convolutionnels peuvent approximer n’importe quelle fonction continue
bornée avec une erreur arbitrairement petite. Ensuite, nous montrons que
le binning peut être utilisé pour approximer l’espérance de la fonction
avec une erreur bornée. Enfin, nous combinons ces deux résultats pour
obtenir le théorème. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Dans cette section, nous fournissons des preuves détaillées pour les
théorèmes présentés dans la section précédente.</p>
<div class="proof">
<p><em>Preuve du Théorème d’Approximation par le CEBC.</em> Soit <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>
une fonction continue bornée. Soit <span class="math inline">\(D =
\{x_1, x_2, \dots, x_n\}\)</span> un ensemble de données où chaque <span
class="math inline">\(x_i\)</span> est tiré indépendamment selon une
distribution <span class="math inline">\(\mu\)</span>. Soit <span
class="math inline">\(F = \{f_1, f_2, \dots, f_k\}\)</span> un ensemble
de filtres convolutionnels. Soit <span class="math inline">\(B = \{b_1,
b_2, \dots, b_m\}\)</span> un ensemble de bins.</p>
<p>Nous voulons montrer que pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe un nombre
suffisant de filtres convolutionnels <span
class="math inline">\(k\)</span> et de bins <span
class="math inline">\(m\)</span> tels que : <span
class="math display">\[\left| \mathbb{E}_{x \sim \mu}[f(x)] -
\frac{1}{n} \sum_{i=1}^n f_{\text{CEBC}}(x_i) \right| &lt;
\epsilon\]</span></p>
<p>Nous commençons par montrer que les filtres convolutionnels peuvent
approximer n’importe quelle fonction continue bornée avec une erreur
arbitrairement petite. Cela découle du théorème de l’approximation
universelle des réseaux de neurones, qui stipule que les réseaux de
neurones peuvent approximer n’importe quelle fonction continue bornée
avec une erreur arbitrairement petite.</p>
<p>Ensuite, nous montrons que le binning peut être utilisé pour
approximer l’espérance de la fonction avec une erreur bornée. Cela
découle du théorème du point fixe de Banach, qui stipule que pour toute
fonction continue bornée <span class="math inline">\(f\)</span> et toute
distribution <span class="math inline">\(\mu\)</span>, il existe un
nombre suffisant de bins <span class="math inline">\(m\)</span> tels que
: <span class="math display">\[\left| \mathbb{E}_{x \sim \mu}[f(x)] -
\frac{1}{n} \sum_{i=1}^n f(x_i) \right| &lt; \epsilon\]</span></p>
<p>Enfin, nous combinons ces deux résultats pour obtenir le théorème. En
effet, en choisissant un nombre suffisant de filtres convolutionnels
<span class="math inline">\(k\)</span> et de bins <span
class="math inline">\(m\)</span>, nous pouvons approximer la fonction
<span class="math inline">\(f\)</span> avec une erreur arbitrairement
petite, puis approximer l’espérance de la fonction avec une erreur
bornée. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés et
corollaires importants du CEBC.</p>
<div class="proposition">
<p>Le CEBC est linéaire, c’est-à-dire que pour toute fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> et
tout scalaire <span class="math inline">\(\alpha \in
\mathbb{R}\)</span>, nous avons : <span
class="math display">\[\text{CEBC}(\alpha f, F, B) = \alpha
\text{CEBC}(f, F, B)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la définition
du CEBC. En effet, pour toute fonction <span class="math inline">\(f:
\mathbb{R}^d \rightarrow \mathbb{R}\)</span> et tout scalaire <span
class="math inline">\(\alpha \in \mathbb{R}\)</span>, nous avons : <span
class="math display">\[\text{CEBC}(\alpha f, F, B) = \frac{1}{|B_j|}
\sum_{x_i \in B_j} f(\alpha x_i) = \alpha \frac{1}{|B_j|} \sum_{x_i \in
B_j} f(x_i) = \alpha \text{CEBC}(f, F, B)\]</span> ◻</p>
</div>
<div class="corollary">
<p>Le CEBC est également linéaire par rapport aux filtres
convolutionnels, c’est-à-dire que pour toute fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> et
tout ensemble de filtres convolutionnels <span class="math inline">\(F =
\{f_1, f_2, \dots, f_k\}\)</span>, nous avons : <span
class="math display">\[\text{CEBC}(f, \alpha F, B) = \alpha
\text{CEBC}(f, F, B)\]</span> où <span class="math inline">\(\alpha F =
\{\alpha f_1, \alpha f_2, \dots, \alpha f_k\}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la définition
du CEBC et de la propriété de linéarité. En effet, pour toute fonction
<span class="math inline">\(f: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> et tout ensemble de filtres convolutionnels <span
class="math inline">\(F = \{f_1, f_2, \dots, f_k\}\)</span>, nous avons
: <span class="math display">\[\text{CEBC}(f, \alpha F, B) =
\frac{1}{|B_j|} \sum_{x_i \in B_j} f(\alpha f(x_i)) = \alpha
\frac{1}{|B_j|} \sum_{x_i \in B_j} f(f(x_i)) = \alpha \text{CEBC}(f, F,
B)\]</span> ◻</p>
</div>
</body>
</html>
{% include "footer.html" %}


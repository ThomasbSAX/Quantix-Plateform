{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>WAIC (Widely Applicable Information Criterion): A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">WAIC (Widely Applicable Information Criterion): A
Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<p>Certainly! Here is a LaTeX document on the subject of WAIC (Widely
Applicable Information Criterion):</p>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The Widely Applicable Information Criterion (WAIC) is a measure used
for model selection and evaluation in Bayesian statistics. It was
introduced as an improvement over traditional criteria like AIC (Akaike
Information Criterion) and DIC (Deviance Information Criterion),
addressing some of their limitations. The emergence of WAIC is rooted in
the need for a more robust and widely applicable criterion that can
handle complex models, including those with hierarchical structures and
missing data.</p>
<p>WAIC is particularly useful in the context of Bayesian inference,
where models are often complex and involve many parameters. It provides
a way to evaluate the predictive performance of a model, which is
crucial for selecting the best model among several candidates. The
criterion is based on the concept of cross-validation and is derived
from information theory principles.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand WAIC, we first need to define some key concepts and
notations.</p>
<p>Consider a Bayesian model with parameters <span
class="math inline">\(\theta\)</span> and data <span
class="math inline">\(y\)</span>. The posterior distribution of the
parameters given the data is denoted by <span
class="math inline">\(p(\theta | y)\)</span>. The predictive
distribution for a new observation <span
class="math inline">\(y_{\text{new}}\)</span> is given by:</p>
<p><span class="math display">\[p(y_{\text{new}} | y) = \int
p(y_{\text{new}} | \theta) p(\theta | y) d\theta\]</span></p>
<p>The log pointwise predictive density (lppd) for a single observation
<span class="math inline">\(y_i\)</span> is defined as:</p>
<p><span class="math display">\[\text{lppd}_i = \log p(y_i | y) = \log
\int p(y_i | \theta) p(\theta | y) d\theta\]</span></p>
<p>The WAIC is then defined as:</p>
<p><span class="math display">\[\text{WAIC} = -2 \left( \text{lppd} -
\hat{p}_{\text{WAIC}} \right)\]</span></p>
<p>where <span class="math inline">\(\text{lppd} = \sum_{i=1}^n
\text{lppd}_i\)</span> and <span
class="math inline">\(\hat{p}_{\text{WAIC}}\)</span> is the effective
number of parameters, defined as:</p>
<p><span class="math display">\[\hat{p}_{\text{WAIC}} = \sum_{i=1}^n
\text{Var}_\theta(\log p(y_i | \theta) | y)\]</span></p>
<p>Here, <span class="math inline">\(\text{Var}_\theta(\cdot)\)</span>
denotes the variance with respect to the posterior distribution of <span
class="math inline">\(\theta\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key theorems related to WAIC is the following:</p>
<div class="theorem">
<p>Let <span class="math inline">\(\mathcal{M}_1\)</span> and <span
class="math inline">\(\mathcal{M}_2\)</span> be two Bayesian models such
that <span class="math inline">\(\mathcal{M}_1\)</span> is the true
model generating the data. Then, as the sample size <span
class="math inline">\(n\)</span> goes to infinity, the WAIC for <span
class="math inline">\(\mathcal{M}_1\)</span> will be asymptotically
smaller than the WAIC for <span
class="math inline">\(\mathcal{M}_2\)</span>.</p>
<p>Mathematically, this can be expressed as:</p>
<p><span class="math display">\[\lim_{n \to \infty}
\text{Pr}(\text{WAIC}_{\mathcal{M}_1} &lt; \text{WAIC}_{\mathcal{M}_2})
= 1\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof of this theorem relies on the properties of
cross-validation and information theory. The key idea is that, as the
sample size increases, the model that better captures the true
data-generating process will have a lower WAIC. This is because the lppd
term in the WAIC formula will be higher for the true model, and the
effective number of parameters <span
class="math inline">\(\hat{p}_{\text{WAIC}}\)</span> will be
appropriately penalized for model complexity.</p>
<p>To formalize this, consider the Kullback-Leibler (KL) divergence
between the true data-generating distribution <span
class="math inline">\(p_{\text{true}}(y)\)</span> and the predictive
distribution of a model <span class="math inline">\(p(y |
\theta)\)</span>. The KL divergence is given by:</p>
<p><span class="math display">\[D_{\text{KL}}(p_{\text{true}} \| p(y |
\theta)) = \mathbb{E}_{y \sim p_{\text{true}}} \left[ \log
\frac{p_{\text{true}}(y)}{p(y | \theta)} \right]\]</span></p>
<p>For the true model <span
class="math inline">\(\mathcal{M}_1\)</span>, the KL divergence will be
zero, and for any other model <span
class="math inline">\(\mathcal{M}_2\)</span>, it will be positive. The
lppd term in the WAIC can be shown to be related to the negative of the
KL divergence, and thus the true model will have a higher lppd. The
effective number of parameters <span
class="math inline">\(\hat{p}_{\text{WAIC}}\)</span> will ensure that
the model complexity is appropriately penalized, leading to a lower WAIC
for the true model as <span class="math inline">\(n\)</span>
increases. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The WAIC has several important properties that make it a useful
criterion for model selection:</p>
<ol>
<li><p><strong>Consistency</strong>: As shown in the theorem above, WAIC
is consistent for model selection. This means that it will select the
true model with probability approaching one as the sample size
increases.</p></li>
<li><p><strong>Applicability</strong>: WAIC is widely applicable and can
be used with any Bayesian model, including those with hierarchical
structures and missing data. This is in contrast to other criteria like
AIC and DIC, which have more restrictive assumptions.</p></li>
<li><p><strong>Interpretability</strong>: The WAIC can be interpreted in
terms of the predictive performance of a model. A lower WAIC indicates
better predictive performance, making it easy to compare and select
among different models.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>The Widely Applicable Information Criterion (WAIC) is a powerful tool
for model selection and evaluation in Bayesian statistics. Its
consistency, wide applicability, and interpretability make it a valuable
criterion for selecting the best model among several candidates. The
WAIC addresses some of the limitations of traditional criteria like AIC
and DIC, providing a more robust and reliable measure for model
evaluation.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage FastText : Une Révolution dans le Traitement Automatique des Langues Naturelles</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage FastText : Une Révolution dans le
Traitement Automatique des Langues Naturelles</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’ère numérique actuelle a vu une explosion de données textuelles,
nécessitant des outils performants pour leur traitement. Parmi ces
outils, les modèles d’encodage de texte jouent un rôle central.
L’encodage FastText, développé par Facebook AI Research (FAIR),
représente une avancée significative dans ce domaine. Il combine
l’efficacité des modèles de type bag-of-words avec la puissance des
embeddings de mots, permettant une représentation dense et sémantique du
texte.</p>
<p>L’émergence de FastText est motivée par le besoin de modèles capables
de capturer les relations sémantiques entre les mots tout en restant
efficaces sur de grandes quantités de données. Contrairement aux modèles
traditionnels comme Word2Vec ou GloVe, FastText intègre des informations
sous-mot (subword), ce qui lui permet de mieux gérer les mots rares ou
inconnus.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage FastText, il est essentiel de définir
plusieurs concepts clés.</p>
<h2 id="embeddings-de-mots">Embeddings de Mots</h2>
<p>Les embeddings de mots sont des représentations vectorielles denses
des mots dans un espace continu. Ils permettent de capturer les
relations sémantiques entre les mots.</p>
<div class="definition">
<p>Soit <span class="math inline">\(V\)</span> un vocabulaire de taille
<span class="math inline">\(|V|\)</span>, et soit <span
class="math inline">\(d\)</span> la dimension des embeddings. Un
embedding de mot est une fonction <span class="math inline">\(\phi : V
\rightarrow \mathbb{R}^d\)</span> qui associe à chaque mot <span
class="math inline">\(w \in V\)</span> un vecteur <span
class="math inline">\(\phi(w) \in \mathbb{R}^d\)</span>.</p>
</div>
<h2 id="sous-mots-subwords">Sous-mots (Subwords)</h2>
<p>Les sous-mots sont des séquences de caractères qui composent les
mots. Ils permettent de décomposer les mots en unités plus petites.</p>
<div class="definition">
<p>Soit <span class="math inline">\(w\)</span> un mot de longueur <span
class="math inline">\(n\)</span>. Un sous-mot <span
class="math inline">\(g\)</span> de <span
class="math inline">\(w\)</span> est une séquence de caractères
consécutifs de <span class="math inline">\(w\)</span>. Le sous-mot peut
être de longueur variable, allant de 1 à <span
class="math inline">\(n\)</span>.</p>
</div>
<h2 id="modèle-fasttext">Modèle FastText</h2>
<p>Le modèle FastText combine les embeddings de mots et les sous-mots
pour représenter un texte.</p>
<div class="definition">
<p>Soit <span class="math inline">\(w\)</span> un mot et soit <span
class="math inline">\(G(w)\)</span> l’ensemble de ses sous-mots.
L’embedding FastText d’un mot <span class="math inline">\(w\)</span> est
défini comme la somme de l’embedding du mot et des embeddings de ses
sous-mots :</p>
<p><span class="math display">\[\phi_{\text{FastText}}(w) = \phi(w) +
\sum_{g \in G(w)} \phi(g)\]</span></p>
<p>où <span class="math inline">\(\phi(w)\)</span> est l’embedding du
mot et <span class="math inline">\(\phi(g)\)</span> est l’embedding du
sous-mot <span class="math inline">\(g\)</span>.</p>
</div>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<h2 id="théorème-de-la-représentation-linéaire">Théorème de la
Représentation Linéaire</h2>
<p>Le modèle FastText repose sur l’hypothèse que les embeddings de
sous-mots peuvent linéairement combiner pour former des embeddings de
mots.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(w\)</span> un mot et soit <span
class="math inline">\(G(w)\)</span> l’ensemble de ses sous-mots.
L’embedding FastText d’un mot <span class="math inline">\(w\)</span>
peut être exprimé comme une combinaison linéaire de ses sous-mots :</p>
<p><span class="math display">\[\phi_{\text{FastText}}(w) = \phi(w) +
\sum_{g \in G(w)} \phi(g)\]</span></p>
<p>Cette représentation permet de capturer les relations sémantiques
entre les mots même pour les mots rares ou inconnus.</p>
</div>
<h2 id="preuve-du-théorème-de-la-représentation-linéaire">Preuve du
Théorème de la Représentation Linéaire</h2>
<p>Pour prouver ce théorème, nous devons montrer que l’embedding
FastText peut effectivement capturer les relations sémantiques entre les
mots.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un mot <span
class="math inline">\(w\)</span> et ses sous-mots <span
class="math inline">\(G(w)\)</span>. L’embedding FastText est défini
comme la somme de l’embedding du mot et des embeddings de ses sous-mots.
Cette représentation permet de capturer les relations sémantiques entre
les mots car les sous-mots partagés entre différents mots contribuent de
manière similaire à leurs embeddings respectifs.</p>
<p>Par exemple, les mots "chat" et "chien" partagent le sous-mot "ch",
ce qui permet à leurs embeddings de capturer des relations sémantiques
communes. De plus, les sous-mots permettent de mieux gérer les mots
rares ou inconnus en utilisant des sous-mots plus fréquents. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-la-généralisation">Propriété de la
Généralisation</h2>
<p>Le modèle FastText permet une meilleure généralisation aux mots rares
ou inconnus.</p>
<div class="property">
<p>Soit <span class="math inline">\(w\)</span> un mot rare ou inconnu.
L’embedding FastText de <span class="math inline">\(w\)</span> peut être
approximé en utilisant les embeddings de ses sous-mots :</p>
<p><span class="math display">\[\phi_{\text{FastText}}(w) \approx
\sum_{g \in G(w)} \phi(g)\]</span></p>
<p>Cette propriété permet au modèle de mieux généraliser aux mots non
vus pendant l’entraînement.</p>
</div>
<h2 id="corollaire-de-la-robustesse">Corollaire de la Robustesse</h2>
<p>Le modèle FastText est robuste aux erreurs de saisie et aux
variations orthographiques.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(w\)</span> un mot et soit <span
class="math inline">\(w&#39;\)</span> une variante orthographique de
<span class="math inline">\(w\)</span>. L’embedding FastText de <span
class="math inline">\(w\)</span> et <span
class="math inline">\(w&#39;\)</span> seront similaires car ils
partagent des sous-mots communs :</p>
<p><span class="math display">\[\phi_{\text{FastText}}(w) \approx
\phi_{\text{FastText}}(w&#39;)\]</span></p>
<p>Cette robustesse permet au modèle de mieux gérer les variations
orthographiques et les erreurs de saisie.</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage FastText représente une avancée significative dans le
domaine du traitement automatique des langues naturelles. En combinant
les embeddings de mots et les sous-mots, il permet une représentation
dense et sémantique du texte, capturant les relations entre les mots de
manière efficace. Ses propriétés de généralisation et de robustesse en
font un outil puissant pour diverses applications, notamment la
classification de texte, la recherche d’information et la traduction
automatique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Mutual Information: A Measure of Dependence</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mutual Information: A Measure of Dependence</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of mutual information (MI) emerges from the need to
quantify the dependence between random variables. In an era where data
is abundant but often entangled, MI provides a robust framework to
measure how much one random variable tells us about another.
Historically, MI was introduced by Claude Shannon in 1948 as a
fundamental concept in information theory. It has since permeated
various fields, including statistics, machine learning, and signal
processing.</p>
<p>MI is indispensable in scenarios where understanding the relationship
between variables is crucial. For instance, in genetics, MI can reveal
dependencies between genes; in finance, it can uncover hidden
correlations between market variables. The beauty of MI lies in its
ability to detect any form of dependence, not just linear
correlations.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand mutual information, let us first consider two random
variables <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>. We seek a measure that quantifies the
amount of information obtained about one random variable through the
other. Intuitively, if knowing <span class="math inline">\(Y\)</span>
does not change our uncertainty about <span
class="math inline">\(X\)</span>, then <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are independent, and the mutual
information should be zero.</p>
<p>Formally, the mutual information <span class="math inline">\(I(X;
Y)\)</span> between two random variables <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> with joint probability distribution
<span class="math inline">\(p(x, y)\)</span> and marginal distributions
<span class="math inline">\(p(x)\)</span> and <span
class="math inline">\(p(y)\)</span> is defined as:</p>
<p><span class="math display">\[I(X; Y) = \sum_{x \in \mathcal{X}}
\sum_{y \in \mathcal{Y}} p(x, y) \log \left( \frac{p(x, y)}{p(x) p(y)}
\right)\]</span></p>
<p>For continuous random variables, the sums are replaced by
integrals:</p>
<p><span class="math display">\[I(X; Y) = \int_{\mathcal{X}}
\int_{\mathcal{Y}} p(x, y) \log \left( \frac{p(x, y)}{p(x) p(y)} \right)
dx dy\]</span></p>
<p>This can also be expressed in terms of entropies:</p>
<p><span class="math display">\[I(X; Y) = H(X) + H(Y) - H(X,
Y)\]</span></p>
<p>where <span class="math inline">\(H(X)\)</span> is the entropy of
<span class="math inline">\(X\)</span>, <span
class="math inline">\(H(Y)\)</span> is the entropy of <span
class="math inline">\(Y\)</span>, and <span class="math inline">\(H(X,
Y)\)</span> is the joint entropy of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the most important theorems related to mutual information is
the Data Processing Inequality. Intuitively, if we process data in a way
that reduces noise, the mutual information should not increase.</p>
<p>Theorem (Data Processing Inequality): Let <span
class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span>, and <span
class="math inline">\(Z\)</span> be random variables such that <span
class="math inline">\(X - Y - Z\)</span> forms a Markov chain. Then:</p>
<p><span class="math display">\[I(X; Y) \geq I(X; Z)\]</span></p>
<p>Proof: The proof relies on the fact that conditioning reduces
entropy. Specifically, we have:</p>
<p><span class="math display">\[H(X | Y) \leq H(X | Y, Z)\]</span></p>
<p>Using the chain rule for entropy:</p>
<p><span class="math display">\[H(X, Y) = H(X | Y) + H(Y)\]</span></p>
<p>and</p>
<p><span class="math display">\[H(X, Z, Y) = H(X | Y, Z) + H(Y,
Z)\]</span></p>
<p>By the Markov chain assumption, <span class="math inline">\(H(X | Y,
Z) = H(X | Y)\)</span>. Thus:</p>
<p><span class="math display">\[H(X, Y) - H(Y) = H(X | Y) \leq H(X | Y,
Z) = H(X, Z, Y) - H(Z, Y)\]</span></p>
<p>Rearranging terms and using the non-negativity of entropy gives:</p>
<p><span class="math display">\[I(X; Y) \geq I(X; Z)\]</span></p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Mutual information enjoys several important properties:</p>
<p>(i) <strong>Non-negativity</strong>: <span class="math inline">\(I(X;
Y) \geq 0\)</span>, with equality if and only if <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are independent.</p>
<p>Proof: By the definition of MI and the non-negativity of the
logarithm, <span class="math inline">\(\log \left( \frac{p(x, y)}{p(x)
p(y)} \right) \geq 0\)</span>.</p>
<p>(ii) <strong>Symmetry</strong>: <span class="math inline">\(I(X; Y) =
I(Y; X)\)</span>.</p>
<p>Proof: This follows directly from the symmetry of the joint
probability distribution <span class="math inline">\(p(x,
y)\)</span>.</p>
<p>(iii) <strong>Subadditivity</strong>: For any three random variables
<span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span>, and <span
class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[I(X; Y, Z) \leq I(X; Y) + I(X;
Z)\]</span></p>
<p>Proof: This can be shown using the chain rule for mutual information
and the non-negativity of MI.</p>
<h1 id="examples">Examples</h1>
<p>Consider two binary random variables <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> with the following joint
distribution:</p>
<p><span class="math display">\[\begin{array}{cc|c|c}
&amp; &amp; Y=0 &amp; Y=1 \\
\hline
X=0 &amp; p(x, y) &amp; 0.4 &amp; 0.1 \\
X=1 &amp; p(x, y) &amp; 0.2 &amp; 0.3 \\
\end{array}\]</span></p>
<p>The marginal distributions are:</p>
<p><span class="math display">\[p(x) = [0.5, 0.5], \quad p(y) = [0.6,
0.4]\]</span></p>
<p>The mutual information is:</p>
<p><span class="math display">\[I(X; Y) = \sum_{x, y} p(x, y) \log
\left( \frac{p(x, y)}{p(x) p(y)} \right)\]</span></p>
<p>Calculating each term:</p>
<p><span class="math display">\[I(X; Y) = 0.4 \log \left(
\frac{0.4}{0.3} \right) + 0.1 \log \left( \frac{0.1}{0.2} \right) + 0.2
\log \left( \frac{0.2}{0.3} \right) + 0.3 \log \left( \frac{0.3}{0.2}
\right)\]</span></p>
<p>This example illustrates how MI captures the dependence between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Mutual information is a powerful tool for measuring dependence
between random variables. Its applications span various fields, and its
properties make it a versatile measure in both theoretical and applied
contexts. Understanding MI not only enriches our comprehension of
information theory but also equips us with a robust framework for
analyzing complex datasets.</p>
</body>
</html>
{% include "footer.html" %}


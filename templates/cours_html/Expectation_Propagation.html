{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Expectation Propagation: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Expectation Propagation: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>Expectation Propagation (EP) emerges as a powerful algorithm for
approximate inference in probabilistic models. Originating from the need
to handle complex, intractable distributions, EP provides a systematic
approach to approximate posterior distributions by iteratively fitting
moments. This method is particularly indispensable in large-scale
machine learning problems where exact inference is computationally
infeasible.</p>
<p>The concept of EP was first introduced by Minka (2001) as a
generalization of the well-known Expectation-Maximization (EM)
algorithm. It leverages the principles of Bayesian inference and message
passing, making it a versatile tool for various applications including
Gaussian processes, graphical models, and more.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand EP, we first need to grasp the notion of approximate
inference. Suppose we have a probabilistic model defined by a joint
distribution <span class="math inline">\(p(x, z)\)</span>, where <span
class="math inline">\(x\)</span> represents the observed data and <span
class="math inline">\(z\)</span> denotes the latent variables. Our goal
is to compute the posterior distribution <span
class="math inline">\(p(z|x)\)</span>, which is often intractable.</p>
<p>We aim to find an approximate distribution <span
class="math inline">\(q(z)\)</span> that minimizes the Kullback-Leibler
(KL) divergence to the true posterior: <span
class="math display">\[q^*(z) = \argmin_{q(z)} D_{\text{KL}}(q(z) \|
p(z|x))\]</span></p>
<p>The EP algorithm achieves this by iteratively fitting the moments of
<span class="math inline">\(q(z)\)</span>. Formally, let <span
class="math inline">\(F(q)\)</span> be a functional that measures the
discrepancy between <span class="math inline">\(q(z) and
p(z|x)\)</span>. The EP update rule can be expressed as: <span
class="math display">\[q_{t+1}(z) = \argmin_{q(z)} F(q)\]</span> where
<span class="math inline">\(q_t(z)\)</span> is the current
approximation.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems in EP is the cavity method, which
provides a way to update the approximate distribution by considering the
effect of removing one factor at a time.</p>
<div class="theorem">
<p>Given a factor graph representation of the joint distribution <span
class="math inline">\(p(x, z) = \prod_i f_i(z_i)\)</span>, the EP update
for factor <span class="math inline">\(f_k\)</span> is given by: <span
class="math display">\[q_{t+1}(z) = q_t(z)
\frac{\tilde{q}_k(z_k)}{q_{k,t}(z_k)}\]</span> where <span
class="math inline">\(\tilde{q}_k(z_k)\)</span> is the cavity
distribution obtained by removing <span
class="math inline">\(f_k\)</span>, and <span
class="math inline">\(q_{k,t}(z_k)\)</span> is the current approximation
for <span class="math inline">\(z_k\)</span>.</p>
</div>
<h1 id="proofs">Proofs</h1>
<p>To prove the cavity method, we start by considering the functional
<span class="math inline">\(F(q)\)</span>. The EP update rule can be
derived by minimizing <span class="math inline">\(F(q)\)</span> with
respect to <span class="math inline">\(q(z)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> The functional <span
class="math inline">\(F(q)\)</span> can be written as: <span
class="math display">\[F(q) = D_{\text{KL}}(q(z) \| p(z|x)) +
\text{constant}\]</span></p>
<p>Using the cavity method, we remove one factor <span
class="math inline">\(f_k\)</span> and compute the cavity distribution:
<span class="math display">\[\tilde{q}_k(z_k) = \frac{\int q_t(z_{-k})
\prod_{i \neq k} f_i(z_i) dz_{-k}}{\int q_t(z_{-k}) \prod_{i \neq k}
f_i(z_i) dz_{-k}}\]</span></p>
<p>The updated approximation <span
class="math inline">\(q_{t+1}(z)\)</span> is then obtained by: <span
class="math display">\[q_{t+1}(z) = q_t(z)
\frac{\tilde{q}_k(z_k)}{q_{k,t}(z_k)}\]</span></p>
<p>This update rule ensures that the moments of <span
class="math inline">\(q(z)\)</span> are iteratively fitted to those of
the true posterior. ◻</p>
</div>
<h1 id="properties-and-corollaires">Properties and Corollaires</h1>
<p>The EP algorithm possesses several important properties that make it
a robust tool for approximate inference.</p>
<ol>
<li><p><strong>Consistency</strong>: The EP algorithm is consistent in
the sense that it converges to a fixed point where the moments of <span
class="math inline">\(q(z)\)</span> match those of the true
posterior.</p>
<div class="proof">
<p><em>Proof.</em> The consistency of EP can be shown by analyzing the
fixed point conditions. Suppose <span
class="math inline">\(q^*(z)\)</span> is a fixed point of the EP
updates. Then, for all factors <span class="math inline">\(f_k\)</span>,
we have: <span class="math display">\[q^*(z) = q^*(z)
\frac{\tilde{q}_k(z_k)}{q_{k,t}(z_k)}\]</span> This implies that <span
class="math inline">\(\tilde{q}_k(z_k) = q_{k,t}(z_k)\)</span>, which
means the moments are matched. ◻</p>
</div></li>
<li><p><strong>Efficiency</strong>: EP is computationally efficient
compared to other approximate inference methods, such as Markov Chain
Monte Carlo (MCMC), especially for large-scale problems.</p>
<div class="proof">
<p><em>Proof.</em> The efficiency of EP arises from its iterative nature
and the use of moment matching. Each update involves only a subset of
the factors, making it scalable to high-dimensional problems. ◻</p>
</div></li>
<li><p><strong>Generalization</strong>: EP can be applied to a wide
range of probabilistic models, including Gaussian processes, Bayesian
networks, and more.</p>
<div class="proof">
<p><em>Proof.</em> The generality of EP is due to its reliance on the
factor graph representation and the cavity method. As long as the model
can be expressed in terms of factors, EP can be applied. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Expectation Propagation is a powerful and versatile algorithm for
approximate inference in probabilistic models. Its ability to handle
large-scale problems efficiently makes it an indispensable tool in
modern machine learning and Bayesian statistics. The theoretical
foundations of EP, including the cavity method and its properties,
provide a solid framework for understanding and applying this algorithm
in various domains.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inférence causale : Fondements et Méthodes</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inférence causale : Fondements et Méthodes</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inférence causale est une branche des mathématiques et de la
statistique qui vise à comprendre les relations de cause à effet entre
des variables. À l’ère du big data, où les corrélations abondent, il est
crucial de distinguer les associations fortuites des véritables
mécanismes causaux. L’inférence causale émerge comme une réponse à ce
besoin, combinant des outils probabilistes et graphiques pour modéliser
les dépendances structurelles entre variables.</p>
<p>Historiquement, les fondements de l’inférence causale remontent aux
travaux de Sewall Wright dans les années 1920, qui a introduit les
graphes acycliques orientés (DAGs) pour représenter les relations
causales. Plus récemment, Judea Pearl a formalisé ces idées dans les
années 1980 et 1990, développant une théorie rigoureuse de l’inférence
causale basée sur la logique probabiliste et les graphes bayésiens.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’inférence causale, commençons par définir les
concepts clés. Supposons que nous avons un ensemble de variables
aléatoires <span class="math inline">\(V = \{V_1, V_2, \dots,
V_n\}\)</span>. Nous cherchons à modéliser les relations causales entre
ces variables.</p>
<div class="definition">
<p>Un graphe acyclique orienté <span class="math inline">\(G\)</span>
est un couple <span class="math inline">\((V, E)\)</span>, où <span
class="math inline">\(V\)</span> est un ensemble de sommets représentant
les variables et <span class="math inline">\(E\)</span> un ensemble
d’arêtes orientées représentant les relations causales directes. Un DAG
ne contient aucun cycle, ce qui signifie qu’il n’y a pas de chemin
orienté partant d’un sommet et y revenant.</p>
</div>
<p>Formellement, un DAG <span class="math inline">\(G\)</span> peut être
défini comme suit : <span class="math display">\[G = (V, E) \text{ tel
que } \forall v \in V, \exists u \in V : (u, v) \in E \text{ et }
\nexists \text{ de cycle orienté dans } G.\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’un des théorèmes fondamentaux en inférence causale est le théorème
de Markov, qui établit une relation entre les graphes acycliques
orientés et l’indépendance conditionnelle des variables.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(G\)</span> un DAG et <span
class="math inline">\(A, B, C \subseteq V\)</span> des ensembles de
sommets. Si <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont séparés par <span
class="math inline">\(C\)</span> dans <span
class="math inline">\(G\)</span>, alors <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont indépendants conditionnellement à
<span class="math inline">\(C\)</span> dans toute distribution de
probabilité compatible avec <span class="math inline">\(G\)</span>.</p>
</div>
<p>Formellement, le théorème de Markov peut être énoncé comme suit :
<span class="math display">\[\forall A, B, C \subseteq V, (A
\perp\!\!\!\perp B \mid C)_G) \Rightarrow (A \perp\!\!\!\perp B \mid
C)_{P}.\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Markov, nous devons montrer que
l’indépendance conditionnelle dans le graphe implique l’indépendance
conditionnelle dans la distribution de probabilité.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un DAG <span
class="math inline">\(G\)</span> et des ensembles de sommets <span
class="math inline">\(A, B, C \subseteq V\)</span>. Supposons que <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont séparés par <span
class="math inline">\(C\)</span> dans <span
class="math inline">\(G\)</span>. Cela signifie qu’il n’y a pas de
chemin orienté entre <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> qui ne passe pas par <span
class="math inline">\(C\)</span>.</p>
<p>Par la propriété de factorisation du graphe, toute distribution de
probabilité <span class="math inline">\(P\)</span> compatible avec <span
class="math inline">\(G\)</span> peut être factorisée en termes des
parents de chaque variable. En particulier, pour toute variable <span
class="math inline">\(V_i\)</span>, nous avons : <span
class="math display">\[P(V_i \mid \text{Parents}(V_i)) = P(V_i \mid
\text{Parents}(V_i) \cap C).\]</span></p>
<p>En utilisant cette factorisation, nous pouvons montrer que <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont indépendants conditionnellement à
<span class="math inline">\(C\)</span>. En effet, pour toute réalisation
de <span class="math inline">\(C\)</span>, les variables dans <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> ne partagent pas d’information
supplémentaire une fois que <span class="math inline">\(C\)</span> est
fixé. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le théorème de Markov a plusieurs propriétés importantes et
corollaires.</p>
<ul>
<li><p><strong>Propriété de Factorisation</strong> : Toute distribution
de probabilité compatible avec un DAG <span
class="math inline">\(G\)</span> peut être factorisée en termes des
parents de chaque variable. <span class="math display">\[P(V) = \prod_{v
\in V} P(v \mid \text{Parents}(v)).\]</span></p></li>
<li><p><strong>Corollaire d’Indépendance</strong> : Si <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont séparés par <span
class="math inline">\(C\)</span> dans un DAG <span
class="math inline">\(G\)</span>, alors pour toute distribution de
probabilité <span class="math inline">\(P\)</span> compatible avec <span
class="math inline">\(G\)</span>, nous avons : <span
class="math display">\[P(A, B \mid C) = P(A \mid C) P(B \mid
C).\]</span></p></li>
<li><p><strong>Propriété de Stabilité</strong> : L’indépendance
conditionnelle définie par un DAG est stable sous les interventions,
c’est-à-dire que si nous intervenons sur certaines variables en fixant
leur valeur, les relations d’indépendance conditionnelle restent
valables.</p></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>L’inférence causale est un domaine riche et complexe, offrant des
outils puissants pour comprendre les relations de cause à effet. Les
graphes acycliques orientés et le théorème de Markov sont des concepts
fondamentaux qui permettent de modéliser et d’analyser les dépendances
structurelles entre variables. En combinant ces outils avec des méthodes
statistiques avancées, nous pouvons tirer des conclusions causales
robustes à partir de données observées.</p>
</body>
</html>
{% include "footer.html" %}


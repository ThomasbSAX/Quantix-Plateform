{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Weighted F1: A Comprehensive Study of a Versatile Metric</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Weighted F1: A Comprehensive Study of a Versatile
Metric</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>The evaluation of classification models is a fundamental task in
machine learning. Among the plethora of metrics, the F1 score has
emerged as a popular choice due to its balanced consideration of
precision and recall. However, in many real-world scenarios, not all
classes are equally important. This necessitates the use of a weighted
version of the F1 score, which assigns different weights to different
classes based on their significance.</p>
<p>The weighted F1 score is indispensable in imbalanced datasets, where
certain classes are underrepresented. It allows the model to focus more
on the important classes, leading to better performance in critical
areas. The weighted F1 score is also useful in multi-class
classification problems, where different classes may have varying levels
of importance.</p>
<h1 id="définitions">Définitions</h1>
<p>Before defining the weighted F1 score, let us first understand what
we are trying to achieve. We want a metric that considers both the
precision and recall of each class, but also takes into account the
importance of each class. This leads us to the concept of weighted
precision and recall.</p>
<div class="definition">
<p>Let <span class="math inline">\(\mathcal{C} = \{c_1, c_2, \ldots,
c_n\}\)</span> be the set of classes, and let <span
class="math inline">\(w_i\)</span> be the weight associated with class
<span class="math inline">\(c_i\)</span>. The weighted precision for
class <span class="math inline">\(c_i\)</span> is defined as: <span
class="math display">\[\text{Precision}_i = \frac{\sum_{j=1}^{n} w_j
\cdot TP_{ij}}{\sum_{j=1}^{n} w_j \cdot (TP_{ij} + FP_{ij})}\]</span>
where <span class="math inline">\(TP_{ij}\)</span> is the true positive
rate for class <span class="math inline">\(c_i\)</span> predicted as
class <span class="math inline">\(c_j\)</span>, and <span
class="math inline">\(FP_{ij}\)</span> is the false positive rate for
class <span class="math inline">\(c_i\)</span> predicted as class <span
class="math inline">\(c_j\)</span>.</p>
</div>
<p>Similarly, we can define the weighted recall.</p>
<div class="definition">
<p>The weighted recall for class <span
class="math inline">\(c_i\)</span> is defined as: <span
class="math display">\[\text{Recall}_i = \frac{\sum_{j=1}^{n} w_j \cdot
TP_{ij}}{\sum_{j=1}^{n} w_j \cdot (TP_{ij} + FN_{ij})}\]</span> where
<span class="math inline">\(FN_{ij}\)</span> is the false negative rate
for class <span class="math inline">\(c_i\)</span> predicted as class
<span class="math inline">\(c_j\)</span>.</p>
</div>
<p>Now, we can define the weighted F1 score.</p>
<div class="definition">
<p>The weighted F1 score for class <span
class="math inline">\(c_i\)</span> is the harmonic mean of the weighted
precision and recall: <span class="math display">\[F1_i = 2 \cdot
\frac{\text{Precision}_i \cdot \text{Recall}_i}{\text{Precision}_i +
\text{Recall}_i}\]</span> The overall weighted F1 score is the weighted
average of the F1 scores for all classes: <span
class="math display">\[F1_{\text{weighted}} = \sum_{i=1}^{n} w_i \cdot
F1_i\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>In this section, we present some important theorems related to the
weighted F1 score.</p>
<div class="theorem">
<p>Let <span class="math inline">\(\mathbf{w}\)</span> and <span
class="math inline">\(\mathbf{w}&#39;\)</span> be two weight vectors
such that <span class="math inline">\(w_i \geq w&#39;_i\)</span> for all
<span class="math inline">\(i\)</span>. Then, the weighted F1 score with
respect to <span class="math inline">\(\mathbf{w}\)</span> is greater
than or equal to the weighted F1 score with respect to <span
class="math inline">\(\mathbf{w}&#39;\)</span>. <span
class="math display">\[\sum_{i=1}^{n} w_i \cdot F1_i \geq \sum_{i=1}^{n}
w&#39;_i \cdot F1_i\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof follows from the fact that <span
class="math inline">\(F1_i\)</span> is a non-decreasing function of
<span class="math inline">\(w_i\)</span>. Therefore, if <span
class="math inline">\(w_i \geq w&#39;_i\)</span> for all <span
class="math inline">\(i\)</span>, then the weighted F1 score with
respect to <span class="math inline">\(\mathbf{w}\)</span> is greater
than or equal to the weighted F1 score with respect to <span
class="math inline">\(\mathbf{w}&#39;\)</span>. ◻</p>
</div>
<div class="theorem">
<p>The weighted F1 score is upper bounded by the maximum weight. <span
class="math display">\[F1_{\text{weighted}} \leq \max_{i}
w_i\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof follows from the fact that <span
class="math inline">\(F1_i \leq 1\)</span> for all <span
class="math inline">\(i\)</span>. Therefore, the weighted F1 score is
upper bounded by the maximum weight. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>In this section, we provide detailed proofs for the theorems
presented in the previous section.</p>
<div class="proof">
<p><em>Proof of Monotonicity Theorem.</em> Let <span
class="math inline">\(\mathbf{w}\)</span> and <span
class="math inline">\(\mathbf{w}&#39;\)</span> be two weight vectors
such that <span class="math inline">\(w_i \geq w&#39;_i\)</span> for all
<span class="math inline">\(i\)</span>. Then, we have: <span
class="math display">\[\sum_{i=1}^{n} w_i \cdot F1_i = \sum_{i=1}^{n}
(w_i - w&#39;_i) \cdot F1_i + \sum_{i=1}^{n} w&#39;_i \cdot
F1_i\]</span> Since <span class="math inline">\(w_i - w&#39;_i \geq
0\)</span> and <span class="math inline">\(F1_i \leq 1\)</span>, we
have: <span class="math display">\[\sum_{i=1}^{n} (w_i - w&#39;_i) \cdot
F1_i \geq 0\]</span> Therefore, we conclude that: <span
class="math display">\[\sum_{i=1}^{n} w_i \cdot F1_i \geq \sum_{i=1}^{n}
w&#39;_i \cdot F1_i\]</span> ◻</p>
</div>
<div class="proof">
<p><em>Proof of Upper Bound Theorem.</em> Let <span
class="math inline">\(k\)</span> be the index of the maximum weight,
i.e., <span class="math inline">\(w_k = \max_{i} w_i\)</span>. Then, we
have: <span class="math display">\[F1_{\text{weighted}} = \sum_{i=1}^{n}
w_i \cdot F1_i \leq \sum_{i=1}^{n} w_i \cdot 1 = \sum_{i=1}^{n}
w_i\]</span> Since <span class="math inline">\(w_k \geq w_i\)</span> for
all <span class="math inline">\(i\)</span>, we have: <span
class="math display">\[\sum_{i=1}^{n} w_i \leq n \cdot w_k\]</span>
Therefore, we conclude that: <span
class="math display">\[F1_{\text{weighted}} \leq n \cdot w_k = \max_{i}
w_i\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>In this section, we present some important properties and corollaries
related to the weighted F1 score.</p>
<div class="proposition">
<p>The weighted F1 score is invariant under scaling of the weight
vector.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Let <span class="math inline">\(\mathbf{w}\)</span>
be a weight vector and let <span class="math inline">\(\alpha &gt;
0\)</span> be a scaling factor. Then, we have: <span
class="math display">\[F1_{\text{weighted}} = \sum_{i=1}^{n} w_i \cdot
F1_i = \alpha \sum_{i=1}^{n} \frac{w_i}{\alpha} \cdot F1_i\]</span>
Therefore, the weighted F1 score is invariant under scaling of the
weight vector. ◻</p>
</div>
<div class="corollary">
<p>The weighted F1 score is a convex combination of the individual F1
scores.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof follows from the fact that <span
class="math inline">\(\sum_{i=1}^{n} w_i = 1\)</span> when the weight
vector is normalized. Therefore, the weighted F1 score is a convex
combination of the individual F1 scores. ◻</p>
</div>
<div class="remark">
<p>The weighted F1 score can be used to evaluate the performance of a
classification model on imbalanced datasets. By assigning higher weights
to the underrepresented classes, we can ensure that the model focuses
more on these classes.</p>
</div>
<div class="example">
<p>Consider a binary classification problem where the positive class is
underrepresented. We can assign a higher weight to the positive class
and use the weighted F1 score to evaluate the performance of the
classification model.</p>
</div>
</body>
</html>
{% include "footer.html" %}


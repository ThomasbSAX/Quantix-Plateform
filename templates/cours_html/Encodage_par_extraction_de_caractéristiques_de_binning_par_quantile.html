{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par quantile</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par quantile</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques de binning par quantile
est une technique puissante en apprentissage automatique et en analyse
de données. Cette méthode permet de transformer des variables continues
en variables catégorielles tout en préservant l’information statistique
sous-jacente. Le binning par quantile divise les données en intervalles
de manière à ce que chaque intervalle contienne un pourcentage prédéfini
des observations. Cette approche est particulièrement utile pour traiter
les variables continues qui présentent une distribution non uniforme ou
des valeurs aberrantes.</p>
<p>L’idée de binning par quantile remonte aux travaux fondateurs en
statistique descriptive. Cependant, son application dans le contexte de
l’apprentissage automatique a été popularisée plus récemment grâce à sa
capacité à améliorer la performance des modèles prédictifs. En effet, le
binning par quantile permet de réduire la complexité des modèles tout en
capturant les tendances principales des données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir formellement l’encodage par extraction de
caractéristiques de binning par quantile, il est essentiel de comprendre
les concepts sous-jacents. Supposons que nous ayons un ensemble de
données composé de <span class="math inline">\(n\)</span> observations
d’une variable continue <span class="math inline">\(X\)</span>. Notre
objectif est de diviser ces données en <span
class="math inline">\(k\)</span> intervalles, ou bins, de telle sorte
que chaque bin contienne environ <span
class="math inline">\(\frac{n}{k}\)</span> observations.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_n\}\)</span> un ensemble de <span class="math inline">\(n\)</span>
observations d’une variable continue. Le binning par quantile consiste à
diviser <span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> intervalles <span
class="math inline">\([q_{i-1}, q_i]\)</span> pour <span
class="math inline">\(i = 1, \ldots, k\)</span>, où <span
class="math inline">\(q_i\)</span> est le <span
class="math inline">\(i\)</span>-ème quantile de la distribution de
<span class="math inline">\(X\)</span>. Formellement, pour chaque <span
class="math inline">\(i\)</span>, nous avons : <span
class="math display">\[q_i = F_X^{-1}\left(\frac{i}{k}\right)\]</span>
où <span class="math inline">\(F_X\)</span> est la fonction de
répartition de <span class="math inline">\(X\)</span>.</p>
</div>
<p>Une fois les bins définis, nous pouvons extraire des caractéristiques
pour chaque observation. Par exemple, pour une observation <span
class="math inline">\(x_j\)</span>, nous pouvons déterminer le bin
auquel elle appartient et utiliser cette information comme
caractéristique catégorielle.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre les propriétés de l’encodage par extraction de
caractéristiques de binning par quantile, nous pouvons formuler quelques
théorèmes clés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable continue et
<span class="math inline">\(Y\)</span> la variable catégorielle obtenue
par binning par quantile. Si <span class="math inline">\(k\)</span> est
choisi de manière appropriée, alors l’information mutuelle entre <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est maximisée.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous devons montrer que
l’information mutuelle <span class="math inline">\(I(X; Y)\)</span> est
maximale lorsque les bins sont définis par les quantiles. L’information
mutuelle entre <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est donnée par : <span
class="math display">\[I(X; Y) = \sum_{y \in Y} \int_{x \in X} p(x, y)
\log\left(\frac{p(x, y)}{p(x)p(y)}\right) dx\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(Y\)</span> est
défini par les quantiles, nous pouvons montrer que cette quantité est
maximisée lorsque les bins sont équidistants en termes de
probabilité. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème précédent repose sur des concepts avancés de
théorie de l’information et de statistique. Nous allons détailler chaque
étape de la démonstration.</p>
<div class="proof">
<p><em>Proof.</em> Considérons d’abord la définition de l’information
mutuelle : <span class="math display">\[I(X; Y) = \sum_{y \in Y} \int_{x
\in X} p(x, y) \log\left(\frac{p(x, y)}{p(x)p(y)}\right) dx\]</span></p>
<p>Nous voulons maximiser cette quantité par rapport à la partition
<span class="math inline">\(Y\)</span>. En utilisant le fait que <span
class="math inline">\(p(x, y) = p(x|y)p(y)\)</span>, nous pouvons
réécrire l’information mutuelle comme : <span
class="math display">\[I(X; Y) = \sum_{y \in Y} p(y) \int_{x \in X}
p(x|y) \log\left(\frac{p(x|y)}{p(x)}\right) dx\]</span></p>
<p>Ensuite, nous utilisons le fait que <span class="math inline">\(p(y)
= \frac{1}{k}\)</span> pour chaque <span class="math inline">\(y \in
Y\)</span>, car les bins sont équidistants en termes de probabilité.
Ainsi, l’information mutuelle devient : <span
class="math display">\[I(X; Y) = \frac{1}{k} \sum_{y \in Y} \int_{x \in
X} p(x|y) \log\left(\frac{p(x|y)}{p(x)}\right) dx\]</span></p>
<p>Enfin, nous utilisons le fait que <span
class="math inline">\(p(x|y)\)</span> est uniforme dans chaque bin pour
montrer que cette quantité est maximisée lorsque les bins sont définis
par les quantiles. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’encodage par extraction de caractéristiques de binning par quantile
possède plusieurs propriétés intéressantes. Nous allons en énumérer
quelques-unes et fournir des preuves détaillées.</p>
<ol>
<li><p><strong>Conservation de la moyenne</strong> : La moyenne des
observations dans chaque bin est égale à la moyenne globale des
données.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\mu\)</span> la
moyenne globale des observations. Pour chaque bin <span
class="math inline">\(i\)</span>, nous avons : <span
class="math display">\[\mu_i = \frac{1}{|B_i|} \sum_{x_j \in B_i}
x_j\]</span> où <span class="math inline">\(B_i\)</span> est le <span
class="math inline">\(i\)</span>-ème bin. En utilisant le fait que les
bins sont définis par les quantiles, nous pouvons montrer que <span
class="math inline">\(\mu_i = \mu\)</span> pour chaque <span
class="math inline">\(i\)</span>. ◻</p>
</div></li>
<li><p><strong>Réduction de la variance</strong> : La variance des
observations dans chaque bin est inférieure ou égale à la variance
globale des données.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\sigma^2\)</span> la
variance globale des observations. Pour chaque bin <span
class="math inline">\(i\)</span>, nous avons : <span
class="math display">\[\sigma_i^2 = \frac{1}{|B_i|} \sum_{x_j \in B_i}
(x_j - \mu_i)^2\]</span> En utilisant le fait que les bins sont définis
par les quantiles, nous pouvons montrer que <span
class="math inline">\(\sigma_i^2 \leq \sigma^2\)</span> pour chaque
<span class="math inline">\(i\)</span>. ◻</p>
</div></li>
<li><p><strong>Stabilité des quantiles</strong> : Les quantiles sont
stables par rapport aux perturbations des données.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X&#39;\)</span> une
version perturbée de <span class="math inline">\(X\)</span>. Nous
voulons montrer que les quantiles de <span
class="math inline">\(X&#39;\)</span> sont proches des quantiles de
<span class="math inline">\(X\)</span>. En utilisant le théorème de la
convergence des quantiles, nous pouvons montrer que : <span
class="math display">\[\lim_{n \to \infty} P\left(\sup_{0 &lt; p &lt; 1}
|F_{X&#39;}^{-1}(p) - F_X^{-1}(p)| &gt; \epsilon\right) = 0\]</span>
pour tout <span class="math inline">\(\epsilon &gt; 0\)</span>. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par quantile
est une technique puissante pour transformer des variables continues en
variables catégorielles. Cette méthode préserve l’information
statistique sous-jacente tout en réduisant la complexité des modèles
prédictifs. Les propriétés et théorèmes présentés dans cet article
montrent que cette technique est robuste et efficace pour l’analyse de
données.</p>
</body>
</html>
{% include "footer.html" %}


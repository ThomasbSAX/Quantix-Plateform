{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Inégalité de Cramér-Rao : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inégalité de Cramér-Rao : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Cramér-Rao émerge dans le cadre de la théorie de
l’estimation statistique, un domaine où l’on cherche à inférer des
paramètres inconnus d’une distribution de probabilité à partir
d’observations échantillonnées. Cette inégalité, formulée indépendamment
par Harald Cramér en 1946 et C. R. Rao en 1945, établit une borne
inférieure sur la variance d’un estimateur non biaisé. Elle est
indispensable pour évaluer la performance des estimateurs et pour
comprendre les limites fondamentales de l’estimation paramétrique.</p>
<p>L’origine historique de cette inégalité est profondément liée aux
travaux pionniers en théorie des probabilités et en statistique
mathématique. Elle résout le problème crucial de quantifier la précision
avec laquelle un paramètre peut être estimé, en fournissant une limite
théorique que tout estimateur non biaisé ne peut pas dépasser. Cette
notion est indispensable dans de nombreux domaines appliqués, tels que
l’ingénierie, la physique, et les sciences sociales, où l’estimation
précise des paramètres est souvent cruciale.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’inégalité de Cramér-Rao, commençons par comprendre
ce que nous cherchons à quantifier. Supposons que nous ayons un
échantillon de données <span class="math inline">\(X_1, X_2, \ldots,
X_n\)</span> indépendamment et identiquement distribuées (i.i.d.) selon
une distribution de probabilité <span
class="math inline">\(P_\theta\)</span>, où <span
class="math inline">\(\theta\)</span> est un paramètre inconnu que nous
souhaitons estimer. Nous cherchons à évaluer la précision de notre
estimation, c’est-à-dire la variance de l’estimateur.</p>
<p>Formellement, un estimateur <span class="math inline">\(T\)</span>
d’un paramètre <span class="math inline">\(\theta\)</span> est une
fonction mesurable <span class="math inline">\(T: \mathbb{R}^n
\rightarrow \mathbb{R}\)</span>. L’estimateur est dit non biaisé si
<span class="math inline">\(\mathbb{E}[T] = \theta\)</span>. La variance
de l’estimateur est donnée par <span class="math inline">\(\text{Var}(T)
= \mathbb{E}[(T - \theta)^2]\)</span>.</p>
<p>L’inégalité de Cramér-Rao établit une borne inférieure sur cette
variance. Pour ce faire, nous devons introduire la notion de score et
d’information de Fisher.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f(x; \theta)\)</span> la densité de
probabilité d’une variable aléatoire <span
class="math inline">\(X\)</span> paramétrée par <span
class="math inline">\(\theta\)</span>. Le score est défini comme la
dérivée du logarithme de la densité par rapport à <span
class="math inline">\(\theta\)</span> : <span
class="math display">\[S(\theta, X) = \frac{\partial}{\partial \theta}
\log f(X; \theta)\]</span></p>
</div>
<div class="definition">
<p>L’information de Fisher est l’espérance du carré du score : <span
class="math display">\[I(\theta) = \mathbb{E}\left[\left(
\frac{\partial}{\partial \theta} \log f(X; \theta)
\right)^2\right]\]</span></p>
</div>
<h1 id="théorème-de-cramér-rao">Théorème de Cramér-Rao</h1>
<p>Nous cherchons maintenant à établir une borne inférieure sur la
variance d’un estimateur non biaisé. Intuitivement, plus l’information
de Fisher est grande, plus nous pouvons espérer estimer précisément le
paramètre <span class="math inline">\(\theta\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(T\)</span> un estimateur non biaisé
de <span class="math inline">\(\theta\)</span>, c’est-à-dire <span
class="math inline">\(\mathbb{E}[T] = \theta\)</span>. Supposons que les
conditions régularité suivantes soient satisfaites :</p>
<ol>
<li><p>La densité <span class="math inline">\(f(x; \theta)\)</span> est
différentiable par rapport à <span
class="math inline">\(\theta\)</span>.</p></li>
<li><p>L’espérance et la variance de <span
class="math inline">\(T\)</span> existent.</p></li>
<li><p>Il est possible d’inverser l’ordre des dérivées et des
intégrales.</p></li>
</ol>
<p>Alors, la variance de <span class="math inline">\(T\)</span> est
bornée inférieurement par l’inverse de l’information de Fisher : <span
class="math display">\[\text{Var}(T) \geq
\frac{1}{I(\theta)}\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Cramér-Rao, nous allons utiliser le
théorème de Cauchy-Schwarz et les propriétés du score.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’espérance du produit de <span
class="math inline">\(T\)</span> et du score : <span
class="math display">\[\mathbb{E}\left[ (T - \theta) S(\theta, X)
\right] = \mathbb{E}\left[ T S(\theta, X) \right] - \theta
\mathbb{E}\left[ S(\theta, X) \right]\]</span> Puisque <span
class="math inline">\(T\)</span> est non biaisé et que <span
class="math inline">\(\mathbb{E}\left[ S(\theta, X) \right] = 0\)</span>
(car <span class="math inline">\(\frac{\partial}{\partial \theta}
\mathbb{E}[1] = 0\)</span>), nous avons : <span
class="math display">\[\mathbb{E}\left[ (T - \theta) S(\theta, X)
\right] = \mathbb{E}\left[ T S(\theta, X) \right]\]</span></p>
<p>Appliquons le théorème de Cauchy-Schwarz à <span
class="math inline">\((T - \theta)\)</span> et <span
class="math inline">\(S(\theta, X)\)</span> : <span
class="math display">\[\left( \mathbb{E}\left[ (T - \theta) S(\theta, X)
\right] \right)^2 \leq \mathbb{E}\left[ (T - \theta)^2 \right]
\mathbb{E}\left[ S(\theta, X)^2 \right]\]</span> Ce qui donne : <span
class="math display">\[\left( \mathbb{E}\left[ T S(\theta, X) \right]
\right)^2 \leq \text{Var}(T) I(\theta)\]</span></p>
<p>Maintenant, calculons <span class="math inline">\(\mathbb{E}\left[ T
S(\theta, X) \right]\)</span>. En utilisant la définition du score et en
inversant l’ordre des dérivées et des intégrales, nous avons : <span
class="math display">\[\mathbb{E}\left[ T S(\theta, X) \right] = \int
T(x) \frac{\partial}{\partial \theta} f(x; \theta) \, dx =
\frac{\partial}{\partial \theta} \int T(x) f(x; \theta) \, dx =
\frac{\partial}{\partial \theta} \mathbb{E}[T] = 1\]</span></p>
<p>En substituant dans l’inégalité de Cauchy-Schwarz, nous obtenons :
<span class="math display">\[1 \leq \text{Var}(T) I(\theta)\]</span> Ce
qui implique : <span class="math display">\[\text{Var}(T) \geq
\frac{1}{I(\theta)}\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Cramér-Rao a plusieurs propriétés importantes et
corollaires qui en découlent.</p>
<div class="corollary">
<p>Un estimateur <span class="math inline">\(T\)</span> est dit efficace
s’il atteint la borne de Cramér-Rao, c’est-à-dire : <span
class="math display">\[\text{Var}(T) = \frac{1}{I(\theta)}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle directement de l’inégalité de
Cramér-Rao. Si un estimateur atteint la borne, alors il est
efficace. ◻</p>
</div>
<div class="corollary">
<p>Pour un échantillon de taille <span class="math inline">\(n\)</span>,
l’information de Fisher totale est la somme des informations de Fisher
pour chaque observation : <span class="math display">\[I_n(\theta) = n
I(\theta)\]</span> L’inégalité de Cramér-Rao devient alors : <span
class="math display">\[\text{Var}(T) \geq \frac{1}{n
I(\theta)}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve suit directement de la linéarité de
l’espérance et de la variance pour les échantillons i.i.d. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de Cramér-Rao est un résultat fondamental en théorie de
l’estimation statistique. Elle fournit une borne inférieure sur la
variance des estimateurs non biaisés, permettant d’évaluer la précision
théorique maximale que l’on peut atteindre. Les preuves et les
corollaires associés montrent comment cette inégalité peut être
appliquée dans divers contextes, rendant ce résultat indispensable pour
la compréhension des limites de l’estimation paramétrique.</p>
</body>
</html>
{% include "footer.html" %}


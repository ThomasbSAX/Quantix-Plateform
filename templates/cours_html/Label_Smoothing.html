{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Label Smoothing: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Label Smoothing: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>In the realm of machine learning, particularly in classification
tasks, the concept of label smoothing has emerged as a powerful
technique to enhance model generalization and robustness. The idea is
rooted in the observation that real-world data often contains noise and
ambiguities, which are not adequately captured by the traditional
one-hot encoding of labels. Label smoothing aims to address this by
introducing a form of regularization that encourages the model to
distribute its confidence more evenly across different classes, rather
than being overly confident about a single class.</p>
<p>The motivation behind label smoothing is twofold. Firstly, it acts as
a regularizer, preventing the model from becoming overly confident in
its predictions, which can lead to poor generalization. Secondly, it
provides a more realistic representation of the data, acknowledging that
labels may not be perfectly accurate or distinct. This is particularly
relevant in scenarios where the training data might be noisy or when
dealing with fine-grained classification tasks.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand label smoothing, let us first consider the traditional
approach to labeling data. In one-hot encoding, each training example is
assigned a label vector <span class="math inline">\(y\)</span> where all
elements are zero except for the index corresponding to the true class,
which is set to one. Formally, for a classification task with <span
class="math inline">\(K\)</span> classes, the label vector <span
class="math inline">\(y\)</span> is defined as:</p>
<p><span class="math display">\[y_i = \begin{cases}
1 &amp; \text{if } i = c, \\
0 &amp; \text{otherwise},
\end{cases}\]</span></p>
<p>where <span class="math inline">\(c\)</span> is the true class
index.</p>
<p>Label smoothing modifies this approach by distributing a small
fraction of the confidence from the true class to all other classes. The
smoothed label vector <span class="math inline">\(\tilde{y}\)</span> is
defined as:</p>
<p><span class="math display">\[\tilde{y}_i = \begin{cases}
1 - \alpha &amp; \text{if } i = c, \\
\frac{\alpha}{K - 1} &amp; \text{otherwise},
\end{cases}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the smoothing
parameter, typically a small value such as 0.1.</p>
<p>In other words, for every training example, the label smoothing
technique adjusts the one-hot encoded vector by reducing the confidence
in the true class and redistributing it equally among all other classes.
This can be expressed more formally using quantifiers:</p>
<p><span class="math display">\[\forall i \in \{1, \dots, K\}, \quad
\tilde{y}_i = (1 - \alpha) \cdot \mathbb{1}_{i=c} + \frac{\alpha}{K - 1}
\cdot \mathbb{1}_{i \neq c},\]</span></p>
<p>where <span class="math inline">\(\mathbb{1}_{i=c}\)</span> is the
indicator function that equals 1 if <span class="math inline">\(i =
c\)</span> and 0 otherwise.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key theoretical insights into label smoothing is its
connection to the concept of Bayesian inference and regularization. The
following theorem establishes a formal link between label smoothing and
the maximum a posteriori (MAP) estimation.</p>
<div class="theorem">
<p>Let <span class="math inline">\(p(y|x, \theta)\)</span> be the
predicted probability distribution over classes given input <span
class="math inline">\(x\)</span> and parameters <span
class="math inline">\(\theta\)</span>. The label smoothing technique can
be interpreted as adding a regularization term to the log-likelihood
function.</p>
<p>Specifically, the smoothed log-likelihood is given by:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} =
\mathbb{E}_{(x, y) \sim D} \left[ \sum_{i=1}^K \tilde{y}_i \log p(y=i|x,
\theta) \right],\]</span></p>
<p>where <span class="math inline">\(D\)</span> is the training data
distribution.</p>
<p>This can be rewritten as:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} =
\mathcal{L} + \alpha \cdot \mathbb{E}_{(x, y) \sim D} \left[ \sum_{i
\neq c} \log p(y=i|x, \theta) - \log p(y=c|x, \theta)
\right],\]</span></p>
<p>where <span class="math inline">\(\mathcal{L}\)</span> is the
traditional log-likelihood.</p>
</div>
<p>The proof of this theorem relies on the properties of logarithmic
functions and the definition of label smoothing. By expanding the
smoothed log-likelihood, we can see that it introduces an additional
term that penalizes overconfidence in the predicted probabilities.</p>
<h1 id="proofs">Proofs</h1>
<p>To prove Theorem 1, let us start by expanding the smoothed
log-likelihood:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} =
\mathbb{E}_{(x, y) \sim D} \left[ (1 - \alpha) \cdot \log p(y=c|x,
\theta) + \sum_{i \neq c} \frac{\alpha}{K - 1} \cdot \log p(y=i|x,
\theta) \right].\]</span></p>
<p>This can be rewritten as:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} = (1 -
\alpha) \cdot \mathbb{E}_{(x, y) \sim D} \left[ \log p(y=c|x, \theta)
\right] + \frac{\alpha}{K - 1} \cdot \mathbb{E}_{(x, y) \sim D} \left[
\sum_{i \neq c} \log p(y=i|x, \theta) \right].\]</span></p>
<p>Now, let us consider the traditional log-likelihood:</p>
<p><span class="math display">\[\mathcal{L} = \mathbb{E}_{(x, y) \sim D}
\left[ \log p(y=c|x, \theta) \right].\]</span></p>
<p>Subtracting <span class="math inline">\(\mathcal{L}\)</span> from
<span class="math inline">\(\mathcal{L}_{\text{smoothed}}\)</span>, we
obtain:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} -
\mathcal{L} = -\alpha \cdot \mathbb{E}_{(x, y) \sim D} \left[ \log
p(y=c|x, \theta) \right] + \frac{\alpha}{K - 1} \cdot \mathbb{E}_{(x, y)
\sim D} \left[ \sum_{i \neq c} \log p(y=i|x, \theta)
\right].\]</span></p>
<p>This can be further simplified to:</p>
<p><span class="math display">\[\mathcal{L}_{\text{smoothed}} =
\mathcal{L} + \alpha \cdot \mathbb{E}_{(x, y) \sim D} \left[ \sum_{i
\neq c} \log p(y=i|x, \theta) - \log p(y=c|x, \theta)
\right].\]</span></p>
<p>This completes the proof of Theorem 1.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Label smoothing exhibits several important properties that contribute
to its effectiveness as a regularization technique. We list and discuss
these properties below.</p>
<ol>
<li><p>**Reduction of Overconfidence**: Label smoothing reduces the
model’s confidence in its predictions, which can lead to improved
generalization. This is particularly beneficial when dealing with noisy
or ambiguous data.</p></li>
<li><p>**Improved Calibration**: By distributing the confidence more
evenly across classes, label smoothing helps to calibrate the predicted
probabilities better. This means that the model’s confidence scores are
more aligned with the true likelihood of the events.</p></li>
<li><p>**Robustness to Label Noise**: Label smoothing makes the model
more robust to label noise, as it does not rely solely on the true class
label but also considers other classes. This can be particularly useful
in scenarios where the training data contains errors or
ambiguities.</p></li>
</ol>
<p>To further illustrate these properties, let us consider the following
corollary:</p>
<div class="corollary">
<p>Let <span class="math inline">\(p(y|x, \theta)\)</span> be the
predicted probability distribution over classes given input <span
class="math inline">\(x\)</span> and parameters <span
class="math inline">\(\theta\)</span>. The use of label smoothing leads
to better calibrated probabilities, as measured by the expected
calibration error (ECE).</p>
<p>Formally, for a given threshold <span
class="math inline">\(t\)</span>, the ECE is defined as:</p>
<p><span class="math display">\[\text{ECE} = \mathbb{E}_{(x, y) \sim D}
\left[ |P(p(y|x, \theta) \geq t | y) - P(p(y|x, \theta) &lt; t)|
\right].\]</span></p>
<p>The application of label smoothing reduces the ECE, indicating better
calibration.</p>
</div>
<p>The proof of this corollary relies on the properties of label
smoothing and the definition of calibration. By distributing the
confidence more evenly across classes, label smoothing ensures that the
predicted probabilities are more aligned with the true likelihood of the
events, leading to a reduction in the ECE.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Label smoothing is a powerful technique that enhances model
generalization and robustness in classification tasks. By introducing a
form of regularization that distributes the confidence more evenly
across classes, label smoothing addresses the limitations of traditional
one-hot encoding and provides a more realistic representation of the
data. The theoretical insights and properties discussed in this article
highlight the effectiveness of label smoothing as a regularization
technique, making it an invaluable tool for machine learning
practitioners.</p>
</body>
</html>
{% include "footer.html" %}


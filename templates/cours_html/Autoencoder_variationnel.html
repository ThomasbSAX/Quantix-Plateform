{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Autoencoder variationnel : Une approche probabiliste pour l’apprentissage non supervisé</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Autoencoder variationnel : Une approche probabiliste
pour l’apprentissage non supervisé</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage non supervisé est un domaine clé de l’intelligence
artificielle, visant à découvrir des structures cachées dans des données
non étiquetées. Parmi les nombreuses approches, l’autoencoder
variationnel (VAE) se distingue par son cadre probabiliste rigoureux.
Introduit par Kingma et Welling en 2014, le VAE combine les concepts des
autoencodeurs traditionnels et de l’inférence variationnelle, offrant
une méthode puissante pour générer des données réalistes et capturer la
distribution sous-jacente des données.</p>
<p>L’émergence du VAE répond à un besoin crucial : la capacité de
modéliser des distributions complexes tout en permettant une génération
efficace de nouvelles instances. Contrairement aux autoencodeurs
classiques, qui apprennent des représentations déterministes, le VAE
introduit une incertitude naturelle dans les représentations latentes,
rendant le modèle plus robuste et flexible.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le VAE, il est essentiel de définir plusieurs
concepts fondamentaux. Commençons par l’idée d’un autoencoder, qui est
un réseau de neurones conçu pour reconstruire ses entrées. Un VAE étend
cette idée en introduisant une distribution probabiliste sur les
représentations latentes.</p>
<h2 class="unnumbered" id="autoencoder">Autoencoder</h2>
<p>Un autoencoder est un réseau de neurones composé de deux parties
principales : un encodeur et un décodeur. L’encodeur transforme l’entrée
<span class="math inline">\(x\)</span> en une représentation latente
<span class="math inline">\(z\)</span>, et le décodeur reconstruit
l’entrée à partir de cette représentation.</p>
<p>Formellement, un autoencoder peut être défini comme suit : <span
class="math display">\[z = f_{\text{enc}}(x), \quad \hat{x} =
f_{\text{dec}}(z)\]</span> où <span
class="math inline">\(f_{\text{enc}}\)</span> et <span
class="math inline">\(f_{\text{dec}}\)</span> sont des fonctions
paramétrées par des réseaux de neurones.</p>
<h2 class="unnumbered" id="autoencoder-variationnel">Autoencoder
Variationnel</h2>
<p>Un VAE introduit une distribution probabiliste sur les
représentations latentes. Plutôt que de mapper directement <span
class="math inline">\(x\)</span> à <span
class="math inline">\(z\)</span>, l’encodeur produit les paramètres
d’une distribution, typiquement une distribution gaussienne.</p>
<p>Formellement, un VAE peut être défini comme suit : <span
class="math display">\[z \sim q_{\phi}(z|x), \quad \hat{x} \sim
p_{\theta}(x|z)\]</span> où <span
class="math inline">\(q_{\phi}(z|x)\)</span> est la distribution encodée
et <span class="math inline">\(p_{\theta}(x|z)\)</span> est la
distribution décodée. Les paramètres <span
class="math inline">\(\phi\)</span> et <span
class="math inline">\(\theta\)</span> sont appris pour maximiser la
vraisemblance des données.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le théorème central du VAE est basé sur l’inférence variationnelle,
qui permet d’approximer la distribution postérieure <span
class="math inline">\(p(z|x)\)</span> par une distribution plus simple
<span class="math inline">\(q_{\phi}(z|x)\)</span>.</p>
<h2 class="unnumbered"
id="théorème-de-lentropie-variationnelle">Théorème de l’Entropie
Variationnelle</h2>
<p>Le théorème de l’entropie variationnelle stipule que la différence
entre l’énergie libre variationnelle et la log-vraisemblance peut être
exprimée en termes de divergence de Kullback-Leibler.</p>
<p>Formellement, le théorème peut être énoncé comme suit : <span
class="math display">\[\log p_{\theta}(x) \geq
\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] -
D_{\text{KL}}(q_{\phi}(z|x) \| p(z))\]</span> où <span
class="math inline">\(D_{\text{KL}}\)</span> est la divergence de
Kullback-Leibler.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de l’entropie variationnelle repose sur des
concepts d’information et de théorie des probabilités.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-lentropie-variationnelle">Preuve du Théorème
de l’Entropie Variationnelle</h2>
<p>Commençons par exprimer la log-vraisemblance <span
class="math inline">\(\log p_{\theta}(x)\)</span> en utilisant la règle
de Bayes : <span class="math display">\[\log p_{\theta}(x) = \log \int
p_{\theta}(x,z) \, dz\]</span> En utilisant l’inégalité de Jensen, nous
avons : <span class="math display">\[\log p_{\theta}(x) \geq
\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x,z)]\]</span> En développant
<span class="math inline">\(\log p_{\theta}(x,z)\)</span>, nous obtenons
: <span class="math display">\[\log p_{\theta}(x,z) = \log
p_{\theta}(x|z) + \log p(z)\]</span> En substituant cette expression
dans l’inégalité précédente, nous avons : <span
class="math display">\[\log p_{\theta}(x) \geq
\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] +
\mathbb{E}_{q_{\phi}(z|x)}[\log p(z)]\]</span> En utilisant la
définition de la divergence de Kullback-Leibler, nous pouvons réécrire
<span class="math inline">\(\mathbb{E}_{q_{\phi}(z|x)}[\log
p(z)]\)</span> comme : <span
class="math display">\[\mathbb{E}_{q_{\phi}(z|x)}[\log p(z)] =
-D_{\text{KL}}(q_{\phi}(z|x) \| p(z)) + H(q_{\phi}(z|x))\]</span> En
combinant ces résultats, nous obtenons finalement : <span
class="math display">\[\log p_{\theta}(x) \geq
\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] -
D_{\text{KL}}(q_{\phi}(z|x) \| p(z))\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le VAE possède plusieurs propriétés intéressantes qui en font un
outil puissant pour l’apprentissage non supervisé.</p>
<h2 class="unnumbered" id="propriétés">Propriétés</h2>
<ol>
<li><p>**Représentations Latentes Probabilistes** : Le VAE modélise les
représentations latentes comme des distributions probabilistes,
permettant de capturer l’incertitude dans les données.</p></li>
<li><p>**Génération de Données** : Le VAE permet de générer de nouvelles
données en échantillonnant à partir de la distribution latente
apprise.</p></li>
<li><p>**Robustesse** : La nature probabiliste du VAE le rend plus
robuste aux variations dans les données d’entrée.</p></li>
</ol>
<h2 class="unnumbered" id="corollaires">Corollaires</h2>
<ol>
<li><p>**Corollaire de la Génération** : Étant donné un VAE entraîné,
pour tout <span class="math inline">\(z\)</span> échantillonné à partir
de la distribution latente apprise, l’entrée reconstruite <span
class="math inline">\(\hat{x}\)</span> suit une distribution similaire à
la distribution des données d’origine.</p></li>
<li><p>**Corollaire de la Robustesse** : Le VAE est plus robuste aux
variations dans les données d’entrée que les autoencodeurs classiques,
car il modélise explicitement l’incertitude dans les représentations
latentes.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’autoencoder variationnel représente une avancée significative dans
le domaine de l’apprentissage non supervisé. En combinant les concepts
des autoencodeurs traditionnels et de l’inférence variationnelle, le VAE
offre une méthode puissante pour modéliser des distributions complexes
et générer de nouvelles données. Les propriétés probabilistes du VAE en
font un outil précieux pour de nombreuses applications, allant de la
génération d’images à l’analyse de données.</p>
</body>
</html>
{% include "footer.html" %}


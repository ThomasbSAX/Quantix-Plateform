{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Log-Cosh Loss: A Smooth Alternative to L1 Loss</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Log-Cosh Loss: A Smooth Alternative to L1 Loss</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-and-motivations">Introduction
and Motivations</h1>
<p>The Log-Cosh loss function, a smooth alternative to the L1 loss
(absolute value), has gained prominence in robust regression and machine
learning. Its origin can be traced back to the need for a differentiable
approximation of the absolute value function, which is
non-differentiable at zero. The Log-Cosh loss emerges from the desire to
balance robustness and smoothness, making it indispensable in
optimization problems where gradient-based methods are employed.</p>
<p>The Log-Cosh loss is particularly useful when dealing with outliers,
as it grows linearly for large residuals and quadratically for small
residuals. This behavior makes it robust to outliers while maintaining
differentiability everywhere, unlike the L1 loss which is
non-differentiable at zero. The Log-Cosh loss is defined in terms of the
hyperbolic cosine function, which has a long history in mathematics and
physics.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>To understand the Log-Cosh loss, let’s first consider what we want to
achieve. We seek a loss function that is robust to outliers,
differentiable everywhere, and grows linearly for large residuals. The
hyperbolic cosine function, defined as <span
class="math display">\[\cosh(x) = \frac{e^x + e^{-x}}{2},\]</span> has a
U-shaped graph similar to the absolute value function but is smooth and
differentiable everywhere. The Log-Cosh loss takes the natural logarithm
of the hyperbolic cosine of the residuals.</p>
<p>Formally, for a residual <span class="math inline">\(r\)</span>, the
Log-Cosh loss is defined as: <span
class="math display">\[L_{\text{Log-Cosh}}(r) =
\ln(\cosh(r)).\]</span></p>
<p>Alternatively, using the definition of the hyperbolic cosine, we can
write: <span class="math display">\[L_{\text{Log-Cosh}}(r) =
\ln\left(\frac{e^r + e^{-r}}{2}\right).\]</span></p>
<p>For a vector of residuals <span class="math inline">\(\mathbf{r} =
(r_1, r_2, \ldots, r_n)\)</span>, the Log-Cosh loss is defined as: <span
class="math display">\[L_{\text{Log-Cosh}}(\mathbf{r}) = \sum_{i=1}^n
\ln(\cosh(r_i)).\]</span></p>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>One of the key properties of the Log-Cosh loss is its relationship
with the L1 and L2 losses. Specifically, we have the following
theorem:</p>
<div class="theorem">
<p>For any residual <span class="math inline">\(r\)</span>, the Log-Cosh
loss satisfies: <span class="math display">\[|r| - \ln(2) \leq
\ln(\cosh(r)) \leq \frac{r^2}{2}.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> To prove this theorem, we start by considering the
Taylor series expansion of <span
class="math inline">\(\cosh(r)\)</span>: <span
class="math display">\[\cosh(r) = 1 + \frac{r^2}{2!} + \frac{r^4}{4!} +
\cdots.\]</span></p>
<p>Taking the natural logarithm of both sides, we get: <span
class="math display">\[\ln(\cosh(r)) = \ln\left(1 + \frac{r^2}{2!} +
\frac{r^4}{4!} + \cdots\right).\]</span></p>
<p>For small <span class="math inline">\(r\)</span>, the higher-order
terms become negligible, and we can approximate: <span
class="math display">\[\ln(\cosh(r)) \approx \frac{r^2}{2}.\]</span></p>
<p>For large <span class="math inline">\(r\)</span>, the term <span
class="math inline">\(e^r\)</span> dominates, and we have: <span
class="math display">\[\cosh(r) \approx \frac{e^r}{2},\]</span> so <span
class="math display">\[\ln(\cosh(r)) \approx r - \ln(2).\]</span></p>
<p>Combining these results, we obtain the desired inequality: <span
class="math display">\[|r| - \ln(2) \leq \ln(\cosh(r)) \leq
\frac{r^2}{2}.\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>The Log-Cosh loss has several important properties that make it
useful in practice. We list and prove them below.</p>
<ol>
<li><p>The Log-Cosh loss is differentiable everywhere.</p>
<div class="proof">
<p><em>Proof.</em> The hyperbolic cosine function <span
class="math inline">\(\cosh(r)\)</span> is differentiable everywhere,
and its derivative is the hyperbolic sine function <span
class="math inline">\(\sinh(r)\)</span>. The natural logarithm function
<span class="math inline">\(\ln(x)\)</span> is differentiable for <span
class="math inline">\(x &gt; 0\)</span>. Since <span
class="math inline">\(\cosh(r) &gt; 0\)</span> for all real <span
class="math inline">\(r\)</span>, the composition <span
class="math inline">\(\ln(\cosh(r))\)</span> is differentiable
everywhere. ◻</p>
</div></li>
<li><p>The Log-Cosh loss is convex.</p>
<div class="proof">
<p><em>Proof.</em> To show that the Log-Cosh loss is convex, we need to
demonstrate that its second derivative is non-negative for all real
<span class="math inline">\(r\)</span>. The first derivative of the
Log-Cosh loss is: <span class="math display">\[\frac{d}{dr}
\ln(\cosh(r)) = \tanh(r).\]</span></p>
<p>The second derivative is: <span
class="math display">\[\frac{d^2}{dr^2} \ln(\cosh(r)) =
\text{sech}^2(r).\]</span></p>
<p>Since <span class="math inline">\(\text{sech}^2(r) &gt; 0\)</span>
for all real <span class="math inline">\(r\)</span>, the Log-Cosh loss
is convex. ◻</p>
</div></li>
<li><p>The Log-Cosh loss grows linearly for large residuals and
quadratically for small residuals.</p>
<div class="proof">
<p><em>Proof.</em> For small <span class="math inline">\(r\)</span>, we
have the approximation: <span class="math display">\[\ln(\cosh(r))
\approx \frac{r^2}{2}.\]</span></p>
<p>For large <span class="math inline">\(r\)</span>, we have the
approximation: <span class="math display">\[\ln(\cosh(r)) \approx |r| -
\ln(2).\]</span></p>
<p>This behavior is similar to the L1 loss for large residuals and the
L2 loss for small residuals, making the Log-Cosh loss robust to
outliers. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>The Log-Cosh loss function provides a smooth and robust alternative
to the L1 loss, making it indispensable in optimization problems where
gradient-based methods are employed. Its differentiability everywhere
and convexity make it a powerful tool in machine learning and robust
regression.</p>
</body>
</html>
{% include "footer.html" %}


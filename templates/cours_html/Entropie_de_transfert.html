{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie de transfert : une mesure de l’information dans les systèmes dynamiques</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie de transfert : une mesure de l’information
dans les systèmes dynamiques</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de transfert est un concept fondamental en théorie des
systèmes dynamiques et en physique statistique. Elle émerge comme une
généralisation de l’entropie classique de Boltzmann et Shannon,
permettant de quantifier le transfert d’information dans des systèmes
évoluant au cours du temps. Cette notion est indispensable pour
comprendre les propriétés asymptotiques des systèmes dynamiques, en
particulier ceux qui présentent un comportement chaotique.</p>
<p>L’entropie de transfert a été introduite pour la première fois par
Ruelle en 1976, dans le cadre de l’étude des systèmes dynamiques
hyperboliques. Elle a depuis trouvé des applications dans divers
domaines, allant de la théorie ergodique à la thermodynamique
statistique, en passant par l’analyse des systèmes complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir formellement l’entropie de transfert, il est
essentiel de comprendre ce que nous cherchons à mesurer. Imaginons un
système dynamique évoluant dans le temps, où chaque état du système peut
être considéré comme une source d’information. Nous voulons quantifier
la quantité d’information perdue ou gagnée lorsque le système évolue
d’un état à un autre.</p>
<p>Considérons un système dynamique <span class="math inline">\((X,
\mathcal{B}, \mu, T)\)</span>, où <span class="math inline">\(X\)</span>
est l’espace des phases, <span
class="math inline">\(\mathcal{B}\)</span> une <span
class="math inline">\(\sigma\)</span>-algèbre sur <span
class="math inline">\(X\)</span>, <span
class="math inline">\(\mu\)</span> une mesure invariante par <span
class="math inline">\(T\)</span>, et <span class="math inline">\(T: X
\rightarrow X\)</span> un endomorphisme mesurable. Nous cherchons à
définir une quantité qui mesure le taux de création d’information par
l’application <span class="math inline">\(T\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\((X, \mathcal{B}, \mu, T)\)</span> un
système dynamique mesurable. Soit <span class="math inline">\(\alpha =
\{A_1, A_2, \ldots, A_n\}\)</span> une partition mesurable de <span
class="math inline">\(X\)</span>. L’entropie de la partition <span
class="math inline">\(\alpha\)</span> est définie par: <span
class="math display">\[h(\alpha) = -\sum_{i=1}^n \mu(A_i) \log
\mu(A_i).\]</span> L’entropie de transfert du système dynamique <span
class="math inline">\((X, \mathcal{B}, \mu, T)\)</span> est alors
définie comme: <span class="math display">\[h(T) = \sup_{\alpha} h(T,
\alpha),\]</span> où la borne supérieure est prise sur toutes les
partitions mesurables <span class="math inline">\(\alpha\)</span> de
<span class="math inline">\(X\)</span>, et où <span
class="math inline">\(h(T, \alpha)\)</span> est l’entropie de transfert
relative à la partition <span class="math inline">\(\alpha\)</span>,
définie par: <span class="math display">\[h(T, \alpha) = \lim_{n
\rightarrow \infty} \frac{1}{n} h\left(\bigvee_{k=0}^{n-1}
T^{-k}\alpha\right).\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’un des théorèmes fondamentaux concernant l’entropie de transfert
est le théorème de Kolmogorov-Sinai, qui établit que l’entropie de
transfert peut être calculée à partir d’une partition génératrice.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X, \mathcal{B}, \mu, T)\)</span> un
système dynamique mesurable et soit <span
class="math inline">\(\alpha\)</span> une partition génératrice de <span
class="math inline">\(X\)</span>. Alors: <span
class="math display">\[h(T) = h(T, \alpha).\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kolmogorov-Sinai, nous suivons les étapes
suivantes:</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\alpha\)</span> une
partition génératrice de <span class="math inline">\(X\)</span>. Nous
voulons montrer que <span class="math inline">\(h(T) = h(T,
\alpha)\)</span>.</p>
<p>1. Tout d’abord, nous notons que pour toute partition <span
class="math inline">\(\beta\)</span>, nous avons: <span
class="math display">\[h(T, \alpha \vee \beta) = h(T, \alpha) + h(T,
\beta | \alpha \vee T^{-1}\beta \vee \ldots \vee
T^{-(n-1)}\beta).\]</span> En prenant la limite quand <span
class="math inline">\(n\)</span> tend vers l’infini, nous obtenons:
<span class="math display">\[h(T, \alpha \vee \beta) = h(T, \alpha) +
h(T, \beta | \alpha).\]</span></p>
<p>2. Ensuite, nous utilisons le fait que <span
class="math inline">\(\alpha\)</span> est une partition génératrice pour
montrer que <span class="math inline">\(h(T, \beta | \alpha) =
0\)</span> pour toute partition <span
class="math inline">\(\beta\)</span>. Cela implique que: <span
class="math display">\[h(T, \alpha \vee \beta) = h(T,
\alpha).\]</span></p>
<p>3. Enfin, nous prenons la borne supérieure sur toutes les partitions
<span class="math inline">\(\beta\)</span> pour obtenir: <span
class="math display">\[h(T) = \sup_{\beta} h(T, \alpha \vee \beta) =
h(T, \alpha).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de transfert possède plusieurs propriétés importantes, que
nous énumérons ci-dessous:</p>
<ol>
<li><p><strong>Invariance par conjugaison:</strong> Si <span
class="math inline">\(T_1\)</span> et <span
class="math inline">\(T_2\)</span> sont deux systèmes dynamiques
mesurables qui sont conjugués, alors <span class="math inline">\(h(T_1)
= h(T_2)\)</span>.</p></li>
<li><p><strong>Inégalité de subadditivité:</strong> Pour deux systèmes
dynamiques mesurables <span class="math inline">\(T_1\)</span> et <span
class="math inline">\(T_2\)</span>, nous avons: <span
class="math display">\[h(T_1 \times T_2) \leq h(T_1) +
h(T_2).\]</span></p></li>
<li><p><strong>Continuité:</strong> L’entropie de transfert est une
fonction continue par rapport à la mesure invariante <span
class="math inline">\(\mu\)</span>.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
définitions et théorèmes précédemment établis. Par exemple, la propriété
d’invariance par conjugaison découle directement de la définition de
l’entropie de transfert et du fait que les partitions génératrices sont
préservées par conjugaison.</p>
</body>
</html>
{% include "footer.html" %}


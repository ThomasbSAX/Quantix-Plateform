{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Inégalité de Jensen Matricielle : Une Généralisation Puissante</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inégalité de Jensen Matricielle : Une Généralisation
Puissante</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Jensen, du nom du mathématicien danois Johan Jensen,
est une inégalité fondamentale en analyse convexe. Elle relie la valeur
d’une fonction convexe en un point à la valeur moyenne de cette fonction
sur une distribution de probabilités. L’inégalité classique est énoncée
pour des fonctions scalaires, mais sa généralisation au cadre matriciel
s’est révélée d’une importance capitale en théorie des matrices et en
optimisation convexe.</p>
<p>L’émergence de l’inégalité de Jensen matricielle est motivée par le
besoin de traiter des fonctions convexes définies sur l’espace des
matrices. Cette généralisation permet de capturer des propriétés
structurelles profondes des matrices, notamment en ce qui concerne les
valeurs propres et les inégalités spectrales. L’inégalité de Jensen
matricielle est indispensable dans des domaines tels que la théorie du
contrôle, l’apprentissage automatique et la mécanique quantique.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégité de Jensen matricielle, il est essentiel de
comprendre les concepts fondamentaux qui la sous-tendent. Nous
commençons par rappeler quelques définitions clés.</p>
<h2 id="fonctions-convexes">Fonctions Convexes</h2>
<p>Une fonction <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> est dite convexe si pour tout <span
class="math inline">\(x, y \in \mathbb{R}^n\)</span> et tout <span
class="math inline">\(\lambda \in [0,1]\)</span>, on a : <span
class="math display">\[f(\lambda x + (1-\lambda)y) \leq \lambda f(x) +
(1-\lambda)f(y).\]</span></p>
<p>De manière équivalente, <span class="math inline">\(f\)</span> est
convexe si son épigraphe <span class="math inline">\(\{(x,t) \in
\mathbb{R}^n \times \mathbb{R} : f(x) \leq t\}\)</span> est un ensemble
convexe.</p>
<h2 id="fonctions-convexes-sur-les-matrices">Fonctions Convexes sur les
Matrices</h2>
<p>Une fonction <span class="math inline">\(f: \mathbb{R}^{n \times n}
\rightarrow \mathbb{R}\)</span> est dite convexe si pour toute paire de
matrices <span class="math inline">\(X, Y \in \mathbb{R}^{n \times
n}\)</span> et tout <span class="math inline">\(\lambda \in
[0,1]\)</span>, on a : <span class="math display">\[f(\lambda X +
(1-\lambda)Y) \leq \lambda f(X) + (1-\lambda)f(Y).\]</span></p>
<p>Cette définition peut être étendue aux fonctions définies sur les
matrices hermitiennes <span class="math inline">\(\mathbb{C}^{n \times
n}\)</span>.</p>
<h2 id="inégalité-de-jensen-matricielle">Inégalité de Jensen
Matricielle</h2>
<p>Nous cherchons à généraliser l’inégalité de Jensen classique au cadre
matriciel. L’idée est d’exprimer la valeur d’une fonction convexe en un
point moyen de matrices pondérées par une distribution de
probabilités.</p>
<p>Supposons que <span class="math inline">\(\mu\)</span> soit une
mesure de probabilité sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>, et que <span
class="math inline">\(X: \Omega \rightarrow \mathbb{R}^{n \times
n}\)</span> soit une fonction mesurable telle que <span
class="math inline">\(\int_{\Omega} \|X(\omega)\| d\mu(\omega) &lt;
+\infty\)</span>. Soit <span class="math inline">\(f: \mathbb{R}^{n
\times n} \rightarrow \mathbb{R}\)</span> une fonction convexe.</p>
<p>L’inégalité de Jensen matricielle s’énonce comme suit :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mu\)</span> une mesure de
probabilité sur un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, et soit <span class="math inline">\(X: \Omega
\rightarrow \mathbb{R}^{n \times n}\)</span> une fonction mesurable
telle que <span class="math inline">\(\int_{\Omega} \|X(\omega)\|
d\mu(\omega) &lt; +\infty\)</span>. Soit <span class="math inline">\(f:
\mathbb{R}^{n \times n} \rightarrow \mathbb{R}\)</span> une fonction
convexe. Alors, <span class="math display">\[f\left( \int_{\Omega}
X(\omega) d\mu(\omega) \right) \leq \int_{\Omega} f(X(\omega))
d\mu(\omega).\]</span></p>
</div>
<h1 id="démonstration">Démonstration</h1>
<p>La démonstration de l’inégalité de Jensen matricielle repose sur des
techniques d’optimisation convexe et des propriétés des matrices. Nous
allons procéder par étapes, en utilisant des résultats
intermédiaires.</p>
<h2 id="lemme-de-carathéodory">Lemme de Carathéodory</h2>
<p>Le lemme de Carathéodory est un résultat fondamental en analyse
convexe. Il stipule que tout point d’un ensemble convexe peut être
exprimé comme une combinaison convexe d’un nombre fini de points
extrêmes.</p>
<div class="lemma">
<p>Soit <span class="math inline">\(K \subset \mathbb{R}^n\)</span> un
ensemble convexe compact. Alors, tout point <span
class="math inline">\(x \in K\)</span> peut s’écrire comme une
combinaison convexe d’au plus <span class="math inline">\(n+1\)</span>
points extrêmes de <span class="math inline">\(K\)</span>.</p>
</div>
<h2 id="démonstration-de-linégalité-de-jensen-matricielle">Démonstration
de l’Inégalité de Jensen Matricielle</h2>
<p>Nous allons maintenant démontrer l’inégalité de Jensen matricielle en
utilisant le lemme de Carathéodory et des propriétés de convexité.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\mu\)</span> une
mesure de probabilité sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>, et soit <span
class="math inline">\(X: \Omega \rightarrow \mathbb{R}^{n \times
n}\)</span> une fonction mesurable telle que <span
class="math inline">\(\int_{\Omega} \|X(\omega)\| d\mu(\omega) &lt;
+\infty\)</span>. Soit <span class="math inline">\(f: \mathbb{R}^{n
\times n} \rightarrow \mathbb{R}\)</span> une fonction convexe.</p>
<p>Par le lemme de Carathéodory, il existe un nombre fini de points
<span class="math inline">\(\omega_1, \omega_2, \ldots, \omega_k \in
\Omega\)</span> et des coefficients <span
class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_k \geq
0\)</span> tels que <span class="math inline">\(\sum_{i=1}^k \lambda_i =
1\)</span> et <span class="math display">\[\int_{\Omega} X(\omega)
d\mu(\omega) = \sum_{i=1}^k \lambda_i X(\omega_i).\]</span></p>
<p>Puisque <span class="math inline">\(f\)</span> est convexe, nous
avons <span class="math display">\[f\left( \sum_{i=1}^k \lambda_i
X(\omega_i) \right) \leq \sum_{i=1}^k \lambda_i
f(X(\omega_i)).\]</span></p>
<p>En utilisant la définition de l’intégrale par rapport à une mesure de
probabilité, nous obtenons <span class="math display">\[\sum_{i=1}^k
\lambda_i f(X(\omega_i)) = \int_{\Omega} f(X(\omega))
d\mu(\omega).\]</span></p>
<p>En combinant ces deux inégalités, nous concluons que <span
class="math display">\[f\left( \int_{\Omega} X(\omega) d\mu(\omega)
\right) \leq \int_{\Omega} f(X(\omega)) d\mu(\omega).\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Jensen matricielle possède plusieurs propriétés
intéressantes et corollaires. Nous en présentons quelques-uns
ci-dessous.</p>
<h2 id="propriété-de-sous-additivité">Propriété de Sous-Additivité</h2>
<div class="proposition">
<p>Soit <span class="math inline">\(f: \mathbb{R}^{n \times n}
\rightarrow \mathbb{R}\)</span> une fonction convexe et sous-additive,
c’est-à-dire que pour tout <span class="math inline">\(X, Y \in
\mathbb{R}^{n \times n}\)</span>, on a <span class="math display">\[f(X
+ Y) \leq f(X) + f(Y).\]</span> Alors, pour toute mesure de probabilité
<span class="math inline">\(\mu\)</span> et toute fonction mesurable
<span class="math inline">\(X: \Omega \rightarrow \mathbb{R}^{n \times
n}\)</span>, on a <span class="math display">\[f\left( \int_{\Omega}
X(\omega) d\mu(\omega) \right) \leq \int_{\Omega} f(X(\omega))
d\mu(\omega).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La démonstration suit directement de l’inégalité de
Jensen matricielle et de la sous-additivité de <span
class="math inline">\(f\)</span>. ◻</p>
</div>
<h2 id="corollaire-pour-les-normes-matricielles">Corollaire pour les
Normes Matricielles</h2>
<p>Les normes matricielles sont des exemples importants de fonctions
convexes et sous-additives. Nous en déduisons le corollaire suivant
:</p>
<div class="corollary">
<p>Soit <span class="math inline">\(\| \cdot \|\)</span> une norme
matricielle, et soit <span class="math inline">\(\mu\)</span> une mesure
de probabilité sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Soit <span
class="math inline">\(X: \Omega \rightarrow \mathbb{R}^{n \times
n}\)</span> une fonction mesurable telle que <span
class="math inline">\(\int_{\Omega} \|X(\omega)\| d\mu(\omega) &lt;
+\infty\)</span>. Alors, <span class="math display">\[\left\|
\int_{\Omega} X(\omega) d\mu(\omega) \right\| \leq \int_{\Omega}
\|X(\omega)\| d\mu(\omega).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La démonstration suit directement de l’inégalité de
Jensen matricielle et du fait que les normes matricielles sont convexes
et sous-additives. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de Jensen matricielle est une généralisation puissante et
élégante de l’inégalité de Jensen classique. Elle trouve des
applications dans de nombreux domaines, notamment en théorie des
matrices, en optimisation convexe et en apprentissage automatique. Les
démonstrations et les propriétés présentées dans cet article montrent la
richesse et la profondeur de ce résultat fondamental.</p>
</body>
</html>
{% include "footer.html" %}


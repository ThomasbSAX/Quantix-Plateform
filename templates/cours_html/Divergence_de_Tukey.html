{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Tukey : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Tukey : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Tukey, également connue sous le nom de divergence de
Hellinger, est un concept fondamental en statistique et en théorie des
probabilités. Elle émerge comme une mesure de la distance entre deux
distributions de probabilité, offrant des propriétés remarquables en
termes d’interprétabilité et de robustesse. Historiquement, cette
divergence a été introduite par John Tukey dans les années 1950 pour
répondre à des besoins en analyse statistique, notamment dans le cadre
de l’estimation et du test d’hypothèses.</p>
<p>La divergence de Tukey est indispensable dans divers domaines, tels
que l’apprentissage automatique, la théorie de l’information et la
modélisation statistique. Elle permet de quantifier la différence entre
deux distributions, ce qui est crucial pour évaluer la performance des
modèles et des algorithmes. De plus, elle joue un rôle clé dans
l’optimisation de fonctions et la minimisation des risques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Tukey, commençons par définir ce que
nous cherchons à mesurer. Supposons que nous ayons deux distributions de
probabilité, <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier la différence entre ces deux distributions.</p>
<p>La divergence de Tukey est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La divergence de Tukey entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_T(P \| Q) = 2 \left(1 - \int_{\Omega}
\sqrt{\frac{dP}{dQ}} \, dQ\right)\]</span> où <span
class="math inline">\(\frac{dP}{dQ}\)</span> est la densité de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span>.</p>
</div>
<p>Une autre formulation équivalente de la divergence de Tukey est :</p>
<p><span class="math display">\[D_T(P \| Q) = \int_{\Omega}
\left(\sqrt{\frac{dP}{dQ}} - 1\right)^2 \, dQ\]</span></p>
<p>Cette formulation met en évidence la nature quadratique de la
divergence.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Tukey est le théorème
de l’inégalité de Pinsker, qui relie la divergence de Kullback-Leibler à
la divergence de Tukey.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Alors, <span class="math display">\[D_T(P \| Q)
\leq 2 D_{KL}(P \| Q)\]</span> où <span class="math inline">\(D_{KL}(P
\| Q)\)</span> est la divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Pinsker, nous utilisons la définition de
la divergence de Tukey et des propriétés de l’inégalité de
Cauchy-Schwarz.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la divergence de Tukey : <span
class="math display">\[D_T(P \| Q) = 2 \left(1 - \int_{\Omega}
\sqrt{\frac{dP}{dQ}} \, dQ\right)\]</span></p>
<p>Par l’inégalité de Cauchy-Schwarz, nous avons : <span
class="math display">\[\left(\int_{\Omega} \sqrt{\frac{dP}{dQ}} \,
dQ\right)^2 \leq \int_{\Omega} \frac{dP}{dQ} \, dQ \cdot \int_{\Omega} 1
\, dQ = P(\Omega) \cdot Q(\Omega) = 1\]</span></p>
<p>Ainsi, <span class="math display">\[\int_{\Omega}
\sqrt{\frac{dP}{dQ}} \, dQ \leq 1\]</span></p>
<p>En utilisant la convexité de la fonction <span
class="math inline">\(f(x) = (x - 1)^2\)</span>, nous obtenons : <span
class="math display">\[D_T(P \| Q) = \int_{\Omega}
\left(\sqrt{\frac{dP}{dQ}} - 1\right)^2 \, dQ \leq 2 \int_{\Omega}
\left(\frac{dP}{dQ} - 1\right) \, dQ = 2 D_{KL}(P \| Q)\]</span></p>
<p>Ce qui conclut la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Tukey possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p>**Symétrie** : La divergence de Tukey est symétrique,
c’est-à-dire que <span class="math inline">\(D_T(P \| Q) = D_T(Q \|
P)\)</span>.</p></li>
<li><p>**Positivité** : La divergence de Tukey est toujours non
négative, c’est-à-dire <span class="math inline">\(D_T(P \| Q) \geq
0\)</span>, et elle est nulle si et seulement si <span
class="math inline">\(P = Q\)</span>.</p></li>
<li><p>**Invariance par transformation** : La divergence de Tukey est
invariante sous les transformations mesurables.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Tukey est un outil puissant et polyvalent en
statistique et en théorie des probabilités. Ses propriétés remarquables
en font un choix privilégié pour mesurer la distance entre deux
distributions de probabilité. En comprenant et en appliquant cette
divergence, nous pouvons améliorer nos modèles statistiques et nos
algorithmes d’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Mahalanobis (Bregman)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Mahalanobis (Bregman)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Mahalanobis, également connue sous le nom de
divergence de Bregman, émerge dans un cadre où l’on cherche à mesurer la
distance entre deux points d’un espace vectoriel, en tenant compte de la
structure géométrique sous-jacente. Cette notion est particulièrement
utile dans les domaines de l’apprentissage automatique, de la
statistique et de l’optimisation.</p>
<p>L’idée centrale est de généraliser la notion classique de distance
euclidienne en introduisant une matrice de covariance, ce qui permet de
capturer les corrélations entre les différentes dimensions des données.
La divergence de Bregman, quant à elle, offre un cadre plus général pour
définir des distances basées sur des fonctions convexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Mahalanobis, commençons par rappeler
ce qu’est une distance. Une distance est une fonction qui mesure la
séparation entre deux points dans un espace métrique. Dans le cas de la
divergence de Mahalanobis, cette distance est pondérée par une matrice
de covariance.</p>
<p>Supposons que nous ayons un espace vectoriel <span
class="math inline">\(\mathbb{R}^n\)</span> et une matrice de covariance
symétrique définie positive <span class="math inline">\(\Sigma\)</span>.
La divergence de Mahalanobis entre deux points <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\Sigma\)</span> une matrice de
covariance symétrique définie positive. La divergence de Mahalanobis
entre deux points <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> est donnée par : <span
class="math display">\[D_{\Sigma}(x, y) = (x - y)^T \Sigma^{-1} (x -
y).\]</span></p>
</div>
<p>Cette définition peut être reformulée en utilisant la norme induite
par la matrice <span class="math inline">\(\Sigma\)</span> :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\Sigma\)</span> une matrice de
covariance symétrique définie positive. La divergence de Mahalanobis
entre deux points <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> peut également s’écrire
comme : <span class="math display">\[D_{\Sigma}(x, y) = \| x - y
\|^2_{\Sigma},\]</span> où <span class="math inline">\(\| \cdot
\|_{\Sigma}\)</span> est la norme induite par la matrice <span
class="math inline">\(\Sigma\)</span>, définie par : <span
class="math display">\[\| v \|_{\Sigma} = \sqrt{v^T \Sigma^{-1}
v}.\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence de Mahalanobis est
le suivant :</p>
<div class="theorem">
<p>La divergence de Mahalanobis <span
class="math inline">\(D_{\Sigma}(x, y)\)</span> satisfait les propriétés
suivantes :</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(D_{\Sigma}(x, y) =
D_{\Sigma}(y, x).\)</span></p></li>
<li><p>Positivité : <span class="math inline">\(D_{\Sigma}(x, y) \geq
0.\)</span></p></li>
<li><p>Identité des indiscernables : <span
class="math inline">\(D_{\Sigma}(x, y) = 0\)</span> si et seulement si
<span class="math inline">\(x = y.\)</span></p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver les propriétés de la divergence de Mahalanobis,
commençons par la symétrie :</p>
<div class="proof">
<p><em>Preuve de la Symétrie.</em> Soient <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> deux points dans <span
class="math inline">\(\mathbb{R}^n\)</span>. Nous avons : <span
class="math display">\[D_{\Sigma}(x, y) = (x - y)^T \Sigma^{-1} (x -
y).\]</span> En utilisant la propriété de symétrie du produit scalaire,
nous obtenons : <span class="math display">\[D_{\Sigma}(x, y) = (y -
x)^T \Sigma^{-1} (y - x) = D_{\Sigma}(y, x).\]</span> ◻</p>
</div>
<p>Pour la positivité, nous utilisons le fait que <span
class="math inline">\(\Sigma\)</span> est une matrice symétrique définie
positive :</p>
<div class="proof">
<p><em>Preuve de la Positivité.</em> Soient <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> deux points dans <span
class="math inline">\(\mathbb{R}^n\)</span>. Nous avons : <span
class="math display">\[D_{\Sigma}(x, y) = (x - y)^T \Sigma^{-1} (x -
y).\]</span> Puisque <span class="math inline">\(\Sigma\)</span> est
symétrique définie positive, <span
class="math inline">\(\Sigma^{-1}\)</span> l’est également. Par
conséquent, le produit scalaire <span class="math inline">\((x - y)^T
\Sigma^{-1} (x - y)\)</span> est toujours non négatif. ◻</p>
</div>
<p>Enfin, pour l’identité des indiscernables :</p>
<div class="proof">
<p><em>Preuve de l’Identité des Indiscernables.</em> Soient <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> deux points dans <span
class="math inline">\(\mathbb{R}^n\)</span>. Si <span
class="math inline">\(x = y\)</span>, alors : <span
class="math display">\[D_{\Sigma}(x, y) = (x - x)^T \Sigma^{-1} (x - x)
= 0.\]</span> Réciproquement, si <span
class="math inline">\(D_{\Sigma}(x, y) = 0\)</span>, alors : <span
class="math display">\[(x - y)^T \Sigma^{-1} (x - y) = 0.\]</span>
Puisque <span class="math inline">\(\Sigma^{-1}\)</span> est définie
positive, cela implique que : <span class="math display">\[x - y =
0,\]</span> c’est-à-dire <span class="math inline">\(x =
y\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Mahalanobis possède plusieurs propriétés
intéressantes. En voici quelques-unes :</p>
<div class="corollaire">
<p>La divergence de Mahalanobis est invariante par translation.
Autrement dit, pour tout vecteur <span class="math inline">\(z\)</span>
dans <span class="math inline">\(\mathbb{R}^n\)</span>, nous avons :
<span class="math display">\[D_{\Sigma}(x + z, y + z) = D_{\Sigma}(x,
y).\]</span></p>
</div>
<div class="proof">
<p><em>Preuve de l’Invariance par Translation.</em> Soient <span
class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>
et <span class="math inline">\(z\)</span> trois points dans <span
class="math inline">\(\mathbb{R}^n\)</span>. Nous avons : <span
class="math display">\[D_{\Sigma}(x + z, y + z) = (x + z - y - z)^T
\Sigma^{-1} (x + z - y - z) = (x - y)^T \Sigma^{-1} (x - y) =
D_{\Sigma}(x, y).\]</span> ◻</p>
</div>
<div class="corollaire">
<p>La divergence de Mahalanobis est homogène de degré 2. Autrement dit,
pour tout scalaire <span class="math inline">\(\lambda\)</span>, nous
avons : <span class="math display">\[D_{\Sigma}(\lambda x, \lambda y) =
\lambda^2 D_{\Sigma}(x, y).\]</span></p>
</div>
<div class="proof">
<p><em>Preuve de l’Homogénéité.</em> Soient <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> deux points dans <span
class="math inline">\(\mathbb{R}^n\)</span> et <span
class="math inline">\(\lambda\)</span> un scalaire. Nous avons : <span
class="math display">\[D_{\Sigma}(\lambda x, \lambda y) = (\lambda x -
\lambda y)^T \Sigma^{-1} (\lambda x - \lambda y) = \lambda^2 (x - y)^T
\Sigma^{-1} (x - y) = \lambda^2 D_{\Sigma}(x, y).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Mahalanobis est un outil puissant pour mesurer la
distance entre deux points dans un espace vectoriel, en tenant compte de
la structure géométrique sous-jacente. Ses propriétés fondamentales,
telles que la symétrie, la positivité et l’identité des indiscernables,
en font un outil précieux dans de nombreux domaines d’application.</p>
</body>
</html>
{% include "footer.html" %}


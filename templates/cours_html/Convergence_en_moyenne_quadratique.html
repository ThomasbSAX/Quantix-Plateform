{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Convergence en moyenne quadratique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Convergence en moyenne quadratique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La convergence en moyenne quadratique est un concept fondamental en
analyse probabiliste et en théorie des processus stochastiques. Elle
émerge naturellement dans le cadre de l’étude des suites de variables
aléatoires, où l’on cherche à quantifier la manière dont une suite
converge vers une variable aléatoire limite. Ce type de convergence est
particulièrement utile en traitement du signal, en apprentissage
automatique et en physique statistique, où les approximations et les
estimations sont souvent évaluées en termes de moyenne quadratique.</p>
<p>La convergence en moyenne quadratique (CMQ) est indispensable car
elle permet de mesurer la distance entre deux variables aléatoires en
utilisant l’espérance de la différence de leurs carrés. Cela fournit une
mesure plus robuste que la convergence en probabilité ou en loi, car
elle prend en compte non seulement la fréquence des événements mais
aussi leur amplitude.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la convergence en moyenne quadratique, considérons
une suite de variables aléatoires <span class="math inline">\((X_n)_{n
\in \mathbb{N}}\)</span> et une variable aléatoire <span
class="math inline">\(X\)</span>. Nous cherchons à mesurer comment la
suite <span class="math inline">\((X_n)\)</span> se rapproche de <span
class="math inline">\(X\)</span> en termes d’espérance des carrés des
différences.</p>
<div class="definition">
<p>On dit que la suite <span class="math inline">\((X_n)_{n \in
\mathbb{N}}\)</span> converge en moyenne quadratique vers <span
class="math inline">\(X\)</span> si et seulement si <span
class="math display">\[\lim_{n \to +\infty} \mathbb{E}\left[\left(X_n -
X\right)^2\right] = 0.\]</span> En d’autres termes, pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe un entier
<span class="math inline">\(N \in \mathbb{N}\)</span> tel que pour tout
<span class="math inline">\(n \geq N\)</span>, <span
class="math display">\[\mathbb{E}\left[\left(X_n - X\right)^2\right]
&lt; \epsilon.\]</span></p>
</div>
<p>Cette définition peut être reformulée en utilisant la notion de
distance <span class="math inline">\(L^2\)</span> entre deux variables
aléatoires. La distance <span class="math inline">\(L^2\)</span> entre
<span class="math inline">\(X_n\)</span> et <span
class="math inline">\(X\)</span> est définie par <span
class="math display">\[\|X_n - X\|_{L^2} =
\sqrt{\mathbb{E}\left[\left(X_n - X\right)^2\right]}.\]</span> Ainsi, la
convergence en moyenne quadratique équivaut à la convergence de <span
class="math inline">\(\|X_n - X\|_{L^2}\)</span> vers 0 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la convergence en moyenne
quadratique est le suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires convergeant presque sûrement vers
<span class="math inline">\(X\)</span>. Si la suite <span
class="math inline">\((|X_n|^2)_{n \in \mathbb{N}}\)</span> est
uniformément intégrable, alors <span
class="math inline">\((X_n)\)</span> converge vers <span
class="math inline">\(X\)</span> en moyenne quadratique.</p>
</div>
<p>La démonstration de ce théorème repose sur le théorème de la
convergence dominée de Lebesgue, qui permet d’échanger les limites et
les intégrales sous certaines conditions.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de la convergence dominée pour les suites
de variables aléatoires, nous procédons comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Supposons que <span
class="math inline">\((X_n)\)</span> converge presque sûrement vers
<span class="math inline">\(X\)</span> et que <span
class="math inline">\((|X_n|^2)\)</span> est uniformément intégrable.
Nous devons montrer que <span class="math display">\[\lim_{n \to
+\infty} \mathbb{E}\left[\left(X_n - X\right)^2\right] = 0.\]</span></p>
<p>Par l’inégalité de Markov, pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe un entier
<span class="math inline">\(N\)</span> tel que pour tout <span
class="math inline">\(n \geq N\)</span>, <span
class="math display">\[\mathbb{P}\left(\left|X_n - X\right| &gt;
\epsilon\right) &lt; \frac{\epsilon}{2}.\]</span></p>
<p>Ensuite, nous utilisons le théorème de la convergence dominée pour
obtenir <span class="math display">\[\mathbb{E}\left[\left(X_n -
X\right)^2 \mathbb{I}_{\{|X_n - X| \leq \epsilon\}}\right] \to 0 \quad
\text{quand} \quad n \to +\infty.\]</span></p>
<p>Enfin, nous avons <span
class="math display">\[\mathbb{E}\left[\left(X_n - X\right)^2
\mathbb{I}_{\{|X_n - X| &gt; \epsilon\}}\right] \leq \epsilon^2
\mathbb{P}\left(\left|X_n - X\right| &gt; \epsilon\right) &lt;
\frac{\epsilon^3}{2}.\]</span></p>
<p>En combinant ces résultats, nous obtenons <span
class="math display">\[\mathbb{E}\left[\left(X_n - X\right)^2\right]
\leq \mathbb{E}\left[\left(X_n - X\right)^2 \mathbb{I}_{\{|X_n - X| \leq
\epsilon\}}\right] + \mathbb{E}\left[\left(X_n - X\right)^2
\mathbb{I}_{\{|X_n - X| &gt; \epsilon\}}\right] &lt;
2\epsilon^3.\]</span></p>
<p>En faisant tendre <span class="math inline">\(\epsilon\)</span> vers
0, nous concluons que <span class="math display">\[\lim_{n \to +\infty}
\mathbb{E}\left[\left(X_n - X\right)^2\right] = 0.\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons quelques propriétés importantes de la convergence en
moyenne quadratique :</p>
<ol>
<li><p>La convergence en moyenne quadratique implique la convergence en
probabilité. En effet, par l’inégalité de Markov, <span
class="math display">\[\mathbb{P}\left(\left|X_n - X\right| &gt;
\epsilon\right) \leq \frac{1}{\epsilon^2} \mathbb{E}\left[\left(X_n -
X\right)^2\right].\]</span> Si <span
class="math inline">\((X_n)\)</span> converge vers <span
class="math inline">\(X\)</span> en moyenne quadratique, alors <span
class="math inline">\(\mathbb{E}\left[\left(X_n - X\right)^2\right] \to
0\)</span>, et donc <span
class="math inline">\(\mathbb{P}\left(\left|X_n - X\right| &gt;
\epsilon\right) \to 0\)</span>.</p></li>
<li><p>Si <span class="math inline">\((X_n)\)</span> converge vers <span
class="math inline">\(X\)</span> en moyenne quadratique et que <span
class="math inline">\(g\)</span> est une fonction lipschitzienne, alors
<span class="math inline">\((g(X_n))\)</span> converge vers <span
class="math inline">\(g(X)\)</span> en moyenne quadratique. En effet, il
existe une constante <span class="math inline">\(L &gt; 0\)</span> telle
que <span class="math display">\[|g(X_n) - g(X)| \leq L |X_n -
X|.\]</span> Par conséquent, <span
class="math display">\[\mathbb{E}\left[\left(g(X_n) -
g(X)\right)^2\right] \leq L^2 \mathbb{E}\left[\left(X_n -
X\right)^2\right] \to 0.\]</span></p></li>
<li><p>La convergence en moyenne quadratique est stable par somme. Si
<span class="math inline">\((X_n)\)</span> converge vers <span
class="math inline">\(X\)</span> et <span
class="math inline">\((Y_n)\)</span> converge vers <span
class="math inline">\(Y\)</span> en moyenne quadratique, alors <span
class="math inline">\((X_n + Y_n)\)</span> converge vers <span
class="math inline">\(X + Y\)</span> en moyenne quadratique. En effet,
<span class="math display">\[\mathbb{E}\left[\left(X_n + Y_n - X -
Y\right)^2\right] \leq 2 \mathbb{E}\left[\left(X_n - X\right)^2\right] +
2 \mathbb{E}\left[\left(Y_n - Y\right)^2\right].\]</span> Les deux
termes de droite tendent vers 0, donc le terme de gauche aussi.</p></li>
</ol>
<p>Ces propriétés montrent que la convergence en moyenne quadratique est
un outil puissant pour l’analyse des suites de variables aléatoires,
offrant une mesure robuste et flexible de la convergence.</p>
</body>
</html>
{% include "footer.html" %}


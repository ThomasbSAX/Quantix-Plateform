{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Locally Linear Embedding: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Locally Linear Embedding: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>The manifold learning techniques have revolutionized the field of
dimensionality reduction by preserving the intrinsic geometric structure
of data. Among these techniques, Locally Linear Embedding (LLE) has
emerged as a powerful tool for nonlinear dimensionality reduction.
However, LLE is limited to linear relationships within local
neighborhoods.</p>
<p>Kernelized Locally Linear Embedding (KLLE) extends the LLE algorithm
by incorporating kernel methods, enabling it to capture more complex
nonlinear relationships. This article aims to provide a comprehensive
study of KLLE, covering its historical development, mathematical
foundations, and practical applications.</p>
<h1 class="unnumbered" id="historical-context">Historical Context</h1>
<p>The concept of Locally Linear Embedding was first introduced by
Roweis and Saul in 2000. It quickly gained popularity due to its ability
to uncover nonlinear relationships within data. However, the linear
nature of LLE limited its applicability.</p>
<p>In 2003, Zhang and Zha proposed the Kernelized Locally Linear
Embedding algorithm, extending LLE to capture more complex nonlinear
relationships. This extension has proven invaluable in various fields,
including computer vision, bioinformatics, and signal processing.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>To understand KLLE, we must first define several key concepts.</p>
<h2 class="unnumbered" id="manifold-learning">Manifold Learning</h2>
<p>A manifold is a topological space that locally resembles Euclidean
space. Manifold learning aims to uncover the underlying structure of
high-dimensional data by embedding it into a lower-dimensional
space.</p>
<h2 class="unnumbered" id="kernel-methods">Kernel Methods</h2>
<p>A kernel function measures the similarity between data points. It
enables the transformation of data into a higher-dimensional space where
linear relationships become more apparent.</p>
<h2 class="unnumbered" id="locally-linear-embedding-lle">Locally Linear
Embedding (LLE)</h2>
<p>LLE aims to preserve the local geometric properties of data by
minimizing the reconstruction error within local neighborhoods. Given a
set of high-dimensional data points <span class="math inline">\(X =
\{x_1, x_2, \ldots, x_n\}\)</span>, LLE seeks to find a set of
low-dimensional embeddings <span class="math inline">\(Y = \{y_1, y_2,
\ldots, y_n\}\)</span> that minimizes the following cost function:</p>
<p><span class="math display">\[\min_{Y} \sum_{i=1}^n \left\| x_i -
\sum_{j=1}^n w_{ij} x_j \right\|^2\]</span></p>
<p>where <span class="math inline">\(W = \{w_{ij}\}_{i,j=1}^n\)</span>
is the weight matrix, with <span class="math inline">\(w_{ij} =
0\)</span> if <span class="math inline">\(x_j\)</span> is not in the
neighborhood of <span class="math inline">\(x_i\)</span>.</p>
<h2 class="unnumbered"
id="kernelized-locally-linear-embedding-klle">Kernelized Locally Linear
Embedding (KLLE)</h2>
<p>KLLE extends LLE by incorporating kernel methods. Given a kernel
function <span class="math inline">\(\kappa(x_i, x_j)\)</span>, KLLE
seeks to find embeddings <span class="math inline">\(Y\)</span> that
minimize the following cost function:</p>
<p><span class="math display">\[\min_{Y} \sum_{i=1}^n \left\|
\kappa(x_i, x_i) - 2 \sum_{j=1}^n w_{ij} \kappa(x_i, x_j) + \sum_{j=1}^n
\sum_{l=1}^n w_{ij} w_{il} \kappa(x_j, x_l) \right\|^2\]</span></p>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<h2 class="unnumbered" id="theorem-1-klle-optimization">Theorem 1: KLLE
Optimization</h2>
<p>KLLE optimization can be formulated as an eigenvalue problem. Given
the weight matrix <span class="math inline">\(W\)</span> and the kernel
matrix <span class="math inline">\(K = \{k_{ij}\}_{i,j=1}^n\)</span>,
with <span class="math inline">\(k_{ij} = \kappa(x_i, x_j)\)</span>, the
optimal embeddings <span class="math inline">\(Y\)</span> are given by
the eigenvectors of the matrix <span class="math inline">\(M = (I - W)^T
K (I - W)\)</span>, corresponding to the smallest eigenvalues.</p>
<div class="proof">
<p><em>Proof.</em> The KLLE cost function can be rewritten as:</p>
<p><span class="math display">\[\min_{Y} \text{tr}(Y^T L Y)\]</span></p>
<p>where <span class="math inline">\(L = (I - W)^T K (I - W)\)</span>.
The optimal embeddings <span class="math inline">\(Y\)</span> are given
by the eigenvectors of <span class="math inline">\(L\)</span>,
corresponding to the smallest eigenvalues. ◻</p>
</div>
<h2 class="unnumbered" id="theorem-2-kernel-trick">Theorem 2: Kernel
Trick</h2>
<p>The kernel trick enables the implicit transformation of data into a
higher-dimensional space. Given a kernel function <span
class="math inline">\(\kappa(x_i, x_j)\)</span>, the corresponding
feature map <span class="math inline">\(\phi\)</span> satisfies:</p>
<p><span class="math display">\[\kappa(x_i, x_j) = \langle \phi(x_i),
\phi(x_j) \rangle\]</span></p>
<div class="proof">
<p><em>Proof.</em> By definition, the kernel function <span
class="math inline">\(\kappa(x_i, x_j)\)</span> is a symmetric positive
definite function. Therefore, it can be expressed as the inner product
of some feature map <span class="math inline">\(\phi\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<h2 class="unnumbered"
id="property-1-preservation-of-local-structure">Property 1: Preservation
of Local Structure</h2>
<p>KLLE preserves the local geometric properties of data. Given a set of
high-dimensional data points <span class="math inline">\(X\)</span> and
their corresponding low-dimensional embeddings <span
class="math inline">\(Y\)</span>, the local relationships within <span
class="math inline">\(X\)</span> are preserved in <span
class="math inline">\(Y\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> The KLLE cost function minimizes the reconstruction
error within local neighborhoods. Therefore, the local geometric
properties of <span class="math inline">\(X\)</span> are preserved in
<span class="math inline">\(Y\)</span>. ◻</p>
</div>
<h2 class="unnumbered" id="property-2-nonlinear-embedding">Property 2:
Nonlinear Embedding</h2>
<p>KLLE enables the capture of complex nonlinear relationships within
data. Given a kernel function <span class="math inline">\(\kappa(x_i,
x_j)\)</span>, KLLE can uncover nonlinear relationships that are not
apparent in the original high-dimensional space.</p>
<div class="proof">
<p><em>Proof.</em> The kernel function <span
class="math inline">\(\kappa(x_i, x_j)\)</span> enables the
transformation of data into a higher-dimensional space where linear
relationships become more apparent. Therefore, KLLE can capture complex
nonlinear relationships within data. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Kernelized Locally Linear Embedding is a powerful technique for
nonlinear dimensionality reduction. By incorporating kernel methods,
KLLE extends the capabilities of LLE, enabling it to capture more
complex nonlinear relationships. This article has provided a
comprehensive study of KLLE, covering its historical development,
mathematical foundations, and practical applications.</p>
</body>
</html>
{% include "footer.html" %}


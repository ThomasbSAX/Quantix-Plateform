{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Inégalité de Shannon : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inégalité de Shannon : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Shannon, nommée en l’honneur du mathématicien Claude
Shannon, est un pilier fondamental dans la théorie de l’information.
Elle émerge à l’intersection de plusieurs disciplines, notamment les
mathématiques, la physique et l’ingénierie. Son importance réside dans
sa capacité à quantifier les limites intrinsèques de la compression des
données et de la transmission d’information.</p>
<p>Historiquement, cette inégalité a été formulée dans les années 1940
par Shannon pour répondre à des questions cruciales en
télécommunications. Elle permet de déterminer la capacité maximale d’un
canal de communication bruité, ainsi que les limites de compression sans
perte pour des sources d’information. Ces concepts sont indispensables
dans un monde où la quantité de données à traiter et transmettre ne
cesse de croître.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégalité de Shannon, il est essentiel de définir
quelques notions clés. Commençons par l’entropie d’une source discrète,
qui mesure son incertitude ou son information moyenne.</p>
<h2 id="entropie-dune-source-discrète">Entropie d’une source
discrète</h2>
<p>Considérons une source d’information discrète <span
class="math inline">\(X\)</span> prenant ses valeurs dans un ensemble
fini <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span>. L’entropie <span class="math inline">\(H(X)\)</span> de
cette source est une mesure de l’incertitude associée à la valeur prise
par <span class="math inline">\(X\)</span>. Pour comprendre cette
notion, imaginons que nous avons une source émettant des symboles avec
certaines probabilités. Plus ces probabilités sont uniformément
réparties, plus l’incertitude est grande.</p>
<p>Formellement, l’entropie de <span class="math inline">\(X\)</span>
est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
discrète prenant ses valeurs dans <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
avec une distribution de probabilité <span class="math inline">\(P(X =
x_i) = p_i\)</span> pour tout <span class="math inline">\(i \in \{1, 2,
\ldots, n\}\)</span>. L’entropie de <span
class="math inline">\(X\)</span> est donnée par :</p>
<p><span class="math display">\[H(X) = -\sum_{i=1}^n p_i \log_2
p_i\]</span></p>
<p>où <span class="math inline">\(\log_2\)</span> désigne le logarithme
en base 2.</p>
</div>
<p>Cette définition peut également être exprimée en utilisant le
logarithme naturel :</p>
<p><span class="math display">\[H(X) = -\sum_{i=1}^n p_i \ln p_i / \ln
2\]</span></p>
<h2 id="entropie-conditionnelle">Entropie conditionnelle</h2>
<p>L’entropie conditionnelle mesure l’incertitude d’une variable
aléatoire <span class="math inline">\(X\)</span> sachant qu’une autre
variable <span class="math inline">\(Y\)</span> est connue. Cette notion
est cruciale pour comprendre comment l’information d’une source peut
être réduite en utilisant une autre source corrélée.</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires discrètes
définies sur un espace probabilisé <span class="math inline">\((\Omega,
\mathcal{F}, P)\)</span>. L’entropie conditionnelle de <span
class="math inline">\(X\)</span> sachant <span
class="math inline">\(Y\)</span> est définie par :</p>
<p><span class="math display">\[H(X | Y) = \sum_{y \in \mathcal{Y}} P(Y
= y) H(X | Y = y)\]</span></p>
<p>où <span class="math inline">\(H(X | Y = y)\)</span> est l’entropie
de <span class="math inline">\(X\)</span> conditionnellement à <span
class="math inline">\(Y = y\)</span>, donnée par :</p>
<p><span class="math display">\[H(X | Y = y) = -\sum_{x \in \mathcal{X}}
P(X = x | Y = y) \log_2 P(X = x | Y = y)\]</span></p>
</div>
<h1 id="inégalité-de-shannon">Inégalité de Shannon</h1>
<p>L’inégalité de Shannon est une relation fondamentale entre l’entropie
conjointe et les entropies marginales. Elle est également connue sous le
nom d’inégalité de subadditivité de l’entropie.</p>
<h2 id="énoncé">Énoncé</h2>
<p>Avant d’énoncer l’inégalité, comprenons son intuition. L’entropie
conjointe <span class="math inline">\(H(X, Y)\)</span> mesure
l’incertitude totale de la paire <span class="math inline">\((X,
Y)\)</span>. Si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors <span
class="math inline">\(H(X, Y) = H(X) + H(Y)\)</span>. Cependant, si
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont corrélées, l’entropie conjointe
est inférieure ou égale à la somme des entropies marginales.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires discrètes
définies sur un espace probabilisé <span class="math inline">\((\Omega,
\mathcal{F}, P)\)</span>. Alors :</p>
<p><span class="math display">\[H(X, Y) \leq H(X) + H(Y)\]</span></p>
<p>De plus, l’égalité a lieu si et seulement si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes.</p>
</div>
<h2 id="démonstration">Démonstration</h2>
<p>Pour démontrer cette inégalité, nous allons utiliser les propriétés
de l’entropie et de l’information mutuelle.</p>
<div class="proof">
<p><em>Proof.</em> Par définition, l’entropie conjointe peut être
exprimée comme :</p>
<p><span class="math display">\[H(X, Y) = H(X | Y) + H(Y)\]</span></p>
<p>De même, nous avons :</p>
<p><span class="math display">\[H(X, Y) = H(Y | X) + H(X)\]</span></p>
<p>En utilisant l’inégalité <span class="math inline">\(H(X | Y) \leq
H(X)\)</span>, qui découle du fait que la connaissance de <span
class="math inline">\(Y\)</span> ne peut qu’augmenter l’information sur
<span class="math inline">\(X\)</span>, nous obtenons :</p>
<p><span class="math display">\[H(X, Y) = H(X | Y) + H(Y) \leq H(X) +
H(Y)\]</span></p>
<p>L’égalité a lieu si et seulement si <span class="math inline">\(H(X |
Y) = H(X)\)</span>, ce qui signifie que <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Shannon a plusieurs propriétés intéressantes et
corollaires qui en découlent.</p>
<h2 id="propriétés">Propriétés</h2>
<ol>
<li><p>L’inégalité de Shannon est une généralisation de l’additivité de
l’entropie pour les variables indépendantes. En effet, si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors <span
class="math inline">\(H(X, Y) = H(X) + H(Y)\)</span>.</p></li>
<li><p>L’inégalité de Shannon peut être étendue à plusieurs variables.
Pour <span class="math inline">\(n\)</span> variables aléatoires
discrètes <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>,
nous avons :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_n) \leq
\sum_{i=1}^n H(X_i)\]</span></p></li>
<li><p>L’inégalité de Shannon est une conséquence directe de la
non-négativité de l’information mutuelle. En effet, pour deux variables
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, nous avons :</p>
<p><span class="math display">\[I(X; Y) = H(X) + H(Y) - H(X, Y) \geq
0\]</span></p>
<p>ce qui implique directement <span class="math inline">\(H(X, Y) \leq
H(X) + H(Y)\)</span>.</p></li>
</ol>
<h2 id="corollaires">Corollaires</h2>
<div class="corollary">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(\hat{X}\)</span> deux variables aléatoires
discrètes définies sur un espace probabilisé <span
class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. Alors :</p>
<p><span class="math display">\[H(X | \hat{X}) = h(P_e) - I(X;
\hat{X})\]</span></p>
<p>où <span class="math inline">\(P_e\)</span> est la probabilité
d’erreur et <span class="math inline">\(h\)</span> est l’entropie
binaire.</p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant l’inégalité de Shannon et les propriétés
de l’entropie conditionnelle, nous pouvons dériver cette inégalité en
considérant les relations entre <span class="math inline">\(X\)</span>,
<span class="math inline">\(\hat{X}\)</span> et l’erreur de
reconstruction. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de Shannon est un résultat fondamental en théorie de
l’information, avec des applications dans de nombreux domaines. Elle
fournit une limite théorique sur la compression et la transmission
d’information, ce qui est crucial pour le développement de technologies
modernes. En comprenant et en appliquant cette inégalité, nous pouvons
optimiser les systèmes de communication et de stockage de données.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Perceptual Loss: Une Approche Innovante en Apprentissage Profond</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Perceptual Loss: Une Approche Innovante en
Apprentissage Profond</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage profond a révolutionné de nombreux domaines,
notamment la vision par ordinateur et le traitement d’images. Cependant,
les méthodes traditionnelles de perte, telles que la perte de Mean
Squared Error (MSE), présentent des limitations significatives lorsqu’il
s’agit de capturer les caractéristiques perceptuelles des images. La
perte perceptuelle émerge comme une solution innovante pour résoudre ce
problème en exploitant les réseaux de neurones convolutifs (CNN)
pré-entraînés pour évaluer la similarité entre les images.</p>
<p>Cette approche est indispensable dans des applications où la qualité
visuelle est cruciale, comme le style transfer, la super-résolution et
la génération d’images. En intégrant des caractéristiques perceptuelles
dans la fonction de perte, nous pouvons obtenir des résultats plus
fidèles à la perception humaine, dépassant les limitations des métriques
traditionnelles.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la perte perceptuelle, il est essentiel de définir
d’abord les concepts fondamentaux.</p>
<h2 class="unnumbered" id="réseau-de-neurones-convolutifs-cnn">Réseau de
Neurones Convolutifs (CNN)</h2>
<p>Considérons un réseau de neurones convolutif <span
class="math inline">\(\mathcal{N}\)</span> pré-entraîné sur une vaste
base de données d’images, tel que VGG19 ou ResNet. Ce réseau extrait des
caractéristiques hiérarchiques d’une image <span
class="math inline">\(I\)</span> à travers plusieurs couches de
convolution. Nous notons <span class="math inline">\(\phi_j(I)\)</span>
les caractéristiques extraites par la couche <span
class="math inline">\(j\)</span> du réseau.</p>
<p>Formellement, pour une image <span class="math inline">\(I \in
\mathbb{R}^{H \times W \times C}\)</span>, où <span
class="math inline">\(H\)</span> est la hauteur, <span
class="math inline">\(W\)</span> est la largeur et <span
class="math inline">\(C\)</span> est le nombre de canaux, les
caractéristiques extraites par la couche <span
class="math inline">\(j\)</span> sont définies comme : <span
class="math display">\[\phi_j(I) = \mathcal{N}_j(I)\]</span></p>
<h2 class="unnumbered" id="perte-perceptuelle">Perte Perceptuelle</h2>
<p>La perte perceptuelle mesure la différence entre les caractéristiques
extraites d’une image générée <span class="math inline">\(G\)</span> et
celles d’une image de référence <span class="math inline">\(I\)</span>.
Nous définissons la perte perceptuelle <span
class="math inline">\(L_{\text{perceptual}}\)</span> comme la somme des
différences moyennes quadratiques (MSE) entre les caractéristiques
extraites par plusieurs couches du réseau.</p>
<p>Soit <span class="math inline">\(\mathcal{N}\)</span> un CNN
pré-entraîné, et soit <span class="math inline">\(\phi_j(I)\)</span> et
<span class="math inline">\(\phi_j(G)\)</span> les caractéristiques
extraites par la couche <span class="math inline">\(j\)</span> pour
l’image de référence <span class="math inline">\(I\)</span> et l’image
générée <span class="math inline">\(G\)</span>, respectivement. La perte
perceptuelle est donnée par : <span
class="math display">\[L_{\text{perceptual}}(I, G) = \sum_{j=1}^{J}
\frac{1}{C_j H_j W_j} \|\phi_j(I) - \phi_j(G)\|_2^2\]</span> où <span
class="math inline">\(J\)</span> est le nombre de couches considérées,
et <span class="math inline">\(C_j, H_j, W_j\)</span> sont les
dimensions des caractéristiques extraites par la couche <span
class="math inline">\(j\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered" id="théorème-de-la-perte-perceptuelle">Théorème
de la Perte Perceptuelle</h2>
<p>Nous présentons maintenant un théorème fondamental concernant la
perte perceptuelle.</p>
<h3 class="unnumbered" id="énoncé">Énoncé</h3>
<p>Soit <span class="math inline">\(\mathcal{N}\)</span> un CNN
pré-entraîné, et soit <span class="math inline">\(I\)</span> une image
de référence. Pour toute image générée <span
class="math inline">\(G\)</span>, la perte perceptuelle <span
class="math inline">\(L_{\text{perceptual}}(I, G)\)</span> est minimale
lorsque <span class="math inline">\(G = I\)</span>.</p>
<p>Formellement, pour tout <span class="math inline">\(G \neq
I\)</span>, nous avons : <span
class="math display">\[L_{\text{perceptual}}(I, G) &gt; 0\]</span></p>
<h3 class="unnumbered" id="preuve">Preuve</h3>
<p>Nous procédons par contradiction. Supposons qu’il existe une image
générée <span class="math inline">\(G \neq I\)</span> telle que <span
class="math inline">\(L_{\text{perceptual}}(I, G) = 0\)</span>. Cela
implique que pour toutes les couches <span
class="math inline">\(j\)</span>, nous avons : <span
class="math display">\[\phi_j(I) = \phi_j(G)\]</span></p>
<p>Cependant, comme <span class="math inline">\(\mathcal{N}\)</span> est
un CNN pré-entraîné sur une vaste base de données, les caractéristiques
extraites par différentes couches capturent des informations
hiérarchiques distinctes. Par conséquent, si <span
class="math inline">\(G \neq I\)</span>, il existe au moins une couche
<span class="math inline">\(j\)</span> pour laquelle <span
class="math inline">\(\phi_j(I) \neq \phi_j(G)\)</span>. Cela contredit
notre hypothèse initiale. Par conséquent, nous concluons que : <span
class="math display">\[L_{\text{perceptual}}(I, G) &gt; 0 \quad \forall
G \neq I\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous énumérons maintenant plusieurs propriétés importantes de la
perte perceptuelle.</p>
<h2 class="unnumbered" id="propriété-i">Propriété (i)</h2>
<p>La perte perceptuelle est invariante par translation et rotation
lorsque les caractéristiques extraites sont invariantes par ces
transformations.</p>
<h3 class="unnumbered" id="preuve-1">Preuve</h3>
<p>Soit <span class="math inline">\(T\)</span> une transformation
géométrique telle que la translation ou la rotation. Si les
caractéristiques extraites <span class="math inline">\(\phi_j\)</span>
sont invariantes par <span class="math inline">\(T\)</span>, alors pour
toute image <span class="math inline">\(I\)</span>, nous avons : <span
class="math display">\[\phi_j(T(I)) = \phi_j(I)\]</span></p>
<p>Par conséquent, pour toute image générée <span
class="math inline">\(G\)</span>, nous avons : <span
class="math display">\[L_{\text{perceptual}}(T(I), T(G)) =
\sum_{j=1}^{J} \frac{1}{C_j H_j W_j} \|\phi_j(T(I)) - \phi_j(T(G))\|_2^2
= L_{\text{perceptual}}(I, G)\]</span></p>
<h2 class="unnumbered" id="propriété-ii">Propriété (ii)</h2>
<p>La perte perceptuelle est sensible aux changements de style lorsque
les caractéristiques extraites capturent des informations de style.</p>
<h3 class="unnumbered" id="preuve-2">Preuve</h3>
<p>Soit <span class="math inline">\(S\)</span> une transformation de
style. Si les caractéristiques extraites <span
class="math inline">\(\phi_j\)</span> capturent des informations de
style, alors pour toute image <span class="math inline">\(I\)</span>,
nous avons : <span class="math display">\[\phi_j(S(I)) \neq
\phi_j(I)\]</span></p>
<p>Par conséquent, pour toute image générée <span
class="math inline">\(G\)</span>, nous avons : <span
class="math display">\[L_{\text{perceptual}}(S(I), S(G)) =
\sum_{j=1}^{J} \frac{1}{C_j H_j W_j} \|\phi_j(S(I)) -
\phi_j(S(G))\|_2^2\]</span></p>
<p>Cette perte est non nulle lorsque <span class="math inline">\(G \neq
I\)</span>, ce qui montre que la perte perceptuelle est sensible aux
changements de style.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La perte perceptuelle représente une avancée significative dans le
domaine de l’apprentissage profond, en particulier pour les applications
où la qualité visuelle est cruciale. En exploitant les réseaux de
neurones convolutifs pré-entraînés, cette approche permet de capturer
des caractéristiques perceptuelles essentielles, dépassant les
limitations des métriques traditionnelles. Les propriétés et théorèmes
présentés dans cet article soulignent l’efficacité et la robustesse de
cette méthode, ouvrant la voie à de nouvelles applications
innovantes.</p>
</body>
</html>
{% include "footer.html" %}


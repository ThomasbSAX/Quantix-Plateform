{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Prior informatif : Fondements et Applications en Statistique Bayésienne</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Prior informatif : Fondements et Applications en
Statistique Bayésienne</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’analyse statistique bayésienne repose sur un principe fondamental :
l’incorporation de connaissances a priori dans le processus d’inférence.
Ce paradigme, initié par Thomas Bayes au XVIIIème siècle et formalisé
par Laplace, offre une alternative puissante aux méthodes fréquentielles
classiques. Le prior informatif constitue le cœur de cette approche,
permettant d’exploiter l’intuition et les données historiques pour
enrichir la modélisation statistique.</p>
<p>Pourquoi ce concept émerge-t-il comme indispensable ? Dans un monde
où les données sont souvent rares ou coûteuses à obtenir, le prior
informatif permet de compenser cette pénurie en intégrant des
informations externes. Il joue un rôle crucial dans la régularisation
des modèles complexes, évitant ainsi le surapprentissage. De plus, il
offre une cadre naturel pour l’apprentissage actif et la prise de
décision sous incertitude.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement le concept de prior informatif,
considérons un problème classique : estimer la proportion <span
class="math inline">\(p\)</span> de votants en faveur d’un candidat
politique. Supposons que nous disposions de données partielles issues
d’un sondage, mais aussi d’informations historiques sur les tendances
électorales. Comment intégrer ces connaissances dans notre modèle ?</p>
<p>Nous cherchons à définir une distribution de probabilité <span
class="math inline">\(P\)</span> sur l’espace des paramètres <span
class="math inline">\(\Theta\)</span>, qui encode notre croyance
initiale avant d’observer les données. Cette distribution doit
satisfaire deux propriétés fondamentales :</p>
<ol>
<li><p>Elle doit refléter notre état de connaissance avant l’observation
des données.</p></li>
<li><p>Elle doit être compatible avec les contraintes physiques ou
théoriques du problème étudié.</p></li>
</ol>
<p>Nous sommes maintenant en mesure de formuler la définition centrale
:</p>
<div class="definition">
<p>Soit <span class="math inline">\(\Theta\)</span> l’espace des
paramètres d’un modèle statistique, et soit <span
class="math inline">\(X\)</span> la variable aléatoire observée. Une
distribution de probabilité <span
class="math inline">\(\pi(\cdot)\)</span> sur <span
class="math inline">\(\Theta\)</span> est dite <strong>prior
informatif</strong> s’il existe une fonction mesurable <span
class="math inline">\(g : \Theta \rightarrow [0,1]\)</span> telle que
:</p>
<p><span class="math display">\[\forall \theta \in \Theta, \quad
\pi(\theta) = g(\theta) \cdot h(\theta)\]</span></p>
<p>où <span class="math inline">\(h(\theta)\)</span> est une fonction de
normalisation définie par :</p>
<p><span class="math display">\[h(\theta) = \left( \int_{\Theta} g(\phi)
\, d\phi \right)^{-1}\]</span></p>
<p>De plus, <span class="math inline">\(g\)</span> doit encoder des
informations pertinentes pour le problème étudié.</p>
</div>
<p>Une formulation alternative, souvent utilisée en pratique, est :</p>
<p><span class="math display">\[\pi(\theta) \propto
g(\theta)\]</span></p>
<p>où le symbole <span class="math inline">\(\propto\)</span> signifie
"proportionnel à".</p>
<h1 id="théorèmes-fondamentaux">Théorèmes Fondamentaux</h1>
<p>Le théorème suivant, dû à Bernardo (1979), établit une condition
nécessaire et suffisante pour qu’une distribution soit un prior
informatif.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\pi\)</span> une distribution de
probabilité sur <span class="math inline">\(\Theta\)</span>. Les
assertions suivantes sont équivalentes :</p>
<ol>
<li><p><span class="math inline">\(\pi\)</span> est un prior
informatif.</p></li>
<li><p>Il existe une fonction <span class="math inline">\(g : \Theta
\rightarrow [0,1]\)</span> telle que :</p>
<p><span class="math display">\[\forall \theta \in \Theta, \quad
\frac{\pi(\theta)}{\int_{\Theta} \pi(\phi) \, d\phi} =
g(\theta)\]</span></p>
<p>et <span class="math inline">\(g\)</span> encode des informations
pertinentes pour le problème étudié.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> La démonstration de cette équivalence repose sur des
arguments simples de théorie de la mesure. Nous montrons d’abord que (i)
implique (ii).</p>
<p>Supposons que <span class="math inline">\(\pi\)</span> soit un prior
informatif. Par définition, il existe une fonction <span
class="math inline">\(g\)</span> telle que :</p>
<p><span class="math display">\[\pi(\theta) = g(\theta) \cdot
h(\theta)\]</span></p>
<p>où <span class="math inline">\(h\)</span> est la fonction de
normalisation. En substituant cette expression dans l’intégrale de
normalisation :</p>
<p><span class="math display">\[\int_{\Theta} \pi(\phi) \, d\phi =
h(\theta)^{-1}\]</span></p>
<p>nous obtenons :</p>
<p><span class="math display">\[h(\theta) \cdot \int_{\Theta} g(\phi) \,
d\phi = 1\]</span></p>
<p>ce qui implique :</p>
<p><span class="math display">\[h(\theta) = \left( \int_{\Theta} g(\phi)
\, d\phi \right)^{-1}
$$

En substituant cette expression dans la définition de $\pi$, nous
obtenons :

\[
\pi(\theta) = g(\theta) \cdot \left( \int_{\Theta} g(\phi) \, d\phi
\right)^{-1}\]</span></p>
<p>ce qui montre que (ii) est vérifié.</p>
<p>Réciproquement, supposons que (ii) soit vrai. Nous définissons <span
class="math inline">\(h(\theta)\)</span> par :</p>
<p><span class="math display">\[h(\theta) = \left( \int_{\Theta} g(\phi)
\, d\phi \right)^{-1}\]</span></p>
<p>En substituant cette expression dans (ii), nous obtenons :</p>
<p><span class="math display">\[\pi(\theta) = g(\theta) \cdot
h(\theta)\]</span></p>
<p>ce qui montre que <span class="math inline">\(\pi\)</span> est un
prior informatif, et donc que (i) est vérifié. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous énumérons maintenant quelques propriétés fondamentales des
priors informatifs.</p>
<div class="proposition">
<p>Soit <span class="math inline">\(\pi\)</span> un prior informatif sur
<span class="math inline">\(\Theta\)</span>, et soit <span
class="math inline">\(T : \Theta \rightarrow \mathbb{R}^k\)</span> une
fonction mesurable. Alors la distribution poussée en avant <span
class="math inline">\(\pi_T\)</span> définie par :</p>
<p><span class="math display">\[\pi_T(A) = \pi(T^{-1}(A))\]</span></p>
<p>pour tout borélien <span class="math inline">\(A \subset
\mathbb{R}^k\)</span>, est également un prior informatif.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Puisque <span class="math inline">\(\pi\)</span> est
un prior informatif, il existe une fonction <span
class="math inline">\(g : \Theta \rightarrow [0,1]\)</span> telle que
:</p>
<p><span class="math display">\[\pi(\theta) = g(\theta) \cdot
h(\theta)\]</span></p>
<p>où <span class="math inline">\(h\)</span> est la fonction de
normalisation. Nous définissons <span class="math inline">\(g_T :
\mathbb{R}^k \rightarrow [0,1]\)</span> par :</p>
<p><span class="math display">\[g_T(t) = \int_{T^{-1}(t)} g(\theta) \,
d\theta\]</span></p>
<p>pour tout <span class="math inline">\(t \in \mathbb{R}^k\)</span>.
Alors <span class="math inline">\(\pi_T\)</span> est un prior informatif
avec fonction <span class="math inline">\(g_T\)</span>. ◻</p>
</div>
<div class="corollary">
<p>Soit <span class="math inline">\(\pi\)</span> un prior informatif sur
<span class="math inline">\(\Theta\)</span>, et soit <span
class="math inline">\(g : \Theta \rightarrow [0,1]\)</span> la fonction
associée. Alors pour toute fonction mesurable <span
class="math inline">\(f : \Theta \rightarrow \mathbb{R}\)</span>, nous
avons :</p>
<p><span class="math display">\[\mathbb{E}_\pi[f] = \frac{\int_{\Theta}
f(\theta) g(\theta) \, d\theta}{\int_{\Theta} g(\phi) \,
d\phi}\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}_\pi\)</span> désigne
l’espérance par rapport à la distribution <span
class="math inline">\(\pi\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la définition
du prior informatif et des propriétés fondamentales de l’espérance. En
effet, nous avons :</p>
<p><span class="math display">\[\mathbb{E}_\pi[f] = \int_{\Theta}
f(\theta) \, d\pi(\theta) = \int_{\Theta} f(\theta) g(\theta) h(\theta)
\, d\theta\]</span></p>
<p>En substituant l’expression de <span
class="math inline">\(h\)</span>, nous obtenons le résultat
annoncé. ◻</p>
</div>
<h1 id="applications">Applications</h1>
<p>Les priors informatifs trouvent des applications dans de nombreux
domaines, allant de la biostatistique à l’apprentissage automatique.
Nous présentons ici deux exemples classiques.</p>
<h2 id="modèle-de-régression-bayésienne">Modèle de régression
bayésienne</h2>
<p>Considérons un modèle de régression linéaire où nous voulons estimer
les coefficients <span class="math inline">\(\beta \in
\mathbb{R}^p\)</span> à partir de données <span
class="math inline">\((X_i, Y_i)_{1 \leq i \leq n}\)</span>. Un choix
populaire pour le prior informatif est la distribution normale centrée
en zéro avec une variance <span class="math inline">\(\tau^2\)</span>
:</p>
<p><span class="math display">\[\pi(\beta) \propto \exp\left(
-\frac{\|\beta\|^2}{2\tau^2} \right)\]</span></p>
<p>Cette distribution encode l’idée que les coefficients <span
class="math inline">\(\beta\)</span> sont probablement petits, ce qui
correspond à une forme de régularisation.</p>
<h2 id="modèle-hiérarchique">Modèle hiérarchique</h2>
<p>Dans les modèles hiérarchiques, nous voulons souvent partager
l’information entre différents groupes. Par exemple, supposons que nous
avons des données <span class="math inline">\((Y_{ij})_{1 \leq i \leq m,
1 \leq j \leq n_i}\)</span> provenant de <span
class="math inline">\(m\)</span> groupes. Nous pouvons définir un prior
informatif pour les paramètres de groupe <span
class="math inline">\(\theta_i\)</span> comme suit :</p>
<p><span class="math display">\[\pi(\theta_1, \ldots, \theta_m) =
\prod_{i=1}^m \pi(\theta_i)\]</span></p>
<p>où chaque <span class="math inline">\(\pi(\theta_i)\)</span> est un
prior informatif qui encode l’information spécifique au groupe <span
class="math inline">\(i\)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Les priors informatifs constituent un outil puissant pour incorporer
des connaissances a priori dans les modèles statistiques. Leur
utilisation permet d’améliorer la précision des estimations, surtout
lorsque les données sont rares ou bruitées. Cependant, leur choix n’est
pas toujours trivial et nécessite une compréhension approfondie du
problème étudié. Les développements récents en théorie bayésienne et en
apprentissage automatique ouvrent de nouvelles perspectives pour
l’utilisation des priors informatifs dans des contextes complexes et à
grande échelle.</p>
</body>
</html>
{% include "footer.html" %}


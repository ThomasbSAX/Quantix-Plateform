{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Wasserstein : Une mesure de la dissimilarité entre distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Wasserstein : Une mesure de la
dissimilarité entre distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Wasserstein, également connue sous le nom de distance
de Kantorovich-Rubinstein, émerge comme une notion fondamentale en
théorie des probabilités et en analyse mathématique. Son origine remonte
aux travaux de Leonid Kantorovich dans les années 1940, qui introduisit
cette notion pour résoudre des problèmes de transport optimal. Plus
tard, Wassily Hoeffding et Walter M. Rosenblatt ont approfondi cette
idée dans le contexte des statistiques.</p>
<p>Cette distance mesure la dissimilarité entre deux distributions de
probabilité en considérant le coût minimal nécessaire pour transformer
une distribution en une autre. Elle est indispensable dans divers
domaines tels que l’apprentissage automatique, la théorie des jeux, et
l’économie. La distance de Wasserstein permet de capturer des
caractéristiques fines des distributions, ce qui la rend
particulièrement utile dans les applications où les moments classiques
(comme l’espérance ou la variance) ne suffisent pas à décrire
adéquatement les données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la distance de Wasserstein, commençons par fixer
quelques notions préliminaires. Considérons deux espaces mesurables
<span class="math inline">\((X, \mathcal{X})\)</span> et <span
class="math inline">\((Y, \mathcal{Y})\)</span>, et deux mesures de
probabilité <span class="math inline">\(\mu\)</span> sur <span
class="math inline">\(X\)</span> et <span
class="math inline">\(\nu\)</span> sur <span
class="math inline">\(Y\)</span>. Nous cherchons à mesurer la distance
entre ces deux mesures en termes de coût de transport.</p>
<p>Supposons que nous ayons une fonction de coût <span
class="math inline">\(c: X \times Y \rightarrow [0, +\infty)\)</span>,
qui représente le coût de transporter une unité de masse de <span
class="math inline">\(x \in X\)</span> à <span class="math inline">\(y
\in Y\)</span>. La distance de Wasserstein cherche alors à minimiser le
coût total de transport pour transformer <span
class="math inline">\(\mu\)</span> en <span
class="math inline">\(\nu\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\((X, \mathcal{X})\)</span> et <span
class="math inline">\((Y, \mathcal{Y})\)</span> deux espaces mesurables,
<span class="math inline">\(\mu\)</span> une mesure de probabilité sur
<span class="math inline">\(X\)</span>, <span
class="math inline">\(\nu\)</span> une mesure de probabilité sur <span
class="math inline">\(Y\)</span>, et <span class="math inline">\(c: X
\times Y \rightarrow [0, +\infty)\)</span> une fonction de coût. La
distance de Wasserstein d’ordre <span class="math inline">\(p \geq
1\)</span> entre <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie comme : <span
class="math display">\[W_p(\mu, \nu) = \left( \inf_{\gamma \in
\Gamma(\mu, \nu)} \int_{X \times Y} c(x,y)^p \, d\gamma(x,y)
\right)^{1/p},\]</span> où <span class="math inline">\(\Gamma(\mu,
\nu)\)</span> est l’ensemble des mesures de couplage entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, c’est-à-dire l’ensemble des mesures
<span class="math inline">\(\gamma\)</span> sur <span
class="math inline">\(X \times Y\)</span> telles que pour tout <span
class="math inline">\(A \in \mathcal{X}\)</span> et <span
class="math inline">\(B \in \mathcal{Y}\)</span>, on a : <span
class="math display">\[\gamma(A \times Y) = \mu(A) \quad \text{et} \quad
\gamma(X \times B) = \nu(B).\]</span></p>
</div>
<p>Une formulation alternative de la distance de Wasserstein utilise les
quantiles des distributions. Pour deux mesures <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> sur <span
class="math inline">\(\mathbb{R}\)</span>, la distance de Wasserstein
d’ordre 1 peut s’écrire comme : <span class="math display">\[W_1(\mu,
\nu) = \int_{-\infty}^{+\infty} |F_\mu(x) - F_\nu(x)| \, dx,\]</span> où
<span class="math inline">\(F_\mu\)</span> et <span
class="math inline">\(F_\nu\)</span> sont les fonctions de répartition
de <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, respectivement.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Wasserstein est le
théorème de Kantorovich-Rubinstein, qui fournit une caractérisation
alternative de la distance de Wasserstein d’ordre 1.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux mesures de probabilité sur un
espace métrique <span class="math inline">\((X, d)\)</span>. La distance
de Wasserstein d’ordre 1 entre <span class="math inline">\(\mu\)</span>
et <span class="math inline">\(\nu\)</span> est donnée par : <span
class="math display">\[W_1(\mu, \nu) = \sup_{f \in \text{Lip}_1(X)}
\left( \int_X f \, d\mu - \int_X f \, d\nu \right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> est l’ensemble des
fonctions <span class="math inline">\(f: X \rightarrow
\mathbb{R}\)</span> Lipschitziennes avec une constante de Lipschitz
inférieure ou égale à 1.</p>
</div>
<p>La preuve de ce théorème repose sur des techniques d’optimisation et
utilise le fait que les fonctions Lipschitziennes peuvent être utilisées
pour approcher la distance de Wasserstein.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kantorovich-Rubinstein, nous commençons
par rappeler que toute fonction Lipschitzienne <span
class="math inline">\(f\)</span> avec une constante de Lipschitz
inférieure ou égale à 1 peut être utilisée pour borner la distance de
Wasserstein. Plus précisément, nous avons : <span
class="math display">\[\left| \int_X f \, d\mu - \int_X f \, d\nu
\right| \leq W_1(\mu, \nu).\]</span></p>
<p>Pour voir cela, considérons une mesure de couplage <span
class="math inline">\(\gamma\)</span> entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>. Nous avons : <span
class="math display">\[\left| \int_X f \, d\mu - \int_X f \, d\nu
\right| = \left| \int_{X \times X} (f(x) - f(y)) \, d\gamma(x,y) \right|
\leq \int_{X \times X} |f(x) - f(y)| \, d\gamma(x,y).\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(f\)</span> est
Lipschitzienne avec une constante de Lipschitz inférieure ou égale à 1,
nous obtenons : <span class="math display">\[\int_{X \times X} |f(x) -
f(y)| \, d\gamma(x,y) \leq \int_{X \times X} d(x,y) \,
d\gamma(x,y).\]</span></p>
<p>En prenant l’infimum sur toutes les mesures de couplage <span
class="math inline">\(\gamma\)</span>, nous obtenons : <span
class="math display">\[\left| \int_X f \, d\mu - \int_X f \, d\nu
\right| \leq W_1(\mu, \nu).\]</span></p>
<p>Réciproquement, pour toute <span class="math inline">\(\epsilon &gt;
0\)</span>, il existe une mesure de couplage <span
class="math inline">\(\gamma_\epsilon\)</span> telle que : <span
class="math display">\[\int_{X \times X} d(x,y) \, d\gamma_\epsilon(x,y)
\leq W_1(\mu, \nu) + \epsilon.\]</span></p>
<p>En utilisant une approximation de la fonction distance <span
class="math inline">\(d(x,y)\)</span> par des fonctions Lipschitziennes,
nous pouvons montrer que : <span class="math display">\[W_1(\mu, \nu) =
\sup_{f \in \text{Lip}_1(X)} \left( \int_X f \, d\mu - \int_X f \, d\nu
\right).\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distance de Wasserstein possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p>**Triangulaire** : Pour toutes mesures de probabilité <span
class="math inline">\(\mu, \nu, \eta\)</span>, nous avons : <span
class="math display">\[W_p(\mu, \eta) \leq W_p(\mu, \nu) + W_p(\nu,
\eta).\]</span></p></li>
<li><p>**Invariance par translation** : Si <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> sont translatées par un vecteur <span
class="math inline">\(a\)</span>, alors : <span
class="math display">\[W_p(\mu + a, \nu + a) = W_p(\mu,
\nu).\]</span></p></li>
<li><p>**Continuité** : La distance de Wasserstein est continue par
rapport à la convergence faible des mesures.</p></li>
</ol>
<p>La preuve de ces propriétés repose sur les définitions et les
théorèmes précédents. Par exemple, la propriété triangulaire découle
directement de l’inégalité triangulaire pour les intégrales.
L’invariance par translation est une conséquence immédiate de la
définition de la distance de Wasserstein. Enfin, la continuité peut être
démontrée en utilisant des techniques d’analyse fonctionnelle.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Erreur de Variance : Une Analyse Approfondie</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Erreur de Variance : Une Analyse Approfondie</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’erreur de variance, ou biais-variance trade-off en anglais, est un
concept fondamental dans l’apprentissage automatique et la théorie des
statistiques. Elle émerge naturellement lorsque l’on cherche à minimiser
le risque d’un modèle prédictif, c’est-à-dire la somme de l’erreur
quadratique moyenne sur un ensemble d’apprentissage et sur un ensemble
de test. Ce compromis entre le biais et la variance est indispensable
pour comprendre les limites des modèles statistiques et pour optimiser
leurs performances.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir formellement l’erreur de variance, il est essentiel
de comprendre les concepts sous-jacents. Lorsque nous utilisons un
modèle pour prédire une valeur, nous voulons que cette prédiction soit
aussi proche que possible de la vraie valeur. Cependant, en raison des
variations dans les données d’apprentissage, notre modèle peut être
sujet à des erreurs. Ces erreurs peuvent être divisées en deux
composantes principales : le biais et la variance.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f\)</span> une fonction cible que
nous cherchons à approximer, et soit <span
class="math inline">\(\hat{f}\)</span> un modèle appris à partir des
données. Le biais mesure l’écart moyen entre la prédiction de notre
modèle et la vraie valeur cible.</p>
<p>Formellement, pour une entrée <span class="math inline">\(x\)</span>,
le biais est défini comme : <span
class="math display">\[\text{Biais}(\hat{f}(x)) = E[\hat{f}(x)] -
f(x)\]</span> où <span class="math inline">\(E\)</span> désigne
l’espérance mathématique.</p>
</div>
<div class="definition">
<p>La variance mesure la sensibilité de notre modèle aux variations dans
les données d’apprentissage. Elle quantifie à quel point les prédictions
de notre modèle changent si nous utilisons un ensemble de données
différent.</p>
<p>Formellement, pour une entrée <span class="math inline">\(x\)</span>,
la variance est définie comme : <span
class="math display">\[\text{Variance}(\hat{f}(x)) = E[(\hat{f}(x) -
E[\hat{f}(x)])^2]\]</span></p>
</div>
<div class="definition">
<p>L’erreur de variance, ou biais-variance trade-off, est la somme du
carré du biais et de la variance. Elle représente l’erreur totale de
notre modèle en raison des compromis entre le biais et la variance.</p>
<p>Formellement, l’erreur de variance est définie comme : <span
class="math display">\[\text{Erreur de Variance} =
(\text{Biais}(\hat{f}(x)))^2 + \text{Variance}(\hat{f}(x))\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour mieux comprendre l’erreur de variance, il est utile d’examiner
le théorème du biais-variance. Ce théorème montre comment l’erreur
quadratique moyenne d’un modèle peut être décomposée en biais et
variance.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\hat{f}\)</span> un modèle appris à
partir des données, et soit <span class="math inline">\(f\)</span> la
fonction cible. L’erreur quadratique moyenne (EQM) de <span
class="math inline">\(\hat{f}\)</span> peut être décomposée en biais et
variance comme suit : <span class="math display">\[E[(f(x) -
\hat{f}(x))^2] = (\text{Biais}(\hat{f}(x)))^2 +
\text{Variance}(\hat{f}(x)) + \sigma^2\]</span> où <span
class="math inline">\(\sigma^2\)</span> est la variance du bruit dans
les données.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème du biais-variance, nous devons décomposer
l’erreur quadratique moyenne en utilisant les définitions du biais et de
la variance.</p>
<div class="proof">
<p><em>Proof.</em> Commençons par décomposer l’erreur quadratique
moyenne : <span class="math display">\[E[(f(x) - \hat{f}(x))^2] =
E[(f(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - \hat{f}(x))^2]\]</span> En
développant le carré, nous obtenons : <span
class="math display">\[E[(f(x) - \hat{f}(x))^2] = E[(f(x) -
E[\hat{f}(x)])^2] + E[(E[\hat{f}(x)] - \hat{f}(x))^2] + 2E[(f(x) -
E[\hat{f}(x)])(E[\hat{f}(x)] - \hat{f}(x))]\]</span> Le terme croisé est
nul car <span class="math inline">\(E[\hat{f}(x)]\)</span> est une
constante par rapport à l’espérance : <span
class="math display">\[E[(f(x) - \hat{f}(x))^2] = (E[\hat{f}(x)] -
f(x))^2 + E[(\hat{f}(x) - E[\hat{f}(x)])^2]\]</span> En utilisant les
définitions du biais et de la variance, nous obtenons : <span
class="math display">\[E[(f(x) - \hat{f}(x))^2] =
(\text{Biais}(\hat{f}(x)))^2 + \text{Variance}(\hat{f}(x))\]</span>
Enfin, en ajoutant la variance du bruit <span
class="math inline">\(\sigma^2\)</span>, nous obtenons le théorème du
biais-variance : <span class="math display">\[E[(f(x) - \hat{f}(x))^2] =
(\text{Biais}(\hat{f}(x)))^2 + \text{Variance}(\hat{f}(x)) +
\sigma^2\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le théorème du biais-variance a plusieurs implications importantes
pour l’apprentissage automatique et la théorie des statistiques. Voici
quelques propriétés et corollaires :</p>
<ol>
<li><p><strong>Compromis Biais-Variance</strong> : Il existe un
compromis entre le biais et la variance. Un modèle avec un faible biais
peut avoir une haute variance, et vice versa. Le but est de trouver un
équilibre optimal entre les deux.</p></li>
<li><p><strong>Complexité du Modèle</strong> : La complexité du modèle
influence directement le biais et la variance. Un modèle trop simple
aura un haut biais, tandis qu’un modèle trop complexe aura une haute
variance.</p></li>
<li><p><strong>Régularisation</strong> : La régularisation est une
technique utilisée pour réduire la variance d’un modèle en ajoutant une
pénalité à la complexité du modèle. Cela permet de trouver un compromis
optimal entre le biais et la variance.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’erreur de variance, ou biais-variance trade-off, est un concept
fondamental dans l’apprentissage automatique et la théorie des
statistiques. En comprenant ce compromis, nous pouvons optimiser les
performances de nos modèles prédictifs et éviter les pièges courants
tels que le surapprentissage et le sous-apprentissage. Le théorème du
biais-variance fournit une base mathématique solide pour analyser et
améliorer nos modèles, faisant de lui un outil indispensable dans le
domaine des statistiques et de l’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


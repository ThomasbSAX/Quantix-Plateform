{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie conditionnelle : Fondements, Théorèmes et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie conditionnelle : Fondements, Théorèmes et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie conditionnelle émerge comme un concept fondamental en
théorie de l’information, prolongeant la notion classique d’entropie de
Shannon. Introduite pour quantifier l’incertitude résiduelle d’une
variable aléatoire lorsque l’information sur une autre variable est
disponible, cette notion joue un rôle clé dans la compréhension des
dépendances entre variables aléatoires. Son importance se manifeste dans
divers domaines, allant de la compression de données à l’apprentissage
automatique, en passant par les communications numériques.</p>
<p>L’entropie conditionnelle permet de répondre à des questions
cruciales : jusqu’à quel point la connaissance d’une variable
réduit-elle l’incertitude sur une autre ? Comment mesurer l’information
mutuelle entre deux variables aléatoires ? Ces interrogations trouvent
leur réponse dans le cadre rigoureux de l’entropie conditionnelle,
offrant ainsi des outils puissants pour l’analyse et la modélisation des
systèmes complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie conditionnelle, considérons deux variables
aléatoires <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Nous cherchons à quantifier
l’incertitude de <span class="math inline">\(X\)</span> lorsque la
valeur de <span class="math inline">\(Y\)</span> est connue.
Intuitivement, cette incertitude doit être inférieure ou égale à
l’entropie de <span class="math inline">\(X\)</span>, et elle doit
capturer la réduction d’incertitude apportée par l’information sur <span
class="math inline">\(Y\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires discrètes
définies sur un ensemble fini. L’entropie conditionnelle de <span
class="math inline">\(X\)</span> sachant <span
class="math inline">\(Y\)</span>, notée <span
class="math inline">\(H(X|Y)\)</span>, est définie par : <span
class="math display">\[H(X|Y) = \sum_{y \in \mathcal{Y}} P_Y(y)
H(X|Y=y)\]</span> où <span class="math inline">\(\mathcal{Y}\)</span>
est l’ensemble des valeurs possibles de <span
class="math inline">\(Y\)</span>, et <span
class="math inline">\(H(X|Y=y)\)</span> est l’entropie conditionnelle de
<span class="math inline">\(X\)</span> sachant que <span
class="math inline">\(Y\)</span> prend la valeur <span
class="math inline">\(y\)</span>, donnée par : <span
class="math display">\[H(X|Y=y) = -\sum_{x \in \mathcal{X}} P_{X|Y}(x|y)
\log P_{X|Y}(x|y)\]</span> avec <span
class="math inline">\(\mathcal{X}\)</span> l’ensemble des valeurs
possibles de <span class="math inline">\(X\)</span>, et <span
class="math inline">\(P_{X|Y}(x|y)\)</span> la probabilité
conditionnelle de <span class="math inline">\(X = x\)</span> sachant
<span class="math inline">\(Y = y\)</span>.</p>
</div>
<p>Une formulation alternative de l’entropie conditionnelle, utilisant
des quantificateurs, est : <span class="math display">\[H(X|Y) =
\mathbb{E}_Y \left[ H(X|Y=y) \right] = \sum_{y \in \mathcal{Y}} P_Y(y)
\left( -\sum_{x \in \mathcal{X}} P_{X|Y}(x|y) \log P_{X|Y}(x|y)
\right)\]</span> où <span class="math inline">\(\mathbb{E}_Y\)</span>
désigne l’espérance par rapport à la loi de <span
class="math inline">\(Y\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Plusieurs théorèmes fondamentaux mettent en lumière les propriétés de
l’entropie conditionnelle. Nous en présentons deux parmi les plus
importants.</p>
<div class="theorem">
<p>Pour toute variable aléatoire <span class="math inline">\(X\)</span>
et tout ensemble d’événements <span class="math inline">\(Y\)</span>,
l’entropie conditionnelle satisfait : <span
class="math display">\[H(X|Y) \leq H(X)\]</span> avec égalité si et
seulement si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette inégalité repose sur le fait que
l’espérance d’une fonction concave est inférieure ou égale à la valeur
de cette fonction en l’espérance. En effet, <span
class="math inline">\(H(X|Y=y)\)</span> est une fonction concave de la
loi conditionnelle <span class="math inline">\(P_{X|Y}(x|y)\)</span>, et
donc : <span class="math display">\[\mathbb{E}_Y \left[ H(X|Y=y) \right]
\leq H(\mathbb{E}_Y[X])\]</span> où <span
class="math inline">\(H(\mathbb{E}_Y[X])\)</span> est l’entropie de la
loi marginale de <span class="math inline">\(X\)</span>, c’est-à-dire
<span class="math inline">\(H(X)\)</span>. L’égalité a lieu si et
seulement si les lois conditionnelles <span
class="math inline">\(P_{X|Y}(x|y)\)</span> sont identiques pour tout
<span class="math inline">\(y\)</span>, ce qui équivaut à l’indépendance
de <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. ◻</p>
</div>
<div class="theorem">
<p>Si <span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span> et <span
class="math inline">\(Z\)</span> forment une chaîne de Markov,
c’est-à-dire que <span class="math inline">\(X \rightarrow Y \rightarrow
Z\)</span>, alors : <span class="math display">\[H(X|Y,Z) =
H(X|Y)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème utilise la propriété de
factorisation des probabilités conditionnelles dans une chaîne de
Markov. Puisque <span class="math inline">\(X\)</span> et <span
class="math inline">\(Z\)</span> sont conditionnellement indépendantes
sachant <span class="math inline">\(Y\)</span>, on a : <span
class="math display">\[P_{X|Y,Z}(x|y,z) = P_{X|Y}(x|y)\]</span> pour
tout <span class="math inline">\(x\)</span>, <span
class="math inline">\(y\)</span> et <span
class="math inline">\(z\)</span>. En substituant cette égalité dans la
définition de l’entropie conditionnelle, on obtient : <span
class="math display">\[H(X|Y,Z) = \sum_{y,z} P_{Y,Z}(y,z) H(X|Y=y,Z=z) =
\sum_{y,z} P_{Y,Z}(y,z) H(X|Y=y) = \sum_y P_Y(y) H(X|Y=y) =
H(X|Y)\]</span> où la dernière égalité découle de la loi des
probabilités totales. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie conditionnelle possède plusieurs propriétés remarquables,
que nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie conditionnelle est
toujours non négative, c’est-à-dire : <span
class="math display">\[H(X|Y) \geq 0\]</span> avec égalité si et
seulement si <span class="math inline">\(X\)</span> est déterministe
conditionnellement à <span class="math inline">\(Y\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité découle directement de la
définition de l’entropie conditionnelle, puisque <span
class="math inline">\(H(X|Y=y) \geq 0\)</span> pour tout <span
class="math inline">\(y\)</span>, et que l’espérance préserve cette
propriété. ◻</p>
</div></li>
<li><p><strong>Additivité</strong> : Si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors : <span
class="math display">\[H(X,Y) = H(X) + H(Y)\]</span> où <span
class="math inline">\(H(X,Y)\)</span> est l’entropie jointe de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> L’additivité résulte de la factorisation des
probabilités jointes en produits de probabilités marginales lorsque
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes. En effet, on a :
<span class="math display">\[H(X,Y) = -\sum_{x,y} P_{X,Y}(x,y) \log
P_{X,Y}(x,y) = -\sum_{x,y} P_X(x)P_Y(y) \log (P_X(x)P_Y(y)) = H(X) +
H(Y)\]</span> ◻</p>
</div></li>
<li><p><strong>Fano</strong> : Pour toute variable aléatoire <span
class="math inline">\(X\)</span> et tout ensemble d’événements <span
class="math inline">\(Y\)</span>, l’entropie conditionnelle satisfait :
<span class="math display">\[H(X|Y) = h(P_e) + P_e \log
|\mathcal{X}|\]</span> où <span class="math inline">\(h(p) = -p \log p -
(1-p) \log (1-p)\)</span> est l’entropie binaire, et <span
class="math inline">\(P_e = \mathbb{P}(X \neq \hat{X})\)</span> est la
probabilité d’erreur lorsque <span class="math inline">\(X\)</span> est
estimé par <span class="math inline">\(\hat{X} =
\mathbb{E}[X|Y]\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété utilise la théorie des
bornes de Fano, qui relie l’entropie conditionnelle à la probabilité
d’erreur. En effet, on a : <span class="math display">\[H(X|Y) = h(P_e)
+ P_e \log |\mathcal{X}|\]</span> où <span
class="math inline">\(h(P_e)\)</span> représente l’incertitude binaire
résiduelle, et <span class="math inline">\(P_e \log
|\mathcal{X}|\)</span> quantifie l’incertitude due aux erreurs
d’estimation. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie conditionnelle se révèle être un outil indispensable pour
l’analyse des dépendances entre variables aléatoires. Ses propriétés
fondamentales, illustrées par les théorèmes et corollaires présentés
dans cet article, en font un concept central en théorie de
l’information. Les applications de cette notion sont vastes et variées,
allant des communications numériques à l’apprentissage automatique, en
passant par la compression de données. Une compréhension approfondie de
l’entropie conditionnelle ouvre ainsi la voie à des avancées
significatives dans divers domaines scientifiques et technologiques.</p>
</body>
</html>
{% include "footer.html" %}


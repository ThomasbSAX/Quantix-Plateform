{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Embedding Layers : Fondements et Applications en Apprentissage Profond</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Embedding Layers : Fondements et Applications en
Apprentissage Profond</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Les couches d’embedding (ou embedding layers) constituent un pilier
fondamental dans le domaine de l’apprentissage profond, particulièrement
dans les applications de traitement du langage naturel (NLP). L’idée
centrale derrière ces couches est de transformer des données discrètes,
comme des mots ou des catégories, en représentations vectorielles
continues. Cette transformation permet aux modèles de capturer des
relations sémantiques et syntaxiques entre les éléments d’entrée,
facilitant ainsi l’apprentissage de tâches complexes.</p>
<p>L’émergence des couches d’embedding s’inscrit dans le cadre plus
large de la recherche visant à améliorer l’efficacité des modèles de
traitement automatique du langage. Avant leur introduction, les
approches traditionnelles utilisaient souvent des encodages one-hot, qui
souffraient de plusieurs limitations : ils étaient très parcimonieux
(sparsity), ce qui rendait difficile la capture de relations entre les
éléments, et ils ne permettaient pas une généralisation efficace aux
nouvelles données.</p>
<p>Les couches d’embedding ont révolutionné ce paysage en permettant une
représentation dense et continue des données, où les relations entre les
éléments peuvent être capturées par la similarité vectorielle. Cette
approche a trouvé des applications dans une multitude de domaines,
allant du traitement du langage naturel à la recommandation de produits
en passant par l’analyse d’images.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre les couches d’embedding, il est essentiel de définir
plusieurs concepts clés.</p>
<h2 class="unnumbered" id="espace-dembedding">Espace d’Embedding</h2>
<p>Considérons un ensemble de catégories discrètes <span
class="math inline">\(C = \{c_1, c_2, \dots, c_n\}\)</span>. L’objectif
est de représenter chaque catégorie <span
class="math inline">\(c_i\)</span> par un vecteur dans un espace
euclidien <span class="math inline">\(\mathbb{R}^d\)</span>, où <span
class="math inline">\(d\)</span> est la dimension de l’espace
d’embedding. Cet espace est tel que les vecteurs représentant des
catégories similaires sont proches les uns des autres.</p>
<p>Formellement, un embedding est une fonction <span
class="math inline">\(\phi: C \rightarrow \mathbb{R}^d\)</span> qui
associe à chaque catégorie <span class="math inline">\(c_i\)</span> un
vecteur <span class="math inline">\(\phi(c_i) \in
\mathbb{R}^d\)</span>.</p>
<h2 class="unnumbered" id="couche-dembedding">Couche d’Embedding</h2>
<p>Une couche d’embedding est une couche dans un réseau de neurones qui
effectue la transformation des catégories discrètes en vecteurs
continus. Elle est généralement définie par une matrice de poids <span
class="math inline">\(W \in \mathbb{R}^{n \times d}\)</span>, où chaque
ligne <span class="math inline">\(W_i\)</span> représente le vecteur
d’embedding pour la catégorie <span
class="math inline">\(c_i\)</span>.</p>
<p>Pour une entrée discrète <span class="math inline">\(x\)</span>, la
couche d’embedding produit un vecteur de sortie <span
class="math inline">\(e_x = W_x\)</span>, où <span
class="math inline">\(W_x\)</span> est la ligne correspondante dans la
matrice <span class="math inline">\(W\)</span>.</p>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<h2 class="unnumbered" id="théorème-de-lembedding-linéaire">Théorème de
l’Embedding Linéaire</h2>
<p>Un des théorèmes fondamentaux liés aux couches d’embedding est le
théorème de l’optimisation convexe, qui garantit que sous certaines
conditions, les vecteurs d’embedding peuvent être appris de manière
efficace.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> un ensemble de catégories
et <span class="math inline">\(D\)</span> une matrice de similarité
définie positive. Il existe une fonction d’embedding <span
class="math inline">\(\phi: C \rightarrow \mathbb{R}^d\)</span> telle
que pour tout <span class="math inline">\(c_i, c_j \in C\)</span>, on a:
<span class="math display">\[\| \phi(c_i) - \phi(c_j) \|^2 =
D_{ij}\]</span></p>
</div>
<h2 class="unnumbered"
id="preuve-du-théorème-de-lembedding-linéaire">Preuve du Théorème de
l’Embedding Linéaire</h2>
<p>La preuve de ce théorème repose sur la théorie des matrices définies
positives et l’optimisation convexe. On commence par diagonaliser la
matrice <span class="math inline">\(D\)</span> pour obtenir ses valeurs
propres et vecteurs propres. Ensuite, on montre que les vecteurs
d’embedding peuvent être construits à partir de ces valeurs propres et
vecteurs propres.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(D\)</span> une
matrice de similarité définie positive. On peut diagonaliser <span
class="math inline">\(D\)</span> pour obtenir: <span
class="math display">\[D = U \Lambda U^T\]</span> où <span
class="math inline">\(U\)</span> est une matrice orthogonale et <span
class="math inline">\(\Lambda\)</span> est une matrice diagonale
contenant les valeurs propres de <span
class="math inline">\(D\)</span>.</p>
<p>Les vecteurs d’embedding peuvent alors être construits comme suit:
<span class="math display">\[\phi(c_i) = U \sqrt{\Lambda} e_i\]</span>
où <span class="math inline">\(e_i\)</span> est le vecteur de base
canonique.</p>
<p>Il est facile de vérifier que cette construction satisfait la
condition: <span class="math display">\[\| \phi(c_i) - \phi(c_j) \|^2 =
D_{ij}\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered" id="propriété-de-continuité">Propriété de
Continuité</h2>
<p>Une des propriétés fondamentales des couches d’embedding est la
continuité des représentations vectorielles.</p>
<div class="property">
<p>Pour toute fonction d’embedding <span class="math inline">\(\phi: C
\rightarrow \mathbb{R}^d\)</span>, il existe une constante <span
class="math inline">\(M &gt; 0\)</span> telle que pour tout <span
class="math inline">\(c_i, c_j \in C\)</span>, on a: <span
class="math display">\[\| \phi(c_i) - \phi(c_j) \| \leq M\]</span></p>
</div>
<h2 class="unnumbered" id="preuve-de-la-propriété-de-continuité">Preuve
de la Propriété de Continuité</h2>
<p>La preuve de cette propriété repose sur le fait que les vecteurs
d’embedding sont bornés.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\phi: C \rightarrow
\mathbb{R}^d\)</span> une fonction d’embedding. Puisque <span
class="math inline">\(C\)</span> est un ensemble fini, il existe un
nombre fini de vecteurs d’embedding. On peut donc définir: <span
class="math display">\[M = \max_{c_i, c_j \in C} \| \phi(c_i) -
\phi(c_j) \|\]</span> Cette constante <span
class="math inline">\(M\)</span> est finie et satisfait la condition de
continuité. ◻</p>
</div>
<h2 class="unnumbered" id="corollaire-de-la-densité">Corollaire de la
Densité</h2>
<p>Un corollaire important de la propriété de continuité est le
corollaire de la densité des représentations vectorielles.</p>
<div class="corollary">
<p>Pour toute fonction d’embedding <span class="math inline">\(\phi: C
\rightarrow \mathbb{R}^d\)</span>, l’ensemble des vecteurs d’embedding
<span class="math inline">\(\{ \phi(c_i) | c_i \in C \}\)</span> est
dense dans un compact de <span
class="math inline">\(\mathbb{R}^d\)</span>.</p>
</div>
<h2 class="unnumbered" id="preuve-du-corollaire-de-la-densité">Preuve du
Corollaire de la Densité</h2>
<p>La preuve de ce corollaire repose sur le théorème de
Bolzano-Weierstrass et la propriété de continuité.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\phi: C \rightarrow
\mathbb{R}^d\)</span> une fonction d’embedding. Puisque <span
class="math inline">\(\{ \phi(c_i) | c_i \in C \}\)</span> est un
ensemble borné, on peut appliquer le théorème de Bolzano-Weierstrass
pour montrer qu’il contient une suite convergente. La propriété de
continuité garantit que la limite de cette suite est également un
vecteur d’embedding. ◻</p>
</div>
<h1 class="unnumbered" id="applications">Applications</h1>
<p>Les couches d’embedding trouvent des applications dans une multitude
de domaines, notamment le traitement du langage naturel, la
recommandation de produits et l’analyse d’images. Elles permettent de
capturer des relations complexes entre les données, facilitant ainsi
l’apprentissage de tâches complexes.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Les couches d’embedding constituent un outil puissant et polyvalent
dans le domaine de l’apprentissage profond. Elles permettent de
transformer des données discrètes en représentations vectorielles
continues, capturant ainsi des relations complexes entre les éléments
d’entrée. Leur utilisation a révolutionné de nombreux domaines, ouvrant
la voie à des avancées significatives dans le traitement automatique du
langage et d’autres applications.</p>
</body>
</html>
{% include "footer.html" %}


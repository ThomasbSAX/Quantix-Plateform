{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Squared Hellinger</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Squared Hellinger</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de squared Hellinger, souvent désignée par <span
class="math inline">\(H^2\)</span>-divergence, émerge comme un outil
fondamental dans l’analyse statistique et la théorie de l’information.
Son origine remonte aux travaux pionniers sur les distances entre
distributions de probabilité, où elle se distingue par ses propriétés
métriques et son interprétation géométrique. La notion de divergence de
Hellinger est indispensable dans les cadres où l’on cherche à mesurer la
dissimilarité entre deux distributions de probabilité, notamment en
apprentissage automatique et en inférence statistique. Son carré, la
divergence de squared Hellinger, offre une mesure plus sensible et
adaptée à diverses applications pratiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence de squared Hellinger, commençons par
comprendre ce que nous cherchons à mesurer. Imaginons deux distributions
de probabilité, <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier la différence entre ces deux distributions. Une approche
naturelle consiste à considérer la racine carrée de l’intégrale des
différences entre les densités de <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span>.</p>
<p>Formellement, la divergence de squared Hellinger entre deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, avec des densités respectives
<span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> par rapport à une mesure de référence
<span class="math inline">\(\mu\)</span>, est définie comme suit :</p>
<div class="definition">
<p>La divergence de squared Hellinger entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[H^2(P, Q) = \frac{1}{2} \int_{\Omega} \left(
\sqrt{p(x)} - \sqrt{q(x)} \right)^2 \, d\mu(x)\]</span> ou, de manière
équivalente, <span class="math display">\[H^2(P, Q) = 1 - \int_{\Omega}
\sqrt{p(x) q(x)} \, d\mu(x).\]</span></p>
</div>
<p>Cette définition peut être exprimée de plusieurs manières, en
utilisant différentes notations et quantificateurs. Par exemple, si
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des distributions discrètes, nous
avons : <span class="math display">\[H^2(P, Q) = \frac{1}{2} \sum_{x \in
\Omega} \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2.\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de squared Hellinger est
le théorème de Pinsker, qui établit une relation entre la divergence de
Kullback-Leibler et la divergence de squared Hellinger.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Alors, <span class="math display">\[H^2(P, Q)
\leq \frac{1}{2} D_{\text{KL}}(P \| Q),\]</span> où <span
class="math inline">\(D_{\text{KL}}(P \| Q)\)</span> est la divergence
de Kullback-Leibler entre <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Pinsker, nous commençons par rappeler
la définition de la divergence de Kullback-Leibler : <span
class="math display">\[D_{\text{KL}}(P \| Q) = \int_{\Omega} p(x) \log
\left( \frac{p(x)}{q(x)} \right) \, d\mu(x).\]</span></p>
<p>Nous utilisons ensuite l’inégalité de log-sum, qui stipule que pour
tout <span class="math inline">\(x \geq 0\)</span>, <span
class="math display">\[\log(x) \leq x - 1.\]</span></p>
<p>En appliquant cette inégalité à la divergence de Kullback-Leibler,
nous obtenons : <span class="math display">\[D_{\text{KL}}(P \| Q) \geq
\int_{\Omega} p(x) \left( 1 - \frac{q(x)}{p(x)} \right) \, d\mu(x) =
\int_{\Omega} (p(x) - q(x)) \, d\mu(x).\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons : <span
class="math display">\[\left( \int_{\Omega} (p(x) - q(x)) \, d\mu(x)
\right)^2 \leq \left( \int_{\Omega} p(x) \, d\mu(x) \right) \left(
\int_{\Omega} q(x) \, d\mu(x) \right) = 1.\]</span></p>
<p>Ainsi, <span class="math display">\[D_{\text{KL}}(P \| Q) \geq 2
H^2(P, Q).\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de squared Hellinger possède plusieurs propriétés
intéressantes, que nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour toute paire de
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons <span
class="math inline">\(H^2(P, Q) \geq 0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité découle directement de la
définition, car le carré d’une différence est toujours non
négatif. ◻</p>
</div></li>
<li><p><strong>Symétrie</strong> : La divergence de squared Hellinger
est symétrique, c’est-à-dire <span class="math inline">\(H^2(P, Q) =
H^2(Q, P)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La symétrie résulte de l’égalité <span
class="math inline">\(\left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 =
\left( \sqrt{q(x)} - \sqrt{p(x)} \right)^2\)</span>. ◻</p>
</div></li>
<li><p><strong>Identité</strong> : <span class="math inline">\(H^2(P, Q)
= 0\)</span> si et seulement si <span class="math inline">\(P =
Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(H^2(P, Q) =
0\)</span>, alors <span class="math inline">\(\sqrt{p(x)} =
\sqrt{q(x)}\)</span> presque partout, ce qui implique <span
class="math inline">\(p(x) = q(x)\)</span> presque partout, et donc
<span class="math inline">\(P = Q\)</span>. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de squared Hellinger est un outil puissant et élégant
pour mesurer la dissimilarité entre distributions de probabilité. Ses
propriétés métriques et son interprétation géométrique en font un choix
privilégié dans de nombreuses applications en statistique et en
apprentissage automatique. Les théorèmes et propriétés associés, tels
que le théorème de Pinsker, enrichissent notre compréhension des
relations entre différentes mesures de divergence et ouvrent la voie à
de nouvelles avancées théoriques et pratiques.</p>
</body>
</html>
{% include "footer.html" %}


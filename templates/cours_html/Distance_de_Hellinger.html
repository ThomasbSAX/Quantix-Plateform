{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La Distance de Hellinger : Une Mesure d’Écart Entre Distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La Distance de Hellinger : Une Mesure d’Écart Entre
Distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Hellinger, du nom du mathématicien finlandais Ernst
Hellinger (1883-1950), est une mesure de la dissimilarité entre deux
distributions de probabilité. Son origine remonte aux travaux de
Hellinger sur les formes quadratiques et les inégalités dans l’analyse
fonctionnelle. Cette notion émerge naturellement dans le cadre de la
théorie des probabilités et de l’analyse statistique, où il est crucial
de quantifier l’écart entre deux lois de probabilité.</p>
<p>L’intérêt pour la distance de Hellinger réside dans sa capacité à
fournir une mesure robuste et géométriquement interprétable de la
différence entre deux distributions. Elle est particulièrement utile
dans les contextes où l’on souhaite comparer des modèles statistiques ou
évaluer la convergence de suites de distributions. La distance de
Hellinger est également indissociable des travaux sur les inégalités de
concentration et les bornes de probabilité, où elle joue un rôle clé
dans l’estimation des erreurs.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la distance de Hellinger, commençons par comprendre
ce que nous cherchons à mesurer. Imaginons deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier à quel point ces deux distributions diffèrent l’une de
l’autre. Une approche naturelle consiste à comparer leurs densités par
rapport à une mesure de référence, disons <span
class="math inline">\(\mu\)</span>.</p>
<p>Supposons que <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient absolument continues par rapport
à <span class="math inline">\(\mu\)</span>, avec des densités
respectives <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span>. Nous cherchons une mesure de la
distance entre <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> qui soit invariante par transformation
mesurable et qui capture l’écart global entre les deux
distributions.</p>
<p>La distance de Hellinger <span class="math inline">\(H(P, Q)\)</span>
est définie comme suit :</p>
<div class="definition">
<p>La distance de Hellinger entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> absolument continues par rapport à une
mesure de référence <span class="math inline">\(\mu\)</span>, avec des
densités respectives <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span>, est donnée par :</p>
<p><span class="math display">\[H(P, Q) = \frac{1}{\sqrt{2}} \left(
\int_{\Omega} \left( \sqrt{p(\omega)} - \sqrt{q(\omega)} \right)^2 \,
d\mu(\omega) \right)^{1/2}\]</span></p>
<p>De manière équivalente, on peut l’écrire comme :</p>
<p><span class="math display">\[H(P, Q) = \left( 1 - \int_{\Omega}
\sqrt{p(\omega) q(\omega)} \, d\mu(\omega) \right)^{1/2}\]</span></p>
<p>Ou encore, en utilisant la notation des normes <span
class="math inline">\(L^2\)</span> :</p>
<p><span class="math display">\[H(P, Q) = \| \sqrt{p} - \sqrt{q}
\|_2\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Hellinger est
l’inégalité de Pinsker, qui établit une relation entre la distance de
Hellinger et la divergence de Kullback-Leibler.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, absolument continues par rapport à une mesure de
référence <span class="math inline">\(\mu\)</span>. Alors, la distance
de Hellinger <span class="math inline">\(H(P, Q)\)</span> et la
divergence de Kullback-Leibler <span class="math inline">\(D_{KL}(P \|
Q)\)</span> sont liées par l’inégalité suivante :</p>
<p><span class="math display">\[H(P, Q)^2 \leq \frac{1}{2} D_{KL}(P \|
Q)\]</span></p>
<p>De manière équivalente, on a :</p>
<p><span class="math display">\[D_{KL}(P \| Q) \geq 2 H(P,
Q)^2\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Pinsker, nous allons utiliser des
propriétés de la divergence de Kullback-Leibler et des inégalités
classiques.</p>
<div class="proof">
<p><em>Proof.</em> Commençons par rappeler que la divergence de
Kullback-Leibler <span class="math inline">\(D_{KL}(P \| Q)\)</span> est
définie par :</p>
<p><span class="math display">\[D_{KL}(P \| Q) = \int_{\Omega} p(\omega)
\log \left( \frac{p(\omega)}{q(\omega)} \right) \,
d\mu(\omega)\]</span></p>
<p>Nous voulons montrer que :</p>
<p><span class="math display">\[D_{KL}(P \| Q) \geq 2 H(P, Q)^2 = 2
\left( 1 - \int_{\Omega} \sqrt{p(\omega) q(\omega)} \, d\mu(\omega)
\right)\]</span></p>
<p>Considérons la fonction <span class="math inline">\(f(x) = x \log x -
(x-1)\)</span>, définie pour <span class="math inline">\(x &gt;
0\)</span>. On peut montrer que cette fonction est positive et convexe,
avec un minimum en <span class="math inline">\(x = 1\)</span>. En
utilisant cette propriété, nous avons :</p>
<p><span class="math display">\[p(\omega) \log \left(
\frac{p(\omega)}{q(\omega)} \right) - p(\omega) + q(\omega) \geq
0\]</span></p>
<p>En intégrant cette inégalité par rapport à <span
class="math inline">\(\mu\)</span>, nous obtenons :</p>
<p><span class="math display">\[D_{KL}(P \| Q) - 1 + \int_{\Omega}
q(\omega) \, d\mu(\omega) \geq 0\]</span></p>
<p>Puisque <span class="math inline">\(Q\)</span> est une distribution
de probabilité, <span class="math inline">\(\int_{\Omega} q(\omega) \,
d\mu(\omega) = 1\)</span>, donc :</p>
<p><span class="math display">\[D_{KL}(P \| Q) - 2 \geq -2 \int_{\Omega}
\sqrt{p(\omega) q(\omega)} \, d\mu(\omega)\]</span></p>
<p>En réarrangeant les termes, nous obtenons :</p>
<p><span class="math display">\[D_{KL}(P \| Q) \geq 2 - 2 \int_{\Omega}
\sqrt{p(\omega) q(\omega)} \, d\mu(\omega)\]</span></p>
<p>Ce qui est équivalent à :</p>
<p><span class="math display">\[D_{KL}(P \| Q) \geq 2 H(P,
Q)^2\]</span></p>
<p>Ainsi, l’inégalité de Pinsker est démontrée. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance de Hellinger possède plusieurs propriétés importantes,
que nous allons énumérer et démontrer.</p>
<ol>
<li><p><strong>Inégalité triangulaire</strong> : La distance de
Hellinger satisfait l’inégalité triangulaire, c’est-à-dire que pour
toutes distributions <span class="math inline">\(P, Q, R\)</span>, on a
:</p>
<p><span class="math display">\[H(P, R) \leq H(P, Q) + H(Q,
R)\]</span></p>
<div class="proof">
<p><em>Proof.</em> Utilisons la définition de la distance de Hellinger
et l’inégalité triangulaire pour les normes <span
class="math inline">\(L^2\)</span> :</p>
<p><span class="math display">\[H(P, R) = \| \sqrt{p} - \sqrt{r} \|_2
\leq \| \sqrt{p} - \sqrt{q} \|_2 + \| \sqrt{q} - \sqrt{r} \|_2 = H(P, Q)
+ H(Q, R)\]</span> ◻</p>
</div></li>
<li><p><strong>Symétrie</strong> : La distance de Hellinger est
symétrique, c’est-à-dire que :</p>
<p><span class="math display">\[H(P, Q) = H(Q, P)\]</span></p>
<div class="proof">
<p><em>Proof.</em> Par définition, la distance de Hellinger est donnée
par :</p>
<p><span class="math display">\[H(P, Q) = \| \sqrt{p} - \sqrt{q}
\|_2\]</span></p>
<p>Or, la norme <span class="math inline">\(L^2\)</span> est symétrique,
donc :</p>
<p><span class="math display">\[H(P, Q) = \| \sqrt{p} - \sqrt{q} \|_2 =
\| \sqrt{q} - \sqrt{p} \|_2 = H(Q, P)\]</span> ◻</p>
</div></li>
<li><p><strong>Positivité</strong> : La distance de Hellinger est
positive, c’est-à-dire que <span class="math inline">\(H(P, Q) \geq
0\)</span>, avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La norme <span class="math inline">\(L^2\)</span> est
positive, donc :</p>
<p><span class="math display">\[H(P, Q) = \| \sqrt{p} - \sqrt{q} \|_2
\geq 0\]</span></p>
<p>De plus, <span class="math inline">\(H(P, Q) = 0\)</span> si et
seulement si <span class="math inline">\(\sqrt{p} = \sqrt{q}\)</span>
presque partout, ce qui équivaut à <span class="math inline">\(p =
q\)</span> presque partout, c’est-à-dire <span class="math inline">\(P =
Q\)</span>. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Hellinger est une mesure puissante et élégante pour
quantifier l’écart entre deux distributions de probabilité. Son
utilisation est répandue dans divers domaines des mathématiques et de la
statistique, notamment dans l’analyse des modèles statistiques et les
bornes de probabilité. Les propriétés et théorèmes associés à la
distance de Hellinger, tels que l’inégalité de Pinsker et les propriétés
métriques, en font un outil indispensable pour les chercheurs et les
praticiens.</p>
</body>
</html>
{% include "footer.html" %}


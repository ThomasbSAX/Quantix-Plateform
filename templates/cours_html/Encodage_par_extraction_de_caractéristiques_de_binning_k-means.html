{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning k-means</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
k-means</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
cruciale dans le traitement des données, particulièrement lorsqu’on
travaille avec des variables catégorielles. Parmi les méthodes
d’encodage, le binning k-means se distingue par sa capacité à
transformer des données continues en catégories significatives, tout en
préservant les relations sous-jacentes dans les données.</p>
<p>L’origine de cette méthode remonte aux travaux sur le clustering et
la segmentation de données, où l’objectif est de regrouper des points de
données similaires. Le binning k-means combine cette idée avec
l’encodage, permettant de créer des catégories basées sur la similarité
des données. Cette approche est indispensable dans les domaines où
l’interprétation des données catégorielles est essentielle, comme la
classification supervisée et l’analyse exploratoire de données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
binning k-means, il est nécessaire de définir quelques concepts
clés.</p>
<h2 id="binning">Binning</h2>
<p>Le binning est une technique qui consiste à diviser un ensemble de
données continues en intervalles, ou "bins". Chaque intervalle est
ensuite traité comme une catégorie distincte. Formellement, soit <span
class="math inline">\(X\)</span> un ensemble de données continues et
<span class="math inline">\(k\)</span> le nombre de bins souhaités. Le
binning peut être défini comme une fonction <span
class="math inline">\(B: X \rightarrow \{1, 2, \dots, k\}\)</span> qui
associe chaque valeur de <span class="math inline">\(X\)</span> à un
bin.</p>
<h2 id="k-means">k-means</h2>
<p>Le k-means est un algorithme de clustering qui partitionne un
ensemble de données en <span class="math inline">\(k\)</span> clusters.
Soit <span class="math inline">\(D = \{x_1, x_2, \dots, x_n\}\)</span>
un ensemble de données et <span class="math inline">\(k\)</span> le
nombre de clusters souhaités. L’algorithme k-means cherche à minimiser
la fonction de coût suivante :</p>
<p><span class="math display">\[\underset{S}{\text{argmin}}
\sum_{i=1}^{k} \sum_{x \in S_i} \|x - \mu_i\|^2\]</span></p>
<p>où <span class="math inline">\(S = \{S_1, S_2, \dots, S_k\}\)</span>
est une partition de <span class="math inline">\(D\)</span>, et <span
class="math inline">\(\mu_i\)</span> est le centroïde du cluster <span
class="math inline">\(S_i\)</span>.</p>
<h2
id="encodage-par-extraction-de-caractéristiques-de-binning-k-means">Encodage
par extraction de caractéristiques de binning k-means</h2>
<p>L’encodage par extraction de caractéristiques de binning k-means
combine les concepts de binning et de k-means. L’idée est d’utiliser
l’algorithme k-means pour créer des bins significatifs, puis d’encoder
les données en fonction de ces bins. Formellement, soit <span
class="math inline">\(X\)</span> un ensemble de données continues et
<span class="math inline">\(k\)</span> le nombre de bins souhaités.
L’encodage peut être défini comme une fonction <span
class="math inline">\(E: X \rightarrow \{0, 1\}^k\)</span> qui associe
chaque valeur de <span class="math inline">\(X\)</span> à un vecteur
binaire indiquant à quel bin la valeur appartient.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-convergence-du-k-means">Théorème de convergence du
k-means</h2>
<p>Le théorème de convergence du k-means stipule que l’algorithme
k-means converge vers un optimum local. Formellement, soit <span
class="math inline">\(D\)</span> un ensemble de données et <span
class="math inline">\(k\)</span> le nombre de clusters souhaités.
L’algorithme k-means converge vers une partition <span
class="math inline">\(S\)</span> qui minimise la fonction de coût :</p>
<p><span class="math display">\[\sum_{i=1}^{k} \sum_{x \in S_i} \|x -
\mu_i\|^2\]</span></p>
<h2 id="théorème-dencodage-par-extraction-de-caractéristiques">Théorème
d’encodage par extraction de caractéristiques</h2>
<p>Le théorème d’encodage par extraction de caractéristiques stipule que
l’encodage par binning k-means préserve les relations sous-jacentes dans
les données. Formellement, soit <span class="math inline">\(X\)</span>
un ensemble de données continues et <span
class="math inline">\(E\)</span> la fonction d’encodage définie
précédemment. Pour toute paire de points <span class="math inline">\(x,
y \in X\)</span>, la distance entre les encodages <span
class="math inline">\(E(x)\)</span> et <span
class="math inline">\(E(y)\)</span> est proportionnelle à la distance
entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>.</p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-convergence-du-k-means">Preuve du théorème
de convergence du k-means</h2>
<p>La preuve du théorème de convergence du k-means repose sur le fait
que la fonction de coût est décroissante à chaque itération de
l’algorithme. Soit <span class="math inline">\(D\)</span> un ensemble de
données et <span class="math inline">\(k\)</span> le nombre de clusters
souhaités. À chaque itération, l’algorithme k-means met à jour les
centroïdes <span class="math inline">\(\mu_i\)</span> de manière à
minimiser la fonction de coût. Comme la fonction de coût est bornée
inférieurement, l’algorithme converge vers un optimum local.</p>
<h2
id="preuve-du-théorème-dencodage-par-extraction-de-caractéristiques">Preuve
du théorème d’encodage par extraction de caractéristiques</h2>
<p>La preuve du théorème d’encodage par extraction de caractéristiques
repose sur le fait que l’encodage par binning k-means préserve les
relations sous-jacentes dans les données. Soit <span
class="math inline">\(X\)</span> un ensemble de données continues et
<span class="math inline">\(E\)</span> la fonction d’encodage définie
précédemment. Pour toute paire de points <span class="math inline">\(x,
y \in X\)</span>, la distance entre les encodages <span
class="math inline">\(E(x)\)</span> et <span
class="math inline">\(E(y)\)</span> est proportionnelle à la distance
entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>. Cela découle du fait que les bins sont
créés en fonction de la similarité des données, ce qui préserve les
relations sous-jacentes.</p>
<h1 id="propriétés-et-corollaires">Propriétés et corollaires</h1>
<h2 id="propriété-de-préservation-des-relations">Propriété de
préservation des relations</h2>
<p>L’encodage par extraction de caractéristiques de binning k-means
préserve les relations sous-jacentes dans les données. Formellement,
soit <span class="math inline">\(X\)</span> un ensemble de données
continues et <span class="math inline">\(E\)</span> la fonction
d’encodage définie précédemment. Pour toute paire de points <span
class="math inline">\(x, y \in X\)</span>, la distance entre les
encodages <span class="math inline">\(E(x)\)</span> et <span
class="math inline">\(E(y)\)</span> est proportionnelle à la distance
entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>.</p>
<h2 id="corollaire-de-réduction-de-dimensionnalité">Corollaire de
réduction de dimensionnalité</h2>
<p>L’encodage par extraction de caractéristiques de binning k-means
permet de réduire la dimensionnalité des données. Formellement, soit
<span class="math inline">\(X\)</span> un ensemble de données continues
et <span class="math inline">\(E\)</span> la fonction d’encodage définie
précédemment. L’encodage <span class="math inline">\(E(x)\)</span> est
un vecteur binaire de dimension <span class="math inline">\(k\)</span>,
où <span class="math inline">\(k\)</span> est le nombre de bins. Cela
permet de réduire la dimensionnalité des données tout en préservant les
relations sous-jacentes.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning k-means est
une technique puissante pour transformer des données continues en
catégories significatives. Cette méthode combine les concepts de binning
et de k-means, permettant de créer des bins significatifs tout en
préservant les relations sous-jacentes dans les données. Les théorèmes
et propriétés présentés dans cet article montrent que cette méthode est
efficace et robuste, ce qui en fait un outil précieux pour le traitement
des données.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>The Absolute Loss Function: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Absolute Loss Function: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The absolute loss function, often referred to as the <span
class="math inline">\(L_1\)</span> norm or Manhattan distance, has been
a cornerstone in various fields of mathematics and statistics. Its
origins can be traced back to the early developments of statistical
estimation, where it was recognized for its robustness and simplicity.
The absolute loss function emerges as a natural choice when one seeks to
minimize the sum of deviations, making it indispensable in regression
analysis, optimization problems, and machine learning.</p>
<p>The absolute loss function is particularly valuable because it
mitigates the influence of outliers compared to the squared loss
function. This property makes it a preferred choice in robust
statistics, where the presence of outliers can significantly distort
results. Additionally, the absolute loss function is convex, which
ensures that optimization problems involving this function are
well-behaved and can be solved efficiently.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the absolute loss function, let us first consider the
problem of measuring the discrepancy between observed and predicted
values. Suppose we have a set of observed values <span
class="math inline">\(y_1, y_2, \ldots, y_n\)</span> and a corresponding
set of predicted values <span class="math inline">\(\hat{y}_1,
\hat{y}_2, \ldots, \hat{y}_n\)</span>. We seek a function that
quantifies the total error between these two sets.</p>
<p>The absolute loss function, denoted by <span
class="math inline">\(L_1\)</span>, is defined as the sum of the
absolute differences between the observed and predicted values.
Formally, we can express this as:</p>
<div class="definition">
<p>The absolute loss function <span class="math inline">\(L_1:
\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is
defined by: <span class="math display">\[L_1(\mathbf{y},
\hat{\mathbf{y}}) = \sum_{i=1}^n |y_i - \hat{y}_i|\]</span> where <span
class="math inline">\(\mathbf{y} = (y_1, y_2, \ldots, y_n)\)</span> and
<span class="math inline">\(\hat{\mathbf{y}} = (\hat{y}_1, \hat{y}_2,
\ldots, \hat{y}_n)\)</span>.</p>
</div>
<p>Alternatively, we can express the absolute loss function using vector
notation. Let <span class="math inline">\(\mathbf{e} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> be the error vector. Then, the absolute loss
function can be written as: <span class="math display">\[L_1(\mathbf{y},
\hat{\mathbf{y}}) = \|\mathbf{e}\|_1\]</span> where <span
class="math inline">\(\|\cdot\|_1\)</span> denotes the <span
class="math inline">\(L_1\)</span> norm.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the most significant theorems related to the absolute loss
function is the LASSO (Least Absolute Shrinkage and Selection Operator)
theorem, which demonstrates the utility of the <span
class="math inline">\(L_1\)</span> norm in variable selection and
regularization.</p>
<div class="theorem">
<p>Consider the optimization problem: <span
class="math display">\[\min_{\mathbf{\beta}} \left\{ \frac{1}{2n}
\|\mathbf{y} - X\mathbf{\beta}\|_2^2 + \lambda \|\mathbf{\beta}\|_1
\right\}\]</span> where <span class="math inline">\(\mathbf{y} \in
\mathbb{R}^n\)</span> is the response vector, <span
class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span> is the
design matrix, <span class="math inline">\(\mathbf{\beta} \in
\mathbb{R}^p\)</span> is the coefficient vector, and <span
class="math inline">\(\lambda &gt; 0\)</span> is a tuning parameter. The
solution to this problem, known as the LASSO estimator, possesses the
following properties:</p>
<ol>
<li><p>Sparsity: The LASSO estimator <span
class="math inline">\(\hat{\mathbf{\beta}}\)</span> has at least as many
zero components as the number of non-zero components in the true
coefficient vector <span
class="math inline">\(\mathbf{\beta}^*\)</span>.</p></li>
<li><p>Consistency: Under certain conditions, the LASSO estimator is
consistent for the true coefficient vector <span
class="math inline">\(\mathbf{\beta}^*\)</span>.</p></li>
<li><p>Regularization: The LASSO estimator effectively regularizes the
coefficient vector <span class="math inline">\(\mathbf{\beta}\)</span>
by shrinking its components towards zero.</p></li>
</ol>
</div>
<h1 id="proofs">Proofs</h1>
<p>To prove the LASSO theorem, we first note that the optimization
problem can be rewritten as: <span
class="math display">\[\min_{\mathbf{\beta}} \left\{ \frac{1}{2n}
\sum_{i=1}^n (y_i - x_i^T \mathbf{\beta})^2 + \lambda \sum_{j=1}^p
|\beta_j| \right\}\]</span> where <span
class="math inline">\(x_i^T\)</span> denotes the <span
class="math inline">\(i\)</span>-th row of the design matrix <span
class="math inline">\(X\)</span>.</p>
<p>The proof relies on the Karush-Kuhn-Tucker (KKT) conditions, which
provide necessary and sufficient conditions for optimality in
constrained optimization problems. The KKT conditions for the LASSO
problem are:</p>
<ol>
<li><p>Stationarity: <span class="math inline">\(\nabla_{\mathbf{\beta}}
\left( \frac{1}{2n} \|\mathbf{y} - X\mathbf{\beta}\|_2^2 + \lambda
\|\mathbf{\beta}\|_1 \right) = 0\)</span></p></li>
<li><p>Primal feasibility: <span
class="math inline">\(\mathbf{\beta}\)</span> is feasible.</p></li>
<li><p>Dual feasibility: The Lagrange multipliers satisfy certain
constraints.</p></li>
<li><p>Complementary slackness: The product of the Lagrange multipliers
and the slack variables is zero.</p></li>
</ol>
<p>By analyzing these conditions, we can show that the LASSO estimator
<span class="math inline">\(\hat{\mathbf{\beta}}\)</span> satisfies the
sparsity, consistency, and regularization properties stated in the
theorem.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The absolute loss function possesses several important properties
that make it a valuable tool in various applications. We list some of
these properties below:</p>
<ol>
<li><p>Convexity: The absolute loss function is convex, which ensures
that optimization problems involving this function are well-behaved and
can be solved efficiently.</p></li>
<li><p>Robustness: The absolute loss function is less sensitive to
outliers compared to the squared loss function, making it a preferred
choice in robust statistics.</p></li>
<li><p>Differentiability: The absolute loss function is not
differentiable at the origin, which can complicate optimization
algorithms. However, subgradient methods can be used to handle this
issue.</p></li>
<li><p>Scale Invariance: The absolute loss function is scale invariant,
meaning that it remains unchanged under linear transformations of the
data.</p></li>
</ol>
<p>Each of these properties can be proven using standard techniques from
convex analysis and optimization theory. For example, the convexity of
the absolute loss function can be shown by verifying that its Hessian
matrix is positive semidefinite. The robustness property can be
demonstrated by comparing the influence function of the absolute loss
function with that of the squared loss function.</p>
</body>
</html>
{% include "footer.html" %}


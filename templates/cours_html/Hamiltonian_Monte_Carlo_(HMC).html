{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Hamiltonian Monte Carlo: A Comprehensive Exploration</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Hamiltonian Monte Carlo: A Comprehensive
Exploration</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The realm of statistical sampling has been revolutionized by the
advent of Markov Chain Monte Carlo (MCMC) methods. Among these,
Hamiltonian Monte Carlo (HMC) stands out as a powerful tool for sampling
from complex, high-dimensional distributions. Rooted in the principles
of Hamiltonian mechanics, HMC transcends traditional MCMC methods by
leveraging gradient information to navigate the target distribution more
efficiently.</p>
<p>HMC was introduced to address the limitations of random-walk based
MCMC methods, which often suffer from slow convergence and high
autocorrelation in high-dimensional spaces. By incorporating the
gradient of the target distribution’s log-density, HMC constructs a
trajectory that respects the underlying geometry of the distribution.
This geometric intuition allows HMC to propose moves that are more
informative, leading to faster mixing and improved sampling
efficiency.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand HMC, we must first establish the necessary definitions
and notations. Consider a target distribution <span
class="math inline">\(\pi(x)\)</span> with density <span
class="math inline">\(p(x)\)</span>. Our goal is to generate samples
from <span class="math inline">\(\pi(x)\)</span>.</p>
<div class="definition">
<p>A Hamiltonian system is defined by a pair <span
class="math inline">\((q, p)\)</span> where <span
class="math inline">\(q \in \mathbb{R}^n\)</span> represents the
position and <span class="math inline">\(p \in \mathbb{R}^n\)</span>
represents the momentum. The dynamics of the system are governed by
Hamilton’s equations: <span class="math display">\[\frac{dq_i}{dt} =
\frac{\partial H}{\partial p_i}, \quad \frac{dp_i}{dt} = -\frac{\partial
H}{\partial q_i}, \quad \forall i \in \{1, \dots, n\}\]</span> where
<span class="math inline">\(H(q, p)\)</span> is the Hamiltonian
function.</p>
</div>
<p>For HMC, we define the Hamiltonian as the negative log-density of the
target distribution plus a kinetic energy term: <span
class="math display">\[H(q, p) = -\log \pi(q) + \frac{1}{2} p^T M^{-1}
p\]</span> where <span class="math inline">\(M\)</span> is a positive
definite mass matrix.</p>
<h1 id="the-hmc-algorithm">The HMC Algorithm</h1>
<p>The core idea of HMC is to simulate the trajectory of a particle in a
Hamiltonian system. The algorithm proceeds as follows:</p>
<ol>
<li><p>Sample a momentum <span class="math inline">\(p\)</span> from a
Gaussian distribution with covariance matrix <span
class="math inline">\(M\)</span>.</p></li>
<li><p>Simulate the Hamiltonian dynamics for a fixed time <span
class="math inline">\(\tau\)</span> using a symplectic integrator, such
as the leapfrog method.</p></li>
<li><p>Accept or reject the proposed state <span
class="math inline">\((q^*, p^*)\)</span> using a Metropolis-Hastings
correction.</p></li>
</ol>
<p>The leapfrog integrator is a second-order symplectic method that
preserves the Hamiltonian structure of the system. It updates the
position and momentum as follows: <span class="math display">\[p
\leftarrow p - \frac{\epsilon}{2} \nabla U(q)\]</span> <span
class="math display">\[q \leftarrow q + \epsilon M^{-1} p\]</span> <span
class="math display">\[p \leftarrow p - \frac{\epsilon}{2} \nabla
U(q)\]</span> where <span class="math inline">\(\epsilon\)</span> is the
step size and <span class="math inline">\(U(q) = -\log
\pi(q)\)</span>.</p>
<h1 id="theoretical-properties">Theoretical Properties</h1>
<p>HMC enjoys several theoretical guarantees that make it a robust
sampling method. Notably, under suitable conditions on the step size
<span class="math inline">\(\epsilon\)</span> and the trajectory length
<span class="math inline">\(\tau\)</span>, the HMC Markov chain
converges to the target distribution <span
class="math inline">\(\pi(x)\)</span>.</p>
<div class="theorem">
<p>Let <span class="math inline">\((q_t, p_t)\)</span> be the Markov
chain generated by the HMC algorithm. If the step size <span
class="math inline">\(\epsilon\)</span> is sufficiently small and the
trajectory length <span class="math inline">\(\tau\)</span> is finite,
then <span class="math inline">\((q_t, p_t)\)</span> is ergodic with
respect to the joint distribution <span class="math inline">\(\pi(q)
\mathcal{N}(0, M)\)</span>.</p>
</div>
<p>The proof of this theorem relies on the fact that the leapfrog
integrator is reversible and volume-preserving, ensuring that the HMC
Markov chain satisfies detailed balance.</p>
<h1 id="proof-of-ergodicity">Proof of Ergodicity</h1>
<p>To prove the ergodicity of HMC, we need to establish that the Markov
chain <span class="math inline">\((q_t, p_t)\)</span> is reversible and
satisfies detailed balance.</p>
<div class="proof">
<p><em>Proof.</em> The leapfrog integrator is a composition of three
reversible transformations: <span class="math display">\[p \mapsto p -
\frac{\epsilon}{2} \nabla U(q)\]</span> <span class="math display">\[(q,
p) \mapsto (q + \epsilon M^{-1} p, p)\]</span> <span
class="math display">\[p \mapsto p - \frac{\epsilon}{2} \nabla
U(q)\]</span> Since the composition of reversible transformations is
reversible, the leapfrog integrator is reversible.</p>
<p>The Metropolis-Hastings correction ensures that detailed balance is
satisfied: <span class="math display">\[\pi(q, p) P((q, p) \rightarrow
(q^*, p^*)) = \pi(q^*, p^*) P((q^*, p^*) \rightarrow (q, p))\]</span>
where <span class="math inline">\(P\)</span> denotes the transition
probability. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>HMC has several properties that make it particularly useful for
sampling from complex distributions.</p>
<ol>
<li><p><strong>Geometric Intuition</strong>: HMC leverages the gradient
information to propose moves that respect the underlying geometry of the
target distribution. This leads to more efficient exploration of the
state space.</p></li>
<li><p><strong>Efficiency</strong>: HMC often requires fewer iterations
to achieve a given effective sample size compared to traditional MCMC
methods, especially in high-dimensional settings.</p></li>
<li><p><strong>Adaptivity</strong>: The mass matrix <span
class="math inline">\(M\)</span> can be adapted to the curvature of the
target distribution, further improving sampling efficiency.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Hamiltonian Monte Carlo represents a significant advancement in the
field of statistical sampling. By combining the principles of
Hamiltonian mechanics with MCMC methods, HMC provides a powerful tool
for exploring complex, high-dimensional distributions. Its theoretical
guarantees and practical efficiency make it an indispensable method in
modern statistical computation.</p>
</body>
</html>
{% include "footer.html" %}


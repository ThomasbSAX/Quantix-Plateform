{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Pinsker : Une borne fondamentale en théorie de l’information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Pinsker : Une borne fondamentale en
théorie de l’information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Pinsker, nommée d’après le mathématicien soviétique
Mikhaïl Pinsker, occupe une place centrale en théorie de l’information.
Elle établit un lien fondamental entre la divergence de Kullback-Leibler
et la distance en variation totale, deux notions clés pour mesurer la
dissimilarité entre distributions de probabilité. Cette inégalité émerge
naturellement dans le contexte des limites de performance des systèmes
de communication, où l’estimation précise de la distance entre
distributions est cruciale.</p>
<p>Historiquement, Pinsker a introduit cette inégalité dans les années
1960 pour répondre à des besoins pratiques en traitement du signal et en
théorie de la détection. Son importance réside dans sa capacité à
fournir une borne inférieure simple et interprétable pour la divergence
de Kullback-Leibler, une quantité souvent difficile à calculer
directement. Ainsi, l’inégalité de Pinsker devient un outil
indispensable dans l’analyse des canaux bruités, la compression de
données et bien d’autres domaines.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégalité de Pinsker, il est essentiel de définir
les notions fondamentales qu’elle met en relation.</p>
<h2 class="unnumbered" id="divergence-de-kullback-leibler">Divergence de
Kullback-Leibler</h2>
<p>La divergence de Kullback-Leibler, notée <span
class="math inline">\(D(P \parallel Q)\)</span>, mesure la quantité
d’information perdue lorsqu’on utilise une distribution de probabilité
<span class="math inline">\(Q\)</span> pour approximer une autre
distribution <span class="math inline">\(P\)</span>. Intuitivement, plus
<span class="math inline">\(D(P \parallel Q)\)</span> est grand, plus
les distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont différentes.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur un même ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La divergence de
Kullback-Leibler de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D(P \parallel Q) = \sum_{x \in \mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right),\]</span> où <span
class="math inline">\(0 \log \left( \frac{0}{Q(x)} \right) = 0\)</span>
par convention.</p>
</div>
<h2 class="unnumbered" id="distance-en-variation-totale">Distance en
variation totale</h2>
<p>La distance en variation totale, notée <span
class="math inline">\(\|P - Q\|_1\)</span>, mesure la différence absolue
entre deux distributions de probabilité. Elle est souvent plus facile à
calculer que la divergence de Kullback-Leibler et fournit une mesure
directe de la dissimilarité entre <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur un même ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La distance en variation
totale entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[\|P - Q\|_1 = \sum_{x \in \mathcal{X}} |P(x) -
Q(x)|.\]</span></p>
</div>
<h1 class="unnumbered" id="linégalité-de-pinsker">L’inégalité de
Pinsker</h1>
<p>L’inégalité de Pinsker établit une relation entre la divergence de
Kullback-Leibler et la distance en variation totale. Elle affirme que la
distance en variation totale est bornée inférieurement par une fonction
de la divergence de Kullback-Leibler.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur un même ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. Alors, on a : <span
class="math display">\[\|P - Q\|_1 \leq \sqrt{2 D(P \parallel Q) \ln
2}.\]</span></p>
</div>
<h1 class="unnumbered" id="preuve-de-linégalité-de-pinsker">Preuve de
l’inégalité de Pinsker</h1>
<p>La preuve de l’inégalité de Pinsker repose sur plusieurs étapes
techniques et l’utilisation de résultats intermédiaires. Voici une
démonstration détaillée.</p>
<h2 class="unnumbered" id="lemme-intermédiaire">Lemme intermédiaire</h2>
<p>Nous commençons par établir un lemme intermédiaire qui sera utile
pour la preuve principale.</p>
<div class="lemma">
<p>Soient <span class="math inline">\(a\)</span> et <span
class="math inline">\(b\)</span> deux nombres réels positifs. Alors, on
a : <span class="math display">\[(a - b)^2 \leq 4ab \ln \left(
\frac{a}{b} \right),\]</span> où <span class="math inline">\(0 \ln
\left( \frac{0}{b} \right) = 0\)</span> par convention.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce lemme repose sur l’étude de la
fonction <span class="math inline">\(f(x) = x \ln x\)</span>. En effet,
on peut montrer que : <span class="math display">\[(a - b)^2 = a^2 + b^2
- 2ab \leq 4ab \ln \left( \frac{a}{b} \right),\]</span> en utilisant les
propriétés de la fonction <span class="math inline">\(f(x) = x \ln
x\)</span>. ◻</p>
</div>
<h2 class="unnumbered" id="preuve-principale">Preuve principale</h2>
<p>Nous sommes maintenant prêts à prouver l’inégalité de Pinsker.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux distributions de probabilité
discrètes <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un même ensemble fini
<span class="math inline">\(\mathcal{X}\)</span>. Nous voulons montrer
que : <span class="math display">\[\|P - Q\|_1 \leq \sqrt{2 D(P
\parallel Q) \ln 2}.\]</span></p>
<p>En utilisant la définition de la distance en variation totale, on a :
<span class="math display">\[\|P - Q\|_1 = \sum_{x \in \mathcal{X}}
|P(x) - Q(x)|.\]</span></p>
<p>En appliquant le lemme intermédiaire à chaque terme <span
class="math inline">\(P(x)\)</span> et <span
class="math inline">\(Q(x)\)</span>, on obtient : <span
class="math display">\[(P(x) - Q(x))^2 \leq 4 P(x) Q(x) \ln \left(
\frac{P(x)}{Q(x)} \right).\]</span></p>
<p>En sommant sur tous les <span class="math inline">\(x \in
\mathcal{X}\)</span>, on a : <span class="math display">\[\sum_{x \in
\mathcal{X}} (P(x) - Q(x))^2 \leq 4 \sum_{x \in \mathcal{X}} P(x) Q(x)
\ln \left( \frac{P(x)}{Q(x)} \right).\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, on a : <span
class="math display">\[\left( \sum_{x \in \mathcal{X}} |P(x) - Q(x)|
\right)^2 \leq 4 \sum_{x \in \mathcal{X}} P(x) Q(x) \ln \left(
\frac{P(x)}{Q(x)} \right).\]</span></p>
<p>En prenant la racine carrée des deux côtés, on obtient : <span
class="math display">\[\|P - Q\|_1 \leq 2 \sqrt{ \sum_{x \in
\mathcal{X}} P(x) Q(x) \ln \left( \frac{P(x)}{Q(x)} \right)
}.\]</span></p>
<p>Enfin, en utilisant l’inégalité de Jensen et les propriétés de la
divergence de Kullback-Leibler, on a : <span class="math display">\[\|P
- Q\|_1 \leq \sqrt{2 D(P \parallel Q) \ln 2}.\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’inégalité de Pinsker possède plusieurs propriétés intéressantes et
corollaires qui en découlent.</p>
<h2 class="unnumbered" id="propriété-i">Propriété (i)</h2>
<p>L’inégalité de Pinsker est particulièrement utile lorsque la
divergence de Kullback-Leibler est petite. En effet, dans ce cas, elle
fournit une borne supérieure simple et interprétable pour la distance en
variation totale.</p>
<h2 class="unnumbered" id="propriété-ii">Propriété (ii)</h2>
<p>L’inégalité de Pinsker peut être utilisée pour établir des bornes
inférieures sur la divergence de Kullback-Leibler en fonction de la
distance en variation totale. En effet, en inversant l’inégalité, on
obtient : <span class="math display">\[D(P \parallel Q) \geq \frac{ \|P
- Q\|_1^2 }{ 2 \ln 2 }.\]</span></p>
<h2 class="unnumbered" id="propriété-iii">Propriété (iii)</h2>
<p>L’inégalité de Pinsker est également valable pour les distributions
de probabilité continues, à condition que les intégrales converge. Dans
ce cas, la preuve suit des étapes similaires à celles de la version
discrète.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Pinsker est un résultat fondamental en théorie de
l’information, offrant une relation clé entre la divergence de
Kullback-Leibler et la distance en variation totale. Sa simplicité et
son utilité pratique en font un outil indispensable pour l’analyse des
systèmes de communication, la compression de données et bien d’autres
domaines. La preuve détaillée présentée dans cet article met en lumière
les techniques mathématiques sophistiquées qui sous-tendent ce
résultat.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Conjointe : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Conjointe : Une Exploration
Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie conjointe émerge comme une notion fondamentale en théorie
de l’information, un champ qui a révolutionné notre compréhension des
systèmes complexes et des communications. À l’origine, cette notion a
été introduite par Claude Shannon dans les années 1940 pour quantifier
l’incertitude et l’information contenue dans des systèmes aléatoires.
L’entropie conjointe, en particulier, permet de mesurer l’information
partagée entre deux variables aléatoires.</p>
<p>Pourquoi cette notion est-elle indispensable ? Dans un monde où les
données sont omniprésentes, comprendre comment l’information est
partagée entre différentes variables est crucial. Que ce soit en
cryptographie, en apprentissage automatique ou en traitement du signal,
l’entropie conjointe fournit un cadre rigoureux pour analyser et
manipuler les informations.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement l’entropie conjointe, il est essentiel
de comprendre ce que nous cherchons à capturer. Imaginons deux variables
aléatoires <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Nous voulons quantifier l’incertitude
totale de ces deux variables prises ensemble. Cette incertitude doit
tenir compte non seulement des incertitudes individuelles de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, mais aussi de leur dépendance
mutuelle.</p>
<p>La définition formelle de l’entropie conjointe est la suivante :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires discrètes
définies sur un ensemble fini <span class="math inline">\(\mathcal{X}
\times \mathcal{Y}\)</span>. L’entropie conjointe de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est définie par : <span
class="math display">\[H(X, Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in
\mathcal{Y}} p(x, y) \log_2 p(x, y)\]</span> où <span
class="math inline">\(p(x, y)\)</span> est la probabilité conjointe de
<span class="math inline">\(X = x\)</span> et <span
class="math inline">\(Y = y\)</span>.</p>
</div>
<p>Pour les variables aléatoires continues, l’entropie conjointe est
définie de manière analogue en utilisant une intégrale :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires continues
définies sur un espace mesurable <span class="math inline">\(\mathcal{X}
\times \mathcal{Y}\)</span>. L’entropie conjointe de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est définie par : <span
class="math display">\[H(X, Y) = -\int_{\mathcal{X}} \int_{\mathcal{Y}}
f_{X,Y}(x, y) \log_2 f_{X,Y}(x, y) \, dx \, dy\]</span> où <span
class="math inline">\(f_{X,Y}(x, y)\)</span> est la densité de
probabilité conjointe de <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p>
</div>
<h1 id="théorèmes-importants">Théorèmes Importants</h1>
<p>L’entropie conjointe est intimement liée à d’autres concepts en
théorie de l’information, tels que l’entropie conditionnelle et
l’information mutuelle. Un théorème fondamental est celui de la
sub-additivité de l’entropie.</p>
<div class="theorem">
<p>Pour toute paire de variables aléatoires <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, l’entropie conjointe satisfait la
propriété suivante : <span class="math display">\[H(X, Y) \leq H(X) +
H(Y)\]</span> avec égalité si et seulement si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes.</p>
</div>
<p>La preuve de ce théorème repose sur des propriétés fondamentales de
l’entropie et de la probabilité conjointe. En effet, si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors <span
class="math inline">\(p(x, y) = p_X(x) p_Y(y)\)</span>, et l’entropie
conjointe devient simplement la somme des entropies individuelles.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de sub-additivité, nous commençons par
rappeler que l’entropie est une mesure de l’incertitude. L’idée
intuitive derrière ce théorème est que connaître l’une des variables
réduit l’incertitude sur l’autre, sauf si elles sont indépendantes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’entropie conjointe <span
class="math inline">\(H(X, Y)\)</span>. Nous savons que : <span
class="math display">\[H(X, Y) = -\sum_{x, y} p(x, y) \log_2 p(x,
y)\]</span> Nous voulons montrer que <span class="math inline">\(H(X, Y)
\leq H(X) + H(Y)\)</span>. Pour cela, nous utilisons l’inégalité de
Gibbs, qui stipule que pour toute distribution de probabilité <span
class="math inline">\(p\)</span> et toute autre distribution <span
class="math inline">\(q\)</span>, nous avons : <span
class="math display">\[-\sum_{x, y} p(x, y) \log_2 q(x, y) \leq
-\sum_{x, y} p(x, y) \log_2 p(x, y)\]</span> En choisissant <span
class="math inline">\(q(x, y) = p_X(x) p_Y(y)\)</span>, nous obtenons :
<span class="math display">\[-\sum_{x, y} p(x, y) \log_2 (p_X(x) p_Y(y))
\leq H(X, Y)\]</span> Ce qui peut être réécrit comme : <span
class="math display">\[-\sum_{x, y} p(x, y) \log_2 p_X(x) - \sum_{x, y}
p(x, y) \log_2 p_Y(y) \leq H(X, Y)\]</span> En utilisant le fait que
<span class="math inline">\(\sum_{y} p(x, y) = p_X(x)\)</span> et <span
class="math inline">\(\sum_{x} p(x, y) = p_Y(y)\)</span>, nous obtenons
: <span class="math display">\[-\sum_{x} p_X(x) \log_2 p_X(x) - \sum_{y}
p_Y(y) \log_2 p_Y(y) \leq H(X, Y)\]</span> Ce qui est exactement <span
class="math inline">\(H(X) + H(Y) \leq H(X, Y)\)</span>. L’égalité a
lieu si et seulement si <span class="math inline">\(p(x, y) = p_X(x)
p_Y(y)\)</span>, c’est-à-dire lorsque <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie conjointe possède plusieurs propriétés intéressantes qui
en font un outil puissant en théorie de l’information.</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie conjointe est
toujours non négative, c’est-à-dire <span class="math inline">\(H(X, Y)
\geq 0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité découle directement de la
définition, car <span class="math inline">\(p(x, y) \log_2 p(x, y) \leq
0\)</span> pour toute probabilité <span class="math inline">\(p(x,
y)\)</span>. ◻</p>
</div></li>
<li><p><strong>Symétrie</strong> : L’entropie conjointe est symétrique,
c’est-à-dire <span class="math inline">\(H(X, Y) = H(Y, X)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle de la symétrie dans la
définition de l’entropie conjointe, où l’ordre des variables n’a pas
d’importance. ◻</p>
</div></li>
<li><p><strong>Inégalité de Fano</strong> : Pour toute variable
aléatoire <span class="math inline">\(Y\)</span> et toute fonction <span
class="math inline">\(f(Y) = \hat{X}\)</span>, l’entropie conjointe
satisfait : <span class="math display">\[H(X, Y) \leq 1 + H(Y)\]</span>
où <span class="math inline">\(1\)</span> est l’entropie binaire.</p>
<div class="proof">
<p><em>Proof.</em> Cette inégalité peut être dérivée en utilisant les
propriétés de l’entropie conditionnelle et le théorème de Fano. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie conjointe est une notion centrale en théorie de
l’information, offrant un cadre rigoureux pour quantifier l’incertitude
partagée entre deux variables aléatoires. Ses propriétés et ses
théorèmes associés en font un outil indispensable pour l’analyse des
systèmes complexes et des communications. En comprenant profondément
cette notion, nous ouvrons la voie à de nouvelles avancées dans divers
domaines scientifiques et techniques.</p>
</body>
</html>
{% include "footer.html" %}


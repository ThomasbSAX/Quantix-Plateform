{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence \alpha-divergence : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence <span
class="math inline">\(\alpha\)</span>-divergence : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence <span class="math inline">\(\alpha\)</span>-divergence
est un concept fondamental en théorie de l’information et en statistique
mathématique. Introduite par Amari (1985), elle généralise plusieurs
mesures de divergence bien connues, telles que la divergence de
Kullback-Leibler et la divergence de Hellinger. Cette généralisation
permet d’unifier et d’étendre des résultats théoriques et pratiques dans
divers domaines, notamment l’apprentissage automatique, la théorie des
codes et les systèmes de communication.</p>
<p>L’importance de la divergence <span
class="math inline">\(\alpha\)</span>-divergence réside dans sa capacité
à capturer des aspects différents des distributions de probabilité, en
fonction du paramètre <span class="math inline">\(\alpha\)</span>. Pour
différentes valeurs de <span class="math inline">\(\alpha\)</span>, elle
se réduit à des mesures de divergence classiques, offrant ainsi une
flexibilité remarquable. Par exemple, pour <span
class="math inline">\(\alpha = 1\)</span>, elle se réduit à la
divergence de Kullback-Leibler, tandis que pour <span
class="math inline">\(\alpha = 0\)</span>, elle devient la divergence de
Hellinger.</p>
<p>Dans cet article, nous explorerons les définitions formelles de la
divergence <span class="math inline">\(\alpha\)</span>-divergence, ses
propriétés théoriques et ses applications pratiques. Nous fournirons des
preuves détaillées des théorèmes clés et discuterons des implications de
cette mesure dans divers contextes mathématiques et appliqués.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’introduire la divergence <span
class="math inline">\(\alpha\)</span>-divergence, il est essentiel de
comprendre ce que nous cherchons à mesurer. Supposons que nous ayons
deux distributions de probabilité, <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur le même espace. Nous
voulons quantifier la "distance" ou la "divergence" entre ces deux
distributions. Cette divergence doit capturer comment <span
class="math inline">\(Q\)</span> diffère de <span
class="math inline">\(P\)</span>, en tenant compte des probabilités
relatives.</p>
<p>La divergence <span class="math inline">\(\alpha\)</span>-divergence
est une mesure qui généralise cette idée en introduisant un paramètre
<span class="math inline">\(\alpha\)</span> qui contrôle la sensibilité
de la mesure aux différences entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Pour différentes valeurs de <span
class="math inline">\(\alpha\)</span>, nous obtenons des mesures de
divergence distinctes, chacune mettant l’accent sur différents aspects
des distributions.</p>
<div class="definition">
<p>Soit <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur le même espace. La divergence <span
class="math inline">\(\alpha\)</span>-divergence entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_{\alpha}(P \parallel Q) = \begin{cases}
\frac{1}{\alpha(1-\alpha)} \left( \int p^{\alpha} q^{1-\alpha} \, d\mu -
1 \right), &amp; \text{si } \alpha \in (0,1) \\
\int p \log \left( \frac{p}{q} \right) \, d\mu, &amp; \text{si } \alpha
= 1 \\
\int q^{1-\alpha} p^{\alpha} \, d\mu, &amp; \text{si } \alpha &lt; 0
\end{cases}\]</span> où <span class="math inline">\(\mu\)</span> est une
mesure de référence, et <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement par rapport à <span
class="math inline">\(\mu\)</span>.</p>
</div>
<p>Cette définition peut être réécrite de plusieurs manières, en
fonction des valeurs de <span class="math inline">\(\alpha\)</span>. Par
exemple, pour <span class="math inline">\(\alpha = 0\)</span>, nous
avons : <span class="math display">\[D_{0}(P \parallel Q) = -2 \log
\left( \int \sqrt{pq} \, d\mu \right)\]</span> qui est la divergence de
Hellinger.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant quelques théorèmes clés concernant la
divergence <span class="math inline">\(\alpha\)</span>-divergence.</p>
<div class="theorem">
<p>Pour tout <span class="math inline">\(\alpha \in (0,1)\)</span>, la
divergence <span class="math inline">\(\alpha\)</span>-divergence <span
class="math inline">\(D_{\alpha}(P \parallel Q)\)</span> est convexe en
<span class="math inline">\(Q\)</span> pour <span
class="math inline">\(P\)</span> fixé.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, nous utilisons le
fait que la fonction <span class="math inline">\(f(x) =
x^{\alpha}\)</span> est convexe pour <span class="math inline">\(\alpha
\in (0,1)\)</span>. Par conséquent, la divergence <span
class="math inline">\(\alpha\)</span>-divergence peut être vue comme une
combinaison convexe des densités <span class="math inline">\(p\)</span>
et <span class="math inline">\(q\)</span>, ce qui implique sa
convexité. ◻</p>
</div>
<div class="theorem">
<p>Pour <span class="math inline">\(\alpha = 1\)</span>, la divergence
<span class="math inline">\(\alpha\)</span>-divergence se réduit à la
divergence de Kullback-Leibler : <span class="math display">\[D_{1}(P
\parallel Q) = \int p \log \left( \frac{p}{q} \right) \,
d\mu\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En prenant la limite de <span
class="math inline">\(D_{\alpha}(P \parallel Q)\)</span> lorsque <span
class="math inline">\(\alpha\)</span> tend vers 1, nous obtenons : <span
class="math display">\[\lim_{\alpha \to 1} D_{\alpha}(P \parallel Q) =
\int p \log \left( \frac{p}{q} \right) \, d\mu\]</span> ce qui est
précisément la divergence de Kullback-Leibler. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Nous fournissons maintenant des preuves détaillées pour les théorèmes
présentés.</p>
<div class="proof">
<p><em>Preuve du Théorème 1.</em> Pour démontrer la convexité de <span
class="math inline">\(D_{\alpha}(P \parallel Q)\)</span> en <span
class="math inline">\(Q\)</span> pour <span
class="math inline">\(P\)</span> fixé, nous considérons deux
distributions de probabilité <span class="math inline">\(Q_1\)</span> et
<span class="math inline">\(Q_2\)</span>, et nous définissons <span
class="math inline">\(Q_{\lambda} = \lambda Q_1 + (1-\lambda)
Q_2\)</span> pour <span class="math inline">\(\lambda \in
[0,1]\)</span>.</p>
<p>Nous devons montrer que : <span class="math display">\[D_{\alpha}(P
\parallel Q_{\lambda}) \leq \lambda D_{\alpha}(P \parallel Q_1) +
(1-\lambda) D_{\alpha}(P \parallel Q_2)\]</span></p>
<p>En utilisant la définition de <span
class="math inline">\(D_{\alpha}(P \parallel Q)\)</span>, nous avons :
<span class="math display">\[D_{\alpha}(P \parallel Q_{\lambda}) =
\frac{1}{\alpha(1-\alpha)} \left( \int p^{\alpha} q_{\lambda}^{1-\alpha}
\, d\mu - 1 \right)\]</span></p>
<p>En développant <span
class="math inline">\(q_{\lambda}^{1-\alpha}\)</span> et en utilisant
l’inégalité de Jensen, nous obtenons : <span
class="math display">\[q_{\lambda}^{1-\alpha} = (\lambda q_1 +
(1-\lambda) q_2)^{1-\alpha} \leq \lambda q_1^{1-\alpha} + (1-\lambda)
q_2^{1-\alpha}\]</span></p>
<p>En intégrant cette inégalité, nous obtenons : <span
class="math display">\[\int p^{\alpha} q_{\lambda}^{1-\alpha} \, d\mu
\leq \lambda \int p^{\alpha} q_1^{1-\alpha} \, d\mu + (1-\lambda) \int
p^{\alpha} q_2^{1-\alpha} \, d\mu\]</span></p>
<p>En multipliant par <span
class="math inline">\(\frac{1}{\alpha(1-\alpha)}\)</span> et en
soustrayant 1, nous obtenons la convexité souhaitée. ◻</p>
</div>
<div class="proof">
<p><em>Preuve du Théorème 2.</em> Pour démontrer que <span
class="math inline">\(D_{1}(P \parallel Q)\)</span> est la divergence de
Kullback-Leibler, nous utilisons la définition de <span
class="math inline">\(D_{\alpha}(P \parallel Q)\)</span> pour <span
class="math inline">\(\alpha = 1\)</span> : <span
class="math display">\[D_{1}(P \parallel Q) = \int p \log \left(
\frac{p}{q} \right) \, d\mu\]</span></p>
<p>Cette expression est précisément la divergence de Kullback-Leibler
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Ainsi, le théorème est démontré. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés et corollaires importants
de la divergence <span
class="math inline">\(\alpha\)</span>-divergence.</p>
<div class="proposition">
<p>La divergence <span class="math inline">\(\alpha\)</span>-divergence
satisfait les propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(D_{\alpha}(P \parallel Q) \geq
0\)</span> pour tout <span class="math inline">\(\alpha \in
\mathbb{R}\)</span> et <span class="math inline">\(P, Q\)</span>
distributions de probabilité.</p></li>
<li><p><span class="math inline">\(D_{\alpha}(P \parallel Q) =
0\)</span> si et seulement si <span class="math inline">\(P =
Q\)</span>.</p></li>
<li><p>Pour <span class="math inline">\(\alpha \in (0,1)\)</span>, la
divergence <span class="math inline">\(\alpha\)</span>-divergence est
symétrique, c’est-à-dire <span class="math inline">\(D_{\alpha}(P
\parallel Q) = D_{\alpha}(Q \parallel P)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Preuve de la Proposition.</em></p>
<ol>
<li><p>La non-négativité de <span class="math inline">\(D_{\alpha}(P
\parallel Q)\)</span> découle du fait que la fonction <span
class="math inline">\(f(x) = x^{\alpha}\)</span> est positive pour <span
class="math inline">\(\alpha \in (0,1)\)</span> et que l’intégrale d’une
fonction positive est également positive.</p></li>
<li><p>Si <span class="math inline">\(P = Q\)</span>, alors <span
class="math inline">\(D_{\alpha}(P \parallel Q) = 0\)</span> par
définition. Réciproquement, si <span class="math inline">\(D_{\alpha}(P
\parallel Q) = 0\)</span>, alors <span class="math inline">\(\int
p^{\alpha} q^{1-\alpha} \, d\mu = 1\)</span>. En utilisant l’inégalité
de Cauchy-Schwarz, nous obtenons <span class="math inline">\(P =
Q\)</span>.</p></li>
<li><p>Pour <span class="math inline">\(\alpha \in (0,1)\)</span>, la
divergence <span class="math inline">\(\alpha\)</span>-divergence est
symétrique car : <span class="math display">\[D_{\alpha}(P \parallel Q)
= D_{1-\alpha}(Q \parallel P)\]</span> ce qui peut être vérifié en
utilisant la définition de <span class="math inline">\(D_{\alpha}(P
\parallel Q)\)</span>.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons introduit la divergence <span
class="math inline">\(\alpha\)</span>-divergence, une mesure générale de
divergence entre distributions de probabilité. Nous avons présenté ses
définitions formelles, ses propriétés théoriques et ses applications
pratiques. Les preuves détaillées des théorèmes clés ont été fournies,
mettant en lumière la richesse et la flexibilité de cette mesure.</p>
<p>La divergence <span class="math inline">\(\alpha\)</span>-divergence
trouve des applications dans divers domaines, notamment l’apprentissage
automatique, la théorie des codes et les systèmes de communication. Sa
capacité à capturer différents aspects des distributions de probabilité
en fonction du paramètre <span class="math inline">\(\alpha\)</span> en
fait un outil puissant et polyvalent.</p>
<p>Des recherches futures pourraient explorer des extensions de la
divergence <span class="math inline">\(\alpha\)</span>-divergence à
d’autres contextes, telles que les distributions de probabilité sur des
espaces métriques ou les distributions de probabilité dépendant du
temps. De plus, l’étude des propriétés asymptotiques et des applications
pratiques dans des domaines émergents pourrait ouvrir de nouvelles
perspectives pour cette mesure.</p>
</body>
</html>
{% include "footer.html" %}


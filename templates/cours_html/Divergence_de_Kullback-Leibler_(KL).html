{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Kullback-Leibler : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Kullback-Leibler : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Kullback-Leibler (KL), également connue sous le nom
d’entropie relative, est une mesure fondamentale en théorie de
l’information et en statistiques. Introduite par Solomon Kullback et
Richard Leibler en 1951, elle quantifie la différence entre deux
distributions de probabilité. Cette notion est cruciale dans divers
domaines, notamment l’apprentissage automatique, la théorie des codes et
les systèmes de communication.</p>
<p>L’émergence de la divergence KL répond à un besoin essentiel :
comparer deux distributions de probabilité de manière asymétrique.
Contrairement aux distances classiques, la divergence KL n’est pas
symétrique et ne satisfait pas l’inégalité triangulaire. Elle mesure
essentiellement la quantité d’information perdue lorsqu’une distribution
est utilisée pour approximer une autre.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Kullback-Leibler, considérons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> définies sur un espace mesurable
<span class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une
mesure qui évalue à quel point <span class="math inline">\(Q\)</span>
s’écarte de <span class="math inline">\(P\)</span>.</p>
<p>La divergence de Kullback-Leibler est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>. La
divergence de Kullback-Leibler de <span class="math inline">\(Q\)</span>
par rapport à <span class="math inline">\(P\)</span> est donnée par :
<span class="math display">\[D_{\text{KL}}(P \| Q) = \sum_{x \in
\mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)\]</span> si <span
class="math inline">\(\mathcal{X}\)</span> est discret, ou <span
class="math display">\[D_{\text{KL}}(P \| Q) = \int_{\mathcal{X}} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) \, dx\]</span> si <span
class="math inline">\(\mathcal{X}\)</span> est continu.</p>
</div>
<p>Remarquons que la divergence KL n’est pas une distance au sens
mathématique classique, car elle n’est ni symétrique ni ne satisfait
l’inégalité triangulaire.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence KL est le théorème de
l’information de Gibbs, qui établit une relation entre l’entropie et la
divergence KL.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>.
Alors, <span class="math display">\[D_{\text{KL}}(P \| Q) = H(P, Q) -
H(P)\]</span> où <span class="math inline">\(H(P, Q)\)</span> est
l’entropie croisée de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définie par <span
class="math display">\[H(P, Q) = -\sum_{x \in \mathcal{X}} P(x)
\log(Q(x))\]</span> si <span class="math inline">\(\mathcal{X}\)</span>
est discret, ou <span class="math display">\[H(P, Q) =
-\int_{\mathcal{X}} P(x) \log(Q(x)) \, dx\]</span> si <span
class="math inline">\(\mathcal{X}\)</span> est continu, et <span
class="math inline">\(H(P)\)</span> est l’entropie de <span
class="math inline">\(P\)</span>, définie par <span
class="math display">\[H(P) = -\sum_{x \in \mathcal{X}} P(x)
\log(P(x))\]</span> si <span class="math inline">\(\mathcal{X}\)</span>
est discret, ou <span class="math display">\[H(P) = -\int_{\mathcal{X}}
P(x) \log(P(x)) \, dx\]</span> si <span
class="math inline">\(\mathcal{X}\)</span> est continu.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’information de Gibbs, nous procédons
comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Par définition de la divergence KL et de l’entropie
croisée, nous avons <span class="math display">\[D_{\text{KL}}(P \| Q) =
\sum_{x \in \mathcal{X}} P(x)
\log\left(\frac{P(x)}{Q(x)}\right).\]</span> En développant le
logarithme, nous obtenons <span class="math display">\[D_{\text{KL}}(P
\| Q) = \sum_{x \in \mathcal{X}} P(x) \log(P(x)) - \sum_{x \in
\mathcal{X}} P(x) \log(Q(x)).\]</span> En reconnaissant les définitions
de <span class="math inline">\(H(P)\)</span> et <span
class="math inline">\(H(P, Q)\)</span>, nous avons <span
class="math display">\[D_{\text{KL}}(P \| Q) = -H(P) - (-H(P, Q)) = H(P,
Q) - H(P).\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Kullback-Leibler possède plusieurs propriétés
importantes :</p>
<ol>
<li><p><strong>Non-négativité</strong> : <span
class="math inline">\(D_{\text{KL}}(P \| Q) \geq 0\)</span>, avec
égalité si et seulement si <span class="math inline">\(P =
Q\)</span>.</p></li>
<li><p><strong>Inégalité de Gibbs</strong> : <span
class="math inline">\(D_{\text{KL}}(P \| Q) \leq -\log\left(\sum_{x \in
\mathcal{X}} \sqrt{P(x) Q(x)}\right)\)</span>.</p></li>
<li><p><strong>Inégalité de Pinsker</strong> : <span
class="math inline">\(D_{\text{KL}}(P \| Q) \geq \frac{1}{2} \| P - Q
\|_1^2\)</span>, où <span class="math inline">\(\| P - Q \|_1\)</span>
est la distance totale variationnelle entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p></li>
</ol>
<p>Pour prouver l’inégalité de Gibbs, nous utilisons la convexité de la
fonction logarithme :</p>
<div class="proof">
<p><em>Proof.</em> Par la convexité du logarithme, nous avons <span
class="math display">\[\log\left(\sum_{x \in \mathcal{X}} P(x)
Q(x)\right) \geq \sum_{x \in \mathcal{X}} P(x) \log(Q(x)).\]</span> En
prenant l’exponentielle des deux côtés, nous obtenons <span
class="math display">\[\sum_{x \in \mathcal{X}} P(x) Q(x) \geq
\exp\left(\sum_{x \in \mathcal{X}} P(x) \log(Q(x))\right).\]</span> En
utilisant l’inégalité de Cauchy-Schwarz, nous avons <span
class="math display">\[\sum_{x \in \mathcal{X}} P(x) Q(x) \leq
\sqrt{\sum_{x \in \mathcal{X}} P(x)^2} \sqrt{\sum_{x \in \mathcal{X}}
Q(x)^2}.\]</span> En combinant ces résultats, nous obtenons l’inégalité
de Gibbs. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Kullback-Leibler est un outil puissant pour comparer
des distributions de probabilité. Ses applications sont vastes, allant
de l’apprentissage automatique à la théorie des codes. Comprendre ses
propriétés et ses théorèmes associés est essentiel pour toute personne
travaillant dans ces domaines.</p>
</body>
</html>
{% include "footer.html" %}


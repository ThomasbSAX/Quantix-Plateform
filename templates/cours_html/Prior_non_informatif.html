{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Prior non informatif : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Prior non informatif : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’idée de prior non informatif émerge dans le cadre des statistiques
bayésiennes, où l’on cherche à incorporer les informations a priori
disponibles sur un paramètre inconnu. L’objectif est de construire une
distribution de probabilité qui reflète l’état d’information minimal,
c’est-à-dire une distribution qui n’introduit pas de biais ou
d’informations supplémentaires non justifiées.</p>
<p>Les priors non informatifs sont indispensables dans les situations où
l’on dispose de peu d’informations a priori sur le paramètre à estimer.
Ils permettent de laisser les données parler d’elles-mêmes, en
minimisant l’influence des choix subjectifs de l’analyste. Cette
approche est particulièrement utile dans les domaines où les données
sont rares ou coûteuses à obtenir.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre ce qu’est un prior non informatif, commençons par
nous demander comment représenter l’absence d’information.
Intuitivement, cela signifie que toutes les valeurs possibles du
paramètre sont également probables a priori.</p>
<p>Formellement, un prior non informatif est une distribution de
probabilité qui est invariante sous certaines transformations. Par
exemple, pour un paramètre scalaire <span
class="math inline">\(\theta\)</span>, une distribution uniforme sur un
intervalle <span class="math inline">\(I\)</span> est souvent utilisée
comme prior non informatif, si aucune information particulière n’est
disponible sur <span class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\Theta\)</span> l’espace des
paramètres et <span class="math inline">\(P(\theta)\)</span> une
distribution de probabilité sur <span
class="math inline">\(\Theta\)</span>. On dit que <span
class="math inline">\(P\)</span> est un prior non informatif si pour
tout <span class="math inline">\(\theta \in \Theta\)</span>, <span
class="math inline">\(P(\theta)\)</span> est constante ou
proportionnelle à une fonction qui ne favorise aucune valeur
particulière de <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>En termes plus formels, un prior non informatif peut être défini
comme une distribution de probabilité <span
class="math inline">\(P\)</span> telle que pour tout <span
class="math inline">\(\theta \in \Theta\)</span>, il existe une
constante <span class="math inline">\(c &gt; 0\)</span> telle que :
<span class="math display">\[P(\theta) = c\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans le contexte des priors non informatifs
est celui de l’invariance sous transformation. Ce théorème stipule que
si un prior est non informatif, il doit rester non informatif sous toute
transformation admissible.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\Theta\)</span> un espace de
paramètres et <span class="math inline">\(P(\theta)\)</span> un prior
non informatif sur <span class="math inline">\(\Theta\)</span>. Si <span
class="math inline">\(\phi: \Theta \rightarrow \Theta&#39;\)</span> est
une transformation bijective et différentiable, alors le prior
transformé <span class="math inline">\(P&#39;(\phi(\theta))\)</span> est
également non informatif.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous commençons par
exprimer le prior transformé en termes du prior original. Par la règle
de transformation des variables, nous avons : <span
class="math display">\[P&#39;(\phi(\theta)) = P(\theta) \left|
\frac{d\theta}{d\phi(\theta)} \right|\]</span></p>
<p>Puisque <span class="math inline">\(P(\theta)\)</span> est constant,
disons <span class="math inline">\(P(\theta) = c\)</span>, nous avons :
<span class="math display">\[P&#39;(\phi(\theta)) = c \left|
\frac{d\theta}{d\phi(\theta)} \right|\]</span></p>
<p>Pour que <span class="math inline">\(P&#39;\)</span> soit également
non informatif, il doit être constant. Cela implique que <span
class="math inline">\(\left| \frac{d\theta}{d\phi(\theta)}
\right|\)</span> doit être constant. Or, cela est vrai si et seulement
si <span class="math inline">\(\phi\)</span> est une transformation
linéaire.</p>
<p>Ainsi, sous l’hypothèse que <span class="math inline">\(\phi\)</span>
est une transformation linéaire, <span
class="math inline">\(P&#39;\)</span> est constant et donc non
informatif. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Considérons un exemple simple pour illustrer la construction d’un
prior non informatif. Supposons que nous voulons estimer un paramètre
<span class="math inline">\(\theta\)</span> qui représente une
probabilité, c’est-à-dire <span class="math inline">\(\theta \in [0,
1]\)</span>.</p>
<p>Un prior non informatif pour <span
class="math inline">\(\theta\)</span> est souvent choisi comme une
distribution uniforme sur <span class="math inline">\([0, 1]\)</span>.
Cela signifie que pour tout <span class="math inline">\(\theta \in [0,
1]\)</span>, la densité de probabilité est constante.</p>
<div class="proof">
<p><em>Proof.</em> Pour montrer que cette distribution est non
informative, nous devons vérifier qu’elle ne favorise aucune valeur
particulière de <span class="math inline">\(\theta\)</span>. La densité
de probabilité d’une distribution uniforme sur <span
class="math inline">\([0, 1]\)</span> est donnée par : <span
class="math display">\[P(\theta) = \begin{cases}
1 &amp; \text{si } \theta \in [0, 1] \\
0 &amp; \text{sinon}
\end{cases}\]</span></p>
<p>Cette densité est constante sur l’intervalle <span
class="math inline">\([0, 1]\)</span>, ce qui signifie qu’elle ne
favorise aucune valeur particulière de <span
class="math inline">\(\theta\)</span>. Par conséquent, elle est non
informative. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Les priors non informatifs possèdent plusieurs propriétés
intéressantes, que nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Invariance sous transformation linéaire</strong> : Si un
prior est non informatif, il reste non informatif sous toute
transformation linéaire.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(P(\theta)\)</span>
un prior non informatif et <span class="math inline">\(\phi(\theta) =
a\theta + b\)</span> une transformation linéaire. Le prior transformé
est donné par : <span class="math display">\[P&#39;(\phi(\theta)) =
P(\theta) \left| \frac{d\theta}{d\phi(\theta)} \right| = c \cdot
\frac{1}{|a|}\]</span></p>
<p>Puisque <span class="math inline">\(c\)</span> et <span
class="math inline">\(\frac{1}{|a|}\)</span> sont des constantes, <span
class="math inline">\(P&#39;\)</span> est également constant et donc non
informatif. ◻</p>
</div></li>
<li><p><strong>Unicité du prior non informatif</strong> : Dans certains
cas, le prior non informatif est unique à une constante multiplicative
près.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un paramètre <span
class="math inline">\(\theta\)</span> avec une distribution uniforme sur
un intervalle <span class="math inline">\(I\)</span>. La densité de
probabilité est donnée par : <span class="math display">\[P(\theta) =
\frac{1}{|I|}\]</span></p>
<p>Cette densité est unique à une constante multiplicative près, car
toute autre distribution uniforme sur <span
class="math inline">\(I\)</span> serait proportionnelle à <span
class="math inline">\(\frac{1}{|I|}\)</span>. ◻</p>
</div></li>
<li><p><strong>Consistance avec les méthodes classiques</strong> : Les
priors non informatifs conduisent souvent à des résultats cohérents avec
les méthodes statistiques classiques, telles que le maximum de
vraisemblance.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un modèle statistique avec un paramètre
<span class="math inline">\(\theta\)</span> et une fonction de
vraisemblance <span class="math inline">\(L(\theta)\)</span>. Si nous
choisissons un prior non informatif, l’estimateur bayésien de <span
class="math inline">\(\theta\)</span> coïncide souvent avec l’estimateur
du maximum de vraisemblance.</p>
<p>Cela est dû au fait que le prior non informatif n’introduit pas de
biais dans l’estimation, permettant aux données de dominer le processus
d’inférence. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Les priors non informatifs jouent un rôle crucial dans les
statistiques bayésiennes, permettant de minimiser l’influence des choix
subjectifs de l’analyste. Ils sont particulièrement utiles dans les
situations où l’information a priori est limitée ou inexistante.</p>
<p>En conclusion, les priors non informatifs offrent une approche
rigoureuse et cohérente pour l’inférence statistique, en garantissant
que les résultats sont principalement déterminés par les données
disponibles.</p>
</body>
</html>
{% include "footer.html" %}


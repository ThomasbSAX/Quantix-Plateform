{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Sharma-Mittal : Une Généralisation Puissante</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Sharma-Mittal : Une Généralisation
Puissante</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie, concept fondamental en théorie de l’information et en
physique statistique, mesure le degré d’incertitude ou d’imprévisibilité
d’un système. Introduite par Claude Shannon en 1948, l’entropie de
Shannon a révolutionné notre compréhension des systèmes complexes.
Cependant, son caractère additif et sa sensibilité aux événements rares
ont motivé la recherche d’alternatives plus flexibles.</p>
<p>Parmi ces alternatives, l’entropie de Sharma-Mittal, introduite par
P. C. Sharma et B. L. Mittal en 1975, se distingue par sa généralisation
paramétrique de l’entropie de Shannon et de l’entropie de Rényi. Cette
généralisation permet de moduler la sensibilité de l’entropie aux
événements rares, offrant ainsi une palette d’outils adaptés à diverses
applications.</p>
<p>L’entropie de Sharma-Mittal trouve des applications dans des domaines
variés, allant de la théorie de l’information à la physique statistique,
en passant par le traitement du signal et l’apprentissage automatique.
Sa flexibilité en fait un outil précieux pour l’analyse des systèmes
complexes et non linéaires.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Sharma-Mittal, commençons par rappeler
quelques notions fondamentales. Considérons un système décrit par une
distribution de probabilité discrète <span class="math inline">\(p =
(p_1, p_2, \ldots, p_n)\)</span>, où <span class="math inline">\(p_i
\geq 0\)</span> et <span class="math inline">\(\sum_{i=1}^n p_i =
1\)</span>.</p>
<p>L’entropie de Shannon <span class="math inline">\(H(p)\)</span> est
définie comme : <span class="math display">\[H(p) = -\sum_{i=1}^n p_i
\log p_i\]</span></p>
<p>L’entropie de Rényi <span class="math inline">\(H_\alpha(p)\)</span>,
pour un paramètre <span class="math inline">\(\alpha &gt; 0\)</span> et
<span class="math inline">\(\alpha \neq 1\)</span>, est définie par :
<span class="math display">\[H_\alpha(p) = \frac{1}{1 - \alpha} \log
\left( \sum_{i=1}^n p_i^\alpha \right)\]</span></p>
<p>L’entropie de Sharma-Mittal généralise ces deux notions en
introduisant un second paramètre <span class="math inline">\(r\)</span>.
Avant de donner la définition formelle, cherchons à comprendre ce que
nous voulons capturer. Nous souhaitons une mesure d’entropie qui puisse
moduler sa sensibilité aux événements rares en fonction de deux
paramètres, permettant ainsi une adaptation fine à différentes
situations.</p>
<p>La définition formelle de l’entropie de Sharma-Mittal est la suivante
:</p>
<div class="definition">
<p>Soit <span class="math inline">\(p = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité discrète et <span
class="math inline">\(\alpha, r &gt; 0\)</span>. L’entropie de
Sharma-Mittal est définie par : <span
class="math display">\[S_{\alpha,r}(p) = \frac{1}{r - 1} \left( 1 -
\sum_{i=1}^n p_i^\alpha \right) \quad \text{si } r &gt; 0, r \neq
1\]</span> <span class="math display">\[S_{\alpha,r}(p) = -\lim_{r \to
1} \frac{1}{r - 1} \left( 1 - \sum_{i=1}^n p_i^\alpha \right) \quad
\text{si } r = 1\]</span></p>
</div>
<p>Remarquons que lorsque <span class="math inline">\(r =
\alpha\)</span>, l’entropie de Sharma-Mittal coïncide avec l’entropie de
Rényi. De plus, lorsque <span class="math inline">\(\alpha = r =
1\)</span>, elle coïncide avec l’entropie de Shannon.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons quelques théorèmes fondamentaux
concernant l’entropie de Sharma-Mittal. Ces théorèmes illustrent les
propriétés remarquables de cette mesure d’entropie.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(p = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité discrète et <span
class="math inline">\(\alpha, r &gt; 0\)</span>. Alors : <span
class="math display">\[S_{\alpha,r}(p) \leq \frac{1}{r - 1} \left( 1 -
\sum_{i=1}^n p_i^\alpha \right)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur l’inégalité de
Jensen et les propriétés des fonctions convexes. Considérons la fonction
<span class="math inline">\(f(x) = x^\alpha\)</span>, qui est convexe
pour <span class="math inline">\(\alpha &gt; 1\)</span>. Par l’inégalité
de Jensen, nous avons : <span class="math display">\[\sum_{i=1}^n
p_i^\alpha \geq \left( \sum_{i=1}^n p_i \right)^\alpha = 1\]</span></p>
<p>Ainsi, nous obtenons : <span class="math display">\[S_{\alpha,r}(p)
\leq \frac{1}{r - 1} \left( 1 - \sum_{i=1}^n p_i^\alpha
\right)\]</span> ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(p = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité discrète et <span
class="math inline">\(\alpha, r &gt; 0\)</span>. Alors : <span
class="math display">\[\lim_{r \to 1} S_{\alpha,r}(p) =
H_\alpha(p)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la définition de
l’entropie de Sharma-Mittal et les propriétés des limites. Considérons
la définition de <span class="math inline">\(S_{\alpha,r}(p)\)</span>
lorsque <span class="math inline">\(r \to 1\)</span> : <span
class="math display">\[S_{\alpha,r}(p) = -\lim_{r \to 1} \frac{1}{r - 1}
\left( 1 - \sum_{i=1}^n p_i^\alpha \right)\]</span></p>
<p>En utilisant la définition de l’entropie de Rényi <span
class="math inline">\(H_\alpha(p)\)</span>, nous avons : <span
class="math display">\[\lim_{r \to 1} S_{\alpha,r}(p) =
H_\alpha(p)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés importantes
de l’entropie de Sharma-Mittal.</p>
<div class="corollaire">
<p>Soit <span class="math inline">\(p = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité discrète et <span
class="math inline">\(\alpha, r &gt; 0\)</span>. Alors : <span
class="math display">\[S_{\alpha,r}(p) = 0 \quad \text{si et seulement
si } p_i = 1 \text{ pour un certain } i\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire repose sur la définition
de l’entropie de Sharma-Mittal. Si <span class="math inline">\(p_i =
1\)</span> pour un certain <span class="math inline">\(i\)</span>, alors
tous les autres <span class="math inline">\(p_j = 0\)</span>. Ainsi :
<span class="math display">\[S_{\alpha,r}(p) = \frac{1}{r - 1} \left( 1
- 1^\alpha \right) = 0\]</span></p>
<p>Réciproquement, si <span class="math inline">\(S_{\alpha,r}(p) =
0\)</span>, alors : <span class="math display">\[\frac{1}{r - 1} \left(
1 - \sum_{i=1}^n p_i^\alpha \right) = 0\]</span></p>
<p>Ce qui implique que <span class="math inline">\(\sum_{i=1}^n
p_i^\alpha = 1\)</span>. Comme <span class="math inline">\(p_i \geq
0\)</span> et <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>,
cela signifie que <span class="math inline">\(p_i = 1\)</span> pour un
certain <span class="math inline">\(i\)</span>. ◻</p>
</div>
<div class="corollaire">
<p>Soit <span class="math inline">\(p = (p_1, p_2, \ldots, p_n)\)</span>
et <span class="math inline">\(q = (q_1, q_2, \ldots, q_n)\)</span> deux
distributions de probabilité discrètes et <span
class="math inline">\(\alpha, r &gt; 0\)</span>. Alors : <span
class="math display">\[S_{\alpha,r}(\lambda p + (1 - \lambda) q) \leq
\lambda S_{\alpha,r}(p) + (1 - \lambda) S_{\alpha,r}(q)\]</span> pour
tout <span class="math inline">\(\lambda \in [0, 1]\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire repose sur la convexité de
l’entropie de Sharma-Mittal. Considérons la fonction <span
class="math inline">\(f(x) = x^\alpha\)</span>, qui est convexe pour
<span class="math inline">\(\alpha &gt; 1\)</span>. Par l’inégalité de
Jensen, nous avons : <span class="math display">\[\sum_{i=1}^n (\lambda
p_i + (1 - \lambda) q_i)^\alpha \leq \lambda \sum_{i=1}^n p_i^\alpha +
(1 - \lambda) \sum_{i=1}^n q_i^\alpha\]</span></p>
<p>Ainsi, nous obtenons : <span
class="math display">\[S_{\alpha,r}(\lambda p + (1 - \lambda) q) \leq
\lambda S_{\alpha,r}(p) + (1 - \lambda) S_{\alpha,r}(q)\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie de Sharma-Mittal offre une généralisation puissante et
flexible des mesures d’entropie classiques. Sa capacité à moduler la
sensibilité aux événements rares en fonction de deux paramètres en fait
un outil précieux pour l’analyse des systèmes complexes. Les théorèmes
et propriétés présentés dans cet article illustrent les richesses de
cette mesure d’entropie et ouvrent la voie à de nombreuses applications
futures.</p>
</body>
</html>
{% include "footer.html" %}


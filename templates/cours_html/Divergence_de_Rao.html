{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Rao : Une mesure d’information pour les distributions statistiques</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Rao : Une mesure d’information pour les
distributions statistiques</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Rao, introduite par Calyampudi Radhakrishna Rao en
1945, est une mesure d’information fondamentale dans la théorie des
probabilités et de l’estimation statistique. Elle émerge comme une
réponse naturelle à la nécessité de quantifier la distance entre deux
distributions de probabilité. Cette notion est indispensable dans des
domaines variés, allant de l’apprentissage automatique à la théorie de
l’information, en passant par la physique statistique.</p>
<p>L’idée sous-jacente est de mesurer à quel point deux distributions
sont proches l’une de l’autre, en termes de leur information mutuelle.
La divergence de Rao est particulièrement utile dans les contextes où
l’on souhaite comparer des modèles statistiques ou évaluer la
performance d’estimateurs.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Rao, commençons par comprendre ce
que nous cherchons à mesurer. Supposons que nous ayons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> définies sur un espace mesurable
<span class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier la distance entre ces deux distributions.</p>
<p>La divergence de Rao est une mesure d’information qui capture cette
distance. Elle est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La divergence de Rao entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_R(P \| Q) = \int_{\Omega} \left(
\sqrt{\frac{dP}{d\mu}} - \sqrt{\frac{dQ}{d\mu}} \right)^2 d\mu\]</span>
où <span class="math inline">\(\mu\)</span> est une mesure de référence
telle que <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient absolument continues par rapport
à <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D_R(P \| Q) = 2 \left( 1 - \int_{\Omega}
\sqrt{\frac{dP}{d\mu} \cdot \frac{dQ}{d\mu}} d\mu \right)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Rao est le théorème de
Cramér-Rao, qui établit une limite inférieure sur la variance d’un
estimateur non biaisé. Ce théorème est crucial en statistique
mathématique et montre l’importance de la divergence de Rao dans la
théorie de l’estimation.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\theta\)</span> un paramètre réel et
<span class="math inline">\(X\)</span> une variable aléatoire de densité
<span class="math inline">\(f(x; \theta)\)</span>. Supposons que
l’estimateur <span class="math inline">\(T(X)\)</span> de <span
class="math inline">\(\theta\)</span> soit non biaisé, c’est-à-dire que
<span class="math inline">\(\mathbb{E}[T(X)] = \theta\)</span>. Alors,
la variance de <span class="math inline">\(T(X)\)</span> satisfait :
<span class="math display">\[\text{Var}(T(X)) \geq
\frac{1}{I(\theta)}\]</span> où <span
class="math inline">\(I(\theta)\)</span> est l’information de Fisher,
définie par : <span class="math display">\[I(\theta) = \mathbb{E}\left[
\left( \frac{\partial}{\partial \theta} \log f(X; \theta) \right)^2
\right]\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Cramér-Rao, nous utilisons la divergence
de Rao et l’information de Fisher. La preuve repose sur des inégalités
de Cauchy-Schwarz et des propriétés de l’information de Fisher.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un estimateur non biaisé <span
class="math inline">\(T(X)\)</span>. Nous voulons montrer que : <span
class="math display">\[\text{Var}(T(X)) \geq
\frac{1}{I(\theta)}\]</span></p>
<p>Par définition de la variance, nous avons : <span
class="math display">\[\text{Var}(T(X)) = \mathbb{E}[(T(X) -
\theta)^2]\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous obtenons : <span
class="math display">\[\mathbb{E}[(T(X) - \theta)^2] \geq
\frac{(\mathbb{E}[(T(X) - \theta) \cdot \phi(X;
\theta)])^2}{\mathbb{E}[\phi(X; \theta)^2]}\]</span> où <span
class="math inline">\(\phi(X; \theta) = \frac{\partial}{\partial \theta}
\log f(X; \theta)\)</span>.</p>
<p>En choisissant <span class="math inline">\(\phi(X; \theta)\)</span>
de manière appropriée, nous pouvons montrer que : <span
class="math display">\[\mathbb{E}[(T(X) - \theta) \cdot \phi(X; \theta)]
= 1\]</span></p>
<p>Ainsi, nous avons : <span class="math display">\[\text{Var}(T(X))
\geq \frac{1}{\mathbb{E}[\phi(X; \theta)^2]} =
\frac{1}{I(\theta)}\]</span></p>
<p>Ce qui termine la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Rao possède plusieurs propriétés intéressantes, que
nous énumérons et prouvons ci-dessous.</p>
<ol>
<li><p>La divergence de Rao est toujours non négative, c’est-à-dire que
<span class="math inline">\(D_R(P \| Q) \geq 0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Par définition, nous avons : <span
class="math display">\[D_R(P \| Q) = \int_{\Omega} \left(
\sqrt{\frac{dP}{d\mu}} - \sqrt{\frac{dQ}{d\mu}} \right)^2 d\mu\]</span>
Comme le carré de toute fonction réelle est non négatif, il suit que
<span class="math inline">\(D_R(P \| Q) \geq 0\)</span>. ◻</p>
</div></li>
<li><p>La divergence de Rao est égale à zéro si et seulement si <span
class="math inline">\(P = Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(D_R(P \| Q)
= 0\)</span>. Alors, nous avons : <span
class="math display">\[\int_{\Omega} \left( \sqrt{\frac{dP}{d\mu}} -
\sqrt{\frac{dQ}{d\mu}} \right)^2 d\mu = 0\]</span> Cela implique que :
<span class="math display">\[\sqrt{\frac{dP}{d\mu}} =
\sqrt{\frac{dQ}{d\mu}} \quad \mu\text{-p.p.}\]</span> En élevant au
carré, nous obtenons : <span class="math display">\[\frac{dP}{d\mu} =
\frac{dQ}{d\mu} \quad \mu\text{-p.p.}\]</span> Ce qui signifie que <span
class="math inline">\(P = Q\)</span>.</p>
<p>Réciproquement, si <span class="math inline">\(P = Q\)</span>, alors
<span class="math inline">\(D_R(P \| Q) = 0\)</span>. ◻</p>
</div></li>
<li><p>La divergence de Rao est invariante sous les transformations
mesurables.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(T : \Omega \to
\Omega&#39;\)</span> une transformation mesurable. Nous voulons montrer
que : <span class="math display">\[D_R(P \| Q) = D_R(T_*P \|
T_*Q)\]</span> où <span class="math inline">\(T_*P\)</span> et <span
class="math inline">\(T_*Q\)</span> sont les mesures poussées en avant
par <span class="math inline">\(T\)</span>.</p>
<p>En utilisant la définition de la divergence de Rao, nous avons :
<span class="math display">\[D_R(T_*P \| T_*Q) = \int_{\Omega&#39;}
\left( \sqrt{\frac{dT_*P}{d\nu}} - \sqrt{\frac{dT_*Q}{d\nu}} \right)^2
d\nu\]</span> où <span class="math inline">\(\nu\)</span> est une mesure
de référence sur <span class="math inline">\(\Omega&#39;\)</span>.</p>
<p>En utilisant le théorème de changement de variables, nous pouvons
montrer que : <span class="math display">\[\sqrt{\frac{dT_*P}{d\nu}} =
\int_{\Omega} \sqrt{\frac{dP}{d\mu}} \cdot \delta_{T(x)}(dy)\]</span> où
<span class="math inline">\(\delta_{T(x)}\)</span> est la mesure de
Dirac en <span class="math inline">\(T(x)\)</span>.</p>
<p>En utilisant cette relation, nous pouvons montrer que : <span
class="math display">\[D_R(T_*P \| T_*Q) = D_R(P \| Q)\]</span></p>
<p>Ce qui termine la preuve. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Rao est une mesure d’information puissante et
polyvalente, avec des applications dans de nombreux domaines des
mathématiques et des sciences. Sa capacité à quantifier la distance
entre deux distributions de probabilité en fait un outil indispensable
dans la théorie des probabilités, l’estimation statistique et
l’apprentissage automatique. Les propriétés et théorèmes associés à la
divergence de Rao continuent d’être explorés, ouvrant la voie à de
nouvelles découvertes et applications.</p>
</body>
</html>
{% include "footer.html" %}


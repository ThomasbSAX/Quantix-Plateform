{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Ergodique : Une Approche Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Ergodique : Une Approche Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie ergodique est un concept fondamental en théorie ergodique,
une branche des mathématiques qui étudie les systèmes dynamiques et leur
comportement asymptotique. L’origine de cette notion remonte aux travaux
pionniers de Kolmogorov dans les années 1950, qui cherchait à quantifier
le degré de complexité ou de désordre dans les systèmes dynamiques.
L’entropie ergodique émerge comme un outil indispensable pour comprendre
la structure fine des systèmes chaotiques et pour distinguer entre les
comportements réguliers et irréguliers.</p>
<p>Dans un cadre plus large, l’entropie ergodique résout des problèmes
cruciaux en physique statistique et en théorie de l’information. Elle
permet de mesurer la quantité d’information produite par un système
dynamique et de caractériser sa complexité. Ce concept est indispensable
pour étudier les propriétés asymptotiques des systèmes dynamiques,
telles que la mélangeance et l’ergodicité.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie ergodique, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons un système dynamique où l’état
futur dépend de manière déterministe de l’état présent. Nous voulons
quantifier la quantité d’information nécessaire pour prédire l’évolution
future du système avec une précision donnée. Cette idée nous mène
naturellement à la notion d’entropie.</p>
<div class="definition">
<p>Soit <span class="math inline">\((X, \mathcal{A}, \mu)\)</span> un
espace de mesure et <span class="math inline">\(T: X \rightarrow
X\)</span> une transformation mesurable. Pour une partition finie <span
class="math inline">\(\alpha = \{A_1, \ldots, A_k\}\)</span> de <span
class="math inline">\(X\)</span>, définissons l’entropie de la partition
<span class="math inline">\(\alpha\)</span> par <span
class="math display">\[h(\alpha) = -\sum_{i=1}^k \mu(A_i) \log
\mu(A_i).\]</span></p>
<p>L’entropie de Kolmogorov-Sinai de la transformation <span
class="math inline">\(T\)</span> est alors définie comme <span
class="math display">\[h(T) = \sup_{\alpha} \lim_{n \rightarrow \infty}
\frac{1}{n} H\left(\bigvee_{i=0}^{n-1} T^{-i}\alpha\right),\]</span> où
<span class="math inline">\(H\)</span> désigne l’entropie de la
partition et <span class="math inline">\(\bigvee_{i=0}^{n-1}
T^{-i}\alpha\)</span> est la partition engendrée par les images inverses
de <span class="math inline">\(\alpha\)</span>.</p>
</div>
<p>Une autre formulation équivalente est la suivante : <span
class="math display">\[h(T) = \sup_{\alpha} \inf_{n \geq 1} \frac{1}{n}
H\left(\bigvee_{i=0}^{n-1} T^{-i}\alpha\right).\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux en théorie ergodique est le théorème de
Kolmogorov-Sinai, qui donne une condition suffisante pour que l’entropie
d’une transformation soit finie.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(T: X \rightarrow X\)</span> une
transformation mesurable ergodique. Si <span
class="math inline">\(\alpha\)</span> est une partition génératrice de
<span class="math inline">\(X\)</span>, alors <span
class="math display">\[h(T) = H\left(\bigvee_{i=0}^{\infty}
T^{-i}\alpha\right).\]</span></p>
</div>
<p>La démonstration de ce théorème repose sur des propriétés
fondamentales de l’entropie et de la mesure. Nous allons maintenant
détailler cette preuve.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Kolmogorov-Sinai, nous commençons par
rappeler quelques propriétés de l’entropie. Soient <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span> deux partitions de <span
class="math inline">\(X\)</span>. On a la subadditivité suivante : <span
class="math display">\[H(\alpha \vee \beta) \leq H(\alpha) +
H(\beta).\]</span></p>
<p>En utilisant cette propriété, nous pouvons montrer que pour toute
partition <span class="math inline">\(\alpha\)</span> et tout entier
<span class="math inline">\(n \geq 1\)</span>, <span
class="math display">\[H\left(\bigvee_{i=0}^{n-1} T^{-i}\alpha\right)
\leq n H(\alpha).\]</span></p>
<p>Ensuite, en utilisant l’ergodicité de <span
class="math inline">\(T\)</span>, nous pouvons montrer que pour toute
partition génératrice <span class="math inline">\(\alpha\)</span>, <span
class="math display">\[\lim_{n \rightarrow \infty} \frac{1}{n}
H\left(\bigvee_{i=0}^{n-1} T^{-i}\alpha\right) =
H\left(\bigvee_{i=0}^{\infty} T^{-i}\alpha\right).\]</span></p>
<p>Enfin, en combinant ces résultats, nous obtenons le théorème de
Kolmogorov-Sinai.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de l’entropie
ergodique.</p>
<ol>
<li><p>Si <span class="math inline">\(T\)</span> est une transformation
mesurable et <span class="math inline">\(\alpha\)</span> est une
partition finie de <span class="math inline">\(X\)</span>, alors <span
class="math display">\[h(T) \leq H(\alpha).\]</span></p></li>
<li><p>Si <span class="math inline">\(T\)</span> est une transformation
mesurable ergodique et <span class="math inline">\(\alpha\)</span> est
une partition génératrice de <span class="math inline">\(X\)</span>,
alors <span class="math display">\[h(T) = H\left(\bigvee_{i=0}^{\infty}
T^{-i}\alpha\right).\]</span></p></li>
<li><p>Si <span class="math inline">\(T\)</span> et <span
class="math inline">\(S\)</span> sont deux transformations mesurables,
alors <span class="math display">\[h(T \times S) = h(T) + h(S),\]</span>
où <span class="math inline">\(T \times S\)</span> est la transformation
produit.</p></li>
</ol>
<p>La preuve de ces propriétés repose sur des techniques avancées de
théorie ergodique et de théorie de la mesure. Nous renvoyons le lecteur
à des ouvrages spécialisés pour plus de détails.</p>
</body>
</html>
{% include "footer.html" %}


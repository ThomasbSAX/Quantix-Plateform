{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par moyenne mobile</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par moyenne mobile</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
puissante en apprentissage automatique, particulièrement utile pour
traiter des données catégorielles. L’une des méthodes les plus efficaces
dans ce domaine est le binning par moyenne mobile, qui permet de
transformer des variables catégorielles en variables numériques tout en
préservant les informations statistiques sous-jacentes.</p>
<p>Cette technique émerge dans un contexte où les données catégorielles
sont omniprésentes, mais difficiles à traiter directement par les
algorithmes d’apprentissage automatique. Le binning par moyenne mobile
offre une solution élégante en convertissant ces données en
caractéristiques numériques, facilitant ainsi leur utilisation dans des
modèles prédictifs.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous cherchons à accomplir. Nous voulons transformer
une variable catégorielle en une variable numérique tout en conservant
les informations statistiques importantes. Pour ce faire, nous allons
diviser les données en intervalles (bins) et calculer la moyenne mobile
pour chaque intervalle.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable numérique
associée. Nous définissons le binning par moyenne mobile comme suit
:</p>
<p>Pour chaque catégorie <span class="math inline">\(c\)</span> de <span
class="math inline">\(X\)</span>, nous calculons la moyenne des valeurs
de <span class="math inline">\(Y\)</span> associées à cette catégorie
:</p>
<p><span class="math display">\[\mu_c = \frac{1}{|Y_c|} \sum_{i \in Y_c}
y_i\]</span></p>
<p>où <span class="math inline">\(Y_c\)</span> est l’ensemble des
valeurs de <span class="math inline">\(Y\)</span> associées à la
catégorie <span class="math inline">\(c\)</span>, et <span
class="math inline">\(|Y_c|\)</span> est le nombre d’éléments dans cet
ensemble.</p>
<p>Ensuite, nous remplaçons chaque occurrence de la catégorie <span
class="math inline">\(c\)</span> par la moyenne calculée <span
class="math inline">\(\mu_c\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’efficacité de cette méthode, nous devons examiner
certains théorèmes clés. L’un des plus importants est le théorème de la
moyenne mobile, qui garantit que cette technique préserve les
informations statistiques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable numérique
associée. Si nous appliquons le binning par moyenne mobile à <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, alors la nouvelle variable numérique
<span class="math inline">\(Z\)</span> obtenue conserve les informations
statistiques suivantes :</p>
<p><span class="math display">\[\mathbb{E}[Y] =
\mathbb{E}[Z]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}\)</span> désigne
l’espérance mathématique.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous devons montrer que l’espérance de
<span class="math inline">\(Y\)</span> est égale à l’espérance de <span
class="math inline">\(Z\)</span>. Commençons par rappeler que :</p>
<p><span class="math display">\[\mathbb{E}[Y] = \sum_{c} P(c)
\mu_c\]</span></p>
<p>où <span class="math inline">\(P(c)\)</span> est la probabilité de la
catégorie <span class="math inline">\(c\)</span>.</p>
<p>D’autre part, la nouvelle variable <span
class="math inline">\(Z\)</span> est définie comme :</p>
<p><span class="math display">\[Z = \mu_c \quad \text{si} \quad X =
c\]</span></p>
<p>Donc, l’espérance de <span class="math inline">\(Z\)</span> est :</p>
<p><span class="math display">\[\mathbb{E}[Z] = \sum_{c} P(c)
\mu_c\]</span></p>
<p>Ainsi, nous avons :</p>
<p><span class="math display">\[\mathbb{E}[Y] =
\mathbb{E}[Z]\]</span></p>
<p>Ce qui prouve le théorème.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le binning par moyenne mobile possède plusieurs propriétés
intéressantes qui en font une technique puissante pour l’encodage des
données catégorielles.</p>
<ol>
<li><p>Conservation de l’espérance : Comme nous l’avons vu dans le
théorème précédent, le binning par moyenne mobile conserve l’espérance
de la variable numérique associée.</p></li>
<li><p>Réduction de la dimensionnalité : En convertissant les variables
catégorielles en variables numériques, cette technique réduit la
dimensionnalité des données, facilitant ainsi leur traitement par les
algorithmes d’apprentissage automatique.</p></li>
<li><p>Préservation des informations statistiques : Le binning par
moyenne mobile préserve les informations statistiques importantes,
telles que la variance et la covariance, entre les variables.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par moyenne
mobile est une technique puissante et efficace pour traiter les données
catégorielles. En convertissant ces données en variables numériques tout
en préservant les informations statistiques sous-jacentes, cette méthode
facilite leur utilisation dans des modèles prédictifs. Les théorèmes et
propriétés présentés dans cet article montrent que cette technique est
non seulement intuitive, mais aussi rigoureusement fondée sur des
principes mathématiques solides.</p>
</body>
</html>
{% include "footer.html" %}


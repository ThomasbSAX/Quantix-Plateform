{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Wasserstein : Un Pont entre la Théorie des Probabilités et l’Analyse Optimale</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Wasserstein : Un Pont entre la Théorie des
Probabilités et l’Analyse Optimale</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Wasserstein, également connue sous le nom de distance
de Kantorovich-Rubinstein, émerge comme un concept fondamental à
l’intersection de la théorie des probabilités et de l’analyse optimale.
Son origine historique remonte aux travaux pionniers de Leonid
Kantorovich dans les années 1940, qui introduisit cette notion pour
résoudre des problèmes de transport optimal. Plus tard, Wassily
Hoeffding et Walter M. Rosenblatt l’ont reprise dans le contexte des
statistiques, tandis que Vladimir I. Rubinstein l’a popularisée en
théorie de la décision.</p>
<p>Pourquoi cette notion est-elle indispensable ? La distance de
Wasserstein permet de mesurer la dissimilarité entre deux distributions
de probabilités en tenant compte de la structure géométrique
sous-jacente. Elle est particulièrement utile dans des domaines tels que
l’apprentissage automatique, la finance quantitative et la physique
statistique, où la comparaison de distributions est cruciale. En outre,
elle offre une solution élégante à des problèmes classiques comme le
transport optimal et la théorie de l’approximation.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement la distance de Wasserstein, il est
essentiel de comprendre ce que nous cherchons à capturer. Imaginons deux
distributions de probabilités sur un espace métrique. Nous voulons
mesurer à quel point ces distributions sont "éloignées" l’une de
l’autre, en tenant compte non seulement des différences locales mais
aussi des coûts de transport entre les points.</p>
<p>Pour formaliser cette intuition, nous introduisons d’abord quelques
notations. Soit <span class="math inline">\((X, d)\)</span> un espace
métrique, et soit <span class="math inline">\(\mathcal{P}(X)\)</span>
l’ensemble des mesures de probabilité sur <span
class="math inline">\(X\)</span>. Pour deux mesures <span
class="math inline">\(\mu, \nu \in \mathcal{P}(X)\)</span>, nous
cherchons une manière de quantifier la distance entre elles.</p>
<div class="definition">
<p>La distance de Wasserstein d’ordre 1 entre deux mesures <span
class="math inline">\(\mu, \nu \in \mathcal{P}(X)\)</span> est définie
comme l’infimum des coûts de transport entre ces mesures, où le coût de
transport est minimisé sur l’ensemble des couplages possibles.</p>
<p>Formellement, pour <span class="math inline">\(\pi \in \Pi(\mu,
\nu)\)</span>, où <span class="math inline">\(\Pi(\mu, \nu)\)</span>
désigne l’ensemble des mesures de couplage entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, la distance de Wasserstein d’ordre 1
est donnée par : <span class="math display">\[W_1(\mu, \nu) = \inf_{\pi
\in \Pi(\mu, \nu)} \int_{X \times X} d(x, y) \, d\pi(x, y).\]</span> De
manière équivalente, on peut l’exprimer à l’aide de la formule duale :
<span class="math display">\[W_1(\mu, \nu) = \sup_{f \in
\text{Lip}_1(X)} \left( \int_X f \, d\mu - \int_X f \, d\nu
\right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> désigne l’ensemble des
fonctions <span class="math inline">\(1\)</span>-Lipschitziennes sur
<span class="math inline">\(X\)</span>.</p>
</div>
<p>Pour généraliser cette notion à des ordres supérieurs, nous
introduisons la distance de Wasserstein d’ordre <span
class="math inline">\(p\)</span>.</p>
<div class="definition">
<p>Pour <span class="math inline">\(p \geq 1\)</span>, la distance de
Wasserstein d’ordre <span class="math inline">\(p\)</span> entre deux
mesures <span class="math inline">\(\mu, \nu \in \mathcal{P}(X)\)</span>
est définie comme : <span class="math display">\[W_p(\mu, \nu) = \left(
\inf_{\pi \in \Pi(\mu, \nu)} \int_{X \times X} d(x, y)^p \, d\pi(x, y)
\right)^{1/p}.\]</span> De manière duale, elle peut être exprimée comme
: <span class="math display">\[W_p(\mu, \nu) = \sup_{f \in
\text{Lip}_1(X)} \left( \int_X f \, d\mu - \int_X f \, d\nu
\right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> désigne l’ensemble des
fonctions <span class="math inline">\(p\)</span>-Lipschitziennes sur
<span class="math inline">\(X\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux liés à la distance de Wasserstein est
le théorème de Kantorovich-Rubinstein, qui établit une caractérisation
duale de la distance de Wasserstein d’ordre 1.</p>
<div class="theorem">
<p>Pour deux mesures <span class="math inline">\(\mu, \nu \in
\mathcal{P}(X)\)</span>, la distance de Wasserstein d’ordre 1 peut être
exprimée comme : <span class="math display">\[W_1(\mu, \nu) = \sup_{f
\in \text{Lip}_1(X)} \left( \int_X f \, d\mu - \int_X f \, d\nu
\right).\]</span></p>
</div>
<p>La démonstration de ce théorème repose sur des résultats classiques
d’analyse convexe et de théorie du transport optimal. Nous allons
maintenant détailler cette preuve.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Kantorovich-Rubinstein, nous commençons
par rappeler quelques propriétés fondamentales des mesures de couplage
et des fonctions Lipschitziennes.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\pi \in \Pi(\mu,
\nu)\)</span> un couplage entre <span class="math inline">\(\mu\)</span>
et <span class="math inline">\(\nu\)</span>. Pour toute fonction <span
class="math inline">\(f : X \to \mathbb{R}\)</span> mesurable bornée,
nous avons : <span class="math display">\[\int_X f \, d\mu = \int_{X
\times X} f(x) \, d\pi(x, y),\]</span> <span
class="math display">\[\int_X f \, d\nu = \int_{X \times X} f(y) \,
d\pi(x, y).\]</span> Par conséquent, pour toute fonction <span
class="math inline">\(f \in \text{Lip}_1(X)\)</span>, nous avons : <span
class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu = \int_{X
\times X} (f(x) - f(y)) \, d\pi(x, y).\]</span> Puisque <span
class="math inline">\(f\)</span> est <span
class="math inline">\(1\)</span>-Lipschitzienne, nous avons <span
class="math inline">\(|f(x) - f(y)| \leq d(x, y)\)</span> pour tout
<span class="math inline">\((x, y) \in X \times X\)</span>. Il s’ensuit
que : <span class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu
\leq \int_{X \times X} d(x, y) \, d\pi(x, y).\]</span> En prenant
l’infimum sur tous les couplages <span class="math inline">\(\pi \in
\Pi(\mu, \nu)\)</span>, nous obtenons : <span
class="math display">\[\sup_{f \in \text{Lip}_1(X)} \left( \int_X f \,
d\mu - \int_X f \, d\nu \right) \leq W_1(\mu, \nu).\]</span></p>
<p>Réciproquement, pour toute <span class="math inline">\(\epsilon &gt;
0\)</span>, il existe un couplage <span
class="math inline">\(\pi_\epsilon \in \Pi(\mu, \nu)\)</span> tel que :
<span class="math display">\[\int_{X \times X} d(x, y) \,
d\pi_\epsilon(x, y) \leq W_1(\mu, \nu) + \epsilon.\]</span> Soit <span
class="math inline">\(f : X \to \mathbb{R}\)</span> une fonction
mesurable bornée. Nous pouvons approximer <span
class="math inline">\(f\)</span> par des fonctions Lipschitziennes. Plus
précisément, pour tout <span class="math inline">\(\delta &gt;
0\)</span>, il existe une fonction <span class="math inline">\(f_\delta
\in \text{Lip}_1(X)\)</span> telle que : <span
class="math display">\[\|f - f_\delta\|_{\infty} \leq \delta.\]</span>
En utilisant l’inégalité de triangle, nous avons : <span
class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu = \int_{X
\times X} (f(x) - f(y)) \, d\pi_\epsilon(x, y),\]</span> <span
class="math display">\[\leq \int_{X \times X} (f(x) - f_\delta(x) +
f_\delta(x) - f_\delta(y) + f_\delta(y) - f(y)) \, d\pi_\epsilon(x,
y),\]</span> <span class="math display">\[\leq 2\delta + \int_{X \times
X} (f_\delta(x) - f_\delta(y)) \, d\pi_\epsilon(x, y),\]</span> <span
class="math display">\[\leq 2\delta + W_1(\mu, \nu) + \epsilon.\]</span>
En prenant <span class="math inline">\(\delta\)</span> et <span
class="math inline">\(\epsilon\)</span> arbitrairement petits, nous
obtenons : <span class="math display">\[\int_X f \, d\mu - \int_X f \,
d\nu \leq W_1(\mu, \nu).\]</span> En prenant la borne supérieure sur
toutes les fonctions <span class="math inline">\(f \in
\text{Lip}_1(X)\)</span>, nous concluons que : <span
class="math display">\[W_1(\mu, \nu) = \sup_{f \in \text{Lip}_1(X)}
\left( \int_X f \, d\mu - \int_X f \, d\nu \right).\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous énumérons maintenant quelques propriétés importantes de la
distance de Wasserstein.</p>
<ol>
<li><p><strong>Inégalité triangulaire</strong> : Pour toutes mesures
<span class="math inline">\(\mu, \nu, \rho \in \mathcal{P}(X)\)</span>,
nous avons : <span class="math display">\[W_p(\mu, \rho) \leq W_p(\mu,
\nu) + W_p(\nu, \rho).\]</span> Cette propriété découle directement de
la définition de la distance de Wasserstein et de l’inégalité
triangulaire pour la métrique <span
class="math inline">\(d\)</span>.</p></li>
<li><p><strong>Continuité par rapport à la convergence faible</strong> :
Si <span class="math inline">\(\mu_n\)</span> converge faiblement vers
<span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu_n\)</span> converge faiblement vers <span
class="math inline">\(\nu\)</span>, alors : <span
class="math display">\[\lim_{n \to \infty} W_p(\mu_n, \nu_n) = W_p(\mu,
\nu).\]</span> Cette propriété est cruciale pour l’analyse des limites
de suites de mesures.</p></li>
<li><p><strong>Inégalité de Poincaré-Wasserstein</strong> : Pour toute
mesure <span class="math inline">\(\mu \in \mathcal{P}(X)\)</span> et
toute fonction <span class="math inline">\(f : X \to \mathbb{R}\)</span>
mesurable bornée, nous avons : <span
class="math display">\[\text{Var}(f) \leq W_p(\mu, \delta_x)^p,\]</span>
où <span class="math inline">\(\text{Var}(f)\)</span> désigne la
variance de <span class="math inline">\(f\)</span> sous <span
class="math inline">\(\mu\)</span>, et <span
class="math inline">\(\delta_x\)</span> est la mesure de Dirac en <span
class="math inline">\(x\)</span>.</p>
<p>La démonstration de cette propriété repose sur des techniques
d’analyse fonctionnelle et de théorie de la mesure.</p></li>
</ol>
<p>En conclusion, la distance de Wasserstein est un outil puissant et
polyvalent qui trouve des applications dans de nombreux domaines des
mathématiques appliquées. Son étude continue d’être un sujet actif de
recherche, avec des développements récents dans l’apprentissage
automatique et la physique statistique.</p>
</body>
</html>
{% include "footer.html" %}


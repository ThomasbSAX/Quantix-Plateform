{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Maximale : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Maximale : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie maximale, ou principe de maximum d’entropie, trouve ses
racines dans les travaux de E.T. Jaynes en 1957. Ce principe émerge
comme une réponse élégante aux défis posés par l’estimation de
probabilités à partir d’informations partielles. Dans un contexte où les
données sont insuffisantes pour déterminer une distribution de
probabilité unique, l’entropie maximale offre un cadre rigoureux pour
sélectionner la distribution la plus "neutre" ou "non informative".</p>
<p>L’entropie, concept central en théorie de l’information et en
thermodynamique, mesure le degré d’incertitude ou de désordre. Maximiser
l’entropie sous des contraintes spécifiques permet de capturer cette
incertitude tout en respectant les informations disponibles. Ce principe
est indispensable dans divers domaines, allant de l’apprentissage
automatique à la physique statistique, en passant par la modélisation
économique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de formaliser l’entropie maximale, il est essentiel de
comprendre ce que nous cherchons à capturer. Imaginons une situation où
nous connaissons certaines moyennes ou contraintes sur un système, mais
pas la distribution complète. Nous voulons trouver une distribution qui
respecte ces contraintes tout en étant la plus "uniforme" possible,
c’est-à-dire celle qui ne fait aucune hypothèse supplémentaire non
justifiée par les données.</p>
<p>Pour une variable aléatoire discrète <span
class="math inline">\(X\)</span> prenant des valeurs dans un ensemble
fini <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span>, l’entropie de Shannon est définie comme :</p>
<div class="definition">
<p>L’entropie de Shannon <span class="math inline">\(H(X)\)</span> d’une
variable aléatoire discrète <span class="math inline">\(X\)</span> est
donnée par : <span class="math display">\[H(X) = -\sum_{i=1}^n p(x_i)
\log p(x_i),\]</span> où <span class="math inline">\(p(x_i) = P(X =
x_i)\)</span> est la probabilité que <span
class="math inline">\(X\)</span> prenne la valeur <span
class="math inline">\(x_i\)</span>.</p>
</div>
<p>Pour une variable aléatoire continue, l’entropie différentielle est
définie de manière similaire :</p>
<div class="definition">
<p>L’entropie différentielle <span class="math inline">\(h(X)\)</span>
d’une variable aléatoire continue <span class="math inline">\(X\)</span>
est donnée par : <span class="math display">\[h(X) =
-\int_{-\infty}^{\infty} f(x) \log f(x) \, dx,\]</span> où <span
class="math inline">\(f(x)\)</span> est la densité de probabilité de
<span class="math inline">\(X\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le principe de maximum d’entropie peut être formalisé par le théorème
suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{P}\)</span> l’ensemble des
distributions de probabilité <span class="math inline">\(p(x)\)</span>
sur un espace <span class="math inline">\(\mathcal{X}\)</span>
satisfaisant certaines contraintes <span class="math inline">\(E[p(x)] =
c\)</span>, où <span class="math inline">\(E\)</span> est l’espérance.
La distribution qui maximise l’entropie sous ces contraintes est unique
et satisfait la condition : <span class="math display">\[p^*(x) =
\exp\left(-\lambda_0 - \sum_i \lambda_i E_i(x)\right),\]</span> où <span
class="math inline">\(\lambda_0, \lambda_i\)</span> sont des
multiplicateurs de Lagrange déterminés par les contraintes.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous utilisons la méthode des
multiplicateurs de Lagrange. Nous cherchons à maximiser <span
class="math inline">\(H(X)\)</span> sous les contraintes <span
class="math inline">\(E[p(x)] = c\)</span>.</p>
<p>Considérons le Lagrangien : <span
class="math display">\[\mathcal{L}(p, \lambda) = H(X) - \lambda_0
\left(\sum_{i=1}^n p(x_i) - 1\right) - \sum_j \lambda_j
\left(\sum_{i=1}^n p(x_i) E_j(x_i) - c_j\right).\]</span></p>
<p>En prenant la dérivée partielle par rapport à <span
class="math inline">\(p(x_i)\)</span>, nous obtenons : <span
class="math display">\[\frac{\partial \mathcal{L}}{\partial p(x_i)} =
-\log p(x_i) - 1 - \lambda_0 - \sum_j \lambda_j E_j(x_i).\]</span></p>
<p>En égalisant à zéro pour maximiser <span
class="math inline">\(\mathcal{L}\)</span>, nous avons : <span
class="math display">\[-\log p(x_i) - 1 - \lambda_0 - \sum_j \lambda_j
E_j(x_i) = 0.\]</span></p>
<p>En résolvant pour <span class="math inline">\(p(x_i)\)</span>, nous
obtenons : <span class="math display">\[p^*(x_i) = \exp\left(-1 -
\lambda_0 - \sum_j \lambda_j E_j(x_i)\right).\]</span></p>
<p>En normalisant pour que la somme des probabilités soit égale à 1,
nous obtenons finalement : <span class="math display">\[p^*(x_i) =
\exp\left(-\lambda_0 - \sum_j \lambda_j E_j(x_i)\right).\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<ol>
<li><p>**Unicité de la Solution** : La solution obtenue est unique car
l’entropie est une fonction strictement concave.</p></li>
<li><p>**Consistance avec les Contraintes** : La distribution <span
class="math inline">\(p^*\)</span> satisfait toutes les contraintes
imposées.</p></li>
<li><p>**Minimisation de l’Information** : La distribution <span
class="math inline">\(p^*\)</span> minimise l’information relative (ou
divergence de Kullback-Leibler) par rapport à toute autre distribution
satisfaisant les contraintes.</p></li>
</ol>
<h3 class="unnumbered" id="preuve-de-lunicité">Preuve de l’Unicité</h3>
<p>L’entropie <span class="math inline">\(H(X)\)</span> est une fonction
strictement concave, ce qui signifie que la matrice hessienne de <span
class="math inline">\(H(X)\)</span> est négative définie. Par
conséquent, le maximum de <span class="math inline">\(H(X)\)</span> sous
les contraintes est unique.</p>
<h3 class="unnumbered"
id="preuve-de-la-minimisation-de-linformation">Preuve de la Minimisation
de l’Information</h3>
<p>Pour toute distribution <span class="math inline">\(q(x)\)</span>
satisfaisant les contraintes, la divergence de Kullback-Leibler <span
class="math inline">\(D_{KL}(q || p^*)\)</span> est minimisée lorsque
<span class="math inline">\(q = p^*\)</span>. Cela découle directement
de la propriété de l’entropie maximale.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie maximale offre un cadre puissant pour l’inférence
statistique et la modélisation. En maximisant l’entropie sous des
contraintes spécifiques, nous obtenons une distribution de probabilité
qui est à la fois cohérente avec les données disponibles et non
informative au-delà de ces contraintes. Ce principe trouve des
applications dans divers domaines, de la physique à l’apprentissage
automatique, en passant par l’économie.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Pinsker : Un pont entre l’information et la probabilité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Pinsker : Un pont entre l’information
et la probabilité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Pinsker, nommée en l’honneur du mathématicien russe
Mikhaïl Pinsker, est une pierre angulaire dans la théorie de
l’information et des probabilités. Son émergence s’inscrit dans le cadre
du développement des fondements mathématiques de la théorie de
l’information, initiée par Claude Shannon dans les années 1940. Cette
inégalité établit un lien profond entre la divergence de
Kullback-Leibler, une mesure de distance entre distributions de
probabilité, et la divergence variationnelle, une autre mesure de
dissimilarité. Elle est indispensable dans l’analyse des canaux bruités,
la détection de changements et bien d’autres applications en traitement
du signal et en apprentissage automatique.</p>
<p>L’inégalité de Pinsker répond à une question fondamentale : comment
mesurer la différence entre deux distributions de probabilité de manière
à ce que cette mesure soit à la fois informativement riche et
calculatoirement tractable. Elle apporte une réponse élégante en
fournissant une borne inférieure sur la divergence de Kullback-Leibler
en termes de la divergence variationnelle, ce qui permet de passer d’une
mesure souvent difficile à estimer à une autre plus accessible.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de formuler l’inégalité de Pinsker, il est essentiel de définir
les concepts clés qui y sont associés.</p>
<h2 class="unnumbered" id="divergence-de-kullback-leibler">Divergence de
Kullback-Leibler</h2>
<p>Pour comprendre ce que nous cherchons à mesurer, considérons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier à quel point <span class="math inline">\(Q\)</span> s’écarte
de <span class="math inline">\(P\)</span>, en particulier lorsque <span
class="math inline">\(Q\)</span> est une distribution approchée ou
estimée de <span class="math inline">\(P\)</span>. La divergence de
Kullback-Leibler, souvent notée <span
class="math inline">\(D(P\|Q)\)</span>, est une mesure de cette
dissimilarité.</p>
<div class="definition">
<p>La divergence de Kullback-Leibler entre deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D(P\|Q) = \sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right)\]</span> si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont discrètes, ou <span
class="math display">\[D(P\|Q) = \int_{\Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) \, d\mu(x)\]</span> si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont continues, où <span
class="math inline">\(\mu\)</span> est une mesure de référence.</p>
</div>
<h2 class="unnumbered" id="divergence-variationnelle">Divergence
variationnelle</h2>
<p>La divergence variationnelle, quant à elle, est une mesure plus
simple de la dissimilarité entre deux distributions. Elle est définie
comme la somme des différences absolues entre les probabilités
correspondantes.</p>
<div class="definition">
<p>La divergence variationnelle entre deux distributions de probabilité
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[\delta(P,Q) = \frac{1}{2} \sum_{x \in \Omega}
|P(x) - Q(x)|\]</span> si <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sont discrètes, ou <span
class="math display">\[\delta(P,Q) = \frac{1}{2} \int_{\Omega} |P(x) -
Q(x)| \, d\mu(x)\]</span> si <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sont continues.</p>
</div>
<h1 class="unnumbered" id="linégalité-de-pinsker">L’inégalité de
Pinsker</h1>
<p>Nous sommes maintenant prêts à formuler l’inégalité de Pinsker.
Intuitivement, cette inégalité nous dit que la divergence de
Kullback-Leibler est bornée inférieurement par une fonction de la
divergence variationnelle.</p>
<h2 class="unnumbered" id="formulation">Formulation</h2>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\Omega\)</span>. Alors, on a : <span
class="math display">\[D(P\|Q) \geq \frac{1}{2} \delta(P,Q)^2\]</span>
où <span class="math inline">\(\delta(P,Q)\)</span> est la divergence
variationnelle entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<h2 class="unnumbered" id="preuve">Preuve</h2>
<p>Pour démontrer l’inégalité de Pinsker, nous allons utiliser la
convexité de la fonction <span class="math inline">\(f(x) = x \log
x\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(f(x) = x \log x\)</span>. Cette fonction est
convexe, ce qui signifie que pour tout <span class="math inline">\(x, y
\geq 0\)</span> et <span class="math inline">\(\lambda \in
[0,1]\)</span>, on a : <span class="math display">\[f(\lambda x +
(1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)\]</span></p>
<p>Nous allons utiliser cette propriété de convexité pour borner <span
class="math inline">\(D(P\|Q)\)</span>.</p>
<p>Pour tout <span class="math inline">\(x \in \Omega\)</span>, soit
<span class="math inline">\(p_x = P(x)\)</span> et <span
class="math inline">\(q_x = Q(x)\)</span>. On a : <span
class="math display">\[D(P\|Q) = \sum_{x \in \Omega} p_x
\log\left(\frac{p_x}{q_x}\right)\]</span></p>
<p>En utilisant la convexité de <span class="math inline">\(f\)</span>,
nous pouvons écrire : <span class="math display">\[p_x
\log\left(\frac{p_x}{q_x}\right) = f(p_x) - f(q_x) + q_x
\log\left(\frac{q_x}{p_x}\right)\]</span></p>
<p>Cependant, une approche plus directe consiste à utiliser l’inégalité
de Gibbs, qui stipule que pour tout <span class="math inline">\(x \geq
0\)</span>, on a : <span class="math display">\[x \log x \geq x -
1\]</span></p>
<p>En appliquant cette inégalité à <span
class="math inline">\(p_x\)</span> et <span
class="math inline">\(q_x\)</span>, nous obtenons : <span
class="math display">\[p_x \log\left(\frac{p_x}{q_x}\right) = p_x \log
p_x - p_x \log q_x \geq (p_x - 1) - p_x \log q_x\]</span></p>
<p>En sommant sur <span class="math inline">\(x \in \Omega\)</span>,
nous avons : <span class="math display">\[D(P\|Q) = \sum_{x \in \Omega}
p_x \log\left(\frac{p_x}{q_x}\right) \geq \sum_{x \in \Omega} (p_x - 1)
- \sum_{x \in \Omega} p_x \log q_x\]</span></p>
<p>Or, <span class="math inline">\(\sum_{x \in \Omega} (p_x - 1) =
0\)</span>, donc : <span class="math display">\[D(P\|Q) \geq -\sum_{x
\in \Omega} p_x \log q_x\]</span></p>
<p>En utilisant l’inégalité de Jensen, nous pouvons borner <span
class="math inline">\(-\sum_{x \in \Omega} p_x \log q_x\)</span> par :
<span class="math display">\[-\sum_{x \in \Omega} p_x \log q_x \geq
-\log\left(\sum_{x \in \Omega} p_x q_x\right)\]</span></p>
<p>Cependant, cette approche semble complexe. Une méthode plus simple
consiste à utiliser l’inégalité de Pinsker directement en utilisant la
convexité et les propriétés des normes.</p>
<p>Considérons la fonction <span class="math inline">\(\phi(t) = t \log
t\)</span>. Cette fonction est convexe, et son développement de Taylor
autour de <span class="math inline">\(t = 1\)</span> donne : <span
class="math display">\[\phi(t) = (t-1) - \frac{(t-1)^2}{2} +
o((t-1)^2)\]</span></p>
<p>En utilisant cette approximation, nous pouvons écrire : <span
class="math display">\[p_x \log\left(\frac{p_x}{q_x}\right) = p_x \log
p_x - p_x \log q_x \approx (p_x - 1) - \frac{(p_x - 1)^2}{2} - p_x \log
q_x\]</span></p>
<p>En sommant sur <span class="math inline">\(x \in \Omega\)</span>,
nous avons : <span class="math display">\[D(P\|Q) \approx -\frac{1}{2}
\sum_{x \in \Omega} (p_x - q_x)^2\]</span></p>
<p>Cependant, cette approximation n’est pas suffisante pour obtenir
l’inégalité de Pinsker. Pour une preuve rigoureuse, nous devons utiliser
des outils plus avancés.</p>
<p>Une preuve complète de l’inégalité de Pinsker peut être trouvée dans
les ouvrages de théorie de l’information, où elle est souvent démontrée
en utilisant des techniques d’analyse convexe et des inégalités
fonctionnelles.</p>
<p>Ainsi, nous avons : <span class="math display">\[D(P\|Q) \geq
\frac{1}{2} \delta(P,Q)^2\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’inégalité de Pinsker possède plusieurs propriétés intéressantes et
corollaires qui en découlent.</p>
<h2 class="unnumbered" id="propriété-de-symétrie">Propriété de
Symétrie</h2>
<div class="proposition">
<p>L’inégalité de Pinsker est symétrique en <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, c’est-à-dire que : <span
class="math display">\[D(P\|Q) \geq \frac{1}{2} \delta(P,Q)^2\]</span>
et <span class="math display">\[D(Q\|P) \geq \frac{1}{2}
\delta(P,Q)^2\]</span></p>
</div>
<h2 class="unnumbered" id="borne-supérieure">Borne Supérieure</h2>
<div class="corollary">
<p>Il existe une borne supérieure pour la divergence de Kullback-Leibler
en termes de la divergence variationnelle. Plus précisément, pour tout
<span class="math inline">\(\epsilon &gt; 0\)</span>, il existe des
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> telles que : <span
class="math display">\[D(P\|Q) \leq (1 + o(1)) \frac{1}{2}
\delta(P,Q)^2\]</span> lorsque <span class="math inline">\(\delta(P,Q)
\to 0\)</span>.</p>
</div>
<h2 class="unnumbered" id="application-aux-canaux-bruités">Application
aux Canaux Bruités</h2>
<div class="proposition">
<p>Dans le contexte des canaux bruités, l’inégalité de Pinsker permet de
borner la capacité du canal en termes de la divergence variationnelle
entre les distributions d’entrée et de sortie.</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Pinsker est un résultat fondamental en théorie de
l’information et des probabilités. Elle établit un lien profond entre la
divergence de Kullback-Leibler, une mesure informativement riche mais
souvent difficile à estimer, et la divergence variationnelle, une mesure
plus simple et calculatoirement tractable. Ses applications sont
nombreuses, allant de l’analyse des canaux bruités à la détection de
changements et bien d’autres domaines.</p>
<p>En conclusion, l’inégalité de Pinsker illustre la beauté et
l’élégance des mathématiques appliquées, où des concepts abstraits
trouvent des applications concrètes et profondes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Le coefficient Kappa de Fleiss : une mesure de l’accord inter-juges</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Le coefficient Kappa de Fleiss : une mesure de
l’accord inter-juges</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’évaluation de l’accord inter-juges est une problématique centrale
dans de nombreuses disciplines scientifiques, notamment en médecine,
psychologie et sciences sociales. Lorsque plusieurs experts doivent
classer des sujets selon des catégories prédéfinies, il est crucial de
quantifier la cohérence de leurs évaluations. Le coefficient Kappa de
Fleiss (1971) répond à cette nécessité en fournissant une mesure robuste
de l’accord au-delà du hasard.</p>
<p>Ce coefficient généralise le Kappa de Cohen (1960), initialement
conçu pour deux juges, à un nombre quelconque d’évaluateurs. Il permet
ainsi d’analyser des situations complexes où plusieurs experts
indépendants classent un ensemble de sujets selon plusieurs catégories.
L’importance du Kappa de Fleiss réside dans sa capacité à corriger
l’accord attendu par le hasard, offrant ainsi une évaluation plus
nuancée de la fiabilité des classifications.</p>
<h1 id="définitions">Définitions</h1>
<div class="definition">
<p>Considérons un ensemble de <span class="math inline">\(n\)</span>
juges qui classent indépendamment <span class="math inline">\(m\)</span>
sujets selon <span class="math inline">\(k\)</span> catégories. On note
<span class="math inline">\(n_{ij}\)</span> le nombre de juges ayant
classé le sujet <span class="math inline">\(i\)</span> dans la catégorie
<span class="math inline">\(j\)</span>. La matrice de contingence <span
class="math inline">\(N = (n_{ij})\)</span> est une matrice <span
class="math inline">\(m \times k\)</span> où chaque ligne correspond à
un sujet et chaque colonne à une catégorie.</p>
</div>
<div class="definition">
<p>Pour chaque sujet <span class="math inline">\(i\)</span>, la
proportion de juges ayant classé ce sujet dans la catégorie <span
class="math inline">\(j\)</span> est donnée par : <span
class="math display">\[p_{ij} = \frac{n_{ij}}{n}\]</span> La proportion
marginale pour la catégorie <span class="math inline">\(j\)</span> est
alors : <span class="math display">\[p_j = \frac{1}{m} \sum_{i=1}^{m}
p_{ij}\]</span></p>
</div>
<div class="definition">
<p>La proportion d’accord attendu par le hasard pour la catégorie <span
class="math inline">\(j\)</span> est : <span
class="math display">\[p_j^2\]</span> La proportion totale d’accord
attendu par le hasard est donc : <span class="math display">\[P_e =
\sum_{j=1}^{k} p_j^2\]</span></p>
</div>
<div class="definition">
<p>La proportion d’accord observé est définie comme : <span
class="math display">\[P_o = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k}
p_{ij}^2\]</span></p>
</div>
<h1 id="théorème-de-fleiss">Théorème de Fleiss</h1>
<div class="theorem">
<p>Le coefficient Kappa de Fleiss est défini comme : <span
class="math display">\[\kappa = \frac{P_o - P_e}{1 - P_e}\]</span> où
<span class="math inline">\(P_o\)</span> est la proportion d’accord
observé et <span class="math inline">\(P_e\)</span> la proportion
d’accord attendu par le hasard.</p>
</div>
<h1 id="preuves">Preuves</h1>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la validité du coefficient Kappa de
Fleiss, nous devons montrer que <span
class="math inline">\(\kappa\)</span> mesure effectivement l’accord
au-delà du hasard.</p>
<p>1. **Calcul de <span class="math inline">\(P_o\)</span>** : <span
class="math display">\[P_o = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k}
p_{ij}^2\]</span> Cette expression représente la proportion moyenne
d’accord observé pour tous les sujets et toutes les catégories.</p>
<p>2. **Calcul de <span class="math inline">\(P_e\)</span>** : <span
class="math display">\[P_e = \sum_{j=1}^{k} p_j^2\]</span> Cette
expression représente la proportion d’accord attendu si les juges
classaient les sujets de manière aléatoire selon les proportions
marginales <span class="math inline">\(p_j\)</span>.</p>
<p>3. **Interprétation de <span class="math inline">\(\kappa\)</span>**
: Le numérateur <span class="math inline">\(P_o - P_e\)</span> mesure
l’écart entre l’accord observé et l’accord attendu par le hasard. Le
dénominateur <span class="math inline">\(1 - P_e\)</span> normalise cet
écart pour obtenir une mesure comprise entre -1 et 1. Une valeur de
<span class="math inline">\(\kappa\)</span> proche de 1 indique un
accord parfait, tandis qu’une valeur proche de 0 indique un accord
équivalent au hasard. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<ol>
<li><p>**Invariance par permutation des catégories** : Le coefficient
Kappa de Fleiss est invariant par permutation des catégories. Cela
signifie que le résultat ne dépend pas de l’ordre dans lequel les
catégories sont présentées.</p></li>
<li><p>**Consistance avec le Kappa de Cohen** : Lorsque <span
class="math inline">\(n = 2\)</span> (deux juges), le coefficient Kappa
de Fleiss coïncide avec le Kappa de Cohen, généralisant ainsi ce dernier
à un nombre quelconque d’évaluateurs.</p></li>
<li><p>**Interprétation des valeurs** : Les valeurs de <span
class="math inline">\(\kappa\)</span> peuvent être interprétées comme
suit :</p>
<ul>
<li><p><span class="math inline">\(\kappa \leq 0\)</span> : Accord nul
ou inférieur au hasard.</p></li>
<li><p><span class="math inline">\(0 &lt; \kappa \leq 0.20\)</span> :
Accord faible.</p></li>
<li><p><span class="math inline">\(0.21 &lt; \kappa \leq 0.40\)</span> :
Accord faible à modéré.</p></li>
<li><p><span class="math inline">\(0.41 &lt; \kappa \leq 0.60\)</span> :
Accord modéré.</p></li>
<li><p><span class="math inline">\(0.61 &lt; \kappa \leq 0.80\)</span> :
Accord substantiel.</p></li>
<li><p><span class="math inline">\(0.81 &lt; \kappa \leq 1.00\)</span> :
Accord presque parfait.</p></li>
</ul></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Le coefficient Kappa de Fleiss constitue une avancée majeure dans
l’évaluation de l’accord inter-juges, permettant d’analyser des
situations complexes impliquant plusieurs évaluateurs et catégories. Sa
robustesse et sa généralisation du Kappa de Cohen en font un outil
indispensable dans de nombreuses disciplines scientifiques. Les
propriétés et l’interprétation des valeurs de <span
class="math inline">\(\kappa\)</span> offrent une compréhension fine de
la cohérence des classifications, contribuant ainsi à la fiabilité et à
la validité des études empiriques.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence KL symétrique : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence KL symétrique : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Kullback-Leibler (KL), également connue sous le nom
d’entropie relative, est une mesure fondamentale en théorie de
l’information qui quantifie la différence entre deux distributions de
probabilité. Cependant, cette mesure n’est pas symétrique, ce qui peut
poser des problèmes dans certaines applications où la symétrie est
requise. C’est ici que la divergence KL symétrique entre en jeu.</p>
<p>La divergence KL symétrique, notée souvent <span
class="math inline">\(D_{KL}^{sym}\)</span>, est une version modifiée de
la divergence KL qui possède des propriétés de symétrie. Elle émerge
naturellement dans le cadre de l’apprentissage automatique, des
statistiques bayésiennes et de la théorie des jeux, où la symétrie est
une propriété désirable pour les mesures de divergence.</p>
<p>Dans cet article, nous explorerons en détail la divergence KL
symétrique, ses définitions, ses propriétés et ses applications. Nous
commencerons par rappeler brièvement la divergence KL classique avant de
nous concentrer sur sa version symétrique.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir la divergence KL symétrique, il est essentiel de
rappeler la définition de la divergence KL classique.</p>
<h2 id="divergence-de-kullback-leibler">Divergence de
Kullback-Leibler</h2>
<p>Supposons que <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> soient deux distributions de
probabilité discrètes ou continues sur un espace <span
class="math inline">\(\mathcal{X}\)</span>. La divergence KL de <span
class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span> est définie comme suit :</p>
<div class="definition">
<p>La divergence de Kullback-Leibler <span
class="math inline">\(D_{KL}(P \parallel Q)\)</span> est donnée par :
<span class="math display">\[D_{KL}(P \parallel Q) = \sum_{x \in
\mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)\]</span> pour des
distributions discrètes, et <span class="math display">\[D_{KL}(P
\parallel Q) = \int_{\mathcal{X}} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) dx\]</span> pour des distributions
continues.</p>
</div>
<p>Remarquons que <span class="math inline">\(D_{KL}(P \parallel Q) \neq
D_{KL}(Q \parallel P)\)</span> en général, ce qui signifie que la
divergence KL n’est pas symétrique.</p>
<h2 id="divergence-kl-symétrique">Divergence KL Symétrique</h2>
<p>Pour obtenir une mesure symétrique, nous pouvons considérer la
moyenne de <span class="math inline">\(D_{KL}(P \parallel Q)\)</span> et
<span class="math inline">\(D_{KL}(Q \parallel P)\)</span>. Cela nous
mène à la définition de la divergence KL symétrique.</p>
<div class="definition">
<p>La divergence KL symétrique <span
class="math inline">\(D_{KL}^{sym}(P, Q)\)</span> est définie par :
<span class="math display">\[D_{KL}^{sym}(P, Q) = \frac{1}{2} \left(
D_{KL}(P \parallel Q) + D_{KL}(Q \parallel P) \right)\]</span></p>
</div>
<p>Cette définition garantit que <span
class="math inline">\(D_{KL}^{sym}(P, Q) = D_{KL}^{sym}(Q, P)\)</span>,
ce qui est une propriété cruciale pour de nombreuses applications.</p>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<p>Dans cette section, nous explorons certaines propriétés importantes
de la divergence KL symétrique.</p>
<h2 id="non-négativité">Non-Négativité</h2>
<div class="theorem">
<p>Pour toute paire de distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[D_{KL}^{sym}(P, Q) \geq 0\]</span> avec égalité
si et seulement si <span class="math inline">\(P = Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La non-négativité de <span
class="math inline">\(D_{KL}^{sym}(P, Q)\)</span> découle directement de
la non-négativité de <span class="math inline">\(D_{KL}(P \parallel
Q)\)</span> et <span class="math inline">\(D_{KL}(Q \parallel
P)\)</span>. En effet, par l’inégalité de Gibbs, nous savons que : <span
class="math display">\[D_{KL}(P \parallel Q) \geq 0 \quad \text{et}
\quad D_{KL}(Q \parallel P) \geq 0\]</span> avec égalité si et seulement
si <span class="math inline">\(P = Q\)</span>. Par conséquent, leur
moyenne est également non négative avec la même condition
d’égalité. ◻</p>
</div>
<h2 id="symétrie">Symétrie</h2>
<div class="theorem">
<p>La divergence KL symétrique est symétrique, c’est-à-dire : <span
class="math display">\[D_{KL}^{sym}(P, Q) = D_{KL}^{sym}(Q,
P)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition, <span
class="math display">\[D_{KL}^{sym}(P, Q) = \frac{1}{2} \left( D_{KL}(P
\parallel Q) + D_{KL}(Q \parallel P) \right)\]</span> et <span
class="math display">\[D_{KL}^{sym}(Q, P) = \frac{1}{2} \left( D_{KL}(Q
\parallel P) + D_{KL}(P \parallel Q) \right)\]</span> Il est clair que
ces deux expressions sont égales. ◻</p>
</div>
<h1 id="applications">Applications</h1>
<p>La divergence KL symétrique trouve des applications dans divers
domaines, notamment en apprentissage automatique et en théorie des jeux.
Voici quelques exemples :</p>
<h2 id="apprentissage-automatique">Apprentissage Automatique</h2>
<p>Dans le cadre de l’apprentissage automatique, la divergence KL
symétrique peut être utilisée pour mesurer la différence entre deux
modèles de probabilité. Par exemple, dans les réseaux de neurones
bayésiens, elle peut aider à évaluer la similarité entre deux
distributions postérieures.</p>
<h2 id="théorie-des-jeux">Théorie des Jeux</h2>
<p>En théorie des jeux, la divergence KL symétrique peut être utilisée
pour mesurer la distance entre les stratégies de deux joueurs. Cela
permet d’analyser les équilibres de Nash et d’autres concepts
fondamentaux.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré la divergence KL symétrique, une
mesure de divergence symétrique basée sur la divergence de
Kullback-Leibler. Nous avons défini cette mesure, démontré ses
propriétés fondamentales et discuté de ses applications dans divers
domaines. La symétrie de cette mesure en fait un outil précieux pour les
chercheurs et les praticiens dans des domaines tels que l’apprentissage
automatique et la théorie des jeux.</p>
</body>
</html>
{% include "footer.html" %}


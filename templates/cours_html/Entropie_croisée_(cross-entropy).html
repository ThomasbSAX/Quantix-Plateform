{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Croisée : Une Mesure Fondamentale en Théorie de l’Information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Croisée : Une Mesure Fondamentale en
Théorie de l’Information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie croisée, ou cross-entropy en anglais, est une notion
centrale en théorie de l’information et en apprentissage automatique.
Elle trouve ses racines dans les travaux pionniers de Claude Shannon sur
l’information et la communication. L’entropie croisée émerge comme une
mesure de la différence entre deux distributions de probabilité,
permettant ainsi d’évaluer la performance d’un modèle prédictif par
rapport à une distribution de référence.</p>
<p>L’entropie croisée est indispensable dans divers domaines tels que la
classification, l’apprentissage supervisé, et même en cryptographie.
Elle permet de quantifier la divergence entre une distribution réelle et
une distribution prédite, ce qui est crucial pour l’optimisation des
modèles.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie croisée, commençons par comprendre ce que
nous cherchons à mesurer. Supposons que nous avons une distribution de
probabilité réelle <span class="math inline">\(p\)</span> et une
distribution prédite <span class="math inline">\(q\)</span>. Nous
voulons mesurer à quel point <span class="math inline">\(q\)</span>
s’éloigne de <span class="math inline">\(p\)</span>. L’entropie croisée
est une mesure de cette divergence.</p>
<p>Formellement, l’entropie croisée <span class="math inline">\(H(p,
q)\)</span> entre deux distributions de probabilité discrètes <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(p = (p_1, p_2, \ldots,
p_n)\)</span> et <span class="math inline">\(q = (q_1, q_2, \ldots,
q_n)\)</span> deux distributions de probabilité discrètes sur un
ensemble fini <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_n\}\)</span>. L’entropie croisée <span class="math inline">\(H(p,
q)\)</span> est donnée par : <span class="math display">\[H(p, q) =
-\sum_{i=1}^n p_i \log(q_i)\]</span></p>
</div>
<p>Pour les distributions continues, l’entropie croisée est définie de
manière analogue en utilisant une intégrale :</p>
<div class="definition">
<p>Soient <span class="math inline">\(p(x)\)</span> et <span
class="math inline">\(q(x)\)</span> deux distributions de probabilité
continues sur un espace <span class="math inline">\(X\)</span>.
L’entropie croisée <span class="math inline">\(H(p, q)\)</span> est
donnée par : <span class="math display">\[H(p, q) = -\int_{X} p(x)
\log(q(x)) \, dx\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’entropie croisée possède plusieurs propriétés intéressantes. L’une
des plus importantes est la relation entre l’entropie croisée et
l’entropie de Shannon.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(p\)</span> une distribution de
probabilité et <span class="math inline">\(q\)</span> une autre
distribution. Alors, l’entropie croisée satisfait la relation suivante :
<span class="math display">\[H(p, q) = H(p) + D_{KL}(p \| q)\]</span> où
<span class="math inline">\(H(p)\)</span> est l’entropie de Shannon de
<span class="math inline">\(p\)</span> et <span
class="math inline">\(D_{KL}(p \| q)\)</span> est la divergence de
Kullback-Leibler entre <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver la relation entre l’entropie croisée et l’entropie de
Shannon, commençons par rappeler les définitions :</p>
<div class="proof">
<p><em>Proof.</em> L’entropie de Shannon <span
class="math inline">\(H(p)\)</span> est définie comme : <span
class="math display">\[H(p) = -\sum_{i=1}^n p_i \log(p_i)\]</span></p>
<p>La divergence de Kullback-Leibler <span
class="math inline">\(D_{KL}(p \| q)\)</span> est définie comme : <span
class="math display">\[D_{KL}(p \| q) = \sum_{i=1}^n p_i
\log\left(\frac{p_i}{q_i}\right)\]</span></p>
<p>Maintenant, considérons l’entropie croisée <span
class="math inline">\(H(p, q)\)</span> : <span
class="math display">\[H(p, q) = -\sum_{i=1}^n p_i
\log(q_i)\]</span></p>
<p>Nous pouvons réécrire <span class="math inline">\(H(p, q)\)</span> en
ajoutant et en soustrayant <span class="math inline">\(p_i
\log(p_i)\)</span> : <span class="math display">\[H(p, q) =
-\sum_{i=1}^n p_i \log(p_i) + \sum_{i=1}^n p_i \log(p_i) - \sum_{i=1}^n
p_i \log(q_i)\]</span></p>
<p>Ce qui donne : <span class="math display">\[H(p, q) = H(p) +
\sum_{i=1}^n p_i \log\left(\frac{p_i}{q_i}\right)\]</span></p>
<p>En utilisant la définition de la divergence de Kullback-Leibler, nous
obtenons : <span class="math display">\[H(p, q) = H(p) + D_{KL}(p \|
q)\]</span></p>
<p>Ceci conclut la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie croisée possède plusieurs propriétés intéressantes. En
voici quelques-unes :</p>
<ol>
<li><p><strong>Positivité</strong> : L’entropie croisée <span
class="math inline">\(H(p, q)\)</span> est toujours positive ou nulle.
Elle est nulle si et seulement si <span class="math inline">\(p =
q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La positivité de l’entropie croisée découle
directement de la définition. En effet, pour toute distribution de
probabilité <span class="math inline">\(p\)</span>, nous avons : <span
class="math display">\[H(p, q) = -\sum_{i=1}^n p_i \log(q_i)\]</span> Si
<span class="math inline">\(p = q\)</span>, alors <span
class="math inline">\(H(p, q) = -\sum_{i=1}^n p_i \log(p_i) = H(p) \geq
0\)</span>. La nullité est atteinte lorsque <span
class="math inline">\(p = q\)</span>. ◻</p>
</div></li>
<li><p><strong>Inégalité de Gibbs</strong> : Pour toute distribution
<span class="math inline">\(p\)</span>, l’entropie croisée satisfait :
<span class="math display">\[H(p, q) \geq H(p)\]</span> avec égalité si
et seulement si <span class="math inline">\(p = q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la relation
entre l’entropie croisée et la divergence de Kullback-Leibler. En effet,
nous avons : <span class="math display">\[H(p, q) = H(p) + D_{KL}(p \|
q)\]</span> Puisque <span class="math inline">\(D_{KL}(p \| q) \geq
0\)</span>, il s’ensuit que <span class="math inline">\(H(p, q) \geq
H(p)\)</span>. L’égalité est atteinte lorsque <span
class="math inline">\(D_{KL}(p \| q) = 0\)</span>, c’est-à-dire lorsque
<span class="math inline">\(p = q\)</span>. ◻</p>
</div></li>
<li><p><strong>Convexité</strong> : L’entropie croisée est une fonction
convexe de <span class="math inline">\(q\)</span> pour un <span
class="math inline">\(p\)</span> fixé.</p>
<div class="proof">
<p><em>Proof.</em> La convexité de l’entropie croisée peut être
démontrée en utilisant la définition de la convexité. Pour toute
distribution <span class="math inline">\(q\)</span>, nous avons : <span
class="math display">\[H(p, (1 - \lambda)q + \lambda q&#39;) \leq (1 -
\lambda)H(p, q) + \lambda H(p, q&#39;)\]</span> où <span
class="math inline">\(0 \leq \lambda \leq 1\)</span>. Cette propriété
est cruciale pour les algorithmes d’optimisation dans l’apprentissage
automatique. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie croisée est une mesure fondamentale en théorie de
l’information et en apprentissage automatique. Elle permet de quantifier
la divergence entre deux distributions de probabilité, ce qui est
essentiel pour l’évaluation et l’optimisation des modèles prédictifs.
Ses propriétés, telles que la positivité, l’inégalité de Gibbs, et la
convexité, en font un outil puissant pour diverses applications.</p>
</body>
</html>
{% include "footer.html" %}


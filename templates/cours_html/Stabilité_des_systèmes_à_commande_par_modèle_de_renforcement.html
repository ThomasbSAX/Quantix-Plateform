{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Stabilité des systèmes à commande par modèle de renforcement</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Stabilité des systèmes à commande par modèle de
renforcement</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les systèmes à commande par modèle de renforcement (ou Reinforcement
Learning, RL) représentent une avancée majeure dans le domaine de
l’intelligence artificielle et du contrôle automatique. Ces systèmes
permettent à un agent d’apprendre des stratégies optimales en
interagissant avec un environnement, en maximisant une récompense
cumulative. Cependant, la stabilité de ces systèmes reste un défi
fondamental.</p>
<p>L’étude de la stabilité des systèmes à commande par modèle de
renforcement est cruciale pour garantir que les agents apprennent des
politiques robustes et fiables. Cette stabilité est particulièrement
importante dans des applications critiques telles que la robotique, les
systèmes de transport intelligents et les réseaux électriques.</p>
<p>Dans cet article, nous explorons les concepts mathématiques
sous-jacents à la stabilité des systèmes de RL. Nous commençons par
définir formellement les notions clés, puis nous présentons les
théorèmes et propriétés qui garantissent la stabilité. Enfin, nous
discutons des implications pratiques de ces résultats.</p>
<h1 id="définitions">Définitions</h1>
<h2 id="environnement-de-markov">Environnement de Markov</h2>
<p>Pour comprendre la stabilité des systèmes à commande par modèle de
renforcement, il est essentiel de définir l’environnement dans lequel
l’agent évolue. Un environnement de Markov est un modèle mathématique
qui décrit les transitions entre états en fonction des actions de
l’agent.</p>
<div class="definition">
<p>Un environnement de Markov est un quadruplet <span
class="math inline">\((\mathcal{S}, \mathcal{A}, P, R)\)</span>, où
:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{S}\)</span> est l’ensemble
des états,</p></li>
<li><p><span class="math inline">\(\mathcal{A}\)</span> est l’ensemble
des actions,</p></li>
<li><p><span class="math inline">\(P: \mathcal{S} \times \mathcal{A}
\times \mathcal{S} \rightarrow [0,1]\)</span> est la fonction de
transition,</p></li>
<li><p><span class="math inline">\(R: \mathcal{S} \times \mathcal{A}
\times \mathcal{S} \rightarrow \mathbb{R}\)</span> est la fonction de
récompense.</p></li>
</ul>
</div>
<h2 id="fonction-de-valeur">Fonction de Valeur</h2>
<p>La fonction de valeur est une mesure de la qualité d’un état ou d’une
action en termes de récompense cumulative attendue.</p>
<div class="definition">
<p>La fonction de valeur <span class="math inline">\(V^\pi: \mathcal{S}
\rightarrow \mathbb{R}\)</span> pour une politique <span
class="math inline">\(\pi\)</span> est définie par : <span
class="math display">\[V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty}
\gamma^t R(s_t, a_t, s_{t+1}) \mid s_0 = s, a_t \sim \pi(\cdot |
s_t)\right]\]</span> où <span class="math inline">\(\gamma \in
[0,1]\)</span> est le facteur de discount.</p>
</div>
<h2 id="fonction-davantage">Fonction d’Avantage</h2>
<p>La fonction d’avantage mesure l’avantage relatif d’une action par
rapport à la politique actuelle.</p>
<div class="definition">
<p>La fonction d’avantage <span class="math inline">\(A^\pi: \mathcal{S}
\times \mathcal{A} \rightarrow \mathbb{R}\)</span> pour une politique
<span class="math inline">\(\pi\)</span> est définie par : <span
class="math display">\[A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)\]</span> où
<span class="math inline">\(Q^\pi(s, a)\)</span> est la fonction de
valeur d’action.</p>
</div>
<h1 id="théorèmes-de-stabilité">Théorèmes de Stabilité</h1>
<h2 id="théorème-de-convergence-de-la-fonction-de-valeur">Théorème de
Convergence de la Fonction de Valeur</h2>
<p>Le théorème de convergence de la fonction de valeur garantit que,
sous certaines conditions, la fonction de valeur converge vers sa valeur
optimale.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((\mathcal{S}, \mathcal{A}, P,
R)\)</span> un environnement de Markov. Si la fonction de transition
<span class="math inline">\(P\)</span> est ergodique et le facteur de
discount <span class="math inline">\(\gamma\)</span> satisfait <span
class="math inline">\(\gamma &lt; 1\)</span>, alors pour toute politique
<span class="math inline">\(\pi\)</span>, la fonction de valeur <span
class="math inline">\(V^\pi\)</span> converge vers sa valeur optimale
<span class="math inline">\(V^*\)</span>.</p>
</div>
<h2 id="théorème-de-convergence-de-la-politique-optimale">Théorème de
Convergence de la Politique Optimale</h2>
<p>Le théorème de convergence de la politique optimale garantit que,
sous certaines conditions, la politique optimale converge vers une
politique stable.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((\mathcal{S}, \mathcal{A}, P,
R)\)</span> un environnement de Markov. Si la fonction de transition
<span class="math inline">\(P\)</span> est ergodique et le facteur de
discount <span class="math inline">\(\gamma\)</span> satisfait <span
class="math inline">\(\gamma &lt; 1\)</span>, alors pour toute politique
<span class="math inline">\(\pi\)</span>, la politique optimale <span
class="math inline">\(\pi^*\)</span> converge vers une politique
stable.</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2
id="preuve-du-théorème-de-convergence-de-la-fonction-de-valeur">Preuve
du Théorème de Convergence de la Fonction de Valeur</h2>
<p>Pour prouver le théorème de convergence de la fonction de valeur,
nous utilisons les propriétés des opérateurs de Bellman.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’opérateur de Bellman <span
class="math inline">\(T^\pi\)</span> défini par : <span
class="math display">\[T^\pi V(s) = \mathbb{E}\left[R(s, a, s&#39;) +
\gamma V(s&#39;) \mid s, a \sim \pi(\cdot | s)\right]\]</span>
L’opérateur <span class="math inline">\(T^\pi\)</span> est un contrat,
ce qui signifie qu’il existe une unique fonction de valeur fixe <span
class="math inline">\(V^\pi\)</span> telle que : <span
class="math display">\[V^\pi = T^\pi V^\pi\]</span> Par le théorème de
Banach, la fonction de valeur <span class="math inline">\(V^\pi\)</span>
converge vers sa valeur optimale <span
class="math inline">\(V^*\)</span>. ◻</p>
</div>
<h2
id="preuve-du-théorème-de-convergence-de-la-politique-optimale">Preuve
du Théorème de Convergence de la Politique Optimale</h2>
<p>Pour prouver le théorème de convergence de la politique optimale,
nous utilisons les propriétés des opérateurs de Bellman optimaux.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’opérateur de Bellman optimal <span
class="math inline">\(T^*\)</span> défini par : <span
class="math display">\[T^* V(s) = \max_a \mathbb{E}\left[R(s, a, s&#39;)
+ \gamma V(s&#39;) \mid s\right]\]</span> L’opérateur <span
class="math inline">\(T^*\)</span> est également un contrat, ce qui
signifie qu’il existe une unique fonction de valeur fixe <span
class="math inline">\(V^*\)</span> telle que : <span
class="math display">\[V^* = T^* V^*\]</span> Par le théorème de Banach,
la politique optimale <span class="math inline">\(\pi^*\)</span>
converge vers une politique stable. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-contraction">Propriété de Contraction</h2>
<p>La propriété de contraction garantit que les itérations de
l’opérateur de Bellman convergent vers une solution unique.</p>
<div class="property">
<p>Pour tout <span class="math inline">\(V, V&#39; \in
\mathbb{R}^\mathcal{S}\)</span>, nous avons : <span
class="math display">\[\|T^\pi V - T^\pi V&#39;\|_\infty \leq \gamma \|V
- V&#39;\|_\infty\]</span></p>
</div>
<h2 id="corollaire-de-convergence">Corollaire de Convergence</h2>
<p>Le corollaire de convergence garantit que, sous certaines conditions,
les algorithmes de RL convergent vers des solutions optimales.</p>
<div class="corollary">
<p>Si la fonction de transition <span class="math inline">\(P\)</span>
est ergodique et le facteur de discount <span
class="math inline">\(\gamma\)</span> satisfait <span
class="math inline">\(\gamma &lt; 1\)</span>, alors les algorithmes de
RL convergent vers des solutions optimales.</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré les concepts mathématiques
sous-jacents à la stabilité des systèmes à commande par modèle de
renforcement. Nous avons défini formellement les notions clés, présenté
les théorèmes et propriétés qui garantissent la stabilité, et discuté
des implications pratiques de ces résultats. La compréhension de ces
concepts est essentielle pour développer des systèmes de RL robustes et
fiables.</p>
</body>
</html>
{% include "footer.html" %}


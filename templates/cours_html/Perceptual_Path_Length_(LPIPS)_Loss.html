{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Perceptual Path Length (LPIPS) Loss: A Deep Dive into Perceptual Metrics</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Perceptual Path Length (LPIPS) Loss: A Deep Dive into
Perceptual Metrics</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’évaluation des modèles de génération d’images est un défi
fondamental en apprentissage automatique. Les métriques traditionnelles
comme le Peak Signal-to-Noise Ratio (PSNR) ou la Structure Similarity
Index Measure (SSIM) se sont révélées inadéquates pour capturer les
nuances perceptuelles des images générées. C’est dans ce contexte que la
Perceptual Path Length (LPIPS) Loss émerge comme une solution
prometteuse.</p>
<p>La LPIPS Loss trouve son origine dans la nécessité de mesurer la
qualité des images générées par les modèles de réseaux de neurones, en
particulier les Generative Adversarial Networks (GANs). Elle est
indispensable pour évaluer la capacité d’un modèle à générer des images
qui ne sont pas seulement pixel-par-pixel similaires aux images réelles,
mais aussi perceptuellement cohérentes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la LPIPS Loss, il est essentiel de définir d’abord le
concept de distance perceptuelle. Imaginons que nous voulons mesurer la
différence entre deux images, non pas en termes de pixels, mais en
termes de ce que notre cerveau perçoit. Nous cherchons une mesure qui
capture les différences subtiles que l’œil humain peut détecter.</p>
<p>Formellement, la distance perceptuelle entre deux images <span
class="math inline">\(I_1\)</span> et <span
class="math inline">\(I_2\)</span> peut être définie comme suit :</p>
<p><span class="math display">\[d_{\text{perceptual}}(I_1, I_2) =
\sum_{l} w_l \cdot \| \phi_l(I_1) - \phi_l(I_2) \|_2\]</span></p>
<p>où <span class="math inline">\(\phi_l\)</span> représente les
caractéristiques extraites par le réseau de neurones à la couche <span
class="math inline">\(l\)</span>, et <span
class="math inline">\(w_l\)</span> est un poids associé à cette
couche.</p>
<p>La Perceptual Path Length (LPIPS) Loss est alors définie comme la
somme des distances perceptuelles le long d’un chemin dans l’espace
latent du modèle. Pour un chemin <span
class="math inline">\(\gamma(t)\)</span> paramétré par <span
class="math inline">\(t \in [0, 1]\)</span>, la LPIPS Loss est donnée
par :</p>
<p><span class="math display">\[L_{\text{LPIPS}} = \int_{0}^{1}
d_{\text{perceptual}}(\gamma(t), \gamma(t + \epsilon)) \,
dt\]</span></p>
<p>où <span class="math inline">\(\epsilon\)</span> est un petit
incrément.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la LPIPS Loss est le théorème de la
distance perceptuelle, qui stipule que pour deux images <span
class="math inline">\(I_1\)</span> et <span
class="math inline">\(I_2\)</span>, la distance perceptuelle est bornée
par la différence de leurs caractéristiques.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(I_1\)</span> et <span
class="math inline">\(I_2\)</span> deux images, et <span
class="math inline">\(\phi_l\)</span> les caractéristiques extraites par
un réseau de neurones à la couche <span
class="math inline">\(l\)</span>. Alors, la distance perceptuelle entre
<span class="math inline">\(I_1\)</span> et <span
class="math inline">\(I_2\)</span> est bornée par :</p>
<p><span class="math display">\[d_{\text{perceptual}}(I_1, I_2) \leq
\sum_{l} w_l \cdot \| \phi_l(I_1) - \phi_l(I_2) \|_{\infty}\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la distance perceptuelle, nous commençons
par rappeler que la norme <span class="math inline">\(L_2\)</span> est
bornée par la norme <span class="math inline">\(L_{\infty}\)</span>.
Donc, pour chaque couche <span class="math inline">\(l\)</span>, nous
avons :</p>
<p><span class="math display">\[\| \phi_l(I_1) - \phi_l(I_2) \|_2 \leq
\sqrt{n_l} \cdot \| \phi_l(I_1) - \phi_l(I_2) \|_{\infty}\]</span></p>
<p>où <span class="math inline">\(n_l\)</span> est le nombre de
caractéristiques à la couche <span class="math inline">\(l\)</span>. En
substituant cette inégalité dans la définition de la distance
perceptuelle, nous obtenons :</p>
<p><span class="math display">\[d_{\text{perceptual}}(I_1, I_2) \leq
\sum_{l} w_l \cdot \sqrt{n_l} \cdot \| \phi_l(I_1) - \phi_l(I_2)
\|_{\infty}\]</span></p>
<p>En supposant que <span class="math inline">\(w_l \cdot \sqrt{n_l}
\leq 1\)</span> pour toutes les couches <span
class="math inline">\(l\)</span>, nous avons :</p>
<p><span class="math display">\[d_{\text{perceptual}}(I_1, I_2) \leq
\sum_{l} w_l \cdot \| \phi_l(I_1) - \phi_l(I_2) \|_{\infty}\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La LPIPS Loss possède plusieurs propriétés intéressantes qui la
rendent adaptée à l’évaluation des modèles de génération d’images. Nous
en listons quelques-unes ci-dessous :</p>
<ol>
<li><p>La LPIPS Loss est invariante par translation et rotation. Cela
signifie que si deux images sont des translations ou des rotations l’une
de l’autre, leur LPIPS Loss sera nulle.</p></li>
<li><p>La LPIPS Loss est sensible aux changements de texture. Elle peut
détecter des différences subtiles dans les textures des images, ce qui
est crucial pour évaluer la qualité des images générées.</p></li>
<li><p>La LPIPS Loss est computativement efficace. Elle peut être
calculée rapidement grâce à l’utilisation des réseaux de neurones
pré-entraînés pour extraire les caractéristiques des images.</p></li>
</ol>
<p>Pour prouver la première propriété, nous remarquons que les réseaux
de neurones utilisés pour extraire les caractéristiques des images sont
généralement invariants par translation et rotation. Par conséquent, si
<span class="math inline">\(I_1\)</span> et <span
class="math inline">\(I_2\)</span> sont des translations ou des
rotations l’une de l’autre, leurs caractéristiques extraites seront
identiques, et donc leur distance perceptuelle sera nulle.</p>
</body>
</html>
{% include "footer.html" %}


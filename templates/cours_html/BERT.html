{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>BERT: Bidirectional Encoder Representations from Transformers</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">BERT: Bidirectional Encoder Representations from
Transformers</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, en particulier le traitement du langage
naturel (NLP), a connu une révolution avec l’émergence des modèles de
type Transformer. Parmi ces modèles, BERT (Bidirectional Encoder
Representations from Transformers) se distingue par sa capacité à
comprendre le contexte des mots dans les deux directions, ce qui lui
confère une puissance inégalée pour diverses tâches de NLP.</p>
<p>BERT est né de la nécessité de capturer les dépendances
bidirectionnelles dans le texte, un défi que les modèles précédents
comme Word2Vec ou GloVe ne pouvaient pas relever pleinement. En
utilisant une architecture de Transformer, BERT apprend des
représentations contextuelles profondes des mots dans un corpus non
annoté, puis les affine via une fine-tuning sur des tâches
spécifiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre BERT, il est essentiel de définir quelques concepts
clés. Commençons par le modèle Transformer, qui est au cœur de BERT.</p>
<h2 id="le-modèle-transformer">Le Modèle Transformer</h2>
<p>Un modèle Transformer est un type de réseau neuronal introduit par
Vaswani et al. en 2017. Il est conçu pour traiter des séquences de
données, comme du texte, en utilisant une architecture basée sur
l’attention. L’idée est de permettre au modèle de se concentrer sur
différentes parties de la séquence lorsqu’il traite un élément
donné.</p>
<p>Formellement, un Transformer est défini par les équations suivantes.
Soit <span class="math inline">\(X\)</span> une séquence d’entrée de
longueur <span class="math inline">\(n\)</span>, et soit <span
class="math inline">\(h_i\)</span> la représentation de l’élément <span
class="math inline">\(i\)</span>-ème. Le Transformer utilise des
mécanismes d’attention pour calculer une représentation contextuelle
<span class="math inline">\(z_i\)</span> pour chaque élément <span
class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[z_i = \sum_{j=1}^{n} \alpha_{ij}
h_j\]</span></p>
<p>où <span class="math inline">\(\alpha_{ij}\)</span> est le poids
d’attention entre les éléments <span class="math inline">\(i\)</span> et
<span class="math inline">\(j\)</span>, calculé comme:</p>
<p><span class="math display">\[\alpha_{ij} =
\frac{\exp(e_{ij})}{\sum_{k=1}^{n} \exp(e_{ik})}\]</span></p>
<p>et <span class="math inline">\(e_{ij}\)</span> est l’énergie
d’attention, calculée comme:</p>
<p><span class="math display">\[e_{ij} = v^T \tanh(W h_i + U
h_j)\]</span></p>
<h2 id="bert">BERT</h2>
<p>BERT est un modèle basé sur le Transformer, mais il est pré-entraîné
de manière bidirectionnelle. Cela signifie qu’il apprend à représenter
les mots en tenant compte de leur contexte à gauche et à droite
simultanément.</p>
<p>Formellement, BERT est défini comme un Transformer avec <span
class="math inline">\(L\)</span> couches. Soit <span
class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span> une séquence
d’entrée. BERT produit une représentation contextuelle <span
class="math inline">\(H = (h_1, h_2, ..., h_n)\)</span> pour chaque mot
dans la séquence.</p>
<p><span class="math display">\[H = \text{Transformer}(X)\]</span></p>
<p>où <span class="math inline">\(\text{Transformer}\)</span> est une
fonction qui applique <span class="math inline">\(L\)</span> couches de
Transformer à la séquence d’entrée.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>BERT est basé sur plusieurs théorèmes et propriétés clés des réseaux
de neurones et du traitement du langage naturel. Voici quelques-uns des
plus importants.</p>
<h2 id="théorème-de-lattention">Théorème de l’Attention</h2>
<p>Le mécanisme d’attention est au cœur du modèle Transformer. Le
théorème de l’attention stipule que pour toute séquence <span
class="math inline">\(X\)</span>, il existe une représentation
contextuelle <span class="math inline">\(Z\)</span> telle que:</p>
<p><span class="math display">\[Z = \text{Attention}(X)\]</span></p>
<p>où <span class="math inline">\(\text{Attention}\)</span> est une
fonction qui calcule les poids d’attention et produit une représentation
contextuelle.</p>
<h2 id="théorème-de-la-prétraînement">Théorème de la Prétraînement</h2>
<p>Le prétraînement bidirectionnel est une caractéristique clé de BERT.
Le théorème du prétraînement stipule que pour toute tâche de NLP, il
existe une représentation contextuelle <span
class="math inline">\(H\)</span> telle que:</p>
<p><span class="math display">\[H = \text{BERT}(X)\]</span></p>
<p>où <span class="math inline">\(\text{BERT}\)</span> est une fonction
qui applique un Transformer pré-entraîné de manière bidirectionnelle à
la séquence d’entrée.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’attention, nous devons montrer que pour
toute séquence <span class="math inline">\(X\)</span>, il existe une
représentation contextuelle <span class="math inline">\(Z\)</span> telle
que:</p>
<p><span class="math display">\[Z = \text{Attention}(X)\]</span></p>
<p>Nous commençons par définir les poids d’attention <span
class="math inline">\(\alpha_{ij}\)</span> comme:</p>
<p><span class="math display">\[\alpha_{ij} =
\frac{\exp(e_{ij})}{\sum_{k=1}^{n} \exp(e_{ik})}\]</span></p>
<p>où <span class="math inline">\(e_{ij}\)</span> est l’énergie
d’attention, calculée comme:</p>
<p><span class="math display">\[e_{ij} = v^T \tanh(W h_i + U
h_j)\]</span></p>
<p>Ensuite, nous calculons la représentation contextuelle <span
class="math inline">\(z_i\)</span> pour chaque élément <span
class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[z_i = \sum_{j=1}^{n} \alpha_{ij}
h_j\]</span></p>
<p>Ainsi, nous avons montré que pour toute séquence <span
class="math inline">\(X\)</span>, il existe une représentation
contextuelle <span class="math inline">\(Z\)</span> telle que:</p>
<p><span class="math display">\[Z = \text{Attention}(X)\]</span></p>
<p>Pour prouver le théorème du prétraînement, nous devons montrer que
pour toute tâche de NLP, il existe une représentation contextuelle <span
class="math inline">\(H\)</span> telle que:</p>
<p><span class="math display">\[H = \text{BERT}(X)\]</span></p>
<p>Nous commençons par définir BERT comme un Transformer avec <span
class="math inline">\(L\)</span> couches. Soit <span
class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span> une séquence
d’entrée. BERT produit une représentation contextuelle <span
class="math inline">\(H = (h_1, h_2, ..., h_n)\)</span> pour chaque mot
dans la séquence.</p>
<p><span class="math display">\[H = \text{Transformer}(X)\]</span></p>
<p>où <span class="math inline">\(\text{Transformer}\)</span> est une
fonction qui applique <span class="math inline">\(L\)</span> couches de
Transformer à la séquence d’entrée. Ainsi, nous avons montré que pour
toute tâche de NLP, il existe une représentation contextuelle <span
class="math inline">\(H\)</span> telle que:</p>
<p><span class="math display">\[H = \text{BERT}(X)\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>BERT possède plusieurs propriétés intéressantes qui en font un modèle
puissant pour le traitement du langage naturel. Voici quelques-unes des
plus importantes.</p>
<h2 id="propriété-de-la-bidirectionnalité">Propriété de la
Bidirectionnalité</h2>
<p>BERT est pré-entraîné de manière bidirectionnelle, ce qui signifie
qu’il apprend à représenter les mots en tenant compte de leur contexte à
gauche et à droite simultanément. Cette propriété est cruciale pour
capturer les dépendances complexes dans le texte.</p>
<h2 id="propriété-de-la-fine-tuning">Propriété de la Fine-Tuning</h2>
<p>BERT peut être affiné sur des tâches spécifiques en ajoutant une
couche de sortie adaptée à la tâche. Cette propriété permet à BERT
d’être utilisé pour une grande variété de tâches de NLP, comme la
classification de texte, la reconnaissance d’entités nommées, et la
réponse aux questions.</p>
<h2 id="corollaire-de-lefficacité">Corollaire de l’Efficacité</h2>
<p>Grâce à sa capacité à capturer les dépendances bidirectionnelles et à
être affiné sur des tâches spécifiques, BERT est un modèle extrêmement
efficace pour le traitement du langage naturel. Il a établi de nouveaux
records sur plusieurs benchmarks de NLP.</p>
<h1 id="conclusion">Conclusion</h1>
<p>BERT est un modèle révolutionnaire pour le traitement du langage
naturel. Grâce à son architecture de Transformer et à son prétraînement
bidirectionnel, il capture les dépendances complexes dans le texte de
manière efficace. Ses propriétés de fine-tuning et d’efficacité en font
un outil puissant pour une grande variété de tâches de NLP.</p>
</body>
</html>
{% include "footer.html" %}


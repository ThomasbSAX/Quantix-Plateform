{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>V-Measure: Une Métrique pour l’Évaluation de la Classification Non Supervisée</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">V-Measure: Une Métrique pour l’Évaluation de la
Classification Non Supervisée</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La classification non supervisée est une tâche fondamentale en
apprentissage automatique, où l’objectif est de regrouper des données en
clusters sans avoir recours à des étiquettes prédéfinies. L’évaluation
de la qualité de ces classifications pose un défi majeur, surtout en
l’absence de vérité terrain. La V-Measure est une métrique proposée par
Andrew Rosenberg et Julia Hirschberg en 2007 pour répondre à ce besoin.
Elle mesure la similarité entre deux partitions d’un ensemble de données
en évaluant la cohésion et la séparation des clusters.</p>
<p>La V-Measure est particulièrement utile dans les contextes où
l’interprétation des clusters est cruciale. Elle combine deux mesures :
la complétude et l’homogénéité. La complétude évalue si tous les
éléments d’une même classe sont regroupés dans le même cluster, tandis
que l’homogénéité vérifie si un cluster contient principalement des
éléments d’une seule classe. La V-Measure est donc une métrique robuste
et interprétable, indispensable pour l’évaluation des algorithmes de
clustering.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la V-Measure, nous devons d’abord définir les
concepts de complétude et d’homogénéité.</p>
<h2 id="complétude">Complétude</h2>
<p>La complétude mesure dans quelle mesure tous les éléments d’une même
classe sont regroupés dans le même cluster. Intuitivement, une
classification est complète si tous les éléments d’une classe donnée
sont regroupés dans un seul cluster.</p>
<p>Formellement, soit <span class="math inline">\(C\)</span> une
partition de vérité terrain et <span class="math inline">\(K\)</span>
une partition obtenue par un algorithme de clustering. La complétude
peut être définie comme suit :</p>
<p><span class="math display">\[\text{Complétude}(C, K) =
\sum_{i=1}^{|C|} \frac{1}{|C_i|} \log \left(
\frac{|C_i|}{\sum_{j=1}^{|K|} \frac{|C_i \cap K_j|^2}{|K_j|}}
\right)\]</span></p>
<p>où <span class="math inline">\(C_i\)</span> est le <span
class="math inline">\(i\)</span>-ème cluster de la partition <span
class="math inline">\(C\)</span>, et <span
class="math inline">\(K_j\)</span> est le <span
class="math inline">\(j\)</span>-ème cluster de la partition <span
class="math inline">\(K\)</span>.</p>
<h2 id="homogénéité">Homogénéité</h2>
<p>L’homogénéité mesure dans quelle mesure un cluster contient
principalement des éléments d’une seule classe. Intuitivement, une
classification est homogène si tous les éléments dans un cluster
appartiennent à la même classe.</p>
<p>Formellement, l’homogénéité peut être définie comme suit :</p>
<p><span class="math display">\[\text{Homogénéité}(C, K) =
\sum_{j=1}^{|K|} \frac{1}{|K_j|} \log \left(
\frac{|K_j|}{\sum_{i=1}^{|C|} \frac{|C_i \cap K_j|^2}{|C_i|}}
\right)\]</span></p>
<p>où <span class="math inline">\(C_i\)</span> est le <span
class="math inline">\(i\)</span>-ème cluster de la partition <span
class="math inline">\(C\)</span>, et <span
class="math inline">\(K_j\)</span> est le <span
class="math inline">\(j\)</span>-ème cluster de la partition <span
class="math inline">\(K\)</span>.</p>
<h2 id="v-measure">V-Measure</h2>
<p>La V-Measure combine la complétude et l’homogénéité pour évaluer la
qualité d’une classification non supervisée. Elle est définie comme la
moyenne harmonique de la complétude et de l’homogénéité.</p>
<p>Formellement, la V-Measure est donnée par :</p>
<p><span class="math display">\[\text{V-Measure}(C, K) = 2 \cdot
\frac{\text{Complétude}(C, K) \cdot \text{Homogénéité}(C,
K)}{\text{Complétude}(C, K) + \text{Homogénéité}(C, K)}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-v-measure">Théorème de la V-Measure</h2>
<p>Le théorème suivant montre que la V-Measure est une métrique valide
pour évaluer la qualité d’une classification non supervisée.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> une partition de vérité
terrain et <span class="math inline">\(K\)</span> une partition obtenue
par un algorithme de clustering. La V-Measure satisfait les propriétés
suivantes :</p>
<ul>
<li><p><span class="math inline">\(0 \leq \text{V-Measure}(C, K) \leq
1\)</span></p></li>
<li><p><span class="math inline">\(\text{V-Measure}(C, K) = 1\)</span>
si et seulement si <span class="math inline">\(C = K\)</span></p></li>
<li><p><span class="math inline">\(\text{V-Measure}(C, K) = 0\)</span>
si et seulement si les partitions <span class="math inline">\(C\)</span>
et <span class="math inline">\(K\)</span> sont indépendantes.</p></li>
</ul>
</div>
<h2 id="démonstration-du-théorème-de-la-v-measure">Démonstration du
Théorème de la V-Measure</h2>
<p>Pour démontrer ce théorème, nous devons analyser chaque propriété
séparément.</p>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Pour montrer que <span class="math inline">\(0 \leq
\text{V-Measure}(C, K) \leq 1\)</span>, nous utilisons le fait que la
complétude et l’homogénéité sont toutes deux comprises entre 0 et 1. La
moyenne harmonique de deux nombres compris entre 0 et 1 est également
comprise entre 0 et 1.</p></li>
<li><p>Pour montrer que <span class="math inline">\(\text{V-Measure}(C,
K) = 1\)</span> si et seulement si <span class="math inline">\(C =
K\)</span>, nous observons que si <span class="math inline">\(C =
K\)</span>, alors la complétude et l’homogénéité sont toutes deux égales
à 1, ce qui implique que la V-Measure est égale à 1. Réciproquement, si
la V-Measure est égale à 1, alors la complétude et l’homogénéité doivent
être égales à 1, ce qui implique que <span class="math inline">\(C =
K\)</span>.</p></li>
<li><p>Pour montrer que <span class="math inline">\(\text{V-Measure}(C,
K) = 0\)</span> si et seulement si les partitions <span
class="math inline">\(C\)</span> et <span
class="math inline">\(K\)</span> sont indépendantes, nous utilisons le
fait que la complétude et l’homogénéité sont toutes deux égales à 0 si
les partitions sont indépendantes. Réciproquement, si la V-Measure est
égale à 0, alors la complétude et l’homogénéité doivent être égales à 0,
ce qui implique que les partitions sont indépendantes.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-de-la-v-measure">Propriétés de la V-Measure</h2>
<p>La V-Measure possède plusieurs propriétés importantes qui en font une
métrique robuste pour l’évaluation de la classification non
supervisée.</p>
<ol>
<li><p>La V-Measure est symétrique. Cela signifie que <span
class="math inline">\(\text{V-Measure}(C, K) = \text{V-Measure}(K,
C)\)</span>.</p></li>
<li><p>La V-Measure est invariante par permutation des clusters. Cela
signifie que la V-Measure reste inchangée si les étiquettes des clusters
sont permutées.</p></li>
<li><p>La V-Measure est sensible à la taille des clusters. Cela signifie
que la V-Measure prend en compte la taille des clusters lors de
l’évaluation de la qualité de la classification.</p></li>
</ol>
<h2 id="corollaires-de-la-v-measure">Corollaires de la V-Measure</h2>
<p>Les corollaires suivants découlent des propriétés de la
V-Measure.</p>
<div class="corollary">
<p>Si deux partitions <span class="math inline">\(C\)</span> et <span
class="math inline">\(K\)</span> sont identiques, alors la V-Measure est
égale à 1.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ceci découle directement du théorème de la V-Measure,
qui montre que <span class="math inline">\(\text{V-Measure}(C, K) =
1\)</span> si et seulement si <span class="math inline">\(C =
K\)</span>. ◻</p>
</div>
<div class="corollary">
<p>Si deux partitions <span class="math inline">\(C\)</span> et <span
class="math inline">\(K\)</span> sont indépendantes, alors la V-Measure
est égale à 0.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ceci découle également du théorème de la V-Measure,
qui montre que <span class="math inline">\(\text{V-Measure}(C, K) =
0\)</span> si et seulement si les partitions <span
class="math inline">\(C\)</span> et <span
class="math inline">\(K\)</span> sont indépendantes. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La V-Measure est une métrique puissante et interprétable pour
l’évaluation de la classification non supervisée. Elle combine la
complétude et l’homogénéité pour fournir une mesure robuste de la
qualité d’une classification. Les propriétés et les corollaires de la
V-Measure en font un outil indispensable pour l’analyse des algorithmes
de clustering.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Jensen-Shannon : Une Mesure d’Information Discriminante</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Jensen-Shannon : Une Mesure d’Information
Discriminante</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’information mesurable, quantifiable, est une notion centrale en
théorie des probabilités et en traitement du signal. Parmi les outils
permettant de comparer des distributions de probabilité, la distance de
Jensen-Shannon émerge comme une mesure à la fois élégante et puissante.
Introduite par Lin (1991) puis popularisée par des travaux ultérieurs,
cette distance résulte d’un besoin fondamental : comparer des
distributions de manière symétrique et normalisée. Son origine
historique remonte aux travaux sur la divergence de Kullback-Leibler,
mais c’est sa normalisation qui lui confère des propriétés uniques.</p>
<p>La distance de Jensen-Shannon est indispensable dans divers domaines
: apprentissage automatique, bioinformatique, ou encore traitement
d’images. Elle permet de mesurer la similarité entre distributions tout
en étant bornée, ce qui facilite son interprétation et son utilisation
dans des algorithmes d’optimisation. Son émergence répond à un problème
crucial : comment comparer des distributions de manière robuste et
interprétable, sans être confronté aux problèmes de divergence infinie
ou d’asymétrie.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la distance de Jensen-Shannon, commençons par
comprendre ce que nous cherchons à mesurer. Imaginons deux distributions
de probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Nous voulons quantifier à quel point
ces deux distributions sont similaires. Une première idée serait
d’utiliser la divergence de Kullback-Leibler, mais celle-ci est
asymétrique et peut être infinie. Nous cherchons donc une mesure
symétrique, bornée, et qui capture efficacement la dissimilarité entre
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<p>La distance de Jensen-Shannon est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes ou continues définies sur un espace <span
class="math inline">\(X\)</span>. La distance de Jensen-Shannon entre
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[JS(P || Q) = \frac{1}{2} D_{KL}(P || M) +
\frac{1}{2} D_{KL}(Q || M)\]</span> où <span class="math inline">\(M =
\frac{1}{2}(P + Q)\)</span> est la distribution moyenne de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, et <span
class="math inline">\(D_{KL}\)</span> est la divergence de
Kullback-Leibler.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[JS(P || Q) = H(M) - \frac{1}{2} H(P) -
\frac{1}{2} H(Q)\]</span> où <span class="math inline">\(H\)</span>
désigne l’entropie de Shannon.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Jensen-Shannon est celui
de sa bornitude. Nous cherchons à montrer que cette distance est
toujours comprise entre 0 et 1, ce qui en fait une mesure
normalisée.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité.
Alors : <span class="math display">\[0 \leq JS(P || Q) \leq
1\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons les
propriétés de la divergence de Kullback-Leibler et de l’entropie.
D’abord, notons que <span class="math inline">\(JS(P || Q) \geq
0\)</span> car la divergence de Kullback-Leibler est toujours non
négative.</p>
<p>Ensuite, nous montrons que <span class="math inline">\(JS(P || Q)
\leq 1\)</span>. Considérons l’entropie conjointe <span
class="math inline">\(H(P, Q)\)</span>. Nous avons : <span
class="math display">\[H(M) = H\left(\frac{1}{2}P + \frac{1}{2}Q\right)
\leq \frac{1}{2}H(P) + \frac{1}{2}H(Q)\]</span> Cette inégalité découle
de la concavité de l’entropie. En réarrangeant les termes, nous obtenons
: <span class="math display">\[H(M) - \frac{1}{2}H(P) - \frac{1}{2}H(Q)
\leq 0\]</span> Ce qui implique : <span class="math display">\[JS(P ||
Q) = H(M) - \frac{1}{2}H(P) - \frac{1}{2}H(Q) \leq 1\]</span> car
l’entropie est bornée par <span
class="math inline">\(\log(|X|)\)</span>, où <span
class="math inline">\(|X|\)</span> est le cardinal de l’espace <span
class="math inline">\(X\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer les propriétés de la distance de Jensen-Shannon,
considérons un exemple simple. Soient <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions binaires définies
par : <span class="math display">\[P = (p, 1-p) \quad \text{et} \quad Q
= (q, 1-q)\]</span> Calculons <span class="math inline">\(JS(P ||
Q)\)</span>.</p>
<p>D’abord, calculons la distribution moyenne <span
class="math inline">\(M\)</span> : <span class="math display">\[M =
\frac{1}{2}(P + Q) = \left(\frac{p+q}{2}, 1 -
\frac{p+q}{2}\right)\]</span></p>
<p>Ensuite, calculons les divergences de Kullback-Leibler : <span
class="math display">\[D_{KL}(P || M) = p
\log\left(\frac{2p}{p+q}\right) + (1-p) \log\left(\frac{2(1-p)}{2 - p -
q}\right)\]</span> <span class="math display">\[D_{KL}(Q || M) = q
\log\left(\frac{2q}{p+q}\right) + (1-q) \log\left(\frac{2(1-q)}{2 - p -
q}\right)\]</span></p>
<p>Enfin, la distance de Jensen-Shannon est : <span
class="math display">\[JS(P || Q) = \frac{1}{2} D_{KL}(P || M) +
\frac{1}{2} D_{KL}(Q || M)\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distance de Jensen-Shannon possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(JS(P || Q) = JS(Q ||
P)\)</span>.</p></li>
<li><p>Bornitude : <span class="math inline">\(0 \leq JS(P || Q) \leq
1\)</span>.</p></li>
<li><p>Identité : <span class="math inline">\(JS(P || P) =
0\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la symétrie, nous utilisons les
propriétés de la divergence de Kullback-Leibler. Nous avons : <span
class="math display">\[JS(P || Q) = \frac{1}{2} D_{KL}(P || M) +
\frac{1}{2} D_{KL}(Q || M)\]</span> <span class="math display">\[JS(Q ||
P) = \frac{1}{2} D_{KL}(Q || M) + \frac{1}{2} D_{KL}(P || M)\]</span>
Ainsi, <span class="math inline">\(JS(P || Q) = JS(Q || P)\)</span>.</p>
<p>Pour la bornitude, nous avons déjà démontré que <span
class="math inline">\(0 \leq JS(P || Q) \leq 1\)</span>.</p>
<p>Pour l’identité, si <span class="math inline">\(P = Q\)</span>, alors
<span class="math inline">\(M = P = Q\)</span>. Par conséquent : <span
class="math display">\[JS(P || P) = \frac{1}{2} D_{KL}(P || P) +
\frac{1}{2} D_{KL}(P || P) = 0\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de Jensen-Shannon est un outil puissant pour comparer des
distributions de probabilité. Son élégance mathématique et ses
propriétés avantageuses en font un choix privilégié dans de nombreuses
applications. En comprenant ses fondements théoriques et ses
implications pratiques, nous pouvons mieux appréhender les défis de la
mesure de l’information et de la similarité entre distributions.</p>
</body>
</html>
{% include "footer.html" %}


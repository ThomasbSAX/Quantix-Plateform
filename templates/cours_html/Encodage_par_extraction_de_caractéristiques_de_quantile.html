{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage par extraction de caractéristiques de quantile : une approche robuste pour l’apprentissage automatique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage par extraction de caractéristiques de
quantile : une approche robuste pour l’apprentissage automatique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage des variables catégorielles est une étape cruciale dans de
nombreux problèmes d’apprentissage automatique. Les méthodes
traditionnelles, telles que l’encodage one-hot ou l’encodage moyen,
présentent des limitations importantes. L’encodage par extraction de
caractéristiques de quantile (Quantile Encoding) émerge comme une
alternative robuste, combinant la puissance des statistiques
descriptives et la capacité à capturer les relations sous-jacentes entre
les variables.</p>
<p>L’idée centrale derrière le Quantile Encoding est de transformer une
variable catégorielle en un ensemble de caractéristiques numériques
basées sur les quantiles d’une variable cible. Cette approche permet de
préserver l’information statistique tout en réduisant la dimensionnalité
et en minimisant les risques de surajustement.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de formaliser le concept d’encodage par extraction de
caractéristiques de quantile, examinons les éléments clés qui le
composent.</p>
<h2 id="quantiles">Quantiles</h2>
<p>Les quantiles sont des valeurs qui divisent un ensemble de données en
parties égales. Pour une variable aléatoire <span
class="math inline">\(X\)</span> avec une fonction de répartition <span
class="math inline">\(F_X(x) = P(X \leq x)\)</span>, le quantile d’ordre
<span class="math inline">\(p \in [0, 1]\)</span> est défini comme :</p>
<p><span class="math display">\[Q_X(p) = \inf \{ x \in \mathbb{R} :
F_X(x) \geq p \}\]</span></p>
<p>En d’autres termes, le quantile <span
class="math inline">\(Q_X(p)\)</span> est la plus petite valeur <span
class="math inline">\(x\)</span> telle que la probabilité que <span
class="math inline">\(X\)</span> soit inférieur ou égal à <span
class="math inline">\(x\)</span> est au moins <span
class="math inline">\(p\)</span>.</p>
<h2
id="encodage-par-extraction-de-caractéristiques-de-quantile">Encodage
par extraction de caractéristiques de quantile</h2>
<p>Considérons un ensemble de données <span
class="math inline">\(\mathcal{D} = \{ (x_i, y_i) \}_{i=1}^n\)</span>,
où <span class="math inline">\(x_i \in \mathcal{C}\)</span> est une
variable catégorielle et <span class="math inline">\(y_i \in
\mathbb{R}\)</span> est une variable cible. L’encodage par extraction de
caractéristiques de quantile consiste à transformer chaque catégorie
<span class="math inline">\(c \in \mathcal{C}\)</span> en un vecteur de
caractéristiques basé sur les quantiles de <span
class="math inline">\(y_i\)</span> pour cette catégorie.</p>
<p>Formellement, pour un ensemble de niveaux de quantile <span
class="math inline">\(P = \{ p_1, \ldots, p_k \} \subseteq [0,
1]\)</span>, l’encodage de la catégorie <span
class="math inline">\(c\)</span> est défini comme :</p>
<p><span class="math display">\[E(c) = (Q_{Y|C=c}(p_1), \ldots,
Q_{Y|C=c}(p_k))\]</span></p>
<p>où <span class="math inline">\(Q_{Y|C=c}(p_j)\)</span> est le
quantile d’ordre <span class="math inline">\(p_j\)</span> de la variable
cible <span class="math inline">\(Y\)</span> conditionnellement à ce que
la variable catégorielle <span class="math inline">\(C\)</span> prenne
la valeur <span class="math inline">\(c\)</span>.</p>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<h2 id="consistance-de-lencodage">Consistance de l’encodage</h2>
<p>Un des avantages majeurs du Quantile Encoding est sa capacité à
préserver les relations statistiques entre la variable catégorielle et
la variable cible. Nous formalisons cette propriété dans le théorème
suivant.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D} = \{ (x_i, y_i)
\}_{i=1}^n\)</span> un ensemble de données i.i.d. et <span
class="math inline">\(c \in \mathcal{C}\)</span> une catégorie donnée.
Si <span class="math inline">\(n_c \to \infty\)</span> lorsque <span
class="math inline">\(n \to \infty\)</span>, où <span
class="math inline">\(n_c\)</span> est le nombre d’observations dans la
catégorie <span class="math inline">\(c\)</span>, alors :</p>
<p><span class="math display">\[\lim_{n \to \infty}
Q_{\widehat{Y|C=c}}(p) = Q_{Y|C=c}(p) \quad \text{p.s.}\]</span></p>
<p>où <span class="math inline">\(\widehat{Y|C=c}\)</span> est la
distribution empirique de <span class="math inline">\(Y\)</span>
conditionnellement à <span class="math inline">\(C = c\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle du théorème de Glivenko-Cantelli,
qui garantit la convergence uniforme des fonctions de répartition
empiriques vers les fonctions de répartition théoriques lorsque <span
class="math inline">\(n \to \infty\)</span>. Puisque les quantiles sont
des fonctionnelles continues des fonctions de répartition, la
convergence des quantiles empiriques vers les quantiles théoriques
s’ensuit. ◻</p>
</div>
<h2 id="stabilité-de-lencodage">Stabilité de l’encodage</h2>
<p>Une autre propriété importante du Quantile Encoding est sa stabilité
face aux variations des données. Nous formalisons cette propriété dans
le théorème suivant.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D}_1 = \{ (x_i, y_i)
\}_{i=1}^n\)</span> et <span class="math inline">\(\mathcal{D}_2 = \{
(x_i&#39;, y_i&#39;) \}_{i=1}^m\)</span> deux ensembles de données
i.i.d. issus de la même distribution, et <span class="math inline">\(c
\in \mathcal{C}\)</span> une catégorie donnée. Alors :</p>
<p><span class="math display">\[\lim_{n, m \to \infty} \|
E_{\mathcal{D}_1}(c) - E_{\mathcal{D}_2}(c) \|_2 = 0 \quad
\text{p.s.}\]</span></p>
<p>où <span class="math inline">\(E_{\mathcal{D}}(c)\)</span> désigne
l’encodage de la catégorie <span class="math inline">\(c\)</span> basé
sur l’ensemble de données <span
class="math inline">\(\mathcal{D}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle de la loi des grands nombres et du
théorème central limite. En effet, les quantiles empiriques sont des
estimateurs consistants et asymptotiquement normaux des quantiles
théoriques. Par conséquent, la différence entre deux encodages basés sur
des ensembles de données suffisamment grands tend vers zéro. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-réduction-de-dimensionnalité">Propriété de
réduction de dimensionnalité</h2>
<p>Le Quantile Encoding permet de réduire la dimensionnalité des données
tout en préservant l’information statistique. Cette propriété est
formalisée dans le corollaire suivant.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(\mathcal{C}\)</span> un ensemble de
catégories et <span class="math inline">\(P = \{ p_1, \ldots, p_k
\}\)</span> un ensemble de niveaux de quantile. L’encodage par
extraction de caractéristiques de quantile transforme chaque catégorie
<span class="math inline">\(c \in \mathcal{C}\)</span> en un vecteur de
dimension <span class="math inline">\(k\)</span>, où <span
class="math inline">\(k \ll |\mathcal{C}|\)</span> en général.</p>
</div>
<h2 id="propriété-de-robustesse-au-surajustement">Propriété de
robustesse au surajustement</h2>
<p>Le Quantile Encoding est moins sujet au surajustement que les
méthodes d’encodage traditionnelles, telles que l’encodage one-hot.
Cette propriété est formalisée dans le corollaire suivant.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(\mathcal{D} = \{ (x_i, y_i)
\}_{i=1}^n\)</span> un ensemble de données et <span
class="math inline">\(\mathcal{D}_{\text{test}} = \{ (x_i&#39;,
y_i&#39;) \}_{i=1}^{n&#39;}\)</span> un ensemble de données de test.
L’encodage par extraction de caractéristiques de quantile basé sur <span
class="math inline">\(\mathcal{D}\)</span> tend à généraliser mieux que
l’encodage one-hot sur <span
class="math inline">\(\mathcal{D}_{\text{test}}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’encodage one-hot introduit une dimensionnalité
élevée, ce qui peut entraîner un surajustement. En revanche, le Quantile
Encoding réduit la dimensionnalité et capture les relations statistiques
sous-jacentes, ce qui améliore la généralisation. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de quantile est une
méthode puissante et robuste pour traiter les variables catégorielles
dans les problèmes d’apprentissage automatique. En combinant les
statistiques descriptives et la capacité à capturer les relations
sous-jacentes entre les variables, cette approche offre une alternative
prometteuse aux méthodes traditionnelles. Les propriétés de consistance,
de stabilité, de réduction de dimensionnalité et de robustesse au
surajustement en font un outil précieux pour les praticiens et les
chercheurs.</p>
</body>
</html>
{% include "footer.html" %}


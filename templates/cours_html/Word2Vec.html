{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Word2Vec : Une Révolution dans la Représentation des Mots</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Word2Vec : Une Révolution dans la Représentation des
Mots</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le traitement automatique du langage naturel (TALN) a toujours été un
domaine fascinant et complexe. L’un des défis majeurs dans ce domaine
est la représentation des mots de manière à capturer leur sens et leurs
relations sémantiques. Avant l’ère des modèles modernes, les mots
étaient souvent représentés par des vecteurs de caractéristiques
binaires ou des indices dans un dictionnaire. Cependant, ces méthodes ne
capturaient pas les relations subtiles entre les mots.</p>
<p>C’est dans ce contexte que Word2Vec, développé par Tomas Mikolov et
ses collègues chez Google en 2013, a révolutionné le domaine. Word2Vec
est une méthode efficace pour apprendre des représentations vectorielles
de mots à partir de grands corpus textuels. Ces représentations, souvent
appelées embeddings, permettent de capturer les relations sémantiques et
syntaxiques entre les mots. Par exemple, la relation entre "roi" et
"reine" peut être capturée de manière similaire à celle entre "homme" et
"femme".</p>
<p>Dans cet article, nous explorerons les concepts fondamentaux de
Word2Vec, ses différentes architectures, et ses applications pratiques.
Nous verrons comment cette méthode a permis des avancées significatives
dans le TALN et d’autres domaines connexes.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre certains concepts clés.</p>
<h2 id="embeddings-de-mots">Embeddings de Mots</h2>
<p>Considérons un corpus textuel <span class="math inline">\(C\)</span>
composé d’un ensemble de mots <span class="math inline">\(W = \{w_1,
w_2, \dots, w_n\}\)</span>. Notre objectif est de représenter chaque mot
<span class="math inline">\(w_i\)</span> par un vecteur <span
class="math inline">\(v_{w_i} \in \mathbb{R}^d\)</span>, où <span
class="math inline">\(d\)</span> est la dimension de l’espace
d’embedding. Ces vecteurs doivent capturer les relations sémantiques et
syntaxiques entre les mots.</p>
<p>Formellement, un embedding de mot est une fonction <span
class="math inline">\(\phi: W \rightarrow \mathbb{R}^d\)</span> qui
associe à chaque mot un vecteur dans un espace euclidien de dimension
<span class="math inline">\(d\)</span>.</p>
<h2 id="modèle-de-prédiction-de-contexte">Modèle de Prédiction de
Contexte</h2>
<p>L’idée centrale derrière Word2Vec est d’apprendre les embeddings de
mots en prédisant le contexte d’un mot donné ou vice versa. Il existe
deux architectures principales pour Word2Vec : Skip-gram et Continuous
Bag of Words (CBOW).</p>
<h1 id="architectures-de-word2vec">Architectures de Word2Vec</h1>
<h2 id="skip-gram">Skip-gram</h2>
<p>Le modèle Skip-gram prend un mot comme entrée et prédit les mots qui
l’entourent dans le corpus. Formellement, donné un mot <span
class="math inline">\(w_t\)</span> à la position <span
class="math inline">\(t\)</span>, le modèle prédit les mots <span
class="math inline">\(w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots,
w_{t+k}\)</span> pour une fenêtre de contexte de taille <span
class="math inline">\(k\)</span>.</p>
<p>Le modèle est entraîné en maximisant la probabilité conjointe des
mots de contexte donnés le mot central :</p>
<p><span class="math display">\[\max_{\phi} \prod_{t=1}^T \prod_{-k \leq
j \leq k, j \neq 0} p(w_{t+j} | w_t)\]</span></p>
<p>où <span class="math inline">\(p(w_{t+j} | w_t)\)</span> est la
probabilité de prédire le mot <span
class="math inline">\(w_{t+j}\)</span> donné le mot central <span
class="math inline">\(w_t\)</span>.</p>
<h2 id="continuous-bag-of-words-cbow">Continuous Bag of Words
(CBOW)</h2>
<p>Le modèle CBOW prend les mots de contexte comme entrée et prédit le
mot central. Formellement, donné un ensemble de mots de contexte <span
class="math inline">\(\{w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots,
w_{t+k}\}\)</span>, le modèle prédit le mot central <span
class="math inline">\(w_t\)</span>.</p>
<p>Le modèle est entraîné en maximisant la probabilité conjointe des
mots centraux donnés les mots de contexte :</p>
<p><span class="math display">\[\max_{\phi} \prod_{t=1}^T p(w_t |
\{w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+k}\})\]</span></p>
<p>où <span class="math inline">\(p(w_t | \{w_{t-k}, \dots, w_{t-1},
w_{t+1}, \dots, w_{t+k}\})\)</span> est la probabilité de prédire le mot
central <span class="math inline">\(w_t\)</span> donné les mots de
contexte.</p>
<h1 id="théorèmes-et-preuves">Théorèmes et Preuves</h1>
<h2 id="théorème-de-la-relation-linéaire">Théorème de la Relation
Linéaire</h2>
<p>L’un des résultats les plus fascinants de Word2Vec est que les
relations sémantiques entre les mots peuvent être capturées par des
opérations linéaires sur leurs embeddings. Par exemple, la relation
entre "roi" et "reine" peut être capturée de manière similaire à celle
entre "homme" et "femme".</p>
<p>Formellement, si <span class="math inline">\(v_{\text{roi}}\)</span>,
<span class="math inline">\(v_{\text{reine}}\)</span>, <span
class="math inline">\(v_{\text{homme}}\)</span>, et <span
class="math inline">\(v_{\text{femme}}\)</span> sont les embeddings
respectifs des mots "roi", "reine", "homme", et "femme", alors :</p>
<p><span class="math display">\[v_{\text{roi}} - v_{\text{homme}}
\approx v_{\text{reine}} - v_{\text{femme}}\]</span></p>
<h2 id="preuve-du-théorème-de-la-relation-linéaire">Preuve du Théorème
de la Relation Linéaire</h2>
<p>La preuve de ce théorème repose sur l’hypothèse que les embeddings de
mots capturent les relations sémantiques. En effet, si les embeddings
sont appris à partir de grands corpus textuels, ils peuvent capturer des
relations subtiles entre les mots.</p>
<p>Considérons un corpus textuel où les mots "roi" et "homme"
apparaissent souvent dans des contextes similaires, tout comme les mots
"reine" et "femme". Les embeddings de ces mots seront donc proches dans
l’espace vectoriel. Par conséquent, la différence entre les embeddings
de "roi" et "homme" sera similaire à la différence entre les embeddings
de "reine" et "femme".</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-la-similarité-cosinus">Propriété de la Similarité
Cosinus</h2>
<p>Les embeddings de mots appris par Word2Vec permettent de calculer la
similarité entre les mots en utilisant la similarité cosinus. La
similarité cosinus entre deux vecteurs <span
class="math inline">\(v_{w_i}\)</span> et <span
class="math inline">\(v_{w_j}\)</span> est définie comme :</p>
<p><span class="math display">\[\text{similarité}(v_{w_i}, v_{w_j}) =
\frac{v_{w_i} \cdot v_{w_j}}{\|v_{w_i}\| \|v_{w_j}\|}\]</span></p>
<p>où <span class="math inline">\(\cdot\)</span> désigne le produit
scalaire et <span class="math inline">\(\| \cdot \|\)</span> désigne la
norme euclidienne.</p>
<h2 id="corollaire-de-la-similarité-cosinus">Corollaire de la Similarité
Cosinus</h2>
<p>La similarité cosinus entre les embeddings de mots peut être utilisée
pour trouver des mots similaires dans un corpus. Par exemple, si nous
voulons trouver les mots les plus similaires à "chat", nous pouvons
calculer la similarité cosinus entre l’embedding de "chat" et les
embeddings des autres mots dans le corpus.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Word2Vec a révolutionné la représentation des mots dans le traitement
automatique du langage naturel. En apprenant des embeddings de mots à
partir de grands corpus textuels, Word2Vec permet de capturer les
relations sémantiques et syntaxiques entre les mots. Les architectures
Skip-gram et CBOW offrent des méthodes efficaces pour entraîner ces
embeddings, tandis que le théorème de la relation linéaire démontre la
puissance de cette approche.</p>
<p>À l’avenir, les recherches sur Word2Vec et les embeddings de mots
continueront d’évoluer, ouvrant la voie à de nouvelles applications et
améliorations dans le domaine du TALN.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Spatial Dropout : Une Technique Avancée de Régularisation pour les Réseaux Neuronaux</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Spatial Dropout : Une Technique Avancée de
Régularisation pour les Réseaux Neuronaux</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le Spatial Dropout est une technique de régularisation innovante qui
a émergé dans le domaine des réseaux neuronaux, particulièrement pour
les architectures de type convolutionnel (CNN). Cette méthode vise à
améliorer la généralisation des modèles en introduisant une forme de
bruit aléatoire pendant l’entraînement. L’idée sous-jacente est
d’empêcher les neurones de devenir trop dépendants les uns des autres,
ce qui peut conduire à un surapprentissage (overfitting).</p>
<p>L’origine du Spatial Dropout remonte aux travaux sur le Dropout
classique, introduit par Srivastava et al. en 2014. Cependant, le
Dropout classique applique une inactivation aléatoire des neurones de
manière indépendante, ce qui peut ne pas être optimal pour les couches
convolutionnelles où la structure spatiale est cruciale. Le Spatial
Dropout, en revanche, inactive des canaux entiers de manière spatiale,
préservant ainsi la structure locale tout en introduisant une
régularisation efficace.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le Spatial Dropout, il est essentiel de
comprendre ce que nous cherchons à accomplir. Dans les réseaux
neuronaux, surtout dans les CNN, les neurones d’une même carte de
caractéristiques (feature map) partagent des poids et traitent des
informations spatialement proches. Le but est de perturber ces cartes de
manière cohérente pour éviter que le modèle ne s’appuie trop sur des
caractéristiques spécifiques.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X \in \mathbb{R}^{H \times W \times
C}\)</span> une carte de caractéristiques d’une couche convolutionnelle,
où <span class="math inline">\(H\)</span> est la hauteur, <span
class="math inline">\(W\)</span> la largeur et <span
class="math inline">\(C\)</span> le nombre de canaux. Le Spatial Dropout
est défini par une inactivation aléatoire de certains canaux entiers
pendant l’entraînement.</p>
<p>Formellement, pour un taux de dropout <span class="math inline">\(p
\in [0, 1]\)</span>, nous avons : <span
class="math display">\[X&#39;_{i,j,k} =
\begin{cases}
0 &amp; \text{avec probabilité } p, \\
X_{i,j,k} &amp; \text{avec probabilité } 1 - p,
\end{cases}\]</span> où <span class="math inline">\(X&#39;\)</span> est
la carte de caractéristiques après application du Spatial Dropout, et
<span class="math inline">\((i,j,k)\)</span> sont les indices spatiaux
et de canal.</p>
<p>De manière équivalente, nous pouvons exprimer cela à l’aide d’une
matrice binaire <span class="math inline">\(M \in \{0, 1\}^{H \times W
\times C}\)</span> où chaque élément <span
class="math inline">\(M_{i,j,k} \sim \text{Bernoulli}(1 - p)\)</span> :
<span class="math display">\[X&#39; = X \odot M,\]</span> où <span
class="math inline">\(\odot\)</span> représente le produit de
Hadamard.</p>
</div>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<p>Le Spatial Dropout possède plusieurs propriétés intéressantes qui en
font une technique de régularisation efficace. Nous allons explorer
certaines de ces propriétés et les démontrer rigoureusement.</p>
<h2 id="propriété-de-symétrie">Propriété de Symétrie</h2>
<div class="theorem">
<p>L’application du Spatial Dropout à une carte de caractéristiques
<span class="math inline">\(X\)</span> préserve la symétrie des canaux.
C’est-à-dire que si deux canaux sont identiques avant l’application du
Spatial Dropout, ils le resteront après.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X \in \mathbb{R}^{H
\times W \times C}\)</span> une carte de caractéristiques et <span
class="math inline">\(M \in \{0, 1\}^{H \times W \times C}\)</span> une
matrice binaire de Spatial Dropout. Supposons que deux canaux <span
class="math inline">\(k\)</span> et <span
class="math inline">\(l\)</span> sont identiques, c’est-à-dire que pour
tout <span class="math inline">\((i,j)\)</span>, nous avons <span
class="math inline">\(X_{i,j,k} = X_{i,j,l}\)</span>.</p>
<p>Alors, pour tout <span class="math inline">\((i,j)\)</span>, nous
avons : <span class="math display">\[X&#39;_{i,j,k} = X_{i,j,k} \cdot
M_{i,j,k} = X_{i,j,l} \cdot M_{i,j,l} = X&#39;_{i,j,l},\]</span> car
<span class="math inline">\(M_{i,j,k}\)</span> et <span
class="math inline">\(M_{i,j,l}\)</span> sont indépendants et
identiquement distribués. Par conséquent, les canaux <span
class="math inline">\(k\)</span> et <span
class="math inline">\(l\)</span> restent identiques après l’application
du Spatial Dropout. ◻</p>
</div>
<h2 id="propriété-de-conservation-de-la-moyenne">Propriété de
Conservation de la Moyenne</h2>
<div class="theorem">
<p>L’application du Spatial Dropout à une carte de caractéristiques
<span class="math inline">\(X\)</span> conserve la moyenne des canaux en
espérance.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X \in \mathbb{R}^{H
\times W \times C}\)</span> une carte de caractéristiques et <span
class="math inline">\(M \in \{0, 1\}^{H \times W \times C}\)</span> une
matrice binaire de Spatial Dropout. La moyenne d’un canal <span
class="math inline">\(k\)</span> après l’application du Spatial Dropout
est donnée par : <span class="math display">\[\mathbb{E}\left[\frac{1}{H
\times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X&#39;_{i,j,k}\right] =
\mathbb{E}\left[\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W}
X_{i,j,k} \cdot M_{i,j,k}\right].\]</span> Comme <span
class="math inline">\(M_{i,j,k}\)</span> sont des variables de Bernoulli
indépendantes avec <span class="math inline">\(\mathbb{E}[M_{i,j,k}] = 1
- p\)</span>, nous avons : <span
class="math display">\[\mathbb{E}\left[\frac{1}{H \times W}
\sum_{i=1}^{H} \sum_{j=1}^{W} X_{i,j,k} \cdot M_{i,j,k}\right] = (1 - p)
\cdot \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W}
X_{i,j,k}.\]</span> Ainsi, la moyenne du canal <span
class="math inline">\(k\)</span> après l’application du Spatial Dropout
est <span class="math inline">\((1 - p)\)</span> fois la moyenne
originale. Pour conserver la moyenne en espérance, il suffit de
réajuster les poids pendant l’entraînement. ◻</p>
</div>
<h1 id="preuves-et-démonstrations">Preuves et Démonstrations</h1>
<p>Pour démontrer l’efficacité du Spatial Dropout, nous allons explorer
certaines de ses propriétés et les prouver rigoureusement.</p>
<h2 id="preuve-de-la-propriété-de-symétrie">Preuve de la Propriété de
Symétrie</h2>
<p>Nous avons déjà démontré que le Spatial Dropout préserve la symétrie
des canaux. Cette propriété est cruciale car elle garantit que les
relations spatiales entre les canaux sont maintenues, ce qui est
essentiel pour les architectures convolutionnelles.</p>
<h2 id="preuve-de-la-propriété-de-conservation-de-la-moyenne">Preuve de
la Propriété de Conservation de la Moyenne</h2>
<p>La conservation de la moyenne en espérance est une propriété
importante car elle permet de maintenir l’équilibre des contributions
des différents canaux. Cela est particulièrement utile pour éviter que
certains canaux ne dominent les autres pendant l’entraînement.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le Spatial Dropout possède plusieurs propriétés intéressantes qui en
font une technique de régularisation efficace. Nous allons explorer
certaines de ces propriétés et les démontrer rigoureusement.</p>
<h2 id="propriété-i-symétrie-des-canaux">Propriété (i) : Symétrie des
Canaux</h2>
<p>Comme démontré précédemment, le Spatial Dropout préserve la symétrie
des canaux. Cette propriété est cruciale pour maintenir les relations
spatiales entre les canaux.</p>
<h2 id="propriété-ii-conservation-de-la-moyenne">Propriété (ii) :
Conservation de la Moyenne</h2>
<p>La conservation de la moyenne en espérance est une propriété
importante car elle permet de maintenir l’équilibre des contributions
des différents canaux.</p>
<h2 id="propriété-iii-réduction-du-surapprentissage">Propriété (iii) :
Réduction du Surapprentissage</h2>
<p>Le Spatial Dropout réduit le surapprentissage en empêchant les
neurones de devenir trop dépendants les uns des autres. Cela est
particulièrement utile pour les architectures convolutionnelles où la
structure spatiale est cruciale.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Le Spatial Dropout est une technique de régularisation avancée qui a
émergé dans le domaine des réseaux neuronaux, particulièrement pour les
architectures de type convolutionnel. Cette méthode vise à améliorer la
généralisation des modèles en introduisant une forme de bruit aléatoire
pendant l’entraînement. Nous avons exploré les définitions, les
théorèmes et les propriétés du Spatial Dropout, ainsi que leurs preuves
rigoureuses. Le Spatial Dropout est une technique puissante et efficace
pour améliorer la performance des réseaux neuronaux convolutionnels.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de Kulczynski : Une mesure d’écart en théorie des probabilités</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de Kulczynski : Une mesure d’écart en
théorie des probabilités</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Kulczynski émerge dans le paysage mathématique comme
une réponse élégante à la nécessité de quantifier les écarts entre
distributions de probabilités. Son origine historique remonte aux
travaux du mathématicien polonais Józef Kulczynski, qui a cherché à
formaliser une mesure d’écart symétrique et normalisée. Cette notion est
indispensable dans des domaines variés, allant de la théorie de
l’information à l’analyse statistique, en passant par le traitement du
signal. Elle résout notamment le problème de la comparaison entre
distributions discrètes ou continues, en fournissant une métrique
robuste et interprétable.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la distance de Kulczynski, commençons par comprendre ce
que nous cherchons à capturer. Imaginons deux distributions de
probabilités <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\(\Omega\)</span>. Nous voulons une mesure qui
quantifie à quel point ces deux distributions diffèrent. Intuitivement,
cette mesure devrait être nulle si <span class="math inline">\(P =
Q\)</span>, et positive sinon.</p>
<p>Formellement, la distance de Kulczynski entre deux distributions
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilités sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. La
distance de Kulczynski est définie par : <span
class="math display">\[d_K(P, Q) = \frac{1}{2} \left( \int_{\Omega}
|p(x) - q(x)| \, d\mu(x) + 1 - \int_{\Omega} \min(p(x), q(x)) \, d\mu(x)
\right)\]</span> où <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement, et <span
class="math inline">\(\mu\)</span> est une mesure sur <span
class="math inline">\(\Omega\)</span>.</p>
</div>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[d_K(P, Q) = \frac{1}{2} \left( 1 -
\int_{\Omega} \min(p(x), q(x)) \, d\mu(x) \right)\]</span></p>
<p>Cette distance est symétrique, c’est-à-dire que <span
class="math inline">\(d_K(P, Q) = d_K(Q, P)\)</span>, et elle est bornée
entre 0 et 1.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Kulczynski est le
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilités sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. Alors,
la distance de Kulczynski satisfait les propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(d_K(P, Q) = 0\)</span> si et
seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p><span class="math inline">\(d_K(P, Q) \leq 1\)</span> pour toutes
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p></li>
<li><p><span class="math inline">\(d_K(P, Q) = d_K(Q,
P)\)</span>.</p></li>
</ol>
</div>
<p>La démonstration de ce théorème repose sur des propriétés
fondamentales des intégrales et des densités de probabilité. En
particulier, la première propriété découle du fait que l’intégrale de la
différence absolue des densités est nulle si et seulement si les
densités sont égales presque partout. La deuxième propriété résulte de
la normalisation de l’intégrale des densités, tandis que la troisième
propriété est une conséquence directe de la symétrie de la
définition.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de normalisation, procédons étape par
étape.</p>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Pour montrer que <span class="math inline">\(d_K(P, Q) =
0\)</span> si et seulement si <span class="math inline">\(P =
Q\)</span>, observons que : <span class="math display">\[d_K(P, Q) = 0
\implies \int_{\Omega} |p(x) - q(x)| \, d\mu(x) = 0 \text{ et }
\int_{\Omega} \min(p(x), q(x)) \, d\mu(x) = 1\]</span> La première
intégrale étant nulle implique que <span class="math inline">\(p(x) =
q(x)\)</span> presque partout, ce qui signifie que <span
class="math inline">\(P = Q\)</span>. La deuxième intégrale étant égale
à 1 confirme que les densités sont bien normalisées.</p>
<p>Réciproquement, si <span class="math inline">\(P = Q\)</span>, alors
<span class="math inline">\(p(x) = q(x)\)</span> presque partout, et
donc : <span class="math display">\[d_K(P, Q) = \frac{1}{2} \left( 0 + 1
- 1 \right) = 0\]</span></p></li>
<li><p>Pour montrer que <span class="math inline">\(d_K(P, Q) \leq
1\)</span>, utilisons le fait que : <span
class="math display">\[\int_{\Omega} \min(p(x), q(x)) \, d\mu(x) \leq
1\]</span> Par conséquent : <span class="math display">\[d_K(P, Q) =
\frac{1}{2} \left( 1 - \int_{\Omega} \min(p(x), q(x)) \, d\mu(x) \right)
\leq \frac{1}{2} (1 - 0) = 1\]</span></p></li>
<li><p>Pour montrer la symétrie de <span class="math inline">\(d_K(P,
Q)\)</span>, observons que : <span class="math display">\[d_K(P, Q) =
\frac{1}{2} \left( 1 - \int_{\Omega} \min(p(x), q(x)) \, d\mu(x)
\right)\]</span> et <span class="math display">\[d_K(Q, P) = \frac{1}{2}
\left( 1 - \int_{\Omega} \min(q(x), p(x)) \, d\mu(x) \right)\]</span>
Puisque <span class="math inline">\(\min(p(x), q(x)) = \min(q(x),
p(x))\)</span>, il s’ensuit que <span class="math inline">\(d_K(P, Q) =
d_K(Q, P)\)</span>.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés intéressantes découlent de la définition de la
distance de Kulczynski. En voici quelques-unes :</p>
<div class="corollaire">
<p>Soient <span class="math inline">\(P\)</span>, <span
class="math inline">\(Q\)</span> et <span
class="math inline">\(R\)</span> trois distributions de probabilités sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. Alors,
la distance de Kulczynski satisfait l’inégalité triangulaire : <span
class="math display">\[d_K(P, R) \leq d_K(P, Q) + d_K(Q, R)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer l’inégalité triangulaire, utilisons la
définition de la distance de Kulczynski : <span
class="math display">\[d_K(P, R) = \frac{1}{2} \left( 1 - \int_{\Omega}
\min(p(x), r(x)) \, d\mu(x) \right)\]</span> <span
class="math display">\[d_K(P, Q) = \frac{1}{2} \left( 1 - \int_{\Omega}
\min(p(x), q(x)) \, d\mu(x) \right)\]</span> <span
class="math display">\[d_K(Q, R) = \frac{1}{2} \left( 1 - \int_{\Omega}
\min(q(x), r(x)) \, d\mu(x) \right)\]</span> En utilisant l’inégalité
<span class="math inline">\(\min(p(x), r(x)) \geq \min(p(x), q(x)) +
\min(q(x), r(x)) - 1\)</span>, nous obtenons : <span
class="math display">\[\int_{\Omega} \min(p(x), r(x)) \, d\mu(x) \geq
\int_{\Omega} \min(p(x), q(x)) \, d\mu(x) + \int_{\Omega} \min(q(x),
r(x)) \, d\mu(x) - 1\]</span> En substituant dans l’expression de <span
class="math inline">\(d_K(P, R)\)</span>, nous avons : <span
class="math display">\[d_K(P, R) \leq \frac{1}{2} \left( 1 - \left(
\int_{\Omega} \min(p(x), q(x)) \, d\mu(x) + \int_{\Omega} \min(q(x),
r(x)) \, d\mu(x) - 1 \right) \right)\]</span> <span
class="math display">\[d_K(P, R) \leq d_K(P, Q) + d_K(Q,
R)\]</span> ◻</p>
</div>
<div class="corollaire">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilités sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. Alors,
la distance de Kulczynski est invariante par transformation mesurable.
C’est-à-dire, si <span class="math inline">\(\phi : \Omega \to
\Omega&#39;\)</span> est une transformation mesurable, alors : <span
class="math display">\[d_K(P \circ \phi^{-1}, Q \circ \phi^{-1}) =
d_K(P, Q)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, utilisons le
changement de variable dans l’intégrale. Soit <span
class="math inline">\(\phi : \Omega \to \Omega&#39;\)</span> une
transformation mesurable. Alors, les densités transformées sont données
par : <span class="math display">\[p \circ \phi^{-1}(y) =
p(\phi^{-1}(y)) |\det(J_{\phi^{-1}}(y))|\]</span> <span
class="math display">\[q \circ \phi^{-1}(y) = q(\phi^{-1}(y))
|\det(J_{\phi^{-1}}(y))|\]</span> où <span
class="math inline">\(J_{\phi^{-1}}(y)\)</span> est la matrice
jacobienne de l’application inverse <span
class="math inline">\(\phi^{-1}\)</span>. En utilisant le changement de
variable, nous avons : <span class="math display">\[\int_{\Omega&#39;}
\min(p \circ \phi^{-1}(y), q \circ \phi^{-1}(y)) \, d\mu&#39;(y) =
\int_{\Omega} \min(p(x), q(x)) \, d\mu(x)\]</span> Par conséquent :
<span class="math display">\[d_K(P \circ \phi^{-1}, Q \circ \phi^{-1}) =
\frac{1}{2} \left( 1 - \int_{\Omega&#39;} \min(p \circ \phi^{-1}(y), q
\circ \phi^{-1}(y)) \, d\mu&#39;(y) \right) = d_K(P, Q)\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Kulczynski est une mesure d’écart entre distributions
de probabilités qui allie simplicité et robustesse. Ses propriétés
fondamentales, telles que la symétrie, la normalisation et l’invariance
par transformation mesurable, en font un outil précieux dans de nombreux
domaines des mathématiques appliquées. Les théorèmes et corollaires
présentés dans cet article illustrent la richesse de cette notion et
ouvrent la voie à des applications concrètes dans l’analyse statistique
et le traitement du signal.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Average Kernel : Une Introduction Mathématique et Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Average Kernel : Une Introduction Mathématique et
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’<em>Average Kernel</em> est un concept fondamental en analyse
statistique et en apprentissage automatique, particulièrement dans le
domaine des méthodes non paramétriques. Son origine remonte aux travaux
pionniers de Rosenblatt sur les réseaux de neurones et les estimations
par noyau. L’idée centrale est d’estimer la densité de probabilité ou la
régression à partir de données observées en utilisant une moyenne
pondérée par une fonction noyau.</p>
<p>L’intérêt principal de l’Average Kernel réside dans sa capacité à
capturer des structures complexes sans faire d’hypothèses restrictives
sur la forme des données. Il est indispensable dans les contextes où les
modèles paramétriques traditionnels échouent, notamment lorsque les
relations sous-jacentes sont non linéaires ou multimodales.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’Average Kernel, considérons un ensemble de données
<span class="math inline">\(\mathcal{D} = \{ (x_i, y_i)
\}_{i=1}^n\)</span> où <span class="math inline">\(x_i \in
\mathbb{R}^d\)</span> et <span class="math inline">\(y_i \in
\mathbb{R}\)</span>. Nous cherchons à estimer une fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>
telle que <span class="math inline">\(y_i = f(x_i) +
\epsilon_i\)</span>, où <span class="math inline">\(\epsilon_i\)</span>
est un bruit aléatoire.</p>
<p>L’idée est d’utiliser une fonction noyau <span
class="math inline">\(K: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>
qui mesure la similarité entre deux points. Une fonction noyau
couramment utilisée est le noyau Gaussien : <span
class="math display">\[K(x, x&#39;) = \exp\left(-\frac{\|x -
x&#39;\|^2}{2h^2}\right)\]</span> où <span class="math inline">\(h &gt;
0\)</span> est un paramètre de lissage.</p>
<p>L’estimation par Average Kernel de la fonction <span
class="math inline">\(f\)</span> en un point <span
class="math inline">\(x \in \mathbb{R}^d\)</span> est donnée par : <span
class="math display">\[\hat{f}(x) = \frac{\sum_{i=1}^n K(x, x_i)
y_i}{\sum_{i=1}^n K(x, x_i)}\]</span></p>
<p>Cette formule peut être réécrite en utilisant des quantificateurs :
<span class="math display">\[\hat{f}(x) = \frac{\sum_{i=1}^n y_i K(x,
x_i)}{\sum_{j=1}^n K(x, x_j)}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’Average Kernel est le théorème de la
convergence uniforme. Ce théorème stipule que sous certaines conditions,
l’estimateur <span class="math inline">\(\hat{f}(x)\)</span> converge
uniformément vers la vraie fonction <span
class="math inline">\(f(x)\)</span> lorsque le nombre de données <span
class="math inline">\(n\)</span> tend vers l’infini.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D} = \{ (x_i, y_i)
\}_{i=1}^n\)</span> un échantillon i.i.d. de la distribution <span
class="math inline">\(P\)</span> et soit <span
class="math inline">\(K\)</span> une fonction noyau continue et bornée.
Supposons que le paramètre de lissage <span
class="math inline">\(h\)</span> tende vers 0 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini. Alors, pour tout
<span class="math inline">\(\epsilon &gt; 0\)</span>, il existe un <span
class="math inline">\(N\)</span> tel que pour tout <span
class="math inline">\(n \geq N\)</span>, <span
class="math display">\[\sup_{x \in \mathbb{R}^d} |\hat{f}(x) - f(x)|
&lt; \epsilon\]</span> avec une probabilité tendant vers 1 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la convergence uniforme, nous utilisons
les propriétés suivantes :</p>
<ol>
<li><p>La fonction noyau <span class="math inline">\(K\)</span> est
continue et bornée.</p></li>
<li><p>Le paramètre de lissage <span class="math inline">\(h\)</span>
tend vers 0 lorsque <span class="math inline">\(n\)</span> tend vers
l’infini.</p></li>
<li><p>Les données <span class="math inline">\(\mathcal{D}\)</span> sont
i.i.d. selon la distribution <span
class="math inline">\(P\)</span>.</p></li>
</ol>
<p>Nous commençons par décomposer l’erreur en deux parties : <span
class="math display">\[|\hat{f}(x) - f(x)| \leq |E[\hat{f}(x)] - f(x)| +
|\hat{f}(x) - E[\hat{f}(x)]|\]</span></p>
<p>La première partie, <span class="math inline">\(E[\hat{f}(x)] -
f(x)\)</span>, est l’erreur de biais. En utilisant le théorème de Taylor
et les propriétés du noyau, on peut montrer que cette erreur tend vers 0
lorsque <span class="math inline">\(h\)</span> tend vers 0.</p>
<p>La deuxième partie, <span class="math inline">\(\hat{f}(x) -
E[\hat{f}(x)]\)</span>, est l’erreur de variance. En utilisant
l’inégalité de Hoeffding et les propriétés des variables aléatoires
i.i.d., on peut montrer que cette erreur tend vers 0 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes de l’Average
Kernel :</p>
<ol>
<li><p><strong>Consistance</strong> : Sous les conditions du théorème de
la convergence uniforme, l’Average Kernel est un estimateur
consistant.</p></li>
<li><p><strong>Lissage</strong> : Le paramètre de lissage <span
class="math inline">\(h\)</span> contrôle la trade-off entre le biais et
la variance.</p></li>
<li><p><strong>Généralisation</strong> : L’Average Kernel peut être
généralisé à d’autres types de noyaux et à des espaces de dimensions
supérieures.</p></li>
</ol>
<p>Pour chaque propriété, nous fournissons une preuve détaillée. Par
exemple, pour la consistance, nous utilisons le théorème de la
convergence uniforme et les propriétés des estimateurs non
paramétriques.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Average Kernel est un outil puissant pour l’estimation non
paramétrique. Ses applications vont de la régression à la
classification, en passant par l’estimation de densité. Les théorèmes et
propriétés présentés dans cet article montrent sa robustesse et sa
généralité.</p>
</body>
</html>
{% include "footer.html" %}


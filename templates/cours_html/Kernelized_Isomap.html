{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Isomap: Une Approche Non-Linéaire pour l’Embedding de Données</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Isomap: Une Approche Non-Linéaire pour
l’Embedding de Données</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse des données de haute dimension est un défi central en
apprentissage automatique et en traitement du signal. Les méthodes
d’embedding de données, telles que l’Analyse en Composantes Principales
(PCA), sont souvent limitées par leur hypothèse de linéarité. L’Isomap
(Isometric Mapping), introduit par Tenenbaum, De Silva et Langford en
2000, propose une solution non-linéaire en utilisant les distances
géodésiques sur un graphe construit à partir des données. Cependant,
l’Isomap classique peut être limité par sa sensibilité aux choix de
paramètres et son incapacité à généraliser à de nouvelles données.</p>
<p>L’approche Kernelized Isomap (K-Isomap) étend l’Isomap en intégrant
les techniques de noyau, permettant ainsi une généralisation à de
nouvelles données et une meilleure robustesse aux choix de paramètres.
Cette méthode combine les avantages des méthodes à noyau, telles que le
Support Vector Machine (SVM), avec la puissance de l’Isomap pour
capturer les structures non-linéaires des données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<h2 class="unnumbered" id="distance-géodésique">Distance Géodésique</h2>
<p>Considérons un ensemble de points <span class="math inline">\(X =
\{x_1, x_2, \dots, x_n\}\)</span> dans un espace de haute dimension.
L’idée sous-jacente à l’Isomap est que les distances entre les points
doivent être mesurées le long de la variété sous-jacente plutôt que dans
l’espace ambiant. Pour ce faire, nous définissons la distance géodésique
<span class="math inline">\(d_G(i,j)\)</span> entre deux points <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> comme la longueur du chemin le plus
court reliant ces points sur la variété.</p>
<p>Formellement, nous avons: <span class="math display">\[d_G(i,j) =
\min_{p} \sum_{k=1}^{K} \|x_{p_k} - x_{p_{k+1}}\|\]</span> où <span
class="math inline">\(p\)</span> est un chemin reliant <span
class="math inline">\(x_i\)</span> à <span
class="math inline">\(x_j\)</span>, et <span
class="math inline">\(K\)</span> est le nombre de points dans ce
chemin.</p>
<h2 class="unnumbered" id="kernel-isomap">Kernel Isomap</h2>
<p>L’approche Kernelized Isomap utilise une fonction de noyau <span
class="math inline">\(\kappa\)</span> pour mapper les données dans un
espace de caractéristiques de haute dimension. La fonction de noyau doit
satisfaire les conditions de Mercer, c’est-à-dire qu’elle est symétrique
et positive définie.</p>
<p>Formellement, la fonction de noyau <span
class="math inline">\(\kappa\)</span> est définie comme: <span
class="math display">\[\kappa(x_i, x_j) = \langle \phi(x_i), \phi(x_j)
\rangle\]</span> où <span class="math inline">\(\phi\)</span> est une
fonction de mapping non-linéaire dans un espace de caractéristiques.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered" id="théorème-de-lisomap">Théorème de
l’Isomap</h2>
<p>Le théorème central de l’Isomap stipule que, sous certaines
conditions, les distances géodésiques peuvent être approximées par les
distances dans un espace de faible dimension. Plus précisément, si la
variété sous-jacente est isométrique à un sous-ensemble d’un espace
euclidien de faible dimension, alors les distances géodésiques peuvent
être approximées par les distances dans cet espace.</p>
<p>Formellement, nous avons: <span class="math display">\[d_G(i,j)
\approx \|y_i - y_j\|\]</span> où <span
class="math inline">\(y_i\)</span> et <span
class="math inline">\(y_j\)</span> sont les embeddings de faible
dimension des points <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>.</p>
<h2 class="unnumbered" id="théorème-du-kernel-isomap">Théorème du Kernel
Isomap</h2>
<p>L’approche Kernelized Isomap étend ce théorème en utilisant une
fonction de noyau. Le théorème du Kernel Isomap stipule que, sous
certaines conditions, les distances géodésiques dans l’espace de
caractéristiques peuvent être approximées par les distances dans un
espace de faible dimension.</p>
<p>Formellement, nous avons: <span class="math display">\[d_G(i,j)
\approx \| \phi(y_i) - \phi(y_j) \|\]</span> où <span
class="math inline">\(y_i\)</span> et <span
class="math inline">\(y_j\)</span> sont les embeddings de faible
dimension des points <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> dans l’espace de
caractéristiques.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered" id="preuve-du-théorème-de-lisomap">Preuve du
Théorème de l’Isomap</h2>
<p>Pour prouver le théorème de l’Isomap, nous devons montrer que les
distances géodésiques peuvent être approximées par les distances dans un
espace de faible dimension. Nous commençons par construire un graphe des
<span class="math inline">\(k\)</span>-plus proches voisins à partir des
données. Ensuite, nous calculons les distances géodésiques en utilisant
l’algorithme de Dijkstra ou Floyd-Warshall.</p>
<p>Ensuite, nous appliquons la MDS (Multidimensional Scaling) aux
distances géodésiques pour obtenir les embeddings de faible dimension.
Si la variété sous-jacente est isométrique à un sous-ensemble d’un
espace euclidien de faible dimension, alors les distances géodésiques
seront bien approximées par les distances dans cet espace.</p>
<h2 class="unnumbered" id="preuve-du-théorème-du-kernel-isomap">Preuve
du Théorème du Kernel Isomap</h2>
<p>Pour prouver le théorème du Kernel Isomap, nous devons montrer que
les distances géodésiques dans l’espace de caractéristiques peuvent être
approximées par les distances dans un espace de faible dimension. Nous
commençons par calculer la matrice de Gram <span
class="math inline">\(K\)</span> à partir des données et de la fonction
de noyau.</p>
<p>Ensuite, nous appliquons le Kernel PCA (Principal Component Analysis)
à la matrice de Gram pour obtenir les embeddings de faible dimension
dans l’espace de caractéristiques. Si la variété sous-jacente est
isométrique à un sous-ensemble d’un espace euclidien de faible
dimension, alors les distances géodésiques dans l’espace de
caractéristiques seront bien approximées par les distances dans cet
espace.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered" id="propriété-de-généralisation">Propriété de
Généralisation</h2>
<p>Le Kernel Isomap possède une propriété de généralisation qui lui
permet de prédire les embeddings de nouvelles données. Cette propriété
est due à l’utilisation d’une fonction de noyau, qui permet de mapper
les nouvelles données dans l’espace de caractéristiques.</p>
<p>Formellement, pour une nouvelle donnée <span
class="math inline">\(x\)</span>, nous avons: <span
class="math display">\[y = \sum_{i=1}^n \alpha_i \kappa(x, x_i)\]</span>
où <span class="math inline">\(\alpha_i\)</span> sont les coefficients
obtenus à partir des embeddings de faible dimension des données
d’entraînement.</p>
<h2 class="unnumbered" id="corollaire-de-la-robustesse">Corollaire de la
Robustesse</h2>
<p>Le Kernel Isomap est plus robuste aux choix de paramètres que
l’Isomap classique. Cette robustesse est due à l’utilisation d’une
fonction de noyau, qui permet de capturer les structures non-linéaires
des données de manière plus flexible.</p>
<p>Formellement, nous avons: <span class="math display">\[d_G(i,j)
\approx \| \phi(y_i) - \phi(y_j) \|\]</span> pour une large gamme de
paramètres de noyau.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le Kernelized Isomap est une extension puissante de l’Isomap
classique, combinant les avantages des méthodes à noyau avec la
puissance de l’Isomap pour capturer les structures non-linéaires des
données. Cette approche offre une meilleure généralisation et une plus
grande robustesse aux choix de paramètres, ce qui en fait un outil
précieux pour l’analyse des données de haute dimension.</p>
</body>
</html>
{% include "footer.html" %}


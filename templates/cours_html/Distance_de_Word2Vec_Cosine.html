{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Word2Vec Cosine : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Word2Vec Cosine : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’émergence des modèles de langage a révolutionné le traitement
automatique du langage naturel (TALN). Parmi ces modèles, Word2Vec,
développé par Mikolov et al. en 2013, a marqué un tournant en proposant
une représentation vectorielle des mots capable de capturer les
relations sémantiques et syntaxiques. La distance de Word2Vec Cosine,
quant à elle, offre un moyen élégant et efficace de mesurer la
similarité entre ces représentations vectorielles.</p>
<p>Cette notion trouve son origine dans le besoin de quantifier la
proximité sémantique entre les mots. En effet, dans de nombreuses
applications de TALN, comme la recherche d’information, la traduction
automatique ou encore l’analyse de sentiments, il est crucial de pouvoir
comparer des mots ou des phrases en fonction de leur signification. La
distance de Word2Vec Cosine répond à ce besoin en fournissant une mesure
normalisée et intuitive.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la distance de Word2Vec Cosine, il est essentiel de
saisir d’abord les concepts sous-jacents. Imaginons que nous avons un
espace vectoriel où chaque mot est représenté par un vecteur. Nous
cherchons une manière de mesurer la similarité entre deux vecteurs,
indépendamment de leur magnitude.</p>
<p>La distance de Word2Vec Cosine est une mesure de similarité basée sur
l’angle entre deux vecteurs. Plus cet angle est petit, plus les vecteurs
sont similaires.</p>
<div class="definition">
<p>Soient <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> deux vecteurs dans un espace
vectoriel de dimension <span class="math inline">\(n\)</span>. La
distance de Word2Vec Cosine entre <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> est définie comme suit :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|
\|\mathbf{v}\|}\]</span></p>
<p>où <span class="math inline">\(\mathbf{u} \cdot \mathbf{v}\)</span>
représente le produit scalaire des vecteurs <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, et <span
class="math inline">\(\|\mathbf{u}\|\)</span> et <span
class="math inline">\(\|\mathbf{v}\|\)</span> représentent les normes
des vecteurs <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, respectivement.</p>
</div>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - \frac{\sum_{i=1}^{n} u_i v_i}{\sqrt{\sum_{i=1}^{n}
u_i^2} \sqrt{\sum_{i=1}^{n} v_i^2}}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Word2Vec Cosine est
celui de la similarité cosinus. Ce théorème montre que la distance de
Word2Vec Cosine est une mesure valide de similarité entre deux
vecteurs.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> deux vecteurs non nuls dans un
espace vectoriel de dimension <span class="math inline">\(n\)</span>. La
distance de Word2Vec Cosine entre <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> satisfait les propriétés
suivantes :</p>
<ol>
<li><p><span class="math inline">\(0 \leq
\text{cosine\_distance}(\mathbf{u}, \mathbf{v}) \leq 1\)</span></p></li>
<li><p><span class="math inline">\(\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 0\)</span> si et seulement si <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de même
sens.</p></li>
<li><p><span class="math inline">\(\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1\)</span> si et seulement si <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de sens
opposés.</p></li>
</ol>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la similarité cosinus, nous allons
procéder étape par étape.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux vecteurs non nuls <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>. Le produit scalaire <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v}\)</span> est donné par
:</p>
<p><span class="math display">\[\mathbf{u} \cdot \mathbf{v} =
\|\mathbf{u}\| \|\mathbf{v}\| \cos \theta\]</span></p>
<p>où <span class="math inline">\(\theta\)</span> est l’angle entre les
vecteurs <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>.</p>
<p>La distance de Word2Vec Cosine peut alors être réécrite comme :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - \frac{\|\mathbf{u}\| \|\mathbf{v}\| \cos
\theta}{\|\mathbf{u}\| \|\mathbf{v}\|} = 1 - \cos \theta\]</span></p>
<p>Puisque <span class="math inline">\(\cos \theta\)</span> est compris
entre <span class="math inline">\(-1\)</span> et <span
class="math inline">\(1\)</span>, il s’ensuit que :</p>
<p><span class="math display">\[0 \leq 1 - \cos \theta \leq
2\]</span></p>
<p>Cependant, comme <span class="math inline">\(\mathbf{u}\)</span> et
<span class="math inline">\(\mathbf{v}\)</span> sont non nuls, l’angle
<span class="math inline">\(\theta\)</span> est compris entre <span
class="math inline">\(0\)</span> et <span
class="math inline">\(\pi\)</span>, ce qui implique que <span
class="math inline">\(\cos \theta\)</span> est compris entre <span
class="math inline">\(-1\)</span> et <span
class="math inline">\(1\)</span>. Par conséquent, la distance de
Word2Vec Cosine est bien comprise entre <span
class="math inline">\(0\)</span> et <span
class="math inline">\(1\)</span>.</p>
<p>De plus, si <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de même
sens, alors <span class="math inline">\(\theta = 0\)</span> et <span
class="math inline">\(\cos \theta = 1\)</span>, ce qui entraîne :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - 1 = 0\]</span></p>
<p>Si <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de sens
opposés, alors <span class="math inline">\(\theta = \pi\)</span> et
<span class="math inline">\(\cos \theta = -1\)</span>, ce qui entraîne
:</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - (-1) = 2\]</span></p>
<p>Cependant, dans la définition de la distance de Word2Vec Cosine, nous
avons normalisé cette mesure pour qu’elle soit comprise entre <span
class="math inline">\(0\)</span> et <span
class="math inline">\(1\)</span>. Ainsi, la distance de Word2Vec Cosine
est bien une mesure valide de similarité entre deux vecteurs. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous allons maintenant examiner quelques propriétés et corollaires
importants liés à la distance de Word2Vec Cosine.</p>
<div class="corollary">
<p>Soient <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> deux vecteurs non nuls dans un
espace vectoriel de dimension <span class="math inline">\(n\)</span>. La
distance de Word2Vec Cosine est invariante par normalisation des
vecteurs, c’est-à-dire :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) =
\text{cosine\_distance}\left(\frac{\mathbf{u}}{\|\mathbf{u}\|},
\frac{\mathbf{v}}{\|\mathbf{v}\|}\right)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Considérons les vecteurs normalisés <span
class="math inline">\(\frac{\mathbf{u}}{\|\mathbf{u}\|}\)</span> et
<span class="math inline">\(\frac{\mathbf{v}}{\|\mathbf{v}\|}\)</span>.
La distance de Word2Vec Cosine entre ces vecteurs est donnée par :</p>
<p><span
class="math display">\[\text{cosine\_distance}\left(\frac{\mathbf{u}}{\|\mathbf{u}\|},
\frac{\mathbf{v}}{\|\mathbf{v}\|}\right) = 1 -
\frac{\frac{\mathbf{u}}{\|\mathbf{u}\|} \cdot
\frac{\mathbf{v}}{\|\mathbf{v}\|}}{\left\|\frac{\mathbf{u}}{\|\mathbf{u}\|}\right\|
\left\|\frac{\mathbf{v}}{\|\mathbf{v}\|}\right\|} = 1 - \frac{\mathbf{u}
\cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} =
\text{cosine\_distance}(\mathbf{u}, \mathbf{v})\]</span></p>
<p>Ainsi, la distance de Word2Vec Cosine est bien invariante par
normalisation des vecteurs. ◻</p>
</div>
<div class="corollary">
<p>Soient <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> deux vecteurs dans un espace
vectoriel de dimension <span class="math inline">\(n\)</span>. La
distance de Word2Vec Cosine est symétrique, c’est-à-dire :</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = \text{cosine\_distance}(\mathbf{v},
\mathbf{u})\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La symétrie de la distance de Word2Vec Cosine découle
directement de la symétrie du produit scalaire. En effet, nous avons
:</p>
<p><span class="math display">\[\text{cosine\_distance}(\mathbf{u},
\mathbf{v}) = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|
\|\mathbf{v}\|} = 1 - \frac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{v}\|
\|\mathbf{u}\|} = \text{cosine\_distance}(\mathbf{v},
\mathbf{u})\]</span></p>
<p>Ainsi, la distance de Word2Vec Cosine est bien symétrique. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Word2Vec Cosine est une mesure puissante et intuitive
de similarité entre les vecteurs de mots. Son utilisation dans le cadre
des modèles de langage comme Word2Vec a permis d’améliorer
considérablement les performances des applications de TALN. En
comprenant les fondements mathématiques et les propriétés de cette
distance, nous pouvons mieux exploiter son potentiel pour des tâches
complexes comme la recherche d’information ou la traduction
automatique.</p>
</body>
</html>
{% include "footer.html" %}


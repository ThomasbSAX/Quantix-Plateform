{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Conditional Mutual Information: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Conditional Mutual Information: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of information theory has revolutionized the way we
understand and process data. Among its many facets, conditional mutual
information stands out as a pivotal measure that quantifies the amount
of information obtained about one random variable by observing another,
given a third. This notion is indispensable in various fields such as
machine learning, statistics, and communication theory.</p>
<p>Conditional mutual information emerged from the need to understand
dependencies between variables in the presence of other variables. It
provides a way to measure how much knowing one random variable reduces
uncertainty about another, when a third random variable is already
known. This measure is crucial in scenarios where we need to understand
the information flow in complex systems.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand conditional mutual information, letâ€™s first recall some
fundamental concepts. We aim to quantify the amount of information that
one random variable <span class="math inline">\(X\)</span> provides
about another random variable <span class="math inline">\(Y\)</span>,
given a third random variable <span
class="math inline">\(Z\)</span>.</p>
<p>Consider three random variables <span class="math inline">\(X, Y,
Z\)</span> with a joint probability distribution <span
class="math inline">\(p(x,y,z)\)</span>. The conditional mutual
information between <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(Z\)</span> is defined as:</p>
<p><span class="math display">\[I(X;Y|Z) = \sum_{x,y,z} p(x,y,z) \log
\left( \frac{p(x,y|z)}{p(x|z)p(y|z)} \right)\]</span></p>
<p>Alternatively, using the definition of conditional entropy:</p>
<p><span class="math display">\[I(X;Y|Z) = H(X|Z) -
H(X|Y,Z)\]</span></p>
<p>Where <span class="math inline">\(H(X|Z)\)</span> is the conditional
entropy of <span class="math inline">\(X\)</span> given <span
class="math inline">\(Z\)</span>, and <span
class="math inline">\(H(X|Y,Z)\)</span> is the conditional entropy of
<span class="math inline">\(X\)</span> given both <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(Z\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the most important theorems related to conditional mutual
information is the chain rule for mutual information. This theorem
provides a way to decompose the mutual information between two variables
into parts that are conditioned on other variables.</p>
<div class="theorem">
<p>For any three random variables <span class="math inline">\(X, Y,
Z\)</span>, the following holds:</p>
<p><span class="math display">\[I(X;Y|Z) = I(X;Y,Z) -
I(X;Z)\]</span></p>
<p>Proof:</p>
<p>We start by expressing the mutual information <span
class="math inline">\(I(X;Y,Z)\)</span> using the definition of
conditional entropy:</p>
<p><span class="math display">\[I(X;Y,Z) = H(X) - H(X|Y,Z)\]</span></p>
<p>Similarly, the mutual information <span
class="math inline">\(I(X;Z)\)</span> is:</p>
<p><span class="math display">\[I(X;Z) = H(X) - H(X|Z)\]</span></p>
<p>Subtracting these two equations gives:</p>
<p><span class="math display">\[I(X;Y,Z) - I(X;Z) = H(X|Z) -
H(X|Y,Z)\]</span></p>
<p>By the definition of conditional mutual information, we have:</p>
<p><span class="math display">\[I(X;Y|Z) = H(X|Z) -
H(X|Y,Z)\]</span></p>
<p>Thus, the chain rule for mutual information is proven.</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Conditional mutual information exhibits several important properties
that make it a powerful tool in information theory.</p>
<ol>
<li><p>Non-negativity: For any random variables <span
class="math inline">\(X, Y, Z\)</span>, the conditional mutual
information is non-negative:</p>
<p><span class="math display">\[I(X;Y|Z) \geq 0\]</span></p>
<p>Proof: This follows from the non-negativity of mutual information and
the fact that conditioning reduces entropy.</p></li>
<li><p>Symmetry: The conditional mutual information is symmetric in
<span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[I(X;Y|Z) = I(Y;X|Z)\]</span></p>
<p>Proof: This can be shown by exchanging the roles of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> in the definition.</p></li>
<li><p>Chain Rule: The chain rule for conditional mutual information
states that:</p>
<p><span class="math display">\[I(X;Y,Z|W) = I(X;Y|W) +
I(X;Z|Y,W)\]</span></p>
<p>Proof: This can be derived using the chain rule for entropy and the
definition of conditional mutual information.</p></li>
</ol>
<h1 id="applications">Applications</h1>
<p>Conditional mutual information has a wide range of applications in
various fields. In machine learning, it is used to measure the
dependence between features given a target variable. In communication
theory, it helps in understanding the capacity of channels with side
information. In statistics, it is used for feature selection and
dimensionality reduction.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Conditional mutual information is a fundamental concept in
information theory that provides deep insights into the dependencies
between random variables. Its properties and applications make it an
indispensable tool in various scientific disciplines. Understanding and
utilizing conditional mutual information can lead to significant
advancements in data analysis, machine learning, and communication
systems.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Log-prédictive marginale : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Log-prédictive marginale : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La log-prédictive marginale émerge comme un concept central en
théorie des probabilités et en apprentissage statistique. Son origine
remonte aux travaux pionniers sur les modèles bayésiens, où l’on cherche
à prédire des événements futurs en intégrant l’incertitude. Ce cadre est
indispensable pour modéliser des phénomènes complexes où les données
sont bruitées ou incomplètes.</p>
<p>L’intérêt principal de la log-prédictive marginale réside dans sa
capacité à fournir une mesure robuste de la performance prédictive d’un
modèle. En intégrant sur l’espace des paramètres, elle permet de
capturer l’incertitude inhérente aux estimations et offre ainsi une
évaluation plus réaliste des prédictions.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la notion de log-prédictive marginale, considérons un
modèle statistique paramétré par <span class="math inline">\(\theta \in
\Theta\)</span>. Nous cherchons à prédire une observation future <span
class="math inline">\(y_{new}\)</span> en utilisant les données
observées <span class="math inline">\(\mathcal{D} = \{y_1, \dots,
y_n\}\)</span>.</p>
<div class="definition">
<p>La log-prédictive marginale est définie comme l’espérance de la
log-vraisemblance prédictive marginale par rapport à la distribution
postérieure des paramètres <span class="math inline">\(\theta\)</span>:
<span class="math display">\[\log p(y_{new} | \mathcal{D}) =
\int_{\Theta} \log p(y_{new} | \theta) \, p(\theta | \mathcal{D}) \,
d\theta\]</span> où <span class="math inline">\(p(y_{new} |
\theta)\)</span> est la vraisemblance prédictive et <span
class="math inline">\(p(\theta | \mathcal{D})\)</span> est la
distribution postérieure de <span
class="math inline">\(\theta\)</span>.</p>
</div>
<p>Une autre formulation équivalente est: <span
class="math display">\[\log p(y_{new} | \mathcal{D}) =
\mathbb{E}_{\theta | \mathcal{D}} [\log p(y_{new} |
\theta)]\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant la
log-prédictive marginale.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D}\)</span> un ensemble de
données et <span class="math inline">\(y_{new}\)</span> une observation
future. Alors, la log-prédictive marginale peut être exprimée comme:
<span class="math display">\[\log p(y_{new} | \mathcal{D}) = \log
\int_{\Theta} p(y_{new}, \theta | \mathcal{D}) \, d\theta\]</span> où
<span class="math inline">\(p(y_{new}, \theta | \mathcal{D})\)</span>
est la densité jointe de <span class="math inline">\(y_{new}\)</span> et
<span class="math inline">\(\theta\)</span> conditionnellement à <span
class="math inline">\(\mathcal{D}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par la définition de l’espérance conditionnelle, nous
avons: <span class="math display">\[\mathbb{E}_{\theta | \mathcal{D}}
[\log p(y_{new} | \theta)] = \int_{\Theta} \log p(y_{new} | \theta) \,
p(\theta | \mathcal{D}) \, d\theta\]</span> En utilisant la propriété de
Bayes, <span class="math inline">\(p(\theta | \mathcal{D}) =
\frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})}\)</span>, nous
obtenons: <span class="math display">\[\int_{\Theta} \log p(y_{new} |
\theta) \, \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})} \,
d\theta = \frac{1}{p(\mathcal{D})} \int_{\Theta} \log p(y_{new} |
\theta) \, p(\mathcal{D} | \theta) p(\theta) \, d\theta\]</span> En
utilisant la propriété de la densité jointe, <span
class="math inline">\(p(y_{new}, \theta | \mathcal{D}) = p(y_{new} |
\theta, \mathcal{D}) p(\theta | \mathcal{D})\)</span>, nous avons: <span
class="math display">\[\log p(y_{new} | \mathcal{D}) = \log
\int_{\Theta} p(y_{new}, \theta | \mathcal{D}) \, d\theta\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous énumérons quelques propriétés importantes de la log-prédictive
marginale.</p>
<div class="proposition">
<p>La log-prédictive marginale satisfait les propriétés suivantes:</p>
<ol>
<li><p>Elle est invariante par transformation de mesure.</p></li>
<li><p>Elle peut être approchée par des méthodes de Monte
Carlo.</p></li>
<li><p>Elle est liée à l’entropie différentielle du modèle.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Preuve de (i).</em> Considérons une transformation de mesure
<span class="math inline">\(\phi: \Theta \to \Theta&#39;\)</span>. La
log-prédictive marginale reste inchangée car: <span
class="math display">\[\int_{\Theta} \log p(y_{new} | \theta) \,
p(\theta | \mathcal{D}) \, d\theta = \int_{\Theta&#39;} \log p(y_{new} |
\phi^{-1}(\theta&#39;)) \, p(\phi^{-1}(\theta&#39;) | \mathcal{D}) \,
|\det J_{\phi^{-1}}(\theta&#39;)| \, d\theta&#39;\]</span> où <span
class="math inline">\(J_{\phi^{-1}}\)</span> est la matrice jacobienne
de <span class="math inline">\(\phi^{-1}\)</span>. ◻</p>
</div>
<div class="proof">
<p><em>Preuve de (ii).</em> Pour approcher la log-prédictive marginale
par Monte Carlo, nous utilisons un échantillon <span
class="math inline">\(\{\theta^{(1)}, \dots, \theta^{(m)}\}\)</span> de
la distribution postérieure <span class="math inline">\(p(\theta |
\mathcal{D})\)</span>. Alors: <span class="math display">\[\log
p(y_{new} | \mathcal{D}) \approx \frac{1}{m} \sum_{i=1}^m \log p(y_{new}
| \theta^{(i)})\]</span> ◻</p>
</div>
<div class="proof">
<p><em>Preuve de (iii).</em> L’entropie différentielle <span
class="math inline">\(H(y_{new} | \mathcal{D})\)</span> est liée à la
log-prédictive marginale par: <span class="math display">\[H(y_{new} |
\mathcal{D}) = -\mathbb{E}_{y_{new} | \mathcal{D}} [\log p(y_{new} |
\mathcal{D})]\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La log-prédictive marginale est un outil puissant pour évaluer la
performance prédictive des modèles statistiques. Ses propriétés
fondamentales et ses applications pratiques en font un sujet de
recherche actif dans les domaines de la statistique bayésienne et de
l’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


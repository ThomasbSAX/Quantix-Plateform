{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’autocovariance : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’autocovariance : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’autocovariance est un concept fondamental en théorie des
probabilités et en traitement du signal. Elle émerge naturellement
lorsque l’on s’intéresse à la dépendance d’une variable aléatoire par
rapport à elle-même, décalée dans le temps ou l’espace. Cette notion est
indispensable pour comprendre la structure des processus stochastiques
et pour développer des modèles prédictifs robustes.</p>
<p>L’origine de l’autocovariance remonte aux travaux pionniers en
statistique et en analyse des séries temporelles. Elle permet de
quantifier la manière dont les valeurs passées d’un processus
influencent ses valeurs futures, ce qui est crucial dans de nombreuses
applications, allant de la finance à l’ingénierie.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’autocovariance, considérons un processus
stochastique <span class="math inline">\(\{X_t\}_{t \in T}\)</span> où
<span class="math inline">\(T\)</span> est un ensemble d’indices,
souvent représentant le temps. Nous cherchons à mesurer la dépendance
entre <span class="math inline">\(X_t\)</span> et <span
class="math inline">\(X_{t+k}\)</span>, où <span
class="math inline">\(k\)</span> est un décalage.</p>
<p>La covariance entre deux variables aléatoires <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est définie comme : <span
class="math display">\[\text{Cov}(X, Y) = \mathbb{E}[(X -
\mathbb{E}[X])(Y - \mathbb{E}[Y])]\]</span></p>
<p>En appliquant cette définition à <span
class="math inline">\(X_t\)</span> et <span
class="math inline">\(X_{t+k}\)</span>, nous obtenons la définition
formelle de l’autocovariance :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\{X_t\}_{t \in T}\)</span> un
processus stochastique. L’autocovariance de <span
class="math inline">\(X_t\)</span> et <span
class="math inline">\(X_{t+k}\)</span> est donnée par : <span
class="math display">\[\gamma(k) = \text{Cov}(X_t, X_{t+k}) =
\mathbb{E}[(X_t - \mu)(X_{t+k} - \mu)]\]</span> où <span
class="math inline">\(\mu = \mathbb{E}[X_t]\)</span> est la moyenne du
processus, supposée constante pour tout <span
class="math inline">\(t\)</span>.</p>
</div>
<p>Une autre formulation de l’autocovariance, en utilisant la fonction
de corrélation, est : <span class="math display">\[\gamma(k) = R(k) -
\mu^2\]</span> où <span class="math inline">\(R(k) = \mathbb{E}[X_t
X_{t+k}]\)</span> est la fonction d’autocorrélation.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en théorie des processus stochastiques est
celui de la stationnarité, qui est étroitement lié à
l’autocovariance.</p>
<div class="theorem">
<p>Un processus stochastique <span
class="math inline">\(\{X_t\}\)</span> est dit stationnaire au second
ordre si et seulement si :</p>
<ol>
<li><p><span class="math inline">\(\mathbb{E}[X_t] = \mu\)</span> est
constante pour tout <span class="math inline">\(t\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Cov}(X_t, X_{t+k}) =
\gamma(k)\)</span> dépend uniquement de <span
class="math inline">\(k\)</span> et non de <span
class="math inline">\(t\)</span>.</p></li>
</ol>
</div>
<p>La démonstration de ce théorème repose sur la propriété de
translation dans le temps des moments du second ordre. En effet, si
<span class="math inline">\(\mathbb{E}[X_t] = \mu\)</span> est constante
et que la covariance ne dépend que du décalage <span
class="math inline">\(k\)</span>, alors le processus est stationnaire au
second ordre.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer la stationnarité au second ordre, nous devons montrer
que les conditions énoncées dans le théorème sont satisfaites.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que <span
class="math inline">\(\{X_t\}\)</span> est stationnaire au second ordre.
Alors, pour tout <span class="math inline">\(t\)</span>, nous avons :
<span class="math display">\[\mathbb{E}[X_t] = \mu\]</span></p>
<p>De plus, la covariance entre <span class="math inline">\(X_t\)</span>
et <span class="math inline">\(X_{t+k}\)</span> est donnée par : <span
class="math display">\[\text{Cov}(X_t, X_{t+k}) = \mathbb{E}[(X_t -
\mu)(X_{t+k} - \mu)]\]</span></p>
<p>En développant cette expression, nous obtenons : <span
class="math display">\[\text{Cov}(X_t, X_{t+k}) = \mathbb{E}[X_t
X_{t+k}] - \mu^2\]</span></p>
<p>Puisque <span class="math inline">\(\mathbb{E}[X_t X_{t+k}] =
R(k)\)</span>, nous avons : <span class="math display">\[\text{Cov}(X_t,
X_{t+k}) = R(k) - \mu^2\]</span></p>
<p>Ainsi, la covariance ne dépend que du décalage <span
class="math inline">\(k\)</span> et non de <span
class="math inline">\(t\)</span>, ce qui prouve la stationnarité au
second ordre. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’autocovariance possède plusieurs propriétés importantes qui sont
utilisées dans l’analyse des processus stochastiques.</p>
<ol>
<li><p><strong>Symétrie</strong> : <span class="math inline">\(\gamma(k)
= \gamma(-k)\)</span>. Cette propriété découle du fait que la covariance
est symétrique.</p></li>
<li><p><strong>Positivité</strong> : <span
class="math inline">\(\gamma(k) \leq \gamma(0)\)</span>. Cela signifie
que l’autocovariance est maximale lorsque <span class="math inline">\(k
= 0\)</span>.</p></li>
<li><p><strong>Noyau positif</strong> : La matrice d’autocovariance est
un noyau positif, ce qui est crucial pour l’existence de processus
stochastiques.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la symétrie, nous utilisons la
définition de l’autocovariance : <span class="math display">\[\gamma(k)
= \mathbb{E}[(X_t - \mu)(X_{t+k} - \mu)]\]</span></p>
<p>En remplaçant <span class="math inline">\(k\)</span> par <span
class="math inline">\(-k\)</span>, nous obtenons : <span
class="math display">\[\gamma(-k) = \mathbb{E}[(X_t - \mu)(X_{t-k} -
\mu)]\]</span></p>
<p>En utilisant la propriété de symétrie de la covariance, nous avons :
<span class="math display">\[\gamma(-k) = \mathbb{E}[(X_{t-k} - \mu)(X_t
- \mu)] = \gamma(k)\]</span></p>
<p>Ainsi, <span class="math inline">\(\gamma(k) =
\gamma(-k)\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’autocovariance est un outil puissant pour analyser les processus
stochastiques et comprendre leurs propriétés de dépendance. Elle joue un
rôle central dans la théorie des séries temporelles et trouve des
applications dans de nombreux domaines, de la finance à l’ingénierie. En
maîtrisant les concepts et les théorèmes associés à l’autocovariance,
nous pouvons développer des modèles plus précis et robustes pour prédire
le comportement des systèmes dynamiques.</p>
</body>
</html>
{% include "footer.html" %}


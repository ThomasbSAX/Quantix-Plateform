{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de dissimilarité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de
dissimilarité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques de dissimilarité est
une méthode puissante en apprentissage automatique et en reconnaissance
de motifs. Cette approche émerge d’un besoin crucial : représenter des
données complexes de manière à capturer leurs dissimilarités
intrinsèques. Historiquement, cette idée trouve ses racines dans les
travaux sur l’analyse des données et la réduction de dimensionnalité, où
l’objectif est de transformer les données en un espace où les
dissimilarités sont préservées.</p>
<p>L’importance de cette méthode réside dans sa capacité à identifier et
à exploiter les structures sous-jacentes des données. En effet, en se
concentrant sur les dissimilarités plutôt que sur les similarités, on
peut souvent révéler des informations cachées qui seraient autrement
obscurcies. Cette approche est particulièrement utile dans les domaines
où les données sont de haute dimension ou où les relations entre les
points de données ne sont pas linéaires.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
dissimilarité, commençons par définir ce que nous cherchons à capturer.
Imaginons que nous avons un ensemble de données <span
class="math inline">\(X = \{x_1, x_2, \ldots, x_n\}\)</span>. Nous
voulons représenter chaque point <span
class="math inline">\(x_i\)</span> par un vecteur de caractéristiques
qui capture sa dissimilarité avec les autres points.</p>
<p>Formellement, nous cherchons une fonction <span
class="math inline">\(f\)</span> telle que : <span
class="math display">\[f: X \rightarrow \mathbb{R}^d\]</span> où <span
class="math inline">\(d\)</span> est la dimension de l’espace de
caractéristiques. Cette fonction doit satisfaire certaines propriétés
pour être utile.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données.
Une fonction de dissimilarité est une application <span
class="math inline">\(D: X \times X \rightarrow \mathbb{R}^+\)</span>
telle que pour tout <span class="math inline">\(x, y \in X\)</span>, on
a : <span class="math display">\[D(x, y) = D(y, x) \geq 0\]</span> et
<span class="math inline">\(D(x, y) = 0\)</span> si et seulement si
<span class="math inline">\(x = y\)</span>.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(D\)</span> une fonction de
dissimilarité définie sur un ensemble de données <span
class="math inline">\(X\)</span>. Un encodage par extraction de
caractéristiques de dissimilarité est une fonction <span
class="math inline">\(f: X \rightarrow \mathbb{R}^d\)</span> telle que
pour tout <span class="math inline">\(x, y \in X\)</span>, la
dissimilarité entre <span class="math inline">\(f(x)\)</span> et <span
class="math inline">\(f(y)\)</span> est proportionnelle à <span
class="math inline">\(D(x, y)\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour formaliser davantage cette idée, considérons le théorème suivant
:</p>
<div class="theorem">
<p>Soit <span class="math inline">\(D\)</span> une fonction de
dissimilarité définie sur un ensemble de données <span
class="math inline">\(X\)</span>. Il existe une fonction <span
class="math inline">\(f: X \rightarrow \mathbb{R}^d\)</span> telle que
pour tout <span class="math inline">\(x, y \in X\)</span>, on a : <span
class="math display">\[\|f(x) - f(y)\| = D(x, y)\]</span> si et
seulement si <span class="math inline">\(D\)</span> satisfait les
conditions du théorème de l’indice métrique.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous devons montrer que
<span class="math inline">\(D\)</span> satisfait les conditions du
théorème de l’indice métrique. Cela implique que <span
class="math inline">\(D\)</span> doit être symétrique et satisfaire
l’inégalité triangulaire.</p>
<p>Supposons que <span class="math inline">\(D\)</span> soit une
fonction de dissimilarité. Nous devons montrer qu’il existe une fonction
<span class="math inline">\(f\)</span> telle que <span
class="math inline">\(\|f(x) - f(y)\| = D(x, y)\)</span>.</p>
<p>Considérons l’espace <span
class="math inline">\(\mathbb{R}^d\)</span> et définissons <span
class="math inline">\(f(x)\)</span> comme suit : <span
class="math display">\[f(x) = (D(x, x_1), D(x, x_2), \ldots, D(x,
x_n))\]</span> où <span class="math inline">\(\{x_1, x_2, \ldots,
x_n\}\)</span> est une base de l’ensemble <span
class="math inline">\(X\)</span>.</p>
<p>Nous devons vérifier que cette fonction satisfait les conditions
requises. Pour tout <span class="math inline">\(x, y \in X\)</span>,
nous avons : <span class="math display">\[\|f(x) - f(y)\| =
\sqrt{\sum_{i=1}^n (D(x, x_i) - D(y, x_i))^2}\]</span> En utilisant les
propriétés de <span class="math inline">\(D\)</span>, nous pouvons
montrer que cette expression est égale à <span
class="math inline">\(D(x, y)\)</span>.</p>
<p>Réciproquement, si <span class="math inline">\(f\)</span> satisfait
les conditions du théorème, alors <span class="math inline">\(D\)</span>
doit être symétrique et satisfaire l’inégalité triangulaire. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de l’encodage
par extraction de caractéristiques de dissimilarité :</p>
<ol>
<li><p><strong>Conservation des dissimilarités</strong> : L’encodage
préserve les dissimilarités entre les points de données. Cela signifie
que si deux points sont très dissemblables dans l’espace original, ils
le seront également dans l’espace de caractéristiques.</p></li>
<li><p><strong>Réduction de dimensionnalité</strong> : L’encodage peut
être utilisé pour réduire la dimensionnalité des données tout en
préservant les dissimilarités. Cela est particulièrement utile pour
visualiser les données ou pour appliquer des algorithmes de machine
learning qui nécessitent une faible dimensionnalité.</p></li>
<li><p><strong>Robustesse aux bruits</strong> : L’encodage est robuste
aux bruits et aux variations mineures dans les données. Cela signifie
que de petites perturbations dans les données n’affecteront pas
significativement l’encodage.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de dissimilarité est
une méthode puissante pour représenter des données complexes. En se
concentrant sur les dissimilarités plutôt que sur les similarités, cette
approche permet de révéler des structures cachées et de préserver les
relations intrinsèques entre les points de données. Les théorèmes et
propriétés présentés dans cet article montrent que cette méthode est
rigoureuse et peut être appliquée dans divers domaines de
l’apprentissage automatique et de la reconnaissance de motifs.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par somme</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par somme</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de binning par somme
est une technique puissante dans le domaine du traitement des données,
particulièrement en apprentissage automatique et en analyse statistique.
Cette méthode permet de transformer des variables continues en variables
discrètes, facilitant ainsi leur utilisation dans divers modèles
prédictifs. L’origine de cette technique remonte aux premières
tentatives d’optimisation des données pour les algorithmes de
classification et de régression. L’émergence du binning par somme est
motivée par la nécessité de réduire la complexité computationnelle tout
en préservant les informations essentielles des données.</p>
<p>Le binning par somme est indispensable dans les cas où les données
présentent une grande variabilité ou des valeurs aberrantes. En
regroupant les données en intervalles (ou bins), on peut atténuer
l’impact des valeurs extrêmes et améliorer la robustesse des modèles.
Cette technique est particulièrement utile dans les domaines où les
données sont nombreuses et complexes, comme la finance, la biologie, ou
l’ingénierie.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le binning par somme, il est essentiel
de comprendre ce que nous cherchons à accomplir. Nous voulons
transformer une variable continue en une variable discrète tout en
préservant les informations essentielles. Cela implique de regrouper les
valeurs continues dans des intervalles et de calculer une statistique,
comme la somme, pour chaque intervalle.</p>
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue définie sur un espace probabilisé <span
class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. Nous cherchons
à définir une fonction <span class="math inline">\(f: \mathbb{R}
\rightarrow \{1, 2, \dots, k\}\)</span> qui associe à chaque valeur de
<span class="math inline">\(X\)</span> un intervalle parmi <span
class="math inline">\(k\)</span> intervalles disjoints.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue et <span class="math inline">\(k \in \mathbb{N}^*\)</span>. Le
binning par somme consiste à diviser l’intervalle des valeurs de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> sous-intervalles disjoints <span
class="math inline">\(I_1, I_2, \dots, I_k\)</span> tels que : <span
class="math display">\[\bigcup_{i=1}^k I_i = [a, b] \quad \text{et}
\quad I_i \cap I_j = \emptyset \quad \forall i \neq j\]</span> où <span
class="math inline">\(a\)</span> et <span
class="math inline">\(b\)</span> sont les valeurs minimale et maximale
de <span class="math inline">\(X\)</span>. Pour chaque intervalle <span
class="math inline">\(I_i\)</span>, on calcule la somme des valeurs de
<span class="math inline">\(X\)</span> qui tombent dans cet intervalle :
<span class="math display">\[S_i = \sum_{x_j \in I_i} x_j\]</span> La
fonction de binning par somme <span class="math inline">\(f\)</span> est
alors définie par : <span class="math display">\[f(x) = i \quad
\text{si} \quad x \in I_i\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’impact du binning par somme sur les données, il est
utile de considérer le théorème suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue avec une moyenne finie <span class="math inline">\(\mu =
E[X]\)</span>. Si nous appliquons le binning par somme avec <span
class="math inline">\(k\)</span> intervalles, la moyenne des sommes
<span class="math inline">\(S_i\)</span> est égale à <span
class="math inline">\(k\mu\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La moyenne des sommes <span
class="math inline">\(S_i\)</span> est donnée par : <span
class="math display">\[E\left[\sum_{i=1}^k S_i\right] = \sum_{i=1}^k
E[S_i]\]</span> Pour chaque intervalle <span
class="math inline">\(I_i\)</span>, la somme <span
class="math inline">\(S_i\)</span> peut être exprimée comme : <span
class="math display">\[S_i = \sum_{x_j \in I_i} x_j\]</span> En
utilisant la linéarité de l’espérance, nous avons : <span
class="math display">\[E[S_i] = \sum_{x_j \in I_i} E[x_j]\]</span>
Puisque <span class="math inline">\(x_j\)</span> est une réalisation de
la variable aléatoire <span class="math inline">\(X\)</span>, nous avons
: <span class="math display">\[E[x_j] = \mu\]</span> Ainsi, pour chaque
intervalle <span class="math inline">\(I_i\)</span>, la moyenne de <span
class="math inline">\(S_i\)</span> est : <span
class="math display">\[E[S_i] = |I_i| \mu\]</span> où <span
class="math inline">\(|I_i|\)</span> est le nombre de valeurs de <span
class="math inline">\(X\)</span> dans l’intervalle <span
class="math inline">\(I_i\)</span>. En sommant sur tous les intervalles,
nous obtenons : <span class="math display">\[\sum_{i=1}^k E[S_i] =
\sum_{i=1}^k |I_i| \mu = k\mu\]</span> car <span
class="math inline">\(\sum_{i=1}^k |I_i| = n\)</span>, le nombre total
de valeurs de <span class="math inline">\(X\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le binning par somme possède plusieurs propriétés intéressantes qui
en font une technique puissante pour l’encodage des données.</p>
<ol>
<li><p><strong>Conservation de la variance</strong> : La variance des
sommes <span class="math inline">\(S_i\)</span> est proportionnelle à la
variance de <span class="math inline">\(X\)</span>. Plus précisément, si
<span class="math inline">\(\sigma^2 = Var(X)\)</span>, alors : <span
class="math display">\[Var(S_i) = |I_i| \sigma^2\]</span> Cette
propriété est utile pour comprendre comment le binning affecte la
dispersion des données.</p></li>
<li><p><strong>Réduction de la dimension</strong> : En regroupant les
valeurs continues en intervalles, le binning par somme réduit la
dimension des données. Cela peut améliorer les performances des
algorithmes de machine learning en réduisant la complexité
computationnelle.</p></li>
<li><p><strong>Robustesse aux valeurs aberrantes</strong> : Le binning
par somme atténue l’impact des valeurs aberrantes en les regroupant avec
d’autres valeurs dans le même intervalle. Cela rend les modèles plus
robustes aux variations extrêmes des données.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par somme
est une technique puissante et polyvalente pour le traitement des
données. En transformant les variables continues en variables discrètes,
cette méthode permet de simplifier l’analyse des données tout en
préservant les informations essentielles. Les propriétés et théorèmes
associés au binning par somme en font un outil précieux pour les
chercheurs et les praticiens dans divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie \alpha-\beta : Une généralisation de l’entropie de Rényi</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> : Une généralisation de l’entropie
de Rényi</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie <span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> est une généralisation de
l’entropie de Rényi qui trouve ses racines dans les travaux sur les
mesures d’information et la théorie des probabilités. Introduite pour la
première fois par Kumar et Rajagopalan en 2005, cette notion élargit le
cadre des entropies classiques en introduisant deux paramètres <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>, permettant ainsi une plus grande
flexibilité dans la modélisation des incertitudes.</p>
<p>L’entropie <span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> émerge comme une réponse à la
nécessité de capturer des aspects plus subtils de l’information que ceux
pris en compte par les entropies traditionnelles. Elle est indispensable
dans des domaines tels que la théorie de l’information quantique, le
traitement du signal et l’apprentissage automatique, où une
compréhension fine des distributions de probabilité est cruciale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span>, commençons par rappeler que nous
cherchons une mesure qui généralise l’entropie de Rényi. L’entropie de
Rényi, pour un paramètre <span class="math inline">\(\alpha\)</span>,
est définie comme suit :</p>
<p><span class="math display">\[H_{\alpha}(P) = \frac{1}{1 - \alpha}
\log \left( \sum_{i=1}^{n} p_i^{\alpha} \right)\]</span></p>
<p>où <span class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span>
est une distribution de probabilité. L’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> étend cette notion en introduisant
un second paramètre <span class="math inline">\(\beta\)</span>.</p>
<p>Nous cherchons une mesure <span class="math inline">\(H_{\alpha,
\beta}(P)\)</span> telle que :</p>
<p>1. Pour <span class="math inline">\(\beta = 1\)</span>, <span
class="math inline">\(H_{\alpha, \beta}(P) = H_{\alpha}(P)\)</span>. 2.
Pour <span class="math inline">\(\alpha = 1\)</span>, <span
class="math inline">\(H_{\alpha, \beta}(P) = H_{\beta}(P)\)</span>, où
<span class="math inline">\(H_{\beta}\)</span> est une autre
généralisation de l’entropie.</p>
<p>La définition formelle de l’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> est la suivante :</p>
<p><span class="math display">\[H_{\alpha, \beta}(P) = \frac{1}{1 -
\alpha} \log \left( \sum_{i=1}^{n} p_i^{\alpha \beta}
\right)\]</span></p>
<p>où <span class="math inline">\(\alpha, \beta &gt; 0\)</span> et <span
class="math inline">\(\alpha \neq 1\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant l’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> est la généralisation de
l’inégalité de Gibbs. Rappelons d’abord que pour l’entropie de Shannon,
l’inégalité de Gibbs stipule que :</p>
<p><span class="math display">\[H(P) \leq \log n\]</span></p>
<p>où <span class="math inline">\(n\)</span> est le nombre d’éléments
dans l’espace des événements.</p>
<p>Nous cherchons une inégalité similaire pour l’entropie <span
class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span>. Pour ce faire, nous devons
comprendre comment les paramètres <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span> influencent la mesure.</p>
<p>Le théorème suivant généralise l’inégalité de Gibbs pour l’entropie
<span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité. Alors,</p>
<p><span class="math display">\[H_{\alpha, \beta}(P) \leq \frac{\log
n}{1 - \alpha}\]</span></p>
<p>pour <span class="math inline">\(\alpha &gt; 0\)</span> et <span
class="math inline">\(\beta &gt; 0\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous procédons comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Nous commençons par rappeler que pour toute
distribution de probabilité <span class="math inline">\(P\)</span>, nous
avons :</p>
<p><span class="math display">\[\sum_{i=1}^{n} p_i = 1\]</span></p>
<p>En utilisant l’inégalité de Jensen, nous savons que pour une fonction
convexe <span class="math inline">\(\phi\)</span>, nous avons :</p>
<p><span class="math display">\[\sum_{i=1}^{n} p_i \phi(p_i) \geq
\phi\left( \sum_{i=1}^{n} p_i^2 \right)\]</span></p>
<p>Choisissons <span class="math inline">\(\phi(x) = x^{\alpha
\beta}\)</span>. Alors,</p>
<p><span class="math display">\[\sum_{i=1}^{n} p_i^{\alpha \beta + 1}
\geq \left( \sum_{i=1}^{n} p_i^2 \right)^{\alpha \beta}\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons :</p>
<p><span class="math display">\[\sum_{i=1}^{n} p_i^2 \leq 1\]</span></p>
<p>Donc,</p>
<p><span class="math display">\[\sum_{i=1}^{n} p_i^{\alpha \beta + 1}
\geq 1\]</span></p>
<p>En prenant le logarithme, nous obtenons :</p>
<p><span class="math display">\[\log \left( \sum_{i=1}^{n} p_i^{\alpha
\beta + 1} \right) \geq 0\]</span></p>
<p>En multipliant par <span class="math inline">\(\frac{1}{1 -
\alpha}\)</span>, nous avons :</p>
<p><span class="math display">\[H_{\alpha, \beta}(P) = \frac{1}{1 -
\alpha} \log \left( \sum_{i=1}^{n} p_i^{\alpha \beta + 1} \right) \leq
\frac{\log n}{1 - \alpha}\]</span></p>
<p>Ce qui achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie <span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> possède plusieurs propriétés
intéressantes. Nous en listons quelques-unes ci-dessous :</p>
<ol>
<li><p>Pour <span class="math inline">\(\alpha = 1\)</span>, l’entropie
<span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> se réduit à l’entropie de Rényi
:</p>
<p><span class="math display">\[H_{1, \beta}(P) =
H_{\beta}(P)\]</span></p></li>
<li><p>Pour <span class="math inline">\(\beta = 1\)</span>, l’entropie
<span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> se réduit à l’entropie de Rényi
:</p>
<p><span class="math display">\[H_{\alpha, 1}(P) =
H_{\alpha}(P)\]</span></p></li>
<li><p>L’entropie <span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> est une fonction convexe de la
distribution de probabilité <span
class="math inline">\(P\)</span>.</p></li>
</ol>
<p>Pour prouver la propriété (iii), nous procédons comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Nous devons montrer que pour toute distribution de
probabilité <span class="math inline">\(P\)</span> et tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, nous avons :</p>
<p><span class="math display">\[H_{\alpha, \beta}(\lambda P + (1 -
\lambda) Q) \leq \lambda H_{\alpha, \beta}(P) + (1 - \lambda) H_{\alpha,
\beta}(Q)\]</span></p>
<p>où <span class="math inline">\(Q\)</span> est une autre distribution
de probabilité.</p>
<p>En utilisant la convexité du logarithme et l’inégalité de Jensen,
nous avons :</p>
<p><span class="math display">\[\log \left( \sum_{i=1}^{n} (\lambda p_i
+ (1 - \lambda) q_i)^{\alpha \beta} \right) \leq \lambda \log \left(
\sum_{i=1}^{n} p_i^{\alpha \beta} \right) + (1 - \lambda) \log \left(
\sum_{i=1}^{n} q_i^{\alpha \beta} \right)\]</span></p>
<p>En multipliant par <span class="math inline">\(\frac{1}{1 -
\alpha}\)</span>, nous obtenons :</p>
<p><span class="math display">\[H_{\alpha, \beta}(\lambda P + (1 -
\lambda) Q) \leq \lambda H_{\alpha, \beta}(P) + (1 - \lambda) H_{\alpha,
\beta}(Q)\]</span></p>
<p>Ce qui achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie <span class="math inline">\(\alpha\)</span>-<span
class="math inline">\(\beta\)</span> est une généralisation puissante et
flexible de l’entropie de Rényi. Ses propriétés et ses applications en
font un outil précieux dans divers domaines de la théorie de
l’information et des probabilités. En comprenant cette mesure, nous
pouvons mieux capturer les nuances des distributions de probabilité et
améliorer nos modèles dans des contextes variés.</p>
</body>
</html>
{% include "footer.html" %}


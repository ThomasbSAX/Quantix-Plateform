{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage par Extraction de Caractéristiques de Binning par Kurtosis</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage par Extraction de Caractéristiques de
Binning par Kurtosis</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
puissante en apprentissage automatique, particulièrement utile pour les
variables catégorielles. Parmi les différentes méthodes d’extraction de
caractéristiques, le binning par kurtosis se distingue par son approche
innovante. Le kurtosis, ou l’aplatissement, est une mesure de la forme
de la distribution des données. En utilisant cette caractéristique pour
binning, nous pouvons capturer des informations subtiles sur la
distribution sous-jacente des données.</p>
<p>Cette technique émerge comme une solution élégante pour les problèmes
où les variables catégorielles ont des distributions complexes. Elle est
indispensable dans les domaines où la compréhension fine de la
distribution des données peut conduire à des modèles plus précis et
robustes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Commençons par comprendre ce que nous cherchons à accomplir. Nous
voulons transformer une variable catégorielle en une représentation
numérique qui capture non seulement les statistiques de base, mais aussi
des caractéristiques plus subtiles comme le kurtosis. Le binning par
kurtosis est une méthode qui divise les données en intervalles (bins) en
fonction de leur kurtosis.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire.
Le kurtosis de <span class="math inline">\(X\)</span>, noté <span
class="math inline">\(\kappa(X)\)</span>, est défini comme : <span
class="math display">\[\kappa(X) = \frac{\mathbb{E}[(X -
\mu)^4]}{\sigma^4} - 3\]</span> où <span
class="math inline">\(\mu\)</span> est la moyenne de <span
class="math inline">\(X\)</span>, et <span
class="math inline">\(\sigma\)</span> est l’écart-type de <span
class="math inline">\(X\)</span>.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
avec <span class="math inline">\(n\)</span> catégories. Le binning par
kurtosis consiste à diviser les données en <span
class="math inline">\(k\)</span> bins en fonction du kurtosis des
sous-ensembles de catégories. Formellement, pour chaque bin <span
class="math inline">\(B_i\)</span>, nous avons : <span
class="math display">\[B_i = \{ c \in C \mid \kappa(c) \in [\alpha_i,
\beta_i] \}\]</span> où <span class="math inline">\(\alpha_i\)</span> et
<span class="math inline">\(\beta_i\)</span> sont les bornes du bin
<span class="math inline">\(B_i\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’efficacité du binning par kurtosis, examinons un
théorème clé.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
avec <span class="math inline">\(n\)</span> catégories, et soit <span
class="math inline">\(B\)</span> un ensemble de bins obtenus par binning
par kurtosis. Alors, pour tout modèle d’apprentissage automatique <span
class="math inline">\(M\)</span> utilisant les caractéristiques
extraites de <span class="math inline">\(B\)</span>, la performance de
<span class="math inline">\(M\)</span> est améliorée par rapport à un
modèle utilisant des caractéristiques extraites de bins basés uniquement
sur la moyenne ou l’écart-type.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous devons montrer que le kurtosis capture
des informations supplémentaires qui améliorent la performance du
modèle.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un modèle d’apprentissage automatique
<span class="math inline">\(M\)</span> utilisant les caractéristiques
extraites de bins basés sur la moyenne. La performance de <span
class="math inline">\(M\)</span> est limitée par le fait que la moyenne
ne capture pas la forme de la distribution. En ajoutant le kurtosis,
nous capturons des informations sur la forme de la distribution, ce qui
permet au modèle de mieux distinguer les catégories.</p>
<p>Formellement, soit <span class="math inline">\(\mu(c)\)</span> la
moyenne de la catégorie <span class="math inline">\(c\)</span>, et <span
class="math inline">\(\kappa(c)\)</span> le kurtosis de <span
class="math inline">\(c\)</span>. Nous avons : <span
class="math display">\[\mathbb{E}[M(B_{\mu})] \leq
\mathbb{E}[M(B_{\kappa})]\]</span> où <span
class="math inline">\(B_{\mu}\)</span> est l’ensemble de bins basés sur
la moyenne, et <span class="math inline">\(B_{\kappa}\)</span> est
l’ensemble de bins basés sur le kurtosis. Cette inégalité montre que le
modèle utilisant les bins basés sur le kurtosis a une performance
supérieure ou égale à celle du modèle utilisant les bins basés sur la
moyenne. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Examinons quelques propriétés et corollaires du binning par
kurtosis.</p>
<div class="proposition">
<p>Le binning par kurtosis capture des informations supplémentaires qui
améliorent la performance du modèle.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Comme montré dans la preuve du théorème précédent, le
kurtosis capture des informations sur la forme de la distribution qui ne
sont pas capturées par la moyenne ou l’écart-type. Cela permet au modèle
de mieux distinguer les catégories et d’améliorer sa performance. ◻</p>
</div>
<div class="corollaire">
<p>Le binning par kurtosis est particulièrement utile pour les variables
catégorielles avec des distributions complexes.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour les variables catégorielles avec des
distributions complexes, le kurtosis peut capturer des caractéristiques
subtiles qui sont ignorées par les méthodes de binning basées uniquement
sur la moyenne ou l’écart-type. Cela permet au modèle de mieux
comprendre la structure sous-jacente des données et d’améliorer sa
performance. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par kurtosis
est une technique puissante pour transformer les variables catégorielles
en représentations numériques riches. En capturant des informations sur
la forme de la distribution, cette méthode permet aux modèles
d’apprentissage automatique de mieux distinguer les catégories et
d’améliorer leur performance. Les preuves et propriétés présentées
montrent que cette technique est particulièrement utile pour les
variables catégorielles avec des distributions complexes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Prédiction bayésienne : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>Prédiction bayésienne : Fondements et
Applications</strong></h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La prédiction bayésienne émerge comme une réponse élégante aux défis
posés par l’incertitude dans les modèles statistiques. Son origine
remonte aux travaux de Thomas Bayes au XVIIIe siècle, mais son essor
moderne est lié à la révolution computationnelle et aux besoins
croissants de robustesse dans les prédictions. Cette approche est
indispensable lorsqu’il s’agit d’intégrer des informations a priori, de
quantifier l’incertitude des prédictions et de mettre à jour
dynamiquement les croyances en fonction des données observées. En
combinant la théorie de la probabilité et l’inférence statistique, elle
offre un cadre rigoureux pour traiter des problèmes complexes où les
données sont rares ou bruitées.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la prédiction bayésienne, il est essentiel de définir
les concepts clés. Supposons que nous ayons un modèle paramétrique <span
class="math inline">\(\mathcal{M}\)</span> avec des paramètres <span
class="math inline">\(\theta \in \Theta\)</span>. Nous cherchons à
prédire une quantité d’intérêt <span class="math inline">\(Y\)</span>
basée sur des données observées <span
class="math inline">\(X\)</span>.</p>
<div class="definition">
<p>Considérons un espace de paramètres <span
class="math inline">\(\Theta\)</span> et une distribution a priori <span
class="math inline">\(\pi(\theta)\)</span> sur <span
class="math inline">\(\Theta\)</span>. Soit <span
class="math inline">\(X\)</span> un ensemble de données observées et
<span class="math inline">\(Y\)</span> la quantité à prédire. La
prédiction bayésienne de <span class="math inline">\(Y\)</span> est
donnée par l’espérance a posteriori conditionnelle : <span
class="math display">\[\hat{Y} = \mathbb{E}[Y | X] = \int_{\Theta}
\mathbb{E}[Y | \theta, X] \, d\pi(\theta | X)\]</span> où <span
class="math inline">\(\pi(\theta | X)\)</span> est la distribution a
posteriori de <span class="math inline">\(\theta\)</span>
conditionnellement à <span class="math inline">\(X\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[\hat{Y} = \int_{\Theta} \int_{\mathcal{Y}} y \,
p(y | \theta, X) \, dy \, d\pi(\theta | X)\]</span> où <span
class="math inline">\(p(y | \theta, X)\)</span> est la densité de
probabilité conditionnelle de <span class="math inline">\(Y\)</span>
donné <span class="math inline">\(\theta\)</span> et <span
class="math inline">\(X\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en prédiction bayésienne est le théorème de
Bayes, qui permet de mettre à jour les croyances a priori en fonction
des données observées.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\pi(\theta)\)</span> la distribution
a priori sur les paramètres <span class="math inline">\(\theta\)</span>,
et <span class="math inline">\(p(X | \theta)\)</span> la vraisemblance
des données <span class="math inline">\(X\)</span> conditionnellement à
<span class="math inline">\(\theta\)</span>. La distribution a
posteriori de <span class="math inline">\(\theta\)</span> est donnée par
: <span class="math display">\[\pi(\theta | X) = \frac{p(X | \theta)
\pi(\theta)}{\int_{\Theta} p(X | \theta&#39;) \pi(\theta&#39;) \,
d\theta&#39;}\]</span></p>
</div>
<p>Une autre formulation du théorème de Bayes est : <span
class="math display">\[\pi(\theta | X) \propto p(X | \theta)
\pi(\theta)\]</span> où <span class="math inline">\(\propto\)</span>
signifie "proportionnel à".</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Bayes, nous utilisons la définition de la
distribution conditionnelle et la règle de multiplication des
probabilités.</p>
<div class="proof">
<p><em>Proof.</em> Par définition, la distribution conditionnelle <span
class="math inline">\(\pi(\theta | X)\)</span> est donnée par : <span
class="math display">\[\pi(\theta | X) = \frac{p(X,
\theta)}{p(X)}\]</span> où <span class="math inline">\(p(X,
\theta)\)</span> est la densité jointe de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(\theta\)</span>, et <span
class="math inline">\(p(X)\)</span> est la densité marginale de <span
class="math inline">\(X\)</span>.</p>
<p>En utilisant la règle de multiplication des probabilités, nous avons
: <span class="math display">\[p(X, \theta) = p(X | \theta)
\pi(\theta)\]</span> et <span class="math display">\[p(X) =
\int_{\Theta} p(X | \theta&#39;) \pi(\theta&#39;) \,
d\theta&#39;\]</span></p>
<p>En substituant ces expressions dans la définition de <span
class="math inline">\(\pi(\theta | X)\)</span>, nous obtenons : <span
class="math display">\[\pi(\theta | X) = \frac{p(X | \theta)
\pi(\theta)}{\int_{\Theta} p(X | \theta&#39;) \pi(\theta&#39;) \,
d\theta&#39;}\]</span> ce qui achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La prédiction bayésienne possède plusieurs propriétés importantes qui
en font un outil puissant pour l’inférence statistique.</p>
<ol>
<li><p><strong>Consistance</strong> : Si la distribution a priori <span
class="math inline">\(\pi(\theta)\)</span> est correcte et que les
données <span class="math inline">\(X\)</span> sont suffisamment
informatives, la distribution a posteriori <span
class="math inline">\(\pi(\theta | X)\)</span> converge vers la vraie
valeur des paramètres lorsque le nombre de données tend vers
l’infini.</p></li>
<li><p><strong>Robustesse</strong> : La prédiction bayésienne permet
d’incorporer des informations a priori, ce qui la rend robuste aux
petits échantillons et aux données bruitées.</p></li>
<li><p><strong>Flexibilité</strong> : La prédiction bayésienne peut être
appliquée à une large variété de modèles, y compris les modèles
linéaires, les modèles hiérarchiques et les réseaux bayésiens.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La prédiction bayésienne offre un cadre rigoureux et flexible pour
l’inférence statistique, permettant de quantifier l’incertitude et
d’intégrer des informations a priori. Ses applications sont vastes,
allant de la modélisation statistique à l’apprentissage automatique. En
combinant la théorie de la probabilité et les méthodes
computationnelles, elle constitue un outil indispensable pour traiter
des problèmes complexes dans divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


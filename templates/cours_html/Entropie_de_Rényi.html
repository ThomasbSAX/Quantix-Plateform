{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Rényi : Une Généralisation Puissante de l’Entropie de Shannon</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Rényi : Une Généralisation Puissante de
l’Entropie de Shannon</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie, en théorie de l’information, est une mesure fondamentale
qui quantifie l’incertitude ou le désordre dans un système. Introduite
par Claude Shannon en 1948, l’entropie de Shannon a révolutionné les
communications et l’informatique. Cependant, cette mesure présente
certaines limitations, notamment sa sensibilité aux événements de faible
probabilité.</p>
<p>Pour pallier ces limites, Alfred Rényi a proposé en 1961 une
généralisation de l’entropie de Shannon, désormais connue sous le nom
d’entropie de Rényi. Cette généralisation offre une famille d’entropies
paramétrées par un réel <span class="math inline">\(\alpha &gt;
0\)</span>, permettant ainsi de capturer différentes facettes de
l’incertitude.</p>
<p>L’entropie de Rényi est indispensable dans divers domaines, tels que
la théorie des codes, l’apprentissage automatique et la cryptographie.
Elle permet notamment de mieux comprendre les propriétés asymptotiques
des codes correcteurs d’erreurs et de quantifier la complexité
algorithmique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Rényi, considérons un système aléatoire
décrit par une distribution de probabilité discrète <span
class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots, p_n)\)</span>, où
<span class="math inline">\(p_i &gt; 0\)</span> pour tout <span
class="math inline">\(i\)</span> et <span
class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>.</p>
<p>Nous cherchons une mesure qui généralise l’entropie de Shannon <span
class="math inline">\(H(\mathbf{p}) = -\sum_{i=1}^n p_i \log
p_i\)</span> en introduisant un paramètre <span
class="math inline">\(\alpha &gt; 0\)</span>. Cette mesure doit
satisfaire certaines propriétés fondamentales, telles que la continuité
et la convexité.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots,
p_n)\)</span> une distribution de probabilité discrète et <span
class="math inline">\(\alpha &gt; 0\)</span> un paramètre réel.
L’entropie de Rényi d’ordre <span class="math inline">\(\alpha\)</span>
est définie par : <span class="math display">\[H_\alpha(\mathbf{p}) =
\frac{1}{1 - \alpha} \log \left( \sum_{i=1}^n p_i^\alpha
\right).\]</span></p>
</div>
<p>Cette définition peut également être exprimée sous une forme
équivalente : <span class="math display">\[H_\alpha(\mathbf{p}) = \log
\left( \sum_{i=1}^n p_i^\alpha \right)^{1/(1 - \alpha)}.\]</span></p>
<p>Pour <span class="math inline">\(\alpha = 1\)</span>, l’entropie de
Rényi coïncide avec l’entropie de Shannon, ce qui peut être vu en
prenant la limite : <span class="math display">\[\lim_{\alpha \to 1}
H_\alpha(\mathbf{p}) = -\sum_{i=1}^n p_i \log p_i.\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’entropie de Rényi possède plusieurs propriétés remarquables, dont
certaines sont similaires à celles de l’entropie de Shannon. Nous en
présentons quelques-unes ci-dessous.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{p}\)</span> une distribution
de probabilité discrète. Alors, pour <span class="math inline">\(0 &lt;
\alpha &lt; 1\)</span>, l’entropie de Rényi <span
class="math inline">\(H_\alpha(\mathbf{p})\)</span> est décroissante en
fonction de <span class="math inline">\(\alpha\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons la
convexité de la fonction <span class="math inline">\(f(x) =
x^\alpha\)</span>. En effet, pour <span class="math inline">\(0 &lt;
\alpha &lt; 1\)</span>, la fonction <span
class="math inline">\(f\)</span> est convexe. Par conséquent, l’entropie
de Rényi peut être exprimée comme une fonction convexe de <span
class="math inline">\(\alpha\)</span>, ce qui implique sa
décroissance. ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{p}\)</span> une distribution
de probabilité discrète et <span class="math inline">\(0 &lt; \alpha
&lt; 1\)</span>. Alors, l’entropie de Rényi satisfait l’inégalité
suivante : <span class="math display">\[H_\alpha(\mathbf{p}) \geq
0.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette inégalité découle directement de la définition
de l’entropie de Rényi et du fait que <span
class="math inline">\(\sum_{i=1}^n p_i^\alpha \geq 1\)</span> pour <span
class="math inline">\(0 &lt; \alpha &lt; 1\)</span>. En effet, comme
<span class="math inline">\(\mathbf{p}\)</span> est une distribution de
probabilité, nous avons <span class="math inline">\(\sum_{i=1}^n p_i =
1\)</span>, et par l’inégalité de puissance, <span
class="math inline">\(\sum_{i=1}^n p_i^\alpha \geq 1\)</span> pour <span
class="math inline">\(0 &lt; \alpha &lt; 1\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de Rényi possède plusieurs propriétés intéressantes, que
nous énumérons ci-dessous.</p>
<ol>
<li><p><strong>Continuité</strong> : Pour une distribution de
probabilité discrète <span class="math inline">\(\mathbf{p}\)</span>,
l’entropie de Rényi <span
class="math inline">\(H_\alpha(\mathbf{p})\)</span> est continue en
<span class="math inline">\(\alpha\)</span> pour <span
class="math inline">\(\alpha &gt; 0\)</span>.</p></li>
<li><p><strong>Convexité</strong> : Pour une distribution de probabilité
discrète <span class="math inline">\(\mathbf{p}\)</span>, l’entropie de
Rényi <span class="math inline">\(H_\alpha(\mathbf{p})\)</span> est
convexe en <span class="math inline">\(\mathbf{p}\)</span> pour <span
class="math inline">\(\alpha &gt; 1\)</span>.</p></li>
<li><p><strong>Limite</strong> : Pour une distribution de probabilité
discrète <span class="math inline">\(\mathbf{p}\)</span>, nous avons :
<span class="math display">\[\lim_{\alpha \to 0} H_\alpha(\mathbf{p}) =
\log n,\]</span> où <span class="math inline">\(n\)</span> est le nombre
d’éléments dans la distribution <span
class="math inline">\(\mathbf{p}\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Rényi est une généralisation puissante et flexible de
l’entropie de Shannon. Elle permet de capturer différentes facettes de
l’incertitude et possède des propriétés remarquables qui la rendent
indispensable dans divers domaines de la théorie de l’information.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernel Principal Component Analysis: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernel Principal Component Analysis: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>The Principal Component Analysis (PCA) is a fundamental tool in
multivariate statistics and data analysis, widely used for
dimensionality reduction and feature extraction. However, PCA is
inherently linear, which limits its applicability to datasets with
complex, non-linear structures.</p>
<p>The introduction of Kernel PCA (KPCA) has revolutionized the field by
extending the linear PCA to non-linear settings through the use of
kernel methods. This technique, first introduced by Schölkopf et al.,
leverages the "kernel trick" to implicitly map data into
higher-dimensional feature spaces where linear separation becomes
feasible.</p>
<p>KPCA is indispensable in various domains such as image processing,
bioinformatics, and financial modeling, where data often exhibits
non-linear patterns. Its ability to capture intricate relationships
within data makes it a powerful tool for modern data analysis.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>To understand KPCA, we first need to recall the basic principles of
PCA. Given a dataset <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> where each <span class="math inline">\(x_i \in
\mathbb{R}^d\)</span>, PCA aims to find a set of orthogonal directions
(principal components) that maximize the variance of the projected
data.</p>
<p>The kernel trick allows us to perform PCA in a higher-dimensional
space without explicitly computing the coordinates of the data in that
space. Let <span class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathcal{H}\)</span> be a feature map into a Hilbert space <span
class="math inline">\(\mathcal{H}\)</span>. The kernel function <span
class="math inline">\(k\)</span> is defined as: <span
class="math display">\[k(x_i, x_j) = \langle \phi(x_i), \phi(x_j)
\rangle_{\mathcal{H}}\]</span></p>
<p>The kernel matrix <span class="math inline">\(K\)</span> is then
given by: <span class="math display">\[K_{ij} = k(x_i, x_j)\]</span></p>
<p>The key idea is to perform PCA in the feature space <span
class="math inline">\(\mathcal{H}\)</span> using the kernel matrix <span
class="math inline">\(K\)</span>.</p>
<div class="definition">
<p>Given a dataset <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> and a kernel function <span
class="math inline">\(k\)</span>, Kernel PCA involves the following
steps:</p>
<ol>
<li><p>Compute the kernel matrix <span class="math inline">\(K\)</span>
where <span class="math inline">\(K_{ij} = k(x_i,
x_j)\)</span>.</p></li>
<li><p>Center the kernel matrix: <span class="math display">\[K&#39; = K
- \frac{1}{n} \mathbf{1} K - \frac{1}{n} K \mathbf{1} + \frac{1}{n^2}
\mathbf{1} K \mathbf{1}\]</span> where <span
class="math inline">\(\mathbf{1}\)</span> is an <span
class="math inline">\(n \times n\)</span> matrix of all ones.</p></li>
<li><p>Compute the eigenvalues and eigenvectors of <span
class="math inline">\(K&#39;\)</span>.</p></li>
<li><p>The principal components in the feature space are given by the
eigenvectors corresponding to the largest eigenvalues.</p></li>
</ol>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>The following theorem establishes the relationship between KPCA and
standard PCA in the feature space.</p>
<div class="theorem">
<p>Let <span class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathcal{H}\)</span> be a feature map and <span
class="math inline">\(K\)</span> the corresponding kernel matrix. The
principal components obtained via KPCA are equivalent to those obtained
by performing PCA in the feature space <span
class="math inline">\(\mathcal{H}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> We start by noting that the covariance matrix in the
feature space <span class="math inline">\(\mathcal{H}\)</span> is given
by: <span class="math display">\[C = \frac{1}{n} \sum_{i=1}^n (\phi(x_i)
- \mu)(\phi(x_i) - \mu)^T\]</span> where <span
class="math inline">\(\mu\)</span> is the mean of the data in <span
class="math inline">\(\mathcal{H}\)</span>.</p>
<p>The principal components are the eigenvectors of <span
class="math inline">\(C\)</span>. Using the kernel trick, we can express
<span class="math inline">\(C\)</span> in terms of the kernel matrix
<span class="math inline">\(K\)</span>: <span class="math display">\[C =
\frac{1}{n} \Phi^T \Phi - \mu \mu^T\]</span> where <span
class="math inline">\(\Phi\)</span> is the matrix whose rows are <span
class="math inline">\(\phi(x_i)\)</span>.</p>
<p>By centering the kernel matrix as described in the definition, we
obtain a matrix <span class="math inline">\(K&#39;\)</span> whose
eigenvectors correspond to those of <span
class="math inline">\(C\)</span>. Therefore, the principal components
obtained via KPCA are indeed equivalent to those obtained by performing
PCA in <span class="math inline">\(\mathcal{H}\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>To further illustrate the effectiveness of KPCA, let’s consider a
simple example.</p>
<div class="example">
<p>Consider a dataset <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> where each <span class="math inline">\(x_i \in
\mathbb{R}^2\)</span>. Suppose we want to perform KPCA using the
Gaussian kernel: <span class="math display">\[k(x_i, x_j) =
\exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)\]</span></p>
<p>We first compute the kernel matrix <span
class="math inline">\(K\)</span>. Then, we center <span
class="math inline">\(K\)</span> to obtain <span
class="math inline">\(K&#39;\)</span>. The principal components are the
eigenvectors of <span class="math inline">\(K&#39;\)</span>
corresponding to the largest eigenvalues.</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>KPCA enjoys several important properties that make it a powerful tool
for non-linear dimensionality reduction.</p>
<div class="proposition">
<p>The principal components obtained via KPCA are orthogonal in the
feature space <span class="math inline">\(\mathcal{H}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> This follows directly from the fact that the
eigenvectors of a symmetric matrix (such as <span
class="math inline">\(K&#39;\)</span>) are orthogonal. ◻</p>
</div>
<div class="proposition">
<p>KPCA can be used for non-linear classification tasks by combining it
with a classifier such as Support Vector Machines (SVM).</p>
</div>
<div class="proof">
<p><em>Proof.</em> By performing KPCA, we map the data into a
higher-dimensional feature space where linear separation is possible.
This allows us to use linear classifiers such as SVM in the feature
space. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Kernel PCA is a powerful extension of traditional PCA that enables
non-linear dimensionality reduction. Its ability to capture complex
patterns in data makes it an indispensable tool in modern data analysis.
By leveraging the kernel trick, KPCA provides a flexible and efficient
framework for a wide range of applications.</p>
</body>
</html>
{% include "footer.html" %}


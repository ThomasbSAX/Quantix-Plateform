{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Variational Inference: A Comprehensive Overview</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Variational Inference: A Comprehensive Overview</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>Variational inference has emerged as a powerful tool in the field of
Bayesian statistics and machine learning, particularly for dealing with
complex probabilistic models that are intractable to standard inference
methods. The origins of variational inference can be traced back to the
method of moments and the calculus of variations, but its modern form
was popularized by the work of Jordan et al. in the late 1990s.</p>
<p>The primary motivation behind variational inference is to provide an
efficient approximation to the posterior distribution of a Bayesian
model. Traditional Markov Chain Monte Carlo (MCMC) methods, while
theoretically sound, can be computationally expensive and slow to
converge for high-dimensional models. Variational inference, on the
other hand, offers a deterministic approach that is often faster and
scalable to large datasets.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand variational inference, we first need to define some key
concepts. Consider a probabilistic model with observed data <span
class="math inline">\(\mathbf{X}\)</span> and latent variables <span
class="math inline">\(\mathbf{Z}\)</span>. The goal is to approximate
the posterior distribution <span
class="math inline">\(p(\mathbf{Z}|\mathbf{X})\)</span>.</p>
<div class="definition">
<p>The variational distribution <span
class="math inline">\(q(\mathbf{Z})\)</span> is a parameterized family
of distributions that approximates the true posterior <span
class="math inline">\(p(\mathbf{Z}|\mathbf{X})\)</span>. Formally, we
seek to find <span class="math inline">\(q^*(\mathbf{Z}) = \argmin_{q
\in \mathcal{Q}} D_{\text{KL}}(q(\mathbf{Z}) \|
p(\mathbf{Z}|\mathbf{X}))\)</span>, where <span
class="math inline">\(\mathcal{Q}\)</span> is the space of all possible
variational distributions and <span
class="math inline">\(D_{\text{KL}}\)</span> denotes the
Kullback-Leibler divergence.</p>
</div>
<p>The variational distribution <span
class="math inline">\(q(\mathbf{Z})\)</span> is often chosen to be
factorizable, i.e., <span class="math inline">\(q(\mathbf{Z}) =
\prod_{i=1}^n q_i(\mathbf{Z}_i)\)</span>, where <span
class="math inline">\(\mathbf{Z}_i\)</span> represents the i-th
component of the latent variables.</p>
<h1 id="the-evidence-lower-bound-elbo">The Evidence Lower Bound
(ELBO)</h1>
<p>The optimization problem defined in the previous section is often
intractable due to the presence of the true posterior <span
class="math inline">\(p(\mathbf{Z}|\mathbf{X})\)</span>. To address
this, we introduce the Evidence Lower Bound (ELBO), which provides a
lower bound on the log-evidence <span class="math inline">\(\log
p(\mathbf{X})\)</span>.</p>
<div class="theorem">
<p>The ELBO is defined as: <span class="math display">\[\mathcal{L}(q) =
\mathbb{E}_{q(\mathbf{Z})} \left[ \log p(\mathbf{X}, \mathbf{Z}) - \log
q(\mathbf{Z}) \right]\]</span> It can be shown that: <span
class="math display">\[\log p(\mathbf{X}) = \mathcal{L}(q) +
D_{\text{KL}}(q(\mathbf{Z}) \| p(\mathbf{Z}|\mathbf{X}))\]</span> Since
<span class="math inline">\(D_{\text{KL}}(q(\mathbf{Z}) \|
p(\mathbf{Z}|\mathbf{X})) \geq 0\)</span>, it follows that <span
class="math inline">\(\mathcal{L}(q) \leq \log
p(\mathbf{X})\)</span>.</p>
</div>
<p>The ELBO can be interpreted as the difference between the expected
log-joint distribution and the entropy of the variational distribution.
Maximizing the ELBO is equivalent to minimizing the Kullback-Leibler
divergence between <span class="math inline">\(q(\mathbf{Z})\)</span>
and <span class="math inline">\(p(\mathbf{Z}|\mathbf{X})\)</span>.</p>
<h1 id="variational-inference-algorithm">Variational Inference
Algorithm</h1>
<p>The variational inference algorithm involves the following steps:</p>
<p>1. **Initialization**: Choose an initial parameterization for the
variational distribution <span
class="math inline">\(q(\mathbf{Z})\)</span>.</p>
<p>2. **Optimization**: Maximize the ELBO with respect to the parameters
of <span class="math inline">\(q(\mathbf{Z})\)</span>. This is typically
done using gradient-based optimization methods.</p>
<p>3. **Convergence**: Check for convergence of the ELBO. If the change
in the ELBO is below a predefined threshold, stop the optimization.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<ol>
<li><p>**Factorized Variational Distributions**: For many models, the
variational distribution <span
class="math inline">\(q(\mathbf{Z})\)</span> is chosen to be factorized.
This simplifies the optimization problem and makes it computationally
tractable.</p></li>
<li><p>**Mean-Field Approximation**: The mean-field approximation is a
special case of variational inference where the variational distribution
<span class="math inline">\(q(\mathbf{Z})\)</span> is assumed to be
fully factorized. This approximation is often used in practice due to
its simplicity and efficiency.</p></li>
<li><p>**Stochastic Variational Inference**: To handle large datasets,
stochastic variational inference uses mini-batches of data to estimate
the gradients of the ELBO. This makes the algorithm scalable to
large-scale problems.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Variational inference provides a powerful and efficient framework for
approximating the posterior distribution of complex probabilistic
models. Its deterministic nature makes it suitable for large-scale
applications, and its flexibility allows for a wide range of model
specifications. As the field continues to evolve, variational inference
is poised to play an increasingly important role in Bayesian statistics
and machine learning.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Mahalanobis : Une mesure géométrique et statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Mahalanobis : Une mesure géométrique et
statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Mahalanobis émerge d’un besoin fondamental en
statistique et en géométrie : mesurer la proximité entre des points dans
un espace multidimensionnel tout en tenant compte de la structure de
covariance des données. Introduite par le statisticien indien P.C.
Mahalanobis en 1936, cette distance révolutionne l’analyse des données
en intégrant la corrélation entre variables. Elle est indispensable dans
des domaines variés, de l’apprentissage automatique à la détection
d’anomalies, en passant par l’analyse discriminante. Son pouvoir réside
dans sa capacité à normaliser les données, rendant comparables des
dimensions initialement incompatibles.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la distance de Mahalanobis, considérons un espace
vectoriel <span class="math inline">\(\mathbb{R}^n\)</span> où chaque
point représente un vecteur de données. Nous cherchons une mesure qui
capture non seulement la distance euclidienne entre deux points, mais
aussi la structure de dispersion des données. Imaginez un nuage de
points dans <span class="math inline">\(\mathbb{R}^2\)</span> : la
distance euclidienne ignore que certaines directions peuvent être plus
"étirées" que d’autres. La distance de Mahalanobis corrige cette
limitation.</p>
<p>Formellement, soit <span class="math inline">\(\mathbf{x}, \mathbf{y}
\in \mathbb{R}^n\)</span> deux vecteurs, et <span
class="math inline">\(S\)</span> la matrice de covariance des données.
La distance de Mahalanobis entre <span
class="math inline">\(\mathbf{x}\)</span> et <span
class="math inline">\(\mathbf{y}\)</span> est définie comme :</p>
<p><span class="math display">\[D_M(\mathbf{x}, \mathbf{y}) =
\sqrt{(\mathbf{x} - \mathbf{y})^T S^{-1} (\mathbf{x} -
\mathbf{y})}\]</span></p>
<p>où <span class="math inline">\(S^{-1}\)</span> est l’inverse de la
matrice de covariance. Cette définition peut également s’écrire en
utilisant le produit scalaire induit par <span
class="math inline">\(S^{-1}\)</span> :</p>
<p><span class="math display">\[D_M(\mathbf{x}, \mathbf{y}) =
\sqrt{\langle \mathbf{x} - \mathbf{y}, \mathbf{x} - \mathbf{y}
\rangle_S}\]</span></p>
<p>avec <span class="math inline">\(\langle \cdot, \cdot
\rangle_S\)</span> le produit scalaire défini par <span
class="math inline">\(\langle \mathbf{u}, \mathbf{v} \rangle_S =
\mathbf{u}^T S^{-1} \mathbf{v}\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central lié à la distance de Mahalanobis est celui de la
transformation affine. Ce théorème montre que la distance de Mahalanobis
est invariante sous certaines transformations.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(T\)</span> une transformation affine
de <span class="math inline">\(\mathbb{R}^n\)</span> dans lui-même,
définie par <span class="math inline">\(T(\mathbf{x}) = A\mathbf{x} +
\mathbf{b}\)</span>, où <span class="math inline">\(A\)</span> est une
matrice inversible et <span class="math inline">\(\mathbf{b}\)</span> un
vecteur. Alors, pour deux points <span class="math inline">\(\mathbf{x},
\mathbf{y} \in \mathbb{R}^n\)</span>, on a :</p>
<p><span class="math display">\[D_M(T(\mathbf{x}), T(\mathbf{y})) =
D_M(\mathbf{x}, \mathbf{y})\]</span></p>
<p>si et seulement si <span class="math inline">\(A\)</span> est une
matrice orthogonale.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur le fait que <span
class="math inline">\(A\)</span> préserve les distances si et seulement
si <span class="math inline">\(A^T A = I\)</span>, où <span
class="math inline">\(I\)</span> est la matrice identité. En effet,</p>
<p><span class="math display">\[D_M(T(\mathbf{x}), T(\mathbf{y})) =
\sqrt{(T(\mathbf{x}) - T(\mathbf{y}))^T S^{-1} (T(\mathbf{x}) -
T(\mathbf{y}))}\]</span></p>
<p><span class="math display">\[= \sqrt{(A\mathbf{x} + \mathbf{b} -
A\mathbf{y} - \mathbf{b})^T S^{-1} (A\mathbf{x} + \mathbf{b} -
A\mathbf{y} - \mathbf{b})}\]</span></p>
<p><span class="math display">\[= \sqrt{(A(\mathbf{x} - \mathbf{y}))^T
S^{-1} (A(\mathbf{x} - \mathbf{y}))}\]</span></p>
<p><span class="math display">\[= \sqrt{(\mathbf{x} - \mathbf{y})^T A^T
S^{-1} A (\mathbf{x} - \mathbf{y})}\]</span></p>
<p>Pour que cette expression soit égale à <span
class="math inline">\(D_M(\mathbf{x}, \mathbf{y})\)</span>, il faut que
<span class="math inline">\(A^T S^{-1} A = S^{-1}\)</span>. Cela est
vérifié si et seulement si <span class="math inline">\(A^T A =
I\)</span>, c’est-à-dire si <span class="math inline">\(A\)</span> est
orthogonale. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Considérons la preuve de l’invariance sous transformation affine.
Nous avons montré que <span class="math inline">\(D_M(T(\mathbf{x}),
T(\mathbf{y})) = D_M(\mathbf{x}, \mathbf{y})\)</span> si et seulement si
<span class="math inline">\(A\)</span> est orthogonale. Cette preuve
utilise le fait que la matrice de covariance <span
class="math inline">\(S\)</span> est symétrique définie positive, et que
son inverse <span class="math inline">\(S^{-1}\)</span> l’est également.
La condition <span class="math inline">\(A^T A = I\)</span> garantit que
la transformation <span class="math inline">\(T\)</span> ne déforme pas
les distances, ce qui est crucial pour préserver la structure
géométrique des données.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance de Mahalanobis possède plusieurs propriétés importantes
:</p>
<ol>
<li><p>**Invariance par translation** : La distance de Mahalanobis est
invariante sous les translations. En effet, pour tout vecteur <span
class="math inline">\(\mathbf{b} \in \mathbb{R}^n\)</span>, on a :</p>
<p><span class="math display">\[D_M(\mathbf{x} + \mathbf{b}, \mathbf{y}
+ \mathbf{b}) = D_M(\mathbf{x}, \mathbf{y})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle directement de la linéarité du produit
scalaire :</p>
<p><span class="math display">\[D_M(\mathbf{x} + \mathbf{b}, \mathbf{y}
+ \mathbf{b}) = \sqrt{(\mathbf{x} - \mathbf{y})^T S^{-1} (\mathbf{x} -
\mathbf{y})} = D_M(\mathbf{x}, \mathbf{y})\]</span> ◻</p>
</div></li>
<li><p>**Homogénéité** : La distance de Mahalanobis est homogène de
degré 1. Pour tout scalaire <span class="math inline">\(\lambda \in
\mathbb{R}\)</span>, on a :</p>
<p><span class="math display">\[D_M(\lambda \mathbf{x}, \lambda
\mathbf{y}) = |\lambda| D_M(\mathbf{x}, \mathbf{y})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela résulte de la propriété du produit scalaire
:</p>
<p><span class="math display">\[D_M(\lambda \mathbf{x}, \lambda
\mathbf{y}) = \sqrt{(\lambda \mathbf{x} - \lambda \mathbf{y})^T S^{-1}
(\lambda \mathbf{x} - \lambda \mathbf{y})}\]</span></p>
<p><span class="math display">\[= \sqrt{\lambda^2 (\mathbf{x} -
\mathbf{y})^T S^{-1} (\mathbf{x} - \mathbf{y})}\]</span></p>
<p><span class="math display">\[= |\lambda| \sqrt{(\mathbf{x} -
\mathbf{y})^T S^{-1} (\mathbf{x} - \mathbf{y})}\]</span></p>
<p><span class="math display">\[= |\lambda| D_M(\mathbf{x},
\mathbf{y})\]</span> ◻</p>
</div></li>
<li><p>**Inégalité triangulaire** : La distance de Mahalanobis satisfait
l’inégalité triangulaire. Pour tous <span
class="math inline">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in
\mathbb{R}^n\)</span>, on a :</p>
<p><span class="math display">\[D_M(\mathbf{x}, \mathbf{z}) \leq
D_M(\mathbf{x}, \mathbf{y}) + D_M(\mathbf{y}, \mathbf{z})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle du fait que la distance de
Mahalanobis est une norme induite par le produit scalaire. L’inégalité
triangulaire pour les normes s’applique donc directement. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Mahalanobis est un outil puissant pour l’analyse des
données multidimensionnelles. Son intégration de la structure de
covariance permet une mesure plus précise et plus informative que la
distance euclidienne classique. Les propriétés et théorèmes associés à
cette distance en font un pilier de la statistique moderne, avec des
applications allant de la classification à la détection d’anomalies.</p>
</body>
</html>
{% include "footer.html" %}


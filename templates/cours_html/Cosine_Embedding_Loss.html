{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Cosine Embedding Loss: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cosine Embedding Loss: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage profond a révolutionné de nombreux domaines en
fournissant des modèles capables d’extraire des caractéristiques
complexes à partir de données brutes. Cependant, l’efficacité de ces
modèles repose souvent sur la manière dont les embeddings sont appris et
utilisés. L’objectif est de représenter des données dans un espace
vectoriel où les relations sémantiques sont préservées.</p>
<p>L’émergence de la <em>Cosine Embedding Loss</em> répond à un besoin
crucial : capturer les similarités directionnelles entre vecteurs plutôt
que leurs magnitudes. Cette approche est particulièrement utile dans des
tâches où la direction du vecteur est plus informative que sa norme,
comme dans les systèmes de recommandation ou le traitement du langage
naturel.</p>
<p>La perte d’embedding cosinus est indispensable dans les cadres où la
similarité entre les embeddings doit être mesurée de manière invariante
à l’échelle. Elle permet de comparer des vecteurs en tenant compte
uniquement de leur orientation relative, ce qui est souvent plus
pertinent que la distance euclidienne.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la <em>Cosine Embedding Loss</em>, il est essentiel
de définir d’abord les concepts fondamentaux.</p>
<h2 id="embedding">Embedding</h2>
<p>Considérons un ensemble de données <span
class="math inline">\(X\)</span> que nous souhaitons représenter dans un
espace vectoriel <span class="math inline">\(\mathbb{R}^n\)</span>. Un
embedding est une fonction <span class="math inline">\(f: X \rightarrow
\mathbb{R}^n\)</span> qui mappe chaque élément de <span
class="math inline">\(X\)</span> à un vecteur dans <span
class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>Formellement, pour tout <span class="math inline">\(x \in X\)</span>,
nous avons : <span class="math display">\[f(x) = (f_1(x), f_2(x),
\ldots, f_n(x))\]</span></p>
<h2 id="similarité-cosinus">Similarité Cosinus</h2>
<p>La similarité cosinus entre deux vecteurs <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> est définie comme le cosinus
de l’angle entre eux. Elle est donnée par : <span
class="math display">\[\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|
\|\mathbf{b}\|}\]</span></p>
<h2 id="cosine-embedding-loss">Cosine Embedding Loss</h2>
<p>Nous cherchons à minimiser la différence entre la similarité cosinus
de deux vecteurs <span class="math inline">\(\mathbf{a}\)</span> et
<span class="math inline">\(\mathbf{b}\)</span> et une similarité cible
<span class="math inline">\(y\)</span>. La perte d’embedding cosinus est
définie comme : <span class="math display">\[L(\mathbf{a}, \mathbf{b},
y) = \max(0, 1 - y \cdot \text{cosine\_similarity}(\mathbf{a},
\mathbf{b}))\]</span></p>
<p>Cette perte encourage les vecteurs à avoir une similarité cosinus
proche de la valeur cible <span class="math inline">\(y\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-similarité-cosinus">Théorème de la Similarité
Cosinus</h2>
<p>Le théorème suivant montre que la similarité cosinus est une mesure
de similarité valide.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> deux vecteurs non nuls dans
<span class="math inline">\(\mathbb{R}^n\)</span>. La similarité cosinus
satisfait les propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(-1 \leq
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) \leq
1\)</span></p></li>
<li><p><span class="math inline">\(\text{cosine\_similarity}(\mathbf{a},
\mathbf{b}) = 1\)</span> si et seulement si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont colinéaires et de même
sens.</p></li>
<li><p><span class="math inline">\(\text{cosine\_similarity}(\mathbf{a},
\mathbf{b}) = -1\)</span> si et seulement si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont colinéaires et de sens
opposés.</p></li>
<li><p><span class="math inline">\(\text{cosine\_similarity}(\mathbf{a},
\mathbf{b}) = 0\)</span> si et seulement si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont orthogonaux.</p></li>
</ol>
</div>
<h2
id="démonstration-du-théorème-de-la-similarité-cosinus">Démonstration du
Théorème de la Similarité Cosinus</h2>
<div class="proof">
<p><em>Proof.</em> La démonstration repose sur les propriétés du produit
scalaire et de la norme des vecteurs.</p>
<p>1. **Bornes de la Similarité Cosinus** : <span
class="math display">\[\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|
\|\mathbf{b}\|}\]</span> Puisque <span class="math inline">\(\mathbf{a}
\cdot \mathbf{b}\)</span> est borné par <span
class="math inline">\(-\|\mathbf{a}\| \|\mathbf{b}\| \leq \mathbf{a}
\cdot \mathbf{b} \leq \|\mathbf{a}\| \|\mathbf{b}\|\)</span>, il
s’ensuit que : <span class="math display">\[-1 \leq \frac{\mathbf{a}
\cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} \leq 1\]</span></p>
<p>2. **Colinéarité de Même Sens** : Si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont colinéaires et de même
sens, alors il existe un scalaire <span class="math inline">\(\lambda
&gt; 0\)</span> tel que <span class="math inline">\(\mathbf{b} = \lambda
\mathbf{a}\)</span>. Ainsi : <span
class="math display">\[\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= \frac{\mathbf{a} \cdot (\lambda \mathbf{a})}{\|\mathbf{a}\| \|\lambda
\mathbf{a}\|} = \frac{\lambda \|\mathbf{a}\|^2}{\|\mathbf{a}\|^2
\lambda} = 1\]</span></p>
<p>3. **Colinéarité de Sens Opposés** : Si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont colinéaires et de sens
opposés, alors il existe un scalaire <span class="math inline">\(\lambda
&lt; 0\)</span> tel que <span class="math inline">\(\mathbf{b} = \lambda
\mathbf{a}\)</span>. Ainsi : <span
class="math display">\[\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= \frac{\mathbf{a} \cdot (\lambda \mathbf{a})}{\|\mathbf{a}\| \|\lambda
\mathbf{a}\|} = \frac{\lambda \|\mathbf{a}\|^2}{\|\mathbf{a}\|^2
|\lambda|} = -1\]</span></p>
<p>4. **Orthogonalité** : Si <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> sont orthogonaux, alors <span
class="math inline">\(\mathbf{a} \cdot \mathbf{b} = 0\)</span>. Ainsi :
<span class="math display">\[\text{cosine\_similarity}(\mathbf{a},
\mathbf{b}) = \frac{0}{\|\mathbf{a}\| \|\mathbf{b}\|} = 0\]</span> ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-la-cosine-embedding-loss">Preuve de la Cosine
Embedding Loss</h2>
<p>Nous allons démontrer que la <em>Cosine Embedding Loss</em> est une
fonction de perte valide.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux vecteurs <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> et une similarité cible
<span class="math inline">\(y \in [-1, 1]\)</span>. La perte d’embedding
cosinus est définie comme : <span class="math display">\[L(\mathbf{a},
\mathbf{b}, y) = \max(0, 1 - y \cdot
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}))\]</span></p>
<p>Pour montrer que cette perte est valide, nous devons vérifier les
propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(L(\mathbf{a}, \mathbf{b}, y) \geq
0\)</span></p></li>
<li><p><span class="math inline">\(L(\mathbf{a}, \mathbf{b}, y) =
0\)</span> si et seulement si <span
class="math inline">\(\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= y\)</span></p></li>
</ol>
<p>1. **Non-Négativité de la Perte** : <span
class="math display">\[L(\mathbf{a}, \mathbf{b}, y) = \max(0, 1 - y
\cdot \text{cosine\_similarity}(\mathbf{a}, \mathbf{b}))\]</span>
Puisque la fonction <span class="math inline">\(\max\)</span> renvoie
toujours une valeur non négative, il s’ensuit que <span
class="math inline">\(L(\mathbf{a}, \mathbf{b}, y) \geq 0\)</span>.</p>
<p>2. **Condition de Minimisation** : <span
class="math display">\[L(\mathbf{a}, \mathbf{b}, y) = 0\]</span> Si et
seulement si : <span class="math display">\[1 - y \cdot
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) \leq 0\]</span> Ce qui
équivaut à : <span class="math display">\[y \cdot
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) \geq 1\]</span>
Puisque <span class="math inline">\(y \in [-1, 1]\)</span> et <span
class="math inline">\(\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
\in [-1, 1]\)</span>, la condition est satisfaite si et seulement si :
<span class="math display">\[\text{cosine\_similarity}(\mathbf{a},
\mathbf{b}) = y\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-de-la-cosine-embedding-loss">Propriétés de la Cosine
Embedding Loss</h2>
<ol>
<li><p>**Invariance à l’Échelle** : La perte d’embedding cosinus est
invariante à l’échelle des vecteurs <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span>. Cela signifie que pour tout
scalaire <span class="math inline">\(\lambda \neq 0\)</span>, nous avons
: <span class="math display">\[L(\mathbf{a}, \mathbf{b}, y) = L(\lambda
\mathbf{a}, \lambda \mathbf{b}, y)\]</span></p></li>
<li><p>**Symétrie** : La perte d’embedding cosinus est symétrique par
rapport à <span class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span>. Cela signifie que : <span
class="math display">\[L(\mathbf{a}, \mathbf{b}, y) = L(\mathbf{b},
\mathbf{a}, y)\]</span></p></li>
<li><p>**Bornes de la Perte** : La perte d’embedding cosinus est bornée
par 1. Cela signifie que pour tout <span
class="math inline">\(\mathbf{a}, \mathbf{b} \in \mathbb{R}^n\)</span>
et <span class="math inline">\(y \in [-1, 1]\)</span>, nous avons :
<span class="math display">\[L(\mathbf{a}, \mathbf{b}, y) \leq
1\]</span></p></li>
</ol>
<h2 id="démonstrations-des-propriétés">Démonstrations des
Propriétés</h2>
<div class="proof">
<p><em>Proof.</em> **Invariance à l’Échelle** : Considérons un scalaire
<span class="math inline">\(\lambda \neq 0\)</span>. Nous avons : <span
class="math display">\[L(\lambda \mathbf{a}, \lambda \mathbf{b}, y) =
\max(0, 1 - y \cdot \text{cosine\_similarity}(\lambda \mathbf{a},
\lambda \mathbf{b}))\]</span> Puisque <span
class="math inline">\(\text{cosine\_similarity}(\lambda \mathbf{a},
\lambda \mathbf{b}) = \text{cosine\_similarity}(\mathbf{a},
\mathbf{b})\)</span>, il s’ensuit que : <span
class="math display">\[L(\lambda \mathbf{a}, \lambda \mathbf{b}, y) =
L(\mathbf{a}, \mathbf{b}, y)\]</span></p>
<p>**Symétrie** : Considérons deux vecteurs <span
class="math inline">\(\mathbf{a}\)</span> et <span
class="math inline">\(\mathbf{b}\)</span>. Nous avons : <span
class="math display">\[L(\mathbf{b}, \mathbf{a}, y) = \max(0, 1 - y
\cdot \text{cosine\_similarity}(\mathbf{b}, \mathbf{a}))\]</span>
Puisque <span
class="math inline">\(\text{cosine\_similarity}(\mathbf{a}, \mathbf{b})
= \text{cosine\_similarity}(\mathbf{b}, \mathbf{a})\)</span>, il
s’ensuit que : <span class="math display">\[L(\mathbf{b}, \mathbf{a}, y)
= L(\mathbf{a}, \mathbf{b}, y)\]</span></p>
<p>**Bornes de la Perte** : Considérons <span
class="math inline">\(\mathbf{a}, \mathbf{b} \in \mathbb{R}^n\)</span>
et <span class="math inline">\(y \in [-1, 1]\)</span>. Nous avons :
<span class="math display">\[L(\mathbf{a}, \mathbf{b}, y) = \max(0, 1 -
y \cdot \text{cosine\_similarity}(\mathbf{a}, \mathbf{b}))\]</span>
Puisque <span class="math inline">\(y \cdot
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) \in [-1, 1]\)</span>,
il s’ensuit que : <span class="math display">\[1 - y \cdot
\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) \in [0, 2]\]</span>
Ainsi : <span class="math display">\[L(\mathbf{a}, \mathbf{b}, y) =
\max(0, 1 - y \cdot \text{cosine\_similarity}(\mathbf{a}, \mathbf{b}))
\leq 1\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La <em>Cosine Embedding Loss</em> est une mesure de perte puissante
et flexible pour l’apprentissage des embeddings. Elle capture les
similarités directionnelles entre vecteurs et est invariante à
l’échelle, ce qui la rend particulièrement utile dans de nombreuses
applications. Les propriétés et théorèmes présentés dans cet article
montrent que cette perte est bien définie et possède des
caractéristiques désirables pour l’apprentissage profond.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Régression parallèle : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Régression parallèle : Théorie et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La régression parallèle émerge comme une réponse élégante à la
complexité des modèles de régression linéaire dans les espaces de haute
dimension. Historiquement, cette notion trouve ses racines dans
l’analyse des données multidimensionnelles où la corrélation entre
variables rend les méthodes classiques inefficaces. La régression
parallèle propose une approche innovante en projetant les données sur
des sous-espaces parallèles, permettant ainsi de capturer des relations
linéaires cachées.</p>
<p>Ce cadre théorique est indispensable dans les domaines où la
dimension des données excède le nombre d’observations, rendant les
matrices de covariance singulaires. En introduisant une structure
parallèle, nous pouvons estimer des paramètres même dans ces conditions
défavorables. Les applications vont de la génomique, où les gènes sont
souvent corrélés, à l’économie financière, où les actifs peuvent suivre
des tendances communes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la régression parallèle, commençons par définir ce
que nous cherchons à capturer. Imaginez un ensemble de données où chaque
variable dépend linéairement d’un petit nombre de facteurs latents. La
régression parallèle vise à identifier ces facteurs et à estimer les
coefficients correspondants.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X \in \mathbb{R}^{n \times
p}\)</span> une matrice de données avec <span
class="math inline">\(n\)</span> observations et <span
class="math inline">\(p\)</span> variables. Supposons que chaque colonne
de <span class="math inline">\(X\)</span> peut s’écrire comme une
combinaison linéaire d’un petit nombre <span
class="math inline">\(k\)</span> de facteurs latents <span
class="math inline">\(Z \in \mathbb{R}^{n \times k}\)</span>,
c’est-à-dire : <span class="math display">\[X = Z \Gamma + E\]</span> où
<span class="math inline">\(\Gamma \in \mathbb{R}^{k \times p}\)</span>
est la matrice des coefficients et <span class="math inline">\(E \in
\mathbb{R}^{n \times p}\)</span> est le bruit. La régression parallèle
consiste à estimer <span class="math inline">\(Z\)</span> et <span
class="math inline">\(\Gamma\)</span> à partir de <span
class="math inline">\(X\)</span>.</p>
</div>
<p>Une formulation alternative est la suivante : pour tout <span
class="math inline">\(j = 1, \ldots, p\)</span>, nous avons <span
class="math display">\[X_j = Z \gamma_j + e_j\]</span> où <span
class="math inline">\(X_j\)</span> est la <span
class="math inline">\(j\)</span>-ème colonne de <span
class="math inline">\(X\)</span>, <span
class="math inline">\(\gamma_j\)</span> est le <span
class="math inline">\(j\)</span>-ème vecteur colonne de <span
class="math inline">\(\Gamma\)</span>, et <span
class="math inline">\(e_j\)</span> est le <span
class="math inline">\(j\)</span>-ème vecteur colonne de <span
class="math inline">\(E\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en régression parallèle est celui de
l’estimation des facteurs latents. Ce théorème montre que sous certaines
conditions, les facteurs latents peuvent être estimés de manière
cohérente.</p>
<div class="theorem">
<p>Supposons que les colonnes de <span class="math inline">\(Z\)</span>
sont orthonormales et que le bruit <span
class="math inline">\(E\)</span> est gaussien avec une matrice de
covariance diagonale. Alors, les facteurs latents <span
class="math inline">\(Z\)</span> peuvent être estimés par la
décomposition en valeurs singulières (SVD) de <span
class="math inline">\(X\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur la propriété de la SVD qui
décompose <span class="math inline">\(X\)</span> en <span
class="math inline">\(U \Sigma V^T\)</span>, où <span
class="math inline">\(U\)</span> et <span
class="math inline">\(V\)</span> sont des matrices orthogonales et <span
class="math inline">\(\Sigma\)</span> est diagonale. En supposant que
les <span class="math inline">\(k\)</span> plus grandes valeurs
singulières correspondent aux facteurs latents, nous pouvons estimer
<span class="math inline">\(Z\)</span> par les <span
class="math inline">\(k\)</span> premières colonnes de <span
class="math inline">\(U\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la régression parallèle, considérons
un exemple simple. Supposons que nous avons deux variables <span
class="math inline">\(X_1\)</span> et <span
class="math inline">\(X_2\)</span> qui dépendent d’un seul facteur
latent <span class="math inline">\(Z\)</span>. Nous voulons estimer ce
facteur et les coefficients correspondants.</p>
<div class="example">
<p>Soit <span class="math inline">\(X_1 = Z \gamma_1 + e_1\)</span> et
<span class="math inline">\(X_2 = Z \gamma_2 + e_2\)</span>, où <span
class="math inline">\(Z\)</span> est un vecteur gaussien standard et
<span class="math inline">\(e_1, e_2\)</span> sont des bruits
indépendants. Nous pouvons estimer <span
class="math inline">\(Z\)</span> en résolvant le problème d’optimisation
: <span class="math display">\[\hat{Z} = \arg\max_{Z} \|X - Z
\Gamma\|_F^2\]</span> sous la contrainte <span class="math inline">\(Z^T
Z = I_k\)</span>.</p>
</div>
<p>La solution à ce problème est donnée par la SVD de <span
class="math inline">\(X\)</span>, où <span
class="math inline">\(\hat{Z}\)</span> est constitué des <span
class="math inline">\(k\)</span> premières colonnes de <span
class="math inline">\(U\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La régression parallèle possède plusieurs propriétés intéressantes
qui en font un outil puissant pour l’analyse des données.</p>
<div class="proposition">
<p>(i) <strong>Consistance</strong> : Sous des conditions appropriées,
les estimateurs <span class="math inline">\(\hat{Z}\)</span> et <span
class="math inline">\(\hat{\Gamma}\)</span> sont consistants,
c’est-à-dire qu’ils convergent vers les vrais paramètres lorsque <span
class="math inline">\(n \to \infty\)</span>.</p>
<p>(ii) <strong>Robustesse</strong> : La méthode est robuste aux valeurs
aberrantes et aux données manquantes, car elle repose sur la structure
globale des données plutôt que sur des observations individuelles.</p>
<p>(iii) <strong>Interprétabilité</strong> : Les facteurs latents <span
class="math inline">\(Z\)</span> peuvent souvent être interprétés comme
des tendances sous-jacentes ou des composantes principales, facilitant
l’interprétation des résultats.</p>
</div>
<div class="proof">
<p><em>Proof.</em> (i) La consistance découle du théorème des grands
nombres et de la loi des grands nombres, qui garantissent que les
estimateurs convergent vers leurs vraies valeurs lorsque le nombre
d’observations tend vers l’infini.</p>
<p>(ii) La robustesse est une conséquence de la décomposition en valeurs
singulières, qui est insensible aux valeurs aberrantes et peut être
adaptée pour traiter les données manquantes.</p>
<p>(iii) L’interprétabilité est facilitée par la nature des facteurs
latents, qui capturent les variations principales dans les
données. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La régression parallèle représente une avancée significative dans
l’analyse des données de haute dimension. En exploitant la structure
parallèle des données, elle permet d’estimer des paramètres même dans
des conditions défavorables. Les applications sont vastes et variées,
allant de la génomique à l’économie financière. Les propriétés de
consistance, de robustesse et d’interprétabilité en font un outil
précieux pour les chercheurs et les praticiens.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de Rényi généralisée</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de Rényi généralisée</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie de Rényi généralisée émerge comme une extension naturelle
des concepts d’entropie classiques, permettant de capturer des
informations plus fines sur les distributions de probabilité. Introduite
par Alfréd Rényi en 1961, cette notion généralise l’entropie de Shannon
et offre des outils puissants pour l’analyse des systèmes complexes. Son
importance réside dans sa capacité à fournir une mesure plus flexible et
robuste de l’incertitude, adaptable à divers contextes allant de la
théorie de l’information à la physique statistique.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Rényi généralisée, commençons par
explorer ce que nous cherchons à mesurer. Nous voulons quantifier
l’incertitude ou l’information contenue dans une distribution de
probabilité. Cette mesure doit être capable de capturer des aspects plus
subtils que l’entropie de Shannon, notamment en tenant compte de la
rareté des événements.</p>
<p>Considérons une distribution de probabilité <span
class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span> sur un
ensemble fini <span class="math inline">\(\mathcal{X} = \{x_1, x_2,
\ldots, x_n\}\)</span>. Nous cherchons une fonction qui mesure
l’information contenue dans cette distribution, en tenant compte de la
rareté des événements.</p>
<p>La définition formelle de l’entropie de Rényi généralisée est donnée
par :</p>
<div class="definition">
<p>Soit <span class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité sur un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>.
Pour un paramètre <span class="math inline">\(\alpha &gt; 0\)</span> et
<span class="math inline">\(\alpha \neq 1\)</span>, l’entropie de Rényi
d’ordre <span class="math inline">\(\alpha\)</span> est définie par
:</p>
<p><span class="math display">\[H_\alpha(P) = \frac{1}{1 - \alpha} \log
\left( \sum_{i=1}^n p_i^\alpha \right)\]</span></p>
<p>De manière équivalente, on peut écrire :</p>
<p><span class="math display">\[H_\alpha(P) = \frac{1}{1 - \alpha} \log
\left( \sum_{x \in \mathcal{X}} P(x)^\alpha \right)\]</span></p>
<p>En utilisant des quantificateurs, nous pouvons exprimer cette
définition comme suit :</p>
<p><span class="math display">\[H_\alpha(P) = \frac{1}{1 - \alpha} \log
\left( \sum_{i=1}^n p_i^\alpha \right) = \frac{1}{1 - \alpha} \log
\left( \sum_{x \in \mathcal{X}} P(x)^\alpha \right)\]</span></p>
<p>où <span class="math inline">\(P(x) = p_i\)</span> pour <span
class="math inline">\(x = x_i\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de Rényi est le théorème de
continuité, qui montre que l’entropie de Rényi converge vers l’entropie
de Shannon lorsque le paramètre <span
class="math inline">\(\alpha\)</span> tend vers 1.</p>
<p>Commençons par comprendre ce que nous cherchons à démontrer. Nous
voulons montrer que l’entropie de Rényi d’ordre <span
class="math inline">\(\alpha\)</span> se comporte de manière cohérente
lorsque <span class="math inline">\(\alpha\)</span> approche 1, en
convergeant vers l’entropie de Shannon.</p>
<p>La formulation formelle du théorème est la suivante :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité sur un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>.
Alors,</p>
<p><span class="math display">\[\lim_{\alpha \to 1} H_\alpha(P) =
H_1(P)\]</span></p>
<p>où <span class="math inline">\(H_1(P)\)</span> est l’entropie de
Shannon définie par :</p>
<p><span class="math display">\[H_1(P) = -\sum_{i=1}^n p_i \log
p_i\]</span></p>
<p>De manière équivalente, on peut écrire :</p>
<p><span class="math display">\[\lim_{\alpha \to 1} \frac{1}{1 - \alpha}
\log \left( \sum_{i=1}^n p_i^\alpha \right) = -\sum_{i=1}^n p_i \log
p_i\]</span></p>
<p>En utilisant des quantificateurs, nous pouvons exprimer ce théorème
comme suit :</p>
<p><span class="math display">\[\forall \epsilon &gt; 0, \exists \delta
&gt; 0 \text{ tel que } \forall \alpha, |\alpha - 1| &lt; \delta
\implies |H_\alpha(P) - H_1(P)| &lt; \epsilon\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de continuité, nous utilisons la définition
de l’entropie de Rényi et des techniques de calcul différentiel. Voici
une démonstration détaillée :</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(f(\alpha) = \frac{1}{1 - \alpha} \log \left(
\sum_{i=1}^n p_i^\alpha \right)\)</span>. Nous voulons montrer que <span
class="math inline">\(\lim_{\alpha \to 1} f(\alpha) = -\sum_{i=1}^n p_i
\log p_i\)</span>.</p>
<p>D’abord, nous utilisons la définition de la dérivée et le théorème de
L’Hôpital pour évaluer la limite. Calculons la dérivée de <span
class="math inline">\(f(\alpha)\)</span> par rapport à <span
class="math inline">\(\alpha\)</span> :</p>
<p><span class="math display">\[f&#39;(\alpha) = \frac{d}{d\alpha}
\left( \frac{1}{1 - \alpha} \log \left( \sum_{i=1}^n p_i^\alpha \right)
\right)\]</span></p>
<p>En utilisant la règle du quotient et la dérivée de la fonction
logarithmique, nous obtenons :</p>
<p><span class="math display">\[f&#39;(\alpha) = \frac{-\log \left(
\sum_{i=1}^n p_i^\alpha \right)}{(1 - \alpha)^2} + \frac{1}{1 - \alpha}
\cdot \frac{\sum_{i=1}^n p_i^\alpha \log p_i}{\sum_{i=1}^n
p_i^\alpha}\]</span></p>
<p>En évaluant cette dérivée en <span class="math inline">\(\alpha =
1\)</span>, nous utilisons le théorème de L’Hôpital pour montrer que
:</p>
<p><span class="math display">\[\lim_{\alpha \to 1} f(\alpha) =
-\sum_{i=1}^n p_i \log p_i\]</span></p>
<p>Cette étape utilise les propriétés des limites et la continuité de la
fonction logarithmique. Ainsi, nous avons prouvé le théorème de
continuité. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie de Rényi généralisée possède plusieurs propriétés
intéressantes qui en font un outil puissant pour l’analyse des
distributions de probabilité. Voici quelques-unes de ces propriétés
:</p>
<ol>
<li><p><strong>Monotonicité</strong> : Pour <span
class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, l’entropie de Rényi
est décroissante en fonction de <span
class="math inline">\(\alpha\)</span>. Cela signifie que plus <span
class="math inline">\(\alpha\)</span> est petit, plus l’entropie de
Rényi est grande.</p></li>
<li><p><strong>Concavité</strong> : L’entropie de Rényi est une fonction
concave en <span class="math inline">\(\alpha\)</span>. Cela signifie
que pour toute combinaison convexe de distributions, l’entropie de Rényi
est supérieure à la combinaison convexe des entropies.</p></li>
<li><p><strong>Inégalité de Gibbs</strong> : Pour toute distribution de
probabilité <span class="math inline">\(P\)</span>, l’entropie de Rényi
satisfait l’inégalité de Gibbs :</p>
<p><span class="math display">\[H_\alpha(P) \leq \log n\]</span></p>
<p>où <span class="math inline">\(n\)</span> est le nombre d’éléments
dans l’ensemble <span
class="math inline">\(\mathcal{X}\)</span>.</p></li>
</ol>
<p>Chacune de ces propriétés peut être prouvée en utilisant des
techniques d’analyse et des inégalités classiques. Par exemple, la
monotonicité peut être démontrée en utilisant la dérivée de l’entropie
de Rényi par rapport à <span class="math inline">\(\alpha\)</span>,
tandis que la concavité peut être prouvée en utilisant des inégalités de
Jensen.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Chebyshev : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Chebyshev : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Chebyshev, nommée en l’honneur du mathématicien
russe Pafnuty Chebyshev, est un concept fondamental dans la théorie des
probabilités et de l’analyse mathématique. Elle émerge comme une réponse
aux questions concernant le comportement des suites aléatoires et des
séries infinies. Ce concept est indispensable pour comprendre les
limites de la loi des grands nombres dans des contextes non indépendants
et pour analyser la convergence des séries aléatoires.</p>
<p>Chebyshev a introduit cette divergence dans le cadre de ses travaux
sur les inégalités et les lois des grands nombres. Son importance réside
dans sa capacité à fournir des bornes sur la probabilité que les sommes
partielles d’une suite aléatoire s’écartent significativement de leur
espérance. Cela permet de contrôler les risques dans des applications
pratiques, telles que la finance et l’ingénierie.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Chebyshev, commençons par définir
quelques concepts préliminaires. Supposons que nous avons une suite de
variables aléatoires <span class="math inline">\((X_n)_{n \in
\mathbb{N}}\)</span> définies sur un espace probabilisé <span
class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. Nous cherchons
à comprendre le comportement de la somme partielle <span
class="math inline">\(S_n = X_1 + X_2 + \dots + X_n\)</span>.</p>
<p>La divergence de Chebyshev nous permet de quantifier la probabilité
que <span class="math inline">\(S_n\)</span> s’écarte significativement
de son espérance <span class="math inline">\(E[S_n]\)</span>.
Formellement, nous cherchons à borner la probabilité <span
class="math inline">\(P(|S_n - E[S_n]| \geq \epsilon)\)</span> pour un
certain <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires avec <span
class="math inline">\(E[X_n] = 0\)</span> et <span
class="math inline">\(\text{Var}(X_n) &lt; \infty\)</span>. La
divergence de Chebyshev est donnée par l’inégalité : <span
class="math display">\[P\left(\left|\sum_{k=1}^n X_k\right| \geq
\epsilon\right) \leq \frac{\sum_{k=1}^n
\text{Var}(X_k)}{\epsilon^2}\]</span> pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>.</p>
</div>
<p>Cette inégalité montre que la probabilité que la somme partielle
<span class="math inline">\(S_n\)</span> s’écarte de plus de <span
class="math inline">\(\epsilon\)</span> de son espérance est bornée par
le rapport entre la somme des variances et <span
class="math inline">\(\epsilon^2\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le théorème central limite est un résultat fondamental qui justifie
l’utilisation de la divergence de Chebyshev. Il stipule que sous
certaines conditions, la somme de variables aléatoires indépendantes et
identiquement distribuées converge en loi vers une distribution
normale.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires indépendantes et identiquement
distribuées avec <span class="math inline">\(E[X_n] = 0\)</span> et
<span class="math inline">\(\text{Var}(X_n) = \sigma^2 &lt;
\infty\)</span>. Alors, pour tout <span class="math inline">\(x \in
\mathbb{R}\)</span>, nous avons : <span class="math display">\[\lim_{n
\to \infty} P\left(\frac{S_n}{\sqrt{n}\sigma} \leq x\right) =
\Phi(x)\]</span> où <span class="math inline">\(\Phi(x)\)</span> est la
fonction de répartition de la loi normale standard.</p>
</div>
<p>Ce théorème est crucial pour comprendre le comportement asymptotique
des sommes partielles et justifie l’utilisation de la divergence de
Chebyshev pour borner les écarts.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Chebyshev, nous utilisons l’inégalité de
Markov et la définition de la variance. Supposons que <span
class="math inline">\(Y\)</span> est une variable aléatoire avec <span
class="math inline">\(E[Y] = 0\)</span> et <span
class="math inline">\(\text{Var}(Y) &lt; \infty\)</span>. Nous voulons
montrer que : <span class="math display">\[P(|Y| \geq \epsilon) \leq
\frac{\text{Var}(Y)}{\epsilon^2}\]</span></p>
<div class="proof">
<p><em>Proof.</em> Par l’inégalité de Markov, nous avons : <span
class="math display">\[P(|Y| \geq \epsilon) = P(Y^2 \geq \epsilon^2)
\leq \frac{E[Y^2]}{\epsilon^2}\]</span> Puisque <span
class="math inline">\(E[Y] = 0\)</span>, nous avons <span
class="math inline">\(E[Y^2] = \text{Var}(Y)\)</span>. Par conséquent :
<span class="math display">\[P(|Y| \geq \epsilon) \leq
\frac{\text{Var}(Y)}{\epsilon^2}\]</span> ◻</p>
</div>
<p>Cette preuve montre comment l’inégalité de Chebyshev découle
directement des propriétés fondamentales de la variance et de
l’espérance.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Chebyshev possède plusieurs propriétés importantes
qui en font un outil puissant pour l’analyse des suites aléatoires.</p>
<ol>
<li><p><strong>Inégalité de Chebyshev pour les sommes
partielles</strong> : Pour toute suite <span
class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span> de variables
aléatoires avec <span class="math inline">\(E[X_n] = 0\)</span> et <span
class="math inline">\(\text{Var}(X_n) &lt; \infty\)</span>, nous avons :
<span class="math display">\[P\left(\left|\sum_{k=1}^n X_k\right| \geq
\epsilon\right) \leq \frac{\sum_{k=1}^n
\text{Var}(X_k)}{\epsilon^2}\]</span> Cette propriété est directement
dérivée de la définition de la divergence de Chebyshev.</p></li>
<li><p><strong>Convergence en probabilité</strong> : Si <span
class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span> est une suite de
variables aléatoires avec <span class="math inline">\(E[X_n] =
0\)</span> et <span class="math inline">\(\text{Var}(X_n) \to
0\)</span>, alors <span class="math inline">\(S_n\)</span> converge en
probabilité vers 0. Cela signifie que pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, nous avons : <span
class="math display">\[\lim_{n \to \infty} P(|S_n| \geq \epsilon) =
0\]</span> Cette propriété est une conséquence directe de l’inégalité de
Chebyshev.</p></li>
<li><p><strong>Application à la loi des grands nombres</strong> : La
divergence de Chebyshev peut être utilisée pour prouver la loi faible
des grands nombres. Si <span class="math inline">\((X_n)_{n \in
\mathbb{N}}\)</span> est une suite de variables aléatoires indépendantes
et identiquement distribuées avec <span class="math inline">\(E[X_n] =
\mu\)</span> et <span class="math inline">\(\text{Var}(X_n) = \sigma^2
&lt; \infty\)</span>, alors pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, nous avons : <span
class="math display">\[\lim_{n \to \infty} P\left(\left|\frac{S_n}{n} -
\mu\right| \geq \epsilon\right) = 0\]</span> Cette propriété montre que
la moyenne empirique converge en probabilité vers l’espérance
théorique.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Chebyshev est un outil essentiel pour l’analyse des
suites aléatoires et des séries infinies. Elle fournit des bornes
précieuses sur la probabilité que les sommes partielles s’écartent
significativement de leur espérance, ce qui est crucial pour comprendre
le comportement asymptotique des processus stochastiques. Les propriétés
et corollaires associés à cette divergence en font un outil puissant
pour la théorie des probabilités et ses applications pratiques.</p>
</body>
</html>
{% include "footer.html" %}


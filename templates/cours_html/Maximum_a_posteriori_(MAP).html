{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Maximum a posteriori (MAP)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Maximum a posteriori (MAP)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le concept de <strong>Maximum a Posteriori</strong> (MAP) émerge dans
le cadre de l’inférence bayésienne, une approche fondamentale en
statistique et en apprentissage automatique. L’idée centrale est de
tirer des conclusions sur les paramètres d’un modèle à partir de données
observées, en intégrant des informations a priori. Le MAP est
indispensable lorsque l’on cherche à estimer des paramètres de manière
optimale, en combinant les données observées et les connaissances
préalables.</p>
<p>Historiquement, l’inférence bayésienne trouve ses racines dans les
travaux de Thomas Bayes au XVIIIe siècle. Cependant, c’est avec l’essor
des méthodes computationnelles que le MAP a gagné en popularité,
notamment dans les domaines de la reconnaissance de motifs, du
traitement du signal et de l’apprentissage automatique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire le concept de MAP, considérons un problème
d’estimation où nous avons des données observées <span
class="math inline">\(\mathbf{X} = \{x_1, x_2, \ldots, x_n\}\)</span> et
un paramètre <span class="math inline">\(\theta\)</span> que nous
souhaitons estimer. Supposons que nous avons une distribution a priori
<span class="math inline">\(p(\theta)\)</span> pour ce paramètre. Nous
cherchons à trouver la valeur de <span
class="math inline">\(\theta\)</span> qui maximise la distribution a
posteriori <span class="math inline">\(p(\theta |
\mathbf{X})\)</span>.</p>
<p>La distribution a posteriori est donnée par le théorème de Bayes :
<span class="math display">\[p(\theta | \mathbf{X}) = \frac{p(\mathbf{X}
| \theta) p(\theta)}{p(\mathbf{X})}\]</span></p>
<p>Puisque <span class="math inline">\(p(\mathbf{X})\)</span> ne dépend
pas de <span class="math inline">\(\theta\)</span>, maximiser la
distribution a posteriori revient à maximiser le numérateur : <span
class="math display">\[p(\theta | \mathbf{X}) \propto p(\mathbf{X} |
\theta) p(\theta)\]</span></p>
<p>La valeur de <span class="math inline">\(\theta\)</span> qui maximise
cette expression est appelée le <strong>Maximum a Posteriori</strong>
(MAP).</p>
<p>Formellement, nous avons : <span
class="math display">\[\hat{\theta}_{\text{MAP}} = \arg\max_{\theta}
p(\mathbf{X} | \theta) p(\theta)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié au MAP est le <strong>théorème de
Bayes</strong>, qui relie les distributions a priori et a posteriori. Un
autre résultat important est le <strong>théorème de l’estimateur
MAP</strong>, qui montre que sous certaines conditions, l’estimateur MAP
est asymptotiquement efficient.</p>
<p>Pour formuler le théorème de Bayes, nous avons : <span
class="math display">\[\forall \theta \in \Theta, \quad p(\theta |
\mathbf{X}) = \frac{p(\mathbf{X} | \theta)
p(\theta)}{p(\mathbf{X})}\]</span></p>
<p>où <span class="math inline">\(\Theta\)</span> est l’espace des
paramètres.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver que le MAP maximise la distribution a posteriori, nous
devons montrer que : <span
class="math display">\[\hat{\theta}_{\text{MAP}} = \arg\max_{\theta}
p(\mathbf{X} | \theta) p(\theta)\]</span></p>
<p>Considérons la fonction objectif : <span
class="math display">\[J(\theta) = \log p(\mathbf{X} | \theta) + \log
p(\theta)\]</span></p>
<p>Pour trouver le maximum, nous cherchons à résoudre : <span
class="math display">\[\nabla J(\theta) = 0\]</span></p>
<p>En développant cette équation, nous obtenons : <span
class="math display">\[\nabla J(\theta) = \nabla \log p(\mathbf{X} |
\theta) + \nabla \log p(\theta) = 0\]</span></p>
<p>Cette équation peut être résolue numériquement pour trouver la valeur
de <span class="math inline">\(\theta\)</span> qui maximise <span
class="math inline">\(J(\theta)\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les propriétés du MAP sont nombreuses et importantes. Voici
quelques-unes des plus significatives :</p>
<ol>
<li><p>Sous certaines conditions, l’estimateur MAP est asymptotiquement
normal et efficient.</p></li>
<li><p>Le MAP peut être vu comme une généralisation de l’estimateur du
maximum de vraisemblance (MLE) en intégrant des informations a
priori.</p></li>
<li><p>Le MAP est particulièrement utile lorsque les données sont rares
ou bruitées, car il permet d’incorporer des connaissances
préalables.</p></li>
</ol>
<p>Pour prouver la propriété (i), nous devons montrer que sous certaines
conditions, l’estimateur MAP est asymptotiquement normal. Cela peut être
fait en utilisant le théorème central limite et les propriétés de la
distribution a posteriori.</p>
<p>Pour prouver la propriété (ii), nous devons montrer que lorsque <span
class="math inline">\(p(\theta)\)</span> est une distribution uniforme,
le MAP coïncide avec le MLE. Cela peut être fait en observant que :
<span class="math display">\[\hat{\theta}_{\text{MAP}} =
\arg\max_{\theta} p(\mathbf{X} | \theta)\]</span></p>
<p>qui est exactement la définition du MLE.</p>
<p>Pour prouver la propriété (iii), nous devons montrer que le MAP peut
améliorer les performances de l’estimation lorsque les données sont
rares ou bruitées. Cela peut être fait en analysant la variance de
l’estimateur MAP et en comparant avec celle du MLE.</p>
</body>
</html>
{% include "footer.html" %}


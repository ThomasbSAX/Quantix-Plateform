{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’ADMM (Alternating Direction Method of Multipliers) : Une Méthode Puissante pour l’Optimisation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’ADMM (Alternating Direction Method of Multipliers) :
Une Méthode Puissante pour l’Optimisation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’optimisation est un domaine fondamental en mathématiques
appliquées, avec des applications variées allant de la mécanique
quantique à l’apprentissage automatique. Parmi les nombreuses méthodes
d’optimisation, l’Alternating Direction Method of Multipliers (ADMM) se
distingue par sa capacité à résoudre des problèmes d’optimisation
convexe avec une structure particulière, notamment les problèmes de
minimisation sous contraintes.</p>
<p>L’ADMM trouve ses racines dans les années 1970, avec des
contributions de Gabay et Mercier. Cependant, c’est au cours des deux
dernières décennies que cette méthode a gagné en popularité, notamment
grâce à son efficacité dans la résolution de problèmes de grande
dimension et son application dans des domaines tels que l’imagerie
médicale, le traitement du signal, et l’apprentissage automatique.</p>
<p>L’ADMM est particulièrement utile pour les problèmes qui peuvent être
décomposés en sous-problèmes plus simples. Cette méthode alterne entre
la résolution de ces sous-problèmes et la mise à jour des
multiplicateurs de Lagrange, ce qui permet de traiter efficacement les
contraintes. Dans cet article, nous explorerons les définitions,
théorèmes et propriétés de l’ADMM, ainsi que ses preuves et
applications.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’ADMM, il est essentiel de définir quelques concepts
clés. Considérons un problème d’optimisation convexe de la forme
suivante :</p>
<p><span class="math display">\[\begin{equation}
    \label{eq:problem}
    \min_{x, z} f(x) + g(z) \quad \text{subject to} \quad Ax + Bz = c
\end{equation}\]</span></p>
<p>où <span class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span> sont des fonctions convexes, et <span
class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>
sont des matrices appropriées.</p>
<p>L’idée derrière l’ADMM est de décomposer ce problème en
sous-problèmes plus simples. Pour cela, nous introduisons une variable
auxiliaire <span class="math inline">\(y\)</span> et reformulons le
problème comme suit :</p>
<p><span class="math display">\[\begin{equation}
    \label{eq:augmented_problem}
    \min_{x, z, y} f(x) + g(z) + \frac{\rho}{2} \| Ax + Bz - c \|^2_2
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\rho &gt; 0\)</span> est un paramètre
de pénalité.</p>
<p>La méthode ADMM consiste alors à alterner entre la résolution des
sous-problèmes suivants :</p>
<p><span class="math display">\[\begin{equation}
    \label{eq:subproblems}
    x^{k+1} = \arg\min_x f(x) + \frac{\rho}{2} \| Ax + Bz^k - c + y^k
\|^2_2
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
    \label{eq:subproblems_z}
    z^{k+1} = \arg\min_z g(z) + \frac{\rho}{2} \| Ax^{k+1} + Bz - c +
y^k \|^2_2
\end{equation}\]</span></p>
<p>et la mise à jour des multiplicateurs de Lagrange :</p>
<p><span class="math display">\[\begin{equation}
    \label{eq:update_y}
    y^{k+1} = y^k + Ax^{k+1} + Bz^{k+1} - c
\end{equation}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux concernant l’ADMM est le suivant :</p>
<div class="theorem">
<p>Supposons que <span class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span> soient des fonctions convexes fermées
propres, et que le problème (<a href="#eq:problem"
data-reference-type="ref" data-reference="eq:problem">[eq:problem]</a>)
ait une solution. Alors, pour tout <span class="math inline">\(\rho &gt;
0\)</span>, la séquence générée par l’ADMM converge vers une solution du
problème (<a href="#eq:problem" data-reference-type="ref"
data-reference="eq:problem">[eq:problem]</a>).</p>
</div>
<p>Pour prouver ce théorème, nous avons besoin de quelques lemmes
intermédiaires.</p>
<div class="lemma">
<p>La séquence <span class="math inline">\(\{x^k, z^k\}\)</span> générée
par l’ADMM satisfait la condition de KKT pour le problème (<a
href="#eq:problem" data-reference-type="ref"
data-reference="eq:problem">[eq:problem]</a>).</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce lemme repose sur le fait que chaque
sous-problème est résolu exactement, et que les multiplicateurs de
Lagrange sont mis à jour de manière appropriée. En utilisant la théorie
des points selle et les propriétés des fonctions convexes, on peut
montrer que la séquence <span class="math inline">\(\{x^k,
z^k\}\)</span> satisfait les conditions de KKT. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de convergence, nous utilisons les résultats
du lemme précédent et les propriétés des fonctions convexes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction de Lyapunov définie par :</p>
<p><span class="math display">\[\begin{equation}
        \label{eq:lyapunov}
        V(x, z, y) = f(x) + g(z) + \frac{\rho}{2} \| Ax + Bz - c \|^2_2
\end{equation}\]</span></p>
<p>Nous montrons que cette fonction est décroissante le long de la
séquence générée par l’ADMM. En utilisant les mises à jour des
sous-problèmes et des multiplicateurs de Lagrange, nous pouvons montrer
que :</p>
<p><span class="math display">\[\begin{equation}
        V(x^{k+1}, z^{k+1}, y^{k+1}) \leq V(x^k, z^k, y^k)
\end{equation}\]</span></p>
<p>De plus, en utilisant la coercivité de <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span>, nous pouvons conclure que la séquence
<span class="math inline">\(\{x^k, z^k\}\)</span> est bornée. En
combinant ces résultats avec le lemme précédent, nous obtenons la
convergence de l’ADMM vers une solution du problème (<a
href="#eq:problem" data-reference-type="ref"
data-reference="eq:problem">[eq:problem]</a>). ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’ADMM possède plusieurs propriétés intéressantes qui en font une
méthode puissante pour l’optimisation.</p>
<ol>
<li><p><strong>Décomposabilité</strong> : L’ADMM permet de décomposer un
problème complexe en sous-problèmes plus simples, ce qui facilite la
résolution et permet de tirer parti des structures particulières du
problème.</p></li>
<li><p><strong>Convergence</strong> : Comme montré dans le théorème
précédent, l’ADMM converge vers une solution du problème sous des
conditions raisonnables.</p></li>
<li><p><strong>Parallelisabilité</strong> : Les sous-problèmes de l’ADMM
peuvent être résolus en parallèle, ce qui permet d’exploiter les
architectures de calcul parallèles pour accélérer la
résolution.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’ADMM est une méthode puissante et flexible pour l’optimisation
convexe, avec des applications dans de nombreux domaines. Sa capacité à
décomposer les problèmes complexes en sous-problèmes plus simples, ainsi
que sa convergence garantie sous des conditions raisonnables, en font un
outil précieux pour les chercheurs et les praticiens. Dans cet article,
nous avons exploré les définitions, théorèmes et propriétés de l’ADMM,
ainsi que ses preuves. Nous espérons que cette exploration a permis de
mieux comprendre les fondements mathématiques de cette méthode et son
potentiel pour résoudre des problèmes d’optimisation complexes.</p>
</body>
</html>
{% include "footer.html" %}


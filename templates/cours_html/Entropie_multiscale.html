{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie Multiscale : Une Exploration Mathématique et Conceptuelle</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie Multiscale : Une Exploration Mathématique et
Conceptuelle</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie multiscale émerge comme un concept fondamental dans
l’analyse des systèmes complexes, où les phénomènes à différentes
échelles de temps et d’espace jouent un rôle crucial. Historiquement,
l’entropie a été introduite par Clausius dans le cadre de la
thermodynamique pour quantifier l’irréversibilité des processus
physiques. Cependant, avec l’avènement de la théorie de l’information de
Shannon et de la mécanique statistique, l’entropie a trouvé des
applications bien au-delà de la physique.</p>
<p>L’entropie multiscale vise à capturer l’information contenue dans les
signaux ou les données à travers différentes échelles, permettant ainsi
une compréhension plus profonde des structures sous-jacentes. Cette
approche est indispensable dans des domaines tels que le traitement du
signal, l’analyse de données financières, et la modélisation des
systèmes biologiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie multiscale, commençons par rappeler la
notion d’entropie de Shannon. Supposons que nous ayons un ensemble
discret de symboles <span class="math inline">\(\{x_1, x_2, \ldots,
x_n\}\)</span> avec des probabilités associées <span
class="math inline">\(\{p_1, p_2, \ldots, p_n\}\)</span>. L’entropie de
Shannon <span class="math inline">\(H\)</span> est définie comme :</p>
<p><span class="math display">\[H(X) = -\sum_{i=1}^{n} p_i \log
p_i\]</span></p>
<p>Cette mesure quantifie l’incertitude ou l’information contenue dans
la distribution de probabilité <span
class="math inline">\(p\)</span>.</p>
<p>L’entropie multiscale généralise cette notion en considérant des
échelles de temps ou d’espace différentes. Pour un signal <span
class="math inline">\(\{s_t\}_{t=1}^{T}\)</span>, nous définissons
d’abord une série de moyennes mobiles à différentes échelles <span
class="math inline">\(\tau\)</span> :</p>
<p><span class="math display">\[\bar{s}_t^{(\tau)} = \frac{1}{\tau}
\sum_{k=0}^{\tau-1} s_{t-k}\]</span></p>
<p>Ensuite, pour chaque échelle <span
class="math inline">\(\tau\)</span>, nous calculons l’entropie de
Shannon des symboles obtenus à partir des moyennes mobiles. L’entropie
multiscale <span class="math inline">\(H^{MS}\)</span> est alors définie
comme la somme des entropies à différentes échelles :</p>
<p><span class="math display">\[H^{MS}(X) = \sum_{\tau=1}^{T}
H(\bar{X}^{(\tau)})\]</span></p>
<p>où <span class="math inline">\(\bar{X}^{(\tau)}\)</span> représente
la distribution de probabilité des symboles obtenus à l’échelle <span
class="math inline">\(\tau\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie multiscale est le théorème
de la limite centrale multiscale, qui généralise le théorème classique
de la limite centrale. Ce théorème stipule que pour un signal <span
class="math inline">\(\{s_t\}\)</span> avec une moyenne <span
class="math inline">\(\mu\)</span> et une variance finie <span
class="math inline">\(\sigma^2\)</span>, la distribution des moyennes
mobiles <span class="math inline">\(\bar{s}_t^{(\tau)}\)</span> converge
vers une distribution normale lorsque <span
class="math inline">\(\tau\)</span> tend vers l’infini.</p>
<p>Formellement, nous avons :</p>
<p><span class="math display">\[\lim_{\tau \to \infty} P\left(
\frac{\bar{s}_t^{(\tau)} - \mu}{\sigma/\sqrt{\tau}} \leq x \right) =
\Phi(x)\]</span></p>
<p>où <span class="math inline">\(\Phi(x)\)</span> est la fonction de
répartition de la distribution normale standard.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la limite centrale multiscale, nous
commençons par rappeler le théorème classique de la limite centrale.
Supposons que <span class="math inline">\(\{s_t\}\)</span> est une suite
de variables aléatoires indépendantes et identiquement distribuées
(i.i.d.) avec une moyenne <span class="math inline">\(\mu\)</span> et
une variance <span class="math inline">\(\sigma^2\)</span>. Alors, la
somme normalisée :</p>
<p><span class="math display">\[\frac{1}{\sqrt{n}} \sum_{t=1}^{n} (s_t -
\mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)\]</span></p>
<p>où <span class="math inline">\(\xrightarrow{d}\)</span> désigne la
convergence en loi.</p>
<p>En appliquant ce résultat aux moyennes mobiles <span
class="math inline">\(\bar{s}_t^{(\tau)}\)</span>, nous obtenons :</p>
<p><span class="math display">\[\frac{\sqrt{\tau}}{\sigma}
(\bar{s}_t^{(\tau)} - \mu) = \frac{1}{\sqrt{\tau}} \sum_{k=0}^{\tau-1}
(s_{t-k} - \mu) \xrightarrow{d} \mathcal{N}(0, 1)\]</span></p>
<p>Ce qui prouve le théorème de la limite centrale multiscale.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie multiscale possède plusieurs propriétés intéressantes
:</p>
<p>(i) **Additivité** : L’entropie multiscale est additive pour des
signaux indépendants. Si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont deux signaux indépendants, alors
:</p>
<p><span class="math display">\[H^{MS}(X,Y) = H^{MS}(X) +
H^{MS}(Y)\]</span></p>
<p>(ii) **Invariance par translation** : L’entropie multiscale est
invariante par translation. Si <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span> sont deux signaux tels que <span
class="math inline">\(Y_t = X_t + c\)</span> pour une constante <span
class="math inline">\(c\)</span>, alors :</p>
<p><span class="math display">\[H^{MS}(X) = H^{MS}(Y)\]</span></p>
<p>(iii) **Monotonicité** : L’entropie multiscale est monotone
décroissante avec l’échelle. Pour tout signal <span
class="math inline">\(X\)</span>, nous avons :</p>
<p><span class="math display">\[H^{MS}(X) \geq
H(\bar{X}^{(T)})\]</span></p>
<p>où <span class="math inline">\(T\)</span> est la plus grande échelle
considérée.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie multiscale offre un cadre puissant pour l’analyse des
signaux et des données à différentes échelles. En généralisant les
concepts classiques de l’entropie, elle permet une compréhension plus
profonde des structures complexes et des phénomènes émergents. Les
applications potentielles de cette approche sont vastes, allant du
traitement du signal à la modélisation des systèmes biologiques.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Complexité des problèmes d’optimisation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Complexité des problèmes d’optimisation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’optimisation est une branche des mathématiques qui cherche à
maximiser ou minimiser une fonction sous certaines contraintes. Les
problèmes d’optimisation sont omniprésents dans de nombreux domaines,
allant de l’ingénierie à l’économie en passant par la biologie.
Cependant, tous les problèmes d’optimisation ne sont pas égaux en termes
de difficulté de résolution. La complexité des problèmes d’optimisation
est un sujet crucial qui permet de comprendre les limites et les
possibilités des algorithmes utilisés pour les résoudre.</p>
<p>L’origine historique de l’optimisation remonte à l’Antiquité, avec
des problèmes comme la minimisation des distances ou des coûts.
Conceptuellement, l’optimisation émerge de la nécessité de prendre des
décisions optimales dans un monde contraint. Techniquement, l’étude de
la complexité des problèmes d’optimisation permet de classer ces
problèmes en fonction de leur difficulté algorithmique, ce qui est
indispensable pour le développement d’algorithmes efficaces.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la complexité des problèmes d’optimisation, il est
essentiel de définir certains concepts clés. Supposons que nous
cherchions à minimiser une fonction <span class="math inline">\(f:
\mathbb{R}^n \rightarrow \mathbb{R}\)</span> sous certaines contraintes.
Nous voulons trouver un point <span class="math inline">\(x^* \in
\mathbb{R}^n\)</span> tel que <span class="math inline">\(f(x^*) \leq
f(x)\)</span> pour tout <span class="math inline">\(x\)</span> dans un
ensemble défini.</p>
<div class="definition">
<p>Un problème d’optimisation est défini par un triplet <span
class="math inline">\((f, X, g)\)</span>, où:</p>
<ul>
<li><p><span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> est la fonction objectif à minimiser ou
maximiser,</p></li>
<li><p><span class="math inline">\(X \subseteq \mathbb{R}^n\)</span> est
l’ensemble des variables de décision,</p></li>
<li><p><span class="math inline">\(g: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span> est un vecteur de fonctions de
contraintes.</p></li>
</ul>
<p>Formellement, le problème d’optimisation peut être écrit comme: <span
class="math display">\[\min_{x \in X} f(x) \text{ tel que } g_i(x) \leq
0 \text{ pour tout } i = 1, \ldots, m.\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en optimisation est le théorème de
Karush-Kuhn-Tucker (KKT), qui fournit des conditions nécessaires pour
qu’un point soit optimal dans un problème d’optimisation convexe.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((f, X, g)\)</span> un problème
d’optimisation convexe. Un point <span class="math inline">\(x^* \in
X\)</span> est optimal si et seulement s’il existe des multiplicateurs
de Lagrange <span class="math inline">\(\lambda_i \geq 0\)</span> pour
<span class="math inline">\(i = 1, \ldots, m\)</span> tels que: <span
class="math display">\[\begin{cases}
    \nabla f(x^*) + \sum_{i=1}^m \lambda_i \nabla g_i(x^*) = 0, \\
    g_i(x^*) \leq 0 \text{ pour tout } i = 1, \ldots, m, \\
    \lambda_i g_i(x^*) = 0 \text{ pour tout } i = 1, \ldots, m.
\end{cases}\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Karush-Kuhn-Tucker, nous utilisons les
conditions d’optimalité et la théorie des multiplicateurs de
Lagrange.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un problème d’optimisation convexe <span
class="math inline">\((f, X, g)\)</span>. Supposons que <span
class="math inline">\(x^*\)</span> est un point optimal. Nous devons
montrer qu’il existe des multiplicateurs de Lagrange <span
class="math inline">\(\lambda_i \geq 0\)</span> satisfaisant les
conditions du théorème.</p>
<p>1. **Conditions de stationnarité**: Puisque <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g_i\)</span> sont convexes, le gradient de <span
class="math inline">\(f\)</span> au point optimal <span
class="math inline">\(x^*\)</span> doit être orthogonal à l’espace
tangent des contraintes actives. Cela donne la première condition: <span
class="math display">\[\nabla f(x^*) + \sum_{i=1}^m \lambda_i \nabla
g_i(x^*) = 0.\]</span></p>
<p>2. **Contraintes de faisabilité**: Par définition, <span
class="math inline">\(x^*\)</span> doit satisfaire toutes les
contraintes: <span class="math display">\[g_i(x^*) \leq 0 \text{ pour
tout } i = 1, \ldots, m.\]</span></p>
<p>3. **Conditions de complémentarité**: Les multiplicateurs de Lagrange
<span class="math inline">\(\lambda_i\)</span> doivent être non négatifs
et satisfaire la condition de complémentarité: <span
class="math display">\[\lambda_i g_i(x^*) = 0 \text{ pour tout } i = 1,
\ldots, m.\]</span></p>
<p>Ces conditions sont nécessaires et suffisantes pour que <span
class="math inline">\(x^*\)</span> soit un point optimal. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le théorème de Karush-Kuhn-Tucker a plusieurs propriétés
importantes:</p>
<ol>
<li><p>**Convexité**: Le théorème s’applique uniquement aux problèmes
d’optimisation convexes. Pour les problèmes non convexes, les conditions
de KKT ne sont pas nécessairement suffisantes.</p></li>
<li><p>**Multiplicateurs de Lagrange**: Les multiplicateurs de Lagrange
<span class="math inline">\(\lambda_i\)</span> fournissent des
informations sur l’importance relative des contraintes dans le problème
d’optimisation.</p></li>
<li><p>**Conditions nécessaires et suffisantes**: Dans le cas convexe,
les conditions de KKT sont à la fois nécessaires et suffisantes pour
l’optimalité.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour prouver ces propriétés, nous utilisons les
concepts de convexité et de dualité en optimisation.</p>
<p>1. **Convexité**: La convexité de <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g_i\)</span> garantit que les conditions de KKT
sont suffisantes pour l’optimalité. Sans convexité, ces conditions ne
garantissent pas nécessairement un point optimal global.</p>
<p>2. **Multiplicateurs de Lagrange**: Les multiplicateurs de Lagrange
sont obtenus en résolvant le problème dual du problème d’optimisation
original. Ils représentent l’impact des contraintes sur la fonction
objectif.</p>
<p>3. **Conditions nécessaires et suffisantes**: Dans le cas convexe,
les conditions de KKT sont dérivées des conditions d’optimalité et de la
théorie des multiplicateurs de Lagrange, ce qui les rend à la fois
nécessaires et suffisantes. ◻</p>
</div>
</body>
</html>
{% include "footer.html" %}


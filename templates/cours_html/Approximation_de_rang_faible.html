{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Approximation de rang faible: Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Approximation de rang faible: Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’approximation de rang faible est une notion centrale en analyse
numérique et en traitement du signal. Elle émerge naturellement dans le
contexte de la réduction de dimension, où l’on cherche à représenter des
données complexes par des structures plus simples. Historiquement, cette
idée trouve ses racines dans les travaux de Schmidt sur la décomposition
en valeurs singulières (SVD), mais son importance a été pleinement
reconnue avec l’avènement de l’ère numérique et des big data.</p>
<p>L’approximation de rang faible permet de résoudre des problèmes de
grande dimension en les projetant sur des espaces de plus petite
dimension, tout en préservant les caractéristiques essentielles des
données. Cette approche est indispensable dans de nombreux domaines,
tels que la compression d’images, l’apprentissage automatique, et la
modélisation de systèmes dynamiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement la notion d’approximation de rang
faible, il est utile de comprendre ce que l’on cherche à accomplir.
Supposons que nous ayons une matrice <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span> représentant des données. Nous voulons
trouver une matrice <span class="math inline">\(B\)</span> de rang plus
petit que <span class="math inline">\(A\)</span>, mais qui capture les
caractéristiques principales de <span
class="math inline">\(A\)</span>.</p>
<p>Nous cherchons donc une matrice <span
class="math inline">\(B\)</span> telle que la distance entre <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> soit minimisée. Cette distance est
souvent mesurée par la norme de Frobenius, définie comme suit:</p>
<p><span class="math display">\[\|A - B\|_F = \sqrt{\sum_{i=1}^m
\sum_{j=1}^n |a_{ij} - b_{ij}|^2}\]</span></p>
<p>Maintenant, nous pouvons énoncer formellement la définition de
l’approximation de rang faible:</p>
<div class="definition">
<p>Soit <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span> une matrice et <span class="math inline">\(k \leq \min(m,
n)\)</span>. On appelle approximation de rang faible de <span
class="math inline">\(A\)</span> par rapport à la norme de Frobenius,
toute matrice <span class="math inline">\(B \in \mathbb{R}^{m \times
n}\)</span> de rang au plus <span class="math inline">\(k\)</span> qui
minimise la distance <span class="math inline">\(\|A -
B\|_F\)</span>.</p>
</div>
<p>Une autre manière de formuler cette définition est la suivante:</p>
<p><span class="math display">\[B = \arg\min_{\text{rank}(B) \leq k} \|A
- B\|_F\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans le contexte de l’approximation de rang
faible est le théorème d’Eckart-Young-Mirsky. Ce théorème fournit une
caractérisation explicite de l’approximation de rang faible en termes de
la décomposition en valeurs singulières (SVD) de la matrice <span
class="math inline">\(A\)</span>.</p>
<p>Avant d’énoncer ce théorème, rappelons quelques notions
préliminaires. La SVD d’une matrice <span class="math inline">\(A \in
\mathbb{R}^{m \times n}\)</span> est une factorisation de la forme:</p>
<p><span class="math display">\[A = U \Sigma V^T\]</span></p>
<p>où <span class="math inline">\(U \in \mathbb{R}^{m \times m}\)</span>
et <span class="math inline">\(V \in \mathbb{R}^{n \times n}\)</span>
sont des matrices orthogonales, et <span class="math inline">\(\Sigma
\in \mathbb{R}^{m \times n}\)</span> est une matrice diagonale dont les
éléments diagonaux sont les valeurs singulières de <span
class="math inline">\(A\)</span>, notées <span
class="math inline">\(\sigma_1, \sigma_2, \dots, \sigma_r\)</span> avec
<span class="math inline">\(r = \min(m, n)\)</span>.</p>
<p>Le théorème d’Eckart-Young-Mirsky s’énonce comme suit:</p>
<div class="theorem">
<p>Soit <span class="math inline">\(A \in \mathbb{R}^{m \times
n}\)</span> une matrice de rang <span class="math inline">\(r\)</span>
et <span class="math inline">\(k \leq r\)</span>. L’approximation de
rang faible de <span class="math inline">\(A\)</span> par rapport à la
norme de Frobenius est donnée par:</p>
<p><span class="math display">\[B = U_k \Sigma_k V_k^T\]</span></p>
<p>où <span class="math inline">\(U_k\)</span> est la matrice formée des
<span class="math inline">\(k\)</span> premiers vecteurs singuliers à
gauche de <span class="math inline">\(A\)</span>, <span
class="math inline">\(\Sigma_k\)</span> est la matrice diagonale formée
des <span class="math inline">\(k\)</span> premières valeurs singulières
de <span class="math inline">\(A\)</span>, et <span
class="math inline">\(V_k\)</span> est la matrice formée des <span
class="math inline">\(k\)</span> premiers vecteurs singuliers à droite
de <span class="math inline">\(A\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème d’Eckart-Young-Mirsky, nous allons utiliser
la SVD de la matrice <span class="math inline">\(A\)</span>. Soit <span
class="math inline">\(A = U \Sigma V^T\)</span> la SVD de <span
class="math inline">\(A\)</span>. Nous voulons montrer que
l’approximation de rang faible de <span class="math inline">\(A\)</span>
est donnée par:</p>
<p><span class="math display">\[B = U_k \Sigma_k V_k^T\]</span></p>
<p>où <span class="math inline">\(U_k\)</span>, <span
class="math inline">\(\Sigma_k\)</span>, et <span
class="math inline">\(V_k\)</span> sont définis comme dans l’énoncé du
théorème.</p>
<p>Considérons une matrice <span class="math inline">\(B\)</span> de
rang au plus <span class="math inline">\(k\)</span>. Nous pouvons écrire
<span class="math inline">\(B\)</span> sous la forme:</p>
<p><span class="math display">\[B = U_k \tilde{\Sigma} V_k^T +
E\]</span></p>
<p>où <span class="math inline">\(\tilde{\Sigma}\)</span> est une
matrice diagonale et <span class="math inline">\(E\)</span> est une
matrice de rang au plus <span class="math inline">\(k\)</span>. En
utilisant l’orthogonalité des matrices <span
class="math inline">\(U\)</span> et <span
class="math inline">\(V\)</span>, nous pouvons calculer la norme de
Frobenius de <span class="math inline">\(A - B\)</span>:</p>
<p><span class="math display">\[\|A - B\|_F^2 = \|U \Sigma V^T - U_k
\tilde{\Sigma} V_k^T - E\|_F^2\]</span></p>
<p>En développant cette expression, nous obtenons:</p>
<p><span class="math display">\[\|A - B\|_F^2 = \|\Sigma -
\tilde{\Sigma}\|_F^2 + \|E\|_F^2\]</span></p>
<p>où nous avons utilisé le fait que <span class="math inline">\(U_k^T
U_k = I_k\)</span> et <span class="math inline">\(V_k^T V_k =
I_k\)</span>.</p>
<p>Pour minimiser <span class="math inline">\(\|A - B\|_F^2\)</span>, il
suffit de minimiser <span class="math inline">\(\|\Sigma -
\tilde{\Sigma}\|_F^2\)</span> et <span
class="math inline">\(\|E\|_F^2\)</span>. Il est clair que le minimum de
<span class="math inline">\(\|\Sigma - \tilde{\Sigma}\|_F^2\)</span> est
atteint lorsque <span class="math inline">\(\tilde{\Sigma} =
\Sigma_k\)</span>. De plus, le minimum de <span
class="math inline">\(\|E\|_F^2\)</span> est atteint lorsque <span
class="math inline">\(E = 0\)</span>.</p>
<p>Par conséquent, l’approximation de rang faible de <span
class="math inline">\(A\)</span> est donnée par:</p>
<p><span class="math display">\[B = U_k \Sigma_k V_k^T\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de
l’approximation de rang faible.</p>
<ol>
<li><p>L’approximation de rang faible est unique. En effet, la SVD d’une
matrice est unique à des permutations près des vecteurs singuliers. Par
conséquent, l’approximation de rang faible est également
unique.</p></li>
<li><p>L’erreur d’approximation peut être exprimée en termes des valeurs
singulières de la matrice <span class="math inline">\(A\)</span>. Plus
précisément, si <span class="math inline">\(B\)</span> est
l’approximation de rang faible de <span
class="math inline">\(A\)</span>, alors:</p>
<p><span class="math display">\[\|A - B\|_F = \sqrt{\sum_{i=k+1}^r
\sigma_i^2}\]</span></p>
<p>où <span class="math inline">\(\sigma_{k+1}, \dots, \sigma_r\)</span>
sont les valeurs singulières de <span class="math inline">\(A\)</span>
qui ne sont pas prises en compte dans l’approximation.</p></li>
<li><p>L’approximation de rang faible est stable par rapport aux
perturbations de la matrice <span class="math inline">\(A\)</span>. Plus
précisément, si <span class="math inline">\(\tilde{A}\)</span> est une
perturbation de <span class="math inline">\(A\)</span> telle que <span
class="math inline">\(\|\tilde{A} - A\|_F \leq \epsilon\)</span>, alors
l’approximation de rang faible <span
class="math inline">\(\tilde{B}\)</span> de <span
class="math inline">\(\tilde{A}\)</span> satisfait:</p>
<p><span class="math display">\[\|\tilde{B} - B\|_F \leq
2\epsilon\]</span></p>
<p>où <span class="math inline">\(B\)</span> est l’approximation de rang
faible de <span class="math inline">\(A\)</span>.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’approximation de rang faible est une notion puissante et
polyvalente qui trouve des applications dans de nombreux domaines. Grâce
au théorème d’Eckart-Young-Mirsky, nous disposons d’un outil efficace
pour calculer cette approximation en utilisant la SVD. Les propriétés de
stabilité et d’unicité de l’approximation de rang faible en font un
outil précieux pour la réduction de dimension et le traitement des
données.</p>
</body>
</html>
{% include "footer.html" %}


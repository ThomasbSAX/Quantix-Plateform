{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Csiszár-Kullback</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Csiszár-Kullback</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Csiszár-Kullback, également connue sous le nom
d’inégalité de divergence relative, est un résultat fondamental en
théorie de l’information et en probabilités. Elle établit une borne
supérieure sur la divergence de Kullback-Leibler entre deux mesures de
probabilité, en fonction d’une certaine distance entre ces mesures.
Cette inégalité a des applications dans de nombreux domaines, notamment
en estimation statistique, en apprentissage automatique et en théorie
des codes.</p>
<p>L’origine de cette inégalité remonte aux travaux de Solomon Kullback
et Richard Leibler dans les années 1950, qui ont introduit la notion de
divergence relative pour mesurer la distance entre deux distributions de
probabilité. Imre Csiszár a ensuite généralisé et approfondi ces
résultats dans les années 1960, en établissant des inégalités plus
fortes et plus générales.</p>
<p>Dans cet article, nous présenterons l’inégalité de Csiszár-Kullback,
ses définitions, ses théorèmes associés et leurs preuves détaillées.
Nous explorerons également certaines de ses propriétés et
corollaires.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de présenter l’inégalité de Csiszár-Kullback, nous devons
introduire quelques notions préliminaires.</p>
<h2 class="unnumbered" id="divergence-de-kullback-leibler">Divergence de
Kullback-Leibler</h2>
<p>La divergence de Kullback-Leibler, notée <span
class="math inline">\(D(P \parallel Q)\)</span>, est une mesure de la
distance entre deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Elle est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La divergence de Kullback-Leibler de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D(P \parallel Q) = \begin{cases}
\int_{\Omega} \log\left(\frac{dP}{dQ}\right) dP &amp; \text{si } P \ll
Q, \\
+\infty &amp; \text{sinon.}
\end{cases}\]</span> où <span
class="math inline">\(\frac{dP}{dQ}\)</span> est la densité de
Radon-Nikodym de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span>.</p>
</div>
<h2 class="unnumbered" id="distance-de-variabilité-totale">Distance de
variabilité totale</h2>
<p>La distance de variabilité totale, notée <span
class="math inline">\(\|P - Q\|_{\text{TV}}\)</span>, est une autre
mesure de la distance entre deux distributions de probabilité. Elle est
définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La distance de variabilité totale entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[\|P - Q\|_{\text{TV}} = \sup_{A \in \mathcal{F}}
|P(A) - Q(A)|.\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant l’inégalité de Csiszár-Kullback.</p>
<h2 class="unnumbered" id="inégalité-de-csiszár-kullback">Inégalité de
Csiszár-Kullback</h2>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, avec <span class="math inline">\(P \ll
Q\)</span>. Alors, pour tout <span class="math inline">\(t &gt;
0\)</span>, <span class="math display">\[P\left(\left\{\omega \in \Omega
: \log\left(\frac{dP}{dQ}(\omega)\right) &gt; t\right\}\right) \leq
e^{-t}.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous allons prouver cette inégalité en utilisant le
théorème de Markov et la convexité de la fonction exponentielle.</p>
<p>Soit <span class="math inline">\(t &gt; 0\)</span>. Par le théorème
de Markov, nous avons : <span
class="math display">\[P\left(\left\{\omega \in \Omega :
\log\left(\frac{dP}{dQ}(\omega)\right) &gt; t\right\}\right) \leq
\frac{1}{t} \int_{\Omega} \log\left(\frac{dP}{dQ}\right)
dP.\]</span></p>
<p>En utilisant l’inégalité de Jensen et la convexité de la fonction
exponentielle, nous obtenons : <span
class="math display">\[\int_{\Omega} \log\left(\frac{dP}{dQ}\right) dP
\leq \log\left(\int_{\Omega} \frac{dP}{dQ} dP\right) = \log(1) =
0.\]</span></p>
<p>En combinant ces deux résultats, nous avons : <span
class="math display">\[P\left(\left\{\omega \in \Omega :
\log\left(\frac{dP}{dQ}(\omega)\right) &gt; t\right\}\right) \leq
\frac{1}{t} \cdot 0 = e^{-t}.\]</span></p>
<p>Cela achève la preuve de l’inégalité de Csiszár-Kullback. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous présentons maintenant quelques propriétés et corollaires de
l’inégalité de Csiszár-Kullback.</p>
<h2 class="unnumbered" id="propriétés">Propriétés</h2>
<ol>
<li><p>L’inégalité de Csiszár-Kullback est valable pour toute mesure de
probabilité <span class="math inline">\(P\)</span> absolument continue
par rapport à une autre mesure de probabilité <span
class="math inline">\(Q\)</span>.</p></li>
<li><p>L’inégalité de Csiszár-Kullback peut être utilisée pour établir
des bornes supérieures sur la divergence de Kullback-Leibler entre deux
distributions de probabilité.</p></li>
<li><p>L’inégalité de Csiszár-Kullback est une généralisation de
l’inégalité de Chernoff, qui est un résultat classique en théorie des
probabilités.</p></li>
</ol>
<h2 class="unnumbered" id="corollaires">Corollaires</h2>
<div class="corollary">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, avec <span class="math inline">\(P \ll
Q\)</span>. Alors, <span class="math display">\[D(P \parallel Q) \geq
0.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous allons prouver ce corollaire en utilisant
l’inégalité de Csiszár-Kullback.</p>
<p>Soit <span class="math inline">\(t &gt; 0\)</span>. Par l’inégalité
de Csiszár-Kullback, nous avons : <span
class="math display">\[P\left(\left\{\omega \in \Omega :
\log\left(\frac{dP}{dQ}(\omega)\right) &gt; t\right\}\right) \leq
e^{-t}.\]</span></p>
<p>En faisant tendre <span class="math inline">\(t\)</span> vers
l’infini, nous obtenons : <span
class="math display">\[P\left(\left\{\omega \in \Omega :
\log\left(\frac{dP}{dQ}(\omega)\right) = +\infty\right\}\right) =
0.\]</span></p>
<p>Cela signifie que <span
class="math inline">\(\log\left(\frac{dP}{dQ}\right)\)</span> est
presque sûrement finie, et donc <span class="math inline">\(D(P
\parallel Q)\)</span> est bien définie. De plus, en utilisant
l’inégalité de Jensen et la convexité de la fonction exponentielle, nous
avons : <span class="math display">\[D(P \parallel Q) = \int_{\Omega}
\log\left(\frac{dP}{dQ}\right) dP \geq \log\left(\int_{\Omega}
\frac{dP}{dQ} dP\right) = \log(1) = 0.\]</span></p>
<p>Cela achève la preuve du corollaire. ◻</p>
</div>
<div class="corollary">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, avec <span class="math inline">\(P \ll
Q\)</span>. Alors, <span class="math display">\[D(P \parallel Q) = 0
\iff P = Q.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous allons prouver ce corollaire en utilisant le
corollaire précédent et la définition de la divergence de
Kullback-Leibler.</p>
<p>Supposons d’abord que <span class="math inline">\(D(P \parallel Q) =
0\)</span>. Par le corollaire précédent, nous avons <span
class="math inline">\(D(P \parallel Q) \geq 0\)</span>, et donc <span
class="math inline">\(D(P \parallel Q) = 0\)</span>. En utilisant la
définition de la divergence de Kullback-Leibler, nous avons : <span
class="math display">\[\int_{\Omega} \log\left(\frac{dP}{dQ}\right) dP =
0.\]</span></p>
<p>Cela implique que <span
class="math inline">\(\log\left(\frac{dP}{dQ}\right) = 0\)</span>
presque sûrement, et donc <span class="math inline">\(\frac{dP}{dQ} =
1\)</span> presque sûrement. Par conséquent, <span
class="math inline">\(P = Q\)</span>.</p>
<p>Réciproquement, supposons que <span class="math inline">\(P =
Q\)</span>. Alors, par définition de la divergence de Kullback-Leibler,
nous avons : <span class="math display">\[D(P \parallel Q) =
\int_{\Omega} \log\left(\frac{dP}{dQ}\right) dP = \int_{\Omega} \log(1)
dP = 0.\]</span></p>
<p>Cela achève la preuve du corollaire. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons présenté l’inégalité de
Csiszár-Kullback, ses définitions, ses théorèmes associés et leurs
preuves détaillées. Nous avons également exploré certaines de ses
propriétés et corollaires.</p>
<p>L’inégalité de Csiszár-Kullback est un résultat fondamental en
théorie de l’information et en probabilités, avec des applications dans
de nombreux domaines. Elle établit une borne supérieure sur la
divergence de Kullback-Leibler entre deux mesures de probabilité, en
fonction d’une certaine distance entre ces mesures.</p>
<p>Nous espérons que cet article aura permis de mieux comprendre cette
inégalité et ses implications.</p>
</body>
</html>
{% include "footer.html" %}


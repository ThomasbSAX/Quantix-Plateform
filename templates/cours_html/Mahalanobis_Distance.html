{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>The Mahalanobis Distance: A Fundamental Measure in Multivariate Statistics</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Mahalanobis Distance: A Fundamental Measure in
Multivariate Statistics</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The Mahalanobis distance, introduced by the Indian statistician
Pranab Kumar Mahalanobis in 1936, is a measure of the distance between a
point and a distribution. Unlike the Euclidean distance, which measures
the straight-line distance between two points in Euclidean space, the
Mahalanobis distance takes into account the correlations of the data set
and is scale-invariant. This makes it particularly useful in
multivariate statistics, where variables often have different units and
are correlated.</p>
<p>The Mahalanobis distance is indispensable in various fields such as
pattern recognition, cluster analysis, and anomaly detection. It
provides a way to determine the similarity of a point to a distribution,
which is crucial in identifying outliers and understanding the structure
of high-dimensional data.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand what we are aiming to achieve, consider the following
scenario: We have a set of multivariate data points, and we want to
measure how far a new point is from the center of this distribution.
However, the variables may have different scales and correlations. The
Mahalanobis distance addresses this by standardizing the data and
accounting for these correlations.</p>
<p>Formally, let <span class="math inline">\(\mathbf{X} = (X_1, X_2,
\ldots, X_p)\)</span> be a random vector with mean <span
class="math inline">\(\boldsymbol{\mu} = (\mu_1, \mu_2, \ldots,
\mu_p)\)</span> and covariance matrix <span
class="math inline">\(\boldsymbol{\Sigma}\)</span>. The Mahalanobis
distance <span class="math inline">\(D_M(\mathbf{x})\)</span> of a point
<span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots,
x_p)\)</span> from the distribution is defined as:</p>
<p><span class="math display">\[D_M(\mathbf{x}) = \sqrt{(\mathbf{x} -
\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} -
\boldsymbol{\mu})}\]</span></p>
<p>Alternatively, we can express this in terms of the inverse of the
covariance matrix:</p>
<p><span class="math display">\[D_M(\mathbf{x}) = \sqrt{\sum_{i=1}^p
\sum_{j=1}^p (x_i - \mu_i) \Sigma^{ij} (x_j - \mu_j)}\]</span></p>
<p>where <span class="math inline">\(\Sigma^{ij}\)</span> denotes the
<span class="math inline">\((i,j)\)</span>-th element of the inverse
covariance matrix <span
class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key theorems related to the Mahalanobis distance is that
it measures the number of standard deviations a point is from the mean
in the direction of the principal components. This can be formalized as
follows:</p>
<p>Suppose <span class="math inline">\(\mathbf{X}\)</span> is a
multivariate normal distribution with mean <span
class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix
<span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then, for any
point <span class="math inline">\(\mathbf{x}\)</span>, the Mahalanobis
distance <span class="math inline">\(D_M(\mathbf{x})\)</span> is given
by:</p>
<p><span class="math display">\[D_M(\mathbf{x}) = \sqrt{(\mathbf{x} -
\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} -
\boldsymbol{\mu})}\]</span></p>
<p>This theorem is a consequence of the properties of the multivariate
normal distribution and the definition of the Mahalanobis distance.</p>
<h1 id="proofs">Proofs</h1>
<p>To prove that <span class="math inline">\(D_M(\mathbf{x})\)</span> is
indeed a distance metric, we need to verify the following
properties:</p>
<ol>
<li><p>Non-negativity: <span class="math inline">\(D_M(\mathbf{x}) \geq
0\)</span> for all <span class="math inline">\(\mathbf{x}\)</span>, with
equality if and only if <span class="math inline">\(\mathbf{x} =
\boldsymbol{\mu}\)</span>.</p></li>
<li><p>Identity of indiscernibles: <span
class="math inline">\(D_M(\mathbf{x}) = 0\)</span> if and only if <span
class="math inline">\(\mathbf{x} = \boldsymbol{\mu}\)</span>.</p></li>
<li><p>Symmetry: <span class="math inline">\(D_M(\mathbf{x}, \mathbf{y})
= D_M(\mathbf{y}, \mathbf{x})\)</span> for any two points <span
class="math inline">\(\mathbf{x}\)</span> and <span
class="math inline">\(\mathbf{y}\)</span>.</p></li>
<li><p>Triangle inequality: <span class="math inline">\(D_M(\mathbf{x},
\mathbf{z}) \leq D_M(\mathbf{x}, \mathbf{y}) + D_M(\mathbf{y},
\mathbf{z})\)</span> for any three points <span
class="math inline">\(\mathbf{x}\)</span>, <span
class="math inline">\(\mathbf{y}\)</span>, and <span
class="math inline">\(\mathbf{z}\)</span>.</p></li>
</ol>
<p>We will focus on the first property, non-negativity. The Mahalanobis
distance is defined as a square root of a quadratic form:</p>
<p><span class="math display">\[D_M(\mathbf{x}) = \sqrt{(\mathbf{x} -
\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} -
\boldsymbol{\mu})}\]</span></p>
<p>Since <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span>
is a positive definite matrix, the quadratic form <span
class="math inline">\((\mathbf{x} - \boldsymbol{\mu})^T
\boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\)</span> is
always non-negative. The square root of a non-negative number is also
non-negative, and it equals zero if and only if <span
class="math inline">\(\mathbf{x} - \boldsymbol{\mu} = 0\)</span>, i.e.,
<span class="math inline">\(\mathbf{x} = \boldsymbol{\mu}\)</span>.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Mahalanobis distance has several important properties:</p>
<ol>
<li><p>Scale-invariance: The Mahalanobis distance is invariant under
scaling of the variables. This means that multiplying any variable by a
constant does not change the distance.</p></li>
<li><p>Correlation-aware: The Mahalanobis distance takes into account
the correlations between variables, providing a more accurate measure of
distance in multivariate spaces.</p></li>
<li><p>Ellipsoidal contours: In a multivariate normal distribution, the
contours of equal Mahalanobis distance form ellipsoids centered at the
mean.</p></li>
</ol>
<p>We will prove property (iii). Consider a multivariate normal
distribution with mean <span
class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix
<span class="math inline">\(\boldsymbol{\Sigma}\)</span>. The density
function is given by:</p>
<p><span class="math display">\[f(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}
|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} -
\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} -
\boldsymbol{\mu})\right)\]</span></p>
<p>The contours of equal density correspond to the sets where <span
class="math inline">\((\mathbf{x} - \boldsymbol{\mu})^T
\boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\)</span> is
constant. These are ellipsoids centered at <span
class="math inline">\(\boldsymbol{\mu}\)</span>, confirming property
(iii).</p>
</body>
</html>
{% include "footer.html" %}


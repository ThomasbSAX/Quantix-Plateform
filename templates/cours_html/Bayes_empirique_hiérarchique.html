{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Bayes empirique hiérarchique : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Bayes empirique hiérarchique : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inférence bayésienne a révolutionné l’analyse statistique en
intégrant des informations a priori pour améliorer la précision des
estimations. Cependant, dans un contexte où les données sont abondantes
mais souvent bruitées ou incomplètes, l’approche bayésienne classique
peut se révéler insuffisante. C’est ici qu’intervient le Bayes empirique
hiérarchique, une méthode qui combine la flexibilité des modèles
bayésiens avec la robustesse des approches empiriques.</p>
<p>L’idée centrale est de construire une hiérarchie de modèles où chaque
niveau incorpore des informations empiriques issues des niveaux
inférieurs. Cette approche permet de capturer la complexité des données
tout en maintenant une structure cohérente et interprétable. Le Bayes
empirique hiérarchique est particulièrement utile dans les domaines où
les données sont multi-niveaux, comme en biostatistique, en économie ou
en sciences sociales.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le Bayes empirique hiérarchique, commençons par
définir les concepts de base.</p>
<h2 id="modèle-bayésien">Modèle Bayésien</h2>
<p>Un modèle bayésien est un cadre statistique où les paramètres sont
traités comme des variables aléatoires. On cherche à estimer la
distribution a posteriori d’un paramètre <span
class="math inline">\(\theta\)</span> donnée des données <span
class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[P(\theta | X) = \frac{P(X | \theta)
P(\theta)}{P(X)}\]</span></p>
<p>où <span class="math inline">\(P(\theta)\)</span> est la distribution
a priori, <span class="math inline">\(P(X | \theta)\)</span> est la
vraisemblance et <span class="math inline">\(P(X)\)</span> est une
constante de normalisation.</p>
<h2 id="modèle-empirique">Modèle Empirique</h2>
<p>Un modèle empirique utilise les données pour estimer directement les
paramètres sans faire d’hypothèses a priori. Par exemple, l’estimation
par maximum de vraisemblance:</p>
<p><span class="math display">\[\hat{\theta} = \arg\max_{\theta} P(X |
\theta)\]</span></p>
<h2 id="modèle-hiérarchique">Modèle Hiérarchique</h2>
<p>Un modèle hiérarchique est une structure où les paramètres sont
eux-mêmes modélisés par d’autres paramètres. Par exemple, un modèle à
deux niveaux:</p>
<p><span class="math display">\[\begin{aligned}
X_i | \theta_i &amp;\sim P(X_i | \theta_i) \\
\theta_i | \mu, \tau &amp;\sim P(\theta_i | \mu, \tau) \\
\mu, \tau &amp;\sim P(\mu, \tau)
\end{aligned}\]</span></p>
<h2 id="bayes-empirique-hiérarchique">Bayes Empirique Hiérarchique</h2>
<p>Le Bayes empirique hiérarchique combine ces trois concepts. On
construit une hiérarchie de modèles où chaque niveau utilise des
informations empiriques pour estimer les paramètres du niveau supérieur.
Par exemple, un modèle à deux niveaux:</p>
<p><span class="math display">\[\begin{aligned}
X_i | \theta_i &amp;\sim P(X_i | \theta_i) \\
\theta_i | \mu, \tau &amp;\sim P(\theta_i | \mu, \tau) \\
\mu, \tau | \hat{\mu}, \hat{\tau} &amp;\sim P(\mu, \tau | \hat{\mu},
\hat{\tau})
\end{aligned}\]</span></p>
<p>où <span class="math inline">\(\hat{\mu}\)</span> et <span
class="math inline">\(\hat{\tau}\)</span> sont estimés empiriquement à
partir des données.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-convergence">Théorème de Convergence</h2>
<p>Un théorème fondamental en Bayes empirique hiérarchique est le
théorème de convergence, qui garantit que les estimations empiriques
convergent vers les vraies valeurs des paramètres lorsque le nombre de
données tend vers l’infini.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> un
échantillon i.i.d. de <span class="math inline">\(P(X |
\theta)\)</span>, et <span class="math inline">\(\hat{\theta}_n\)</span>
l’estimateur empirique hiérarchique de <span
class="math inline">\(\theta\)</span>. Alors, sous certaines conditions
régularité:</p>
<p><span class="math display">\[\hat{\theta}_n \xrightarrow{n \to
\infty} \theta\]</span></p>
<p>avec probabilité 1.</p>
</div>
<h2 id="démonstration-du-théorème-de-convergence">Démonstration du
Théorème de Convergence</h2>
<p>Pour démontrer ce théorème, nous utilisons le lemme de la loi des
grands nombres et le théorème central limite.</p>
<div class="proof">
<p><em>Proof.</em> Par la loi des grands nombres, nous savons que:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n X_i
\xrightarrow{n \to \infty} E[X]\]</span></p>
<p>où <span class="math inline">\(E[X]\)</span> est l’espérance de <span
class="math inline">\(X\)</span> sous <span class="math inline">\(P(X |
\theta)\)</span>. En utilisant cet estimateur empirique pour <span
class="math inline">\(\theta\)</span>, nous avons:</p>
<p><span class="math display">\[\hat{\theta}_n = \arg\max_{\theta}
\frac{1}{n} \sum_{i=1}^n \log P(X_i | \theta)\]</span></p>
<p>Par le théorème central limite, nous pouvons montrer que:</p>
<p><span class="math display">\[\sqrt{n} (\hat{\theta}_n - \theta)
\xrightarrow{d} N(0, I(\theta)^{-1})\]</span></p>
<p>où <span class="math inline">\(I(\theta)\)</span> est l’information
de Fisher. Par conséquent, <span
class="math inline">\(\hat{\theta}_n\)</span> converge vers <span
class="math inline">\(\theta\)</span> avec probabilité 1. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-consistance">Propriété de Consistance</h2>
<div class="corollary">
<p>Sous les mêmes conditions que le théorème précédent, l’estimateur
empirique hiérarchique <span
class="math inline">\(\hat{\theta}_n\)</span> est consistant.</p>
</div>
<h2 id="démonstration-de-la-consistance">Démonstration de la
Consistance</h2>
<div class="proof">
<p><em>Proof.</em> La consistance suit directement du théorème de
convergence. En effet, si <span
class="math inline">\(\hat{\theta}_n\)</span> converge vers <span
class="math inline">\(\theta\)</span> avec probabilité 1, alors pour
tout <span class="math inline">\(\epsilon &gt; 0\)</span>, nous
avons:</p>
<p><span class="math display">\[P(|\hat{\theta}_n - \theta| &gt;
\epsilon) \xrightarrow{n \to \infty} 0\]</span></p>
<p>Ce qui montre que <span class="math inline">\(\hat{\theta}_n\)</span>
est un estimateur consistant de <span
class="math inline">\(\theta\)</span>. ◻</p>
</div>
<h2 id="propriété-de-normalité-asymptotique">Propriété de Normalité
Asymptotique</h2>
<div class="corollary">
<p>Sous les mêmes conditions que le théorème précédent, l’estimateur
empirique hiérarchique <span
class="math inline">\(\hat{\theta}_n\)</span> est asymptotiquement
normal.</p>
</div>
<h2 id="démonstration-de-la-normalité-asymptotique">Démonstration de la
Normalité Asymptotique</h2>
<div class="proof">
<p><em>Proof.</em> La normalité asymptotique découle du théorème central
limite. En effet, nous avons montré que:</p>
<p><span class="math display">\[\sqrt{n} (\hat{\theta}_n - \theta)
\xrightarrow{d} N(0, I(\theta)^{-1})\]</span></p>
<p>Ce qui implique que pour <span class="math inline">\(n\)</span>
suffisamment grand, <span class="math inline">\(\hat{\theta}_n\)</span>
suit une distribution normale centrée sur <span
class="math inline">\(\theta\)</span> avec une variance donnée par
l’inverse de l’information de Fisher. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le Bayes empirique hiérarchique offre une approche puissante et
flexible pour l’analyse de données complexes. En combinant les avantages
des modèles bayésiens et empiriques, cette méthode permet de capturer la
structure hiérarchique des données tout en maintenant une grande
robustesse. Les théorèmes et propriétés présentés dans cet article
montrent que cette approche est rigoureuse et efficace, ouvrant ainsi de
nouvelles perspectives pour la recherche en statistique.</p>
</body>
</html>
{% include "footer.html" %}


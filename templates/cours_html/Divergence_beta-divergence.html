{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’étude de la \beta-divergence : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’étude de la <span
class="math inline">\(\beta\)</span>-divergence : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La <span class="math inline">\(\beta\)</span>-divergence, une
généralisation de la divergence de Kullback-Leibler, joue un rôle
fondamental dans l’analyse des données et l’apprentissage automatique.
Introduite initialement pour résoudre des problèmes d’estimation de
densité, cette mesure de divergence a trouvé des applications dans
divers domaines tels que la compression d’images, l’apprentissage non
supervisé et les modèles de mélange.</p>
<p>L’émergence de la <span
class="math inline">\(\beta\)</span>-divergence est motivée par le
besoin d’une mesure flexible capable de capturer différentes formes de
divergences entre distributions. En particulier, elle permet de passer
continûment d’une divergence à une autre en ajustant un paramètre <span
class="math inline">\(\beta\)</span>. Cette flexibilité est
indispensable dans des cadres où la structure des données varie ou où
différentes hypothèses doivent être testées.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la <span
class="math inline">\(\beta\)</span>-divergence, commençons par
considérer deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace <span
class="math inline">\(\Omega\)</span>. Nous cherchons une mesure qui
quantifie la distance entre ces deux distributions. Intuitivement, cette
mesure doit être nulle si et seulement si <span class="math inline">\(P
= Q\)</span> et doit augmenter lorsque les distributions deviennent plus
différentes.</p>
<p>Nous introduisons la <span
class="math inline">\(\beta\)</span>-divergence comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace <span class="math inline">\(\Omega\)</span>, et soit <span
class="math inline">\(\beta \in \mathbb{R}\)</span>. La <span
class="math inline">\(\beta\)</span>-divergence entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_{\beta}(P \parallel Q) =
\frac{1}{\beta(1-\beta)} \left( 1 - \sum_{x \in \Omega} \left(
\frac{q(x)}{p(x)} \right)^{\beta} p(x) \right)\]</span> pour <span
class="math inline">\(\beta \notin \{0, 1\}\)</span>.</p>
</div>
<p>Pour les cas particuliers <span class="math inline">\(\beta =
0\)</span> et <span class="math inline">\(\beta = 1\)</span>, la
divergence est définie par limite :</p>
<div class="definition">
<p>La divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est obtenue comme limite lorsque <span
class="math inline">\(\beta \to 1\)</span> : <span
class="math display">\[D_{KL}(P \parallel Q) = \lim_{\beta \to 1}
D_{\beta}(P \parallel Q) = \sum_{x \in \Omega} p(x) \log \left(
\frac{p(x)}{q(x)} \right)\]</span></p>
</div>
<div class="definition">
<p>La divergence d’Itakura-Saito entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est obtenue comme limite lorsque <span
class="math inline">\(\beta \to 0\)</span> : <span
class="math display">\[D_{IS}(P \parallel Q) = \lim_{\beta \to 0}
D_{\beta}(P \parallel Q) = \sum_{x \in \Omega} \left( \frac{q(x)}{p(x)}
- 1 - \log \left( \frac{q(x)}{p(x)} \right) \right)\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant quelques théorèmes importants concernant
la <span class="math inline">\(\beta\)</span>-divergence.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace <span class="math inline">\(\Omega\)</span>, et soit <span
class="math inline">\(\beta \in \mathbb{R}\)</span>. La <span
class="math inline">\(\beta\)</span>-divergence <span
class="math inline">\(D_{\beta}(P \parallel Q)\)</span> est toujours non
négative, c’est-à-dire : <span class="math display">\[D_{\beta}(P
\parallel Q) \geq 0\]</span> avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons
l’inégalité de Jensen. Considérons la fonction <span
class="math inline">\(\phi(t) = t^{\beta}\)</span>. Cette fonction est
convexe pour <span class="math inline">\(\beta \geq 1\)</span> ou <span
class="math inline">\(\beta \leq 0\)</span>. Par l’inégalité de Jensen,
nous avons : <span class="math display">\[\phi\left( \sum_{x \in \Omega}
p(x) \frac{q(x)}{p(x)} \right) \leq \sum_{x \in \Omega} p(x) \phi\left(
\frac{q(x)}{p(x)} \right)\]</span> En substituant <span
class="math inline">\(\phi(t) = t^{\beta}\)</span>, nous obtenons :
<span class="math display">\[1^{\beta} \leq \sum_{x \in \Omega} p(x)
\left( \frac{q(x)}{p(x)} \right)^{\beta}\]</span> En réarrangeant les
termes, nous obtenons : <span class="math display">\[1 - \sum_{x \in
\Omega} p(x) \left( \frac{q(x)}{p(x)} \right)^{\beta} \leq 0\]</span> En
multipliant par <span
class="math inline">\(\frac{1}{\beta(1-\beta)}\)</span> et en tenant
compte du signe de <span class="math inline">\(\beta(1-\beta)\)</span>,
nous concluons que <span class="math inline">\(D_{\beta}(P \parallel Q)
\geq 0\)</span>.</p>
<p>L’égalité a lieu si et seulement si <span
class="math inline">\(\frac{q(x)}{p(x)}\)</span> est constant pour tout
<span class="math inline">\(x \in \Omega\)</span>, ce qui implique que
<span class="math inline">\(P = Q\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la <span
class="math inline">\(\beta\)</span>-divergence.</p>
<div class="proposition">
<p>La <span class="math inline">\(\beta\)</span>-divergence est
invariante par transformation affine. Plus précisément, pour toute
fonction affine <span class="math inline">\(f\)</span> et pour toutes
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[D_{\beta}(f(P) \parallel f(Q)) = D_{\beta}(P
\parallel Q)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(f(x) = ax +
b\)</span>. Nous avons : <span class="math display">\[D_{\beta}(f(P)
\parallel f(Q)) = \frac{1}{\beta(1-\beta)} \left( 1 - \sum_{x \in
\Omega} \left( \frac{q(f(x))}{p(f(x))} \right)^{\beta} p(f(x))
\right)\]</span> En utilisant le fait que <span
class="math inline">\(f\)</span> est affine, nous pouvons réécrire cette
expression en termes de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Les détails sont omis pour la
concision. ◻</p>
</div>
<div class="corollaire">
<p>La <span class="math inline">\(\beta\)</span>-divergence est convexe
en fonction de <span class="math inline">\(Q\)</span> pour un <span
class="math inline">\(P\)</span> fixé.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La convexité de la <span
class="math inline">\(\beta\)</span>-divergence découle directement de
l’inégalité de Jensen et des propriétés de convexité de la fonction
<span class="math inline">\(\phi(t) = t^{\beta}\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La <span class="math inline">\(\beta\)</span>-divergence est une
mesure de divergence flexible et puissante qui trouve des applications
dans divers domaines. Ses propriétés mathématiques et ses
généralisations en font un outil indispensable pour l’analyse des
données et l’apprentissage automatique. Les théorèmes et propriétés
présentés dans cet article montrent la richesse de cette notion et
ouvrent la voie à de nombreuses recherches futures.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Spectral Embedding : Une Approche Géométrique pour l’Analyse de Données</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Spectral Embedding : Une Approche
Géométrique pour l’Analyse de Données</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse des données est un domaine en pleine expansion, notamment
avec l’essor des méthodes d’apprentissage automatique. Parmi les
techniques les plus puissantes, on trouve l’embedding spectral, qui
permet de représenter des données dans un espace de dimension réduite
tout en préservant certaines propriétés structurelles. Cependant, ces
méthodes classiques sont souvent limitées par leur incapacité à capturer
des relations non linéaires entre les données. C’est ici que l’embedding
spectral kernelisé (Kernelized Spectral Embedding) entre en jeu.</p>
<p>L’idée sous-jacente à cette approche est d’utiliser des noyaux pour
transformer les données dans un espace de features de dimension plus
élevée, où les relations non linéaires deviennent linéaires. Cette
transformation permet ensuite d’appliquer des techniques d’embedding
spectral classiques, mais dans cet espace transformé. L’avantage
principal est la capacité à capturer des structures complexes dans les
données, tout en conservant l’interprétabilité géométrique de
l’embedding spectral.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le Kernelized Spectral Embedding, il est essentiel de
définir quelques concepts clés.</p>
<h2 class="unnumbered" id="noyaux-et-espaces-de-features">Noyaux et
Espaces de Features</h2>
<p>Un noyau est une fonction qui mesure la similarité entre deux
vecteurs. Formellement, un noyau <span class="math inline">\(k\)</span>
est une fonction symétrique et positive définie :</p>
<p><span class="math display">\[k : \mathcal{X} \times \mathcal{X}
\rightarrow \mathbb{R}\]</span></p>
<p><span class="math display">\[\forall x, y \in \mathcal{X}, k(x, y) =
k(y, x)\]</span></p>
<p><span class="math display">\[\forall n \in \mathbb{N}, \forall (x_1,
\ldots, x_n) \in \mathcal{X}^n, \forall (\alpha_1, \ldots, \alpha_n) \in
\mathbb{R}^n,
\sum_{i,j=1}^n \alpha_i \alpha_j k(x_i, x_j) \geq 0\]</span></p>
<p>Un noyau admissible <span class="math inline">\(k\)</span> peut être
vu comme un produit scalaire dans un espace de Hilbert <span
class="math inline">\(\mathcal{H}\)</span>, appelé espace de features
:</p>
<p><span class="math display">\[k(x, y) = \langle \phi(x), \phi(y)
\rangle_{\mathcal{H}}\]</span></p>
<p>où <span class="math inline">\(\phi : \mathcal{X} \rightarrow
\mathcal{H}\)</span> est une fonction de transformation non
linéaire.</p>
<h2 class="unnumbered" id="embedding-spectral">Embedding Spectral</h2>
<p>L’embedding spectral est une technique qui utilise les valeurs
propres et vecteurs propres d’une matrice de similarité pour représenter
des données dans un espace de dimension réduite. Soit <span
class="math inline">\(W\)</span> une matrice de similarité symétrique et
positive définie, l’embedding spectral est défini comme suit :</p>
<p><span class="math display">\[Y = U \Lambda^{1/2}\]</span></p>
<p>où <span class="math inline">\(U\)</span> est la matrice des vecteurs
propres de <span class="math inline">\(W\)</span> associés aux <span
class="math inline">\(d\)</span> plus grandes valeurs propres, et <span
class="math inline">\(\Lambda\)</span> est la matrice diagonale des
<span class="math inline">\(d\)</span> plus grandes valeurs propres.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered" id="théorème-de-representer-kernel">Théorème de
Representer Kernel</h2>
<p>Le théorème de Representer Kernel est fondamental pour comprendre le
Kernelized Spectral Embedding. Il stipule que toute fonction dans
l’espace de Hilbert associé à un noyau peut être exprimée comme une
combinaison linéaire des noyaux centraux.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(k\)</span> un noyau admissible et
<span class="math inline">\(\mathcal{H}\)</span> l’espace de Hilbert
associé. Soit <span class="math inline">\(f : \mathcal{X} \rightarrow
\mathbb{R}\)</span> une fonction dans <span
class="math inline">\(\mathcal{H}\)</span>. Alors, il existe des
coefficients <span class="math inline">\(\alpha_i \in
\mathbb{R}\)</span> tels que :</p>
<p><span class="math display">\[f(x) = \sum_{i=1}^n \alpha_i k(x,
x_i)\]</span></p>
<p>pour tout <span class="math inline">\(x \in \mathcal{X}\)</span>.</p>
</div>
<h2 class="unnumbered"
id="théorème-de-lembedding-spectral-kernelisé">Théorème de l’Embedding
Spectral Kernelisé</h2>
<p>Le théorème suivant montre comment l’embedding spectral peut être
généralisé à des noyaux.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(k\)</span> un noyau admissible et
<span class="math inline">\(W\)</span> la matrice de Gram associée,
définie par <span class="math inline">\(W_{ij} = k(x_i, x_j)\)</span>.
Soit <span class="math inline">\(U\)</span> la matrice des vecteurs
propres de <span class="math inline">\(W\)</span> associés aux <span
class="math inline">\(d\)</span> plus grandes valeurs propres, et <span
class="math inline">\(\Lambda\)</span> la matrice diagonale des <span
class="math inline">\(d\)</span> plus grandes valeurs propres. Alors,
l’embedding spectral kernelisé est défini par :</p>
<p><span class="math display">\[Y = U \Lambda^{1/2}\]</span></p>
<p>où <span class="math inline">\(Y\)</span> est la matrice des
embeddings des données.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered"
id="preuve-du-théorème-de-representer-kernel">Preuve du Théorème de
Representer Kernel</h2>
<p>La preuve du théorème de Representer Kernel repose sur le fait que
l’espace de Hilbert associé à un noyau est engendré par les fonctions
<span class="math inline">\(k(x, \cdot)\)</span>.</p>
<p>Soit <span class="math inline">\(f \in \mathcal{H}\)</span>. Comme
<span class="math inline">\(\mathcal{H}\)</span> est un espace de
Hilbert séparable, il existe une base orthonormée <span
class="math inline">\(\{ e_i \}_{i \in I}\)</span> de <span
class="math inline">\(\mathcal{H}\)</span>. On peut alors écrire :</p>
<p><span class="math display">\[f = \sum_{i \in I} \langle f, e_i
\rangle_{\mathcal{H}} e_i\]</span></p>
<p>En utilisant le théorème de Representer, on sait que chaque <span
class="math inline">\(e_i\)</span> peut être exprimé comme une
combinaison linéaire des noyaux centraux :</p>
<p><span class="math display">\[e_i = \sum_{j=1}^n \alpha_{ij} k(\cdot,
x_j)\]</span></p>
<p>En substituant cette expression dans l’équation précédente, on
obtient :</p>
<p><span class="math display">\[f = \sum_{i \in I} \langle f, e_i
\rangle_{\mathcal{H}} \sum_{j=1}^n \alpha_{ij} k(\cdot, x_j) =
\sum_{j=1}^n \left( \sum_{i \in I} \alpha_{ij} \langle f, e_i
\rangle_{\mathcal{H}} \right) k(\cdot, x_j)\]</span></p>
<p>En posant <span class="math inline">\(\alpha_j = \sum_{i \in I}
\alpha_{ij} \langle f, e_i \rangle_{\mathcal{H}}\)</span>, on obtient
bien :</p>
<p><span class="math display">\[f(x) = \sum_{j=1}^n \alpha_j k(x,
x_j)\]</span></p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-lembedding-spectral-kernelisé">Preuve du
Théorème de l’Embedding Spectral Kernelisé</h2>
<p>La preuve de ce théorème repose sur les propriétés des matrices de
Gram et de l’embedding spectral classique.</p>
<p>Soit <span class="math inline">\(W\)</span> la matrice de Gram
associée au noyau <span class="math inline">\(k\)</span>. Comme <span
class="math inline">\(k\)</span> est symétrique et positive définie,
<span class="math inline">\(W\)</span> est également symétrique et
positive définie. On peut donc diagonaliser <span
class="math inline">\(W\)</span> :</p>
<p><span class="math display">\[W = U \Lambda U^T\]</span></p>
<p>où <span class="math inline">\(U\)</span> est la matrice des vecteurs
propres de <span class="math inline">\(W\)</span> et <span
class="math inline">\(\Lambda\)</span> est la matrice diagonale des
valeurs propres.</p>
<p>L’embedding spectral kernelisé est alors défini par :</p>
<p><span class="math display">\[Y = U \Lambda^{1/2}\]</span></p>
<p>Cette définition est cohérente avec l’embedding spectral classique,
mais appliquée dans l’espace de features associé au noyau <span
class="math inline">\(k\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-de-préservation-des-distances">Propriété de Préservation
des Distances</h2>
<p>L’une des propriétés les plus importantes de l’embedding spectral
kernelisé est sa capacité à préserver les distances entre les points
dans l’espace de features.</p>
<div class="proposition">
<p>Soit <span class="math inline">\(Y\)</span> l’embedding spectral
kernelisé associé à un noyau <span class="math inline">\(k\)</span>.
Alors, pour tout <span class="math inline">\(x_i, x_j \in
\mathcal{X}\)</span>, on a :</p>
<p><span class="math display">\[\| Y_i - Y_j \|^2 = k(x_i, x_i) + k(x_j,
x_j) - 2k(x_i, x_j)\]</span></p>
<p>où <span class="math inline">\(Y_i\)</span> et <span
class="math inline">\(Y_j\)</span> sont les embeddings des points <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span>.</p>
</div>
<h2 class="unnumbered" id="preuve-de-la-proposition">Preuve de la
Proposition</h2>
<p>La preuve de cette proposition découle directement des propriétés des
noyaux et de l’embedding spectral.</p>
<p>Soit <span class="math inline">\(Y\)</span> l’embedding spectral
kernelisé défini par :</p>
<p><span class="math display">\[Y = U \Lambda^{1/2}\]</span></p>
<p>Alors, pour tout <span class="math inline">\(x_i, x_j \in
\mathcal{X}\)</span>, on a :</p>
<p><span class="math display">\[\| Y_i - Y_j \|^2 = \| U_i \Lambda^{1/2}
- U_j \Lambda^{1/2} \|^2 = \| (U_i - U_j) \Lambda^{1/2}
\|^2\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(U\)</span> est
orthonormale, on obtient :</p>
<p><span class="math display">\[\| (U_i - U_j) \Lambda^{1/2} \|^2 =
\text{Tr} \left( (U_i - U_j)^T (U_i - U_j) \Lambda \right)\]</span></p>
<p>En développant cette expression, on obtient :</p>
<p><span class="math display">\[\text{Tr} \left( U_i^T U_i \Lambda +
U_j^T U_j \Lambda - 2 U_i^T U_j \Lambda \right) = k(x_i, x_i) + k(x_j,
x_j) - 2k(x_i, x_j)\]</span></p>
<h2 class="unnumbered" id="corollaire-de-la-dimensionalité">Corollaire
de la Dimensionalité</h2>
<p>Un corollaire important de l’embedding spectral kernelisé est qu’il
permet de réduire la dimensionalité des données tout en préservant les
relations non linéaires.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(Y\)</span> l’embedding spectral
kernelisé associé à un noyau <span class="math inline">\(k\)</span>. Si
le nombre de dimensions <span class="math inline">\(d\)</span> est
choisi de manière appropriée, alors <span
class="math inline">\(Y\)</span> préserve les relations non linéaires
entre les points dans <span
class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
<h2 class="unnumbered" id="preuve-du-corollaire">Preuve du
Corollaire</h2>
<p>La preuve de ce corollaire repose sur le fait que l’embedding
spectral kernelisé capture les structures principales des données dans
l’espace de features.</p>
<p>En choisissant <span class="math inline">\(d\)</span> de manière à
capturer suffisamment de variance dans les données, on peut garantir que
les relations non linéaires sont préservées. Cela est dû au fait que les
noyaux permettent de transformer les données dans un espace où ces
relations deviennent linéaires.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le Kernelized Spectral Embedding est une technique puissante pour
l’analyse des données, combinant les avantages de l’embedding spectral
et des noyaux. En transformant les données dans un espace de features de
dimension plus élevée, cette méthode permet de capturer des relations
non linéaires tout en conservant l’interprétabilité géométrique de
l’embedding spectral. Les théorèmes et propriétés présentés dans cet
article montrent la rigueur mathématique de cette approche et ouvrent la
voie à de nombreuses applications dans le domaine de l’apprentissage
automatique.</p>
</body>
</html>
{% include "footer.html" %}


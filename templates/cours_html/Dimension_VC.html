{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Dimension VC : Une Approche Théorique et Pratique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Dimension VC : Une Approche Théorique et Pratique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La dimension VC, ou dimension de Vapnik-Chervonenkis, est un concept
fondamental en théorie de l’apprentissage statistique. Elle émerge dans
les années 1960-1970 avec les travaux de Vladimir Vapnik et Alexey
Chervonenkis, qui cherchaient à comprendre les limites de la
généralisation des modèles d’apprentissage. La dimension VC permet de
quantifier la capacité d’un modèle à apprendre des données, en mesurant
sa complexité. Elle est indispensable pour éviter le surapprentissage et
pour garantir une bonne généralisation des modèles.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la dimension VC, commençons par comprendre ce que nous
cherchons à mesurer. Imaginons un ensemble de points dans un espace, et
un modèle qui cherche à séparer ces points en deux classes. La dimension
VC mesure le nombre maximal de points que le modèle peut séparer de
toutes les manières possibles.</p>
<p>Formellement, soit <span class="math inline">\(\mathcal{H}\)</span>
un ensemble d’hyperplans dans un espace <span
class="math inline">\(\mathbb{R}^d\)</span>. La dimension VC de <span
class="math inline">\(\mathcal{H}\)</span>, notée <span
class="math inline">\(\text{VC}(\mathcal{H})\)</span>, est le plus grand
entier <span class="math inline">\(d\)</span> tel qu’il existe un
ensemble de <span class="math inline">\(d\)</span> points dans <span
class="math inline">\(\mathbb{R}^d\)</span> que chaque hyperplan de
<span class="math inline">\(\mathcal{H}\)</span> peut séparer de toutes
les manières possibles.</p>
<div class="definition">
<p>Soit <span class="math inline">\(S\)</span> un ensemble de points
dans <span class="math inline">\(\mathbb{R}^d\)</span>. On dit que <span
class="math inline">\(S\)</span> est <em>shattered</em> par <span
class="math inline">\(\mathcal{H}\)</span> si pour toute fonction
booléenne <span class="math inline">\(f: S \rightarrow \{0,
1\}\)</span>, il existe un hyperplan <span class="math inline">\(h \in
\mathcal{H}\)</span> tel que pour tout point <span
class="math inline">\(x \in S\)</span>, <span class="math inline">\(h(x)
= f(x)\)</span>.</p>
</div>
<div class="definition">
<p>La dimension VC de <span class="math inline">\(\mathcal{H}\)</span>,
notée <span class="math inline">\(\text{VC}(\mathcal{H})\)</span>, est
le plus grand entier <span class="math inline">\(d\)</span> tel qu’il
existe un ensemble de <span class="math inline">\(d\)</span> points dans
<span class="math inline">\(\mathbb{R}^d\)</span> qui est shattered par
<span class="math inline">\(\mathcal{H}\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes les plus importants concernant la dimension VC est
le théorème de généralisation, qui donne une borne sur l’erreur de
généralisation d’un modèle en fonction de sa dimension VC.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{H}\)</span> un ensemble
d’hyperplans avec dimension VC <span class="math inline">\(d\)</span>,
et soit <span class="math inline">\(S\)</span> un ensemble de <span
class="math inline">\(m\)</span> points. Alors, pour tout <span
class="math inline">\(h \in \mathcal{H}\)</span>, l’erreur de
généralisation <span class="math inline">\(L(h)\)</span> est bornée par
: <span class="math display">\[L(h) \leq L_S(h) + 4
\sqrt{\frac{d(\ln(2m/d) + 1)}{m}}\]</span> où <span
class="math inline">\(L_S(h)\)</span> est l’erreur empirique sur <span
class="math inline">\(S\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de généralisation, nous utilisons des
techniques de théorie de l’apprentissage statistique. La preuve repose
sur le lemme de symétrisation et le théorème de concentration.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un ensemble <span
class="math inline">\(\mathcal{H}\)</span> d’hyperplans avec dimension
VC <span class="math inline">\(d\)</span>, et un ensemble de points
<span class="math inline">\(S\)</span> de taille <span
class="math inline">\(m\)</span>. Nous voulons borner l’erreur de
généralisation <span class="math inline">\(L(h)\)</span>.</p>
<p>Par le lemme de symétrisation, nous savons que : <span
class="math display">\[\mathbb{E}[L(h) - L_S(h)] \leq 2
\mathbb{E}\left[\sup_{h \in \mathcal{H}} |L(h) -
L_S(h)|\right]\]</span></p>
<p>En utilisant le théorème de concentration, nous obtenons : <span
class="math display">\[\mathbb{E}\left[\sup_{h \in \mathcal{H}} |L(h) -
L_S(h)|\right] \leq 4 \sqrt{\frac{d(\ln(2m/d) + 1)}{m}}\]</span></p>
<p>En combinant ces résultats, nous obtenons la borne souhaitée : <span
class="math display">\[L(h) \leq L_S(h) + 4 \sqrt{\frac{d(\ln(2m/d) +
1)}{m}}\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
dimension VC.</p>
<ol>
<li><p>La dimension VC d’un ensemble d’hyperplans linéaires dans <span
class="math inline">\(\mathbb{R}^d\)</span> est égale à <span
class="math inline">\(d + 1\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cela découle du fait que les hyperplans linéaires
peuvent séparer n’importe quel ensemble de <span class="math inline">\(d
+ 1\)</span> points dans <span
class="math inline">\(\mathbb{R}^d\)</span>. ◻</p>
</div></li>
<li><p>La dimension VC d’un ensemble de fonctions booléennes est finie
si et seulement si l’ensemble est fini.</p>
<div class="proof">
<p><em>Proof.</em> Cela découle du fait que si un ensemble de fonctions
booléennes peut shatter un nombre infini de points, il doit contenir une
infinité de fonctions. ◻</p>
</div></li>
<li><p>La dimension VC d’un ensemble de fonctions est une mesure de sa
complexité.</p>
<div class="proof">
<p><em>Proof.</em> Plus la dimension VC est élevée, plus l’ensemble de
fonctions peut séparer des configurations complexes, ce qui augmente la
complexité du modèle. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La dimension VC est un outil puissant pour comprendre et contrôler la
complexité des modèles d’apprentissage statistique. Elle permet de
garantir une bonne généralisation et d’éviter le surapprentissage. Les
théorèmes et propriétés associés à la dimension VC fournissent des bases
théoriques solides pour le développement de modèles robustes et
performants.</p>
</body>
</html>
{% include "footer.html" %}


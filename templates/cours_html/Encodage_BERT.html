{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage BERT : une révolution dans le traitement automatique du langage naturel</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage BERT : une révolution dans le traitement
automatique du langage naturel</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage BERT (Bidirectional Encoder Representations from
Transformers) représente une avancée majeure dans le domaine du
traitement automatique du langage naturel (NLP). Introduit par Devlin et
al. en 2018, BERT a révolutionné la manière dont les modèles de langage
traitent le contexte bidirectionnel. Contrairement aux approches
traditionnelles qui utilisent des modèles unidirectionnels, BERT
exploite le contexte dans les deux directions, permettant ainsi une
compréhension plus profonde et plus nuancée du langage.</p>
<p>L’émergence de BERT est motivée par le besoin croissant d’améliorer
les performances des systèmes de NLP dans des tâches complexes telles
que la compréhension de texte, la réponse aux questions et la traduction
automatique. Les modèles précédents, comme Word2Vec ou GloVe, étaient
limités par leur incapacité à capturer le contexte complet d’un mot.
BERT, en revanche, utilise un mécanisme d’attention bidirectionnelle
pour encoder des représentations contextuelles riches et précises.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage BERT, il est essentiel de définir
plusieurs concepts clés.</p>
<h2 class="unnumbered" id="modèle-transformer">Modèle Transformer</h2>
<p>Un modèle Transformer est un type d’architecture de réseau neuronal
introduit par Vaswani et al. en 2017. Il repose sur un mécanisme
d’attention auto-supervisée qui permet de capturer les dépendances entre
les éléments d’une séquence. Formellement, un Transformer est défini par
une série de couches d’attention et de réseaux de neurones à
alimentation directe (FFNN).</p>
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_n\}\)</span> une séquence d’entrée. Le Transformer encode cette
séquence en une représentation <span class="math inline">\(Z = \{z_1,
z_2, \ldots, z_n\}\)</span> à l’aide des opérations suivantes :</p>
<p><span class="math display">\[\begin{equation}
    Z = \text{Transformer}(X)
\end{equation}\]</span></p>
<h2 class="unnumbered" id="encodage-bert">Encodage BERT</h2>
<p>L’encodage BERT est une représentation contextuelle d’une séquence de
texte, générée par un modèle pré-entraîné basé sur l’architecture
Transformer. BERT utilise deux types de tâches pour le pré-entraînement
: la prédiction de mots masqués (MLM) et la prédiction de phrases
suivantes (NSP).</p>
<p>Formellement, soit <span class="math inline">\(S = \{s_1, s_2,
\ldots, s_m\}\)</span> une séquence de tokens. L’encodage BERT de <span
class="math inline">\(S\)</span> est donné par :</p>
<p><span class="math display">\[\begin{equation}
    E(S) = \{e_1, e_2, \ldots, e_m\}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(e_i\)</span> est la représentation
contextuelle du token <span class="math inline">\(s_i\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="théorème-de-lattention-auto-supervisée">Théorème de l’Attention
Auto-Supervisée</h2>
<p>Le mécanisme d’attention auto-supervisée est au cœur de
l’architecture Transformer. Il permet de capturer les dépendances entre
les éléments d’une séquence en calculant des scores d’attention.</p>
<p><strong>Théorème</strong> : Soit <span class="math inline">\(X =
\{x_1, x_2, \ldots, x_n\}\)</span> une séquence d’entrée. L’attention
auto-supervisée est définie par :</p>
<p><span class="math display">\[\begin{equation}
    \text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}\]</span></p>
<p>où <span class="math inline">\(Q, K, V\)</span> sont les matrices de
requêtes, clés et valeurs respectivement, et <span
class="math inline">\(d_k\)</span> est la dimension des clés.</p>
<h2 class="unnumbered" id="théorème-du-pré-entraînement-bert">Théorème
du Pré-Entraînement BERT</h2>
<p>Le pré-entraînement de BERT repose sur deux tâches principales : la
prédiction de mots masqués (MLM) et la prédiction de phrases suivantes
(NSP).</p>
<p><strong>Théorème</strong> : Soit <span class="math inline">\(S =
\{s_1, s_2, \ldots, s_m\}\)</span> une séquence de tokens. Le
pré-entraînement BERT est défini par :</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{MLM}} + \mathcal{L}_{\text{NSP}}
\end{equation}\]</span></p>
<p>où <span class="math inline">\(\mathcal{L}_{\text{MLM}}\)</span> est
la perte de prédiction de mots masqués et <span
class="math inline">\(\mathcal{L}_{\text{NSP}}\)</span> est la perte de
prédiction de phrases suivantes.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered"
id="preuve-du-théorème-de-lattention-auto-supervisée">Preuve du Théorème
de l’Attention Auto-Supervisée</h2>
<p>Pour prouver le théorème de l’attention auto-supervisée, nous devons
montrer que les scores d’attention sont calculés correctement.</p>
<p>Soit <span class="math inline">\(Q, K, V\)</span> les matrices de
requêtes, clés et valeurs respectivement. Les scores d’attention sont
calculés comme suit :</p>
<p><span class="math display">\[\begin{equation}
    \text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}\]</span></p>
<p>La fonction softmax assure que les scores d’attention sont
normalisés, permettant ainsi de capturer les dépendances entre les
éléments de la séquence.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-du-pré-entraînement-bert">Preuve du Théorème du
Pré-Entraînement BERT</h2>
<p>Pour prouver le théorème du pré-entraînement BERT, nous devons
montrer que les pertes <span
class="math inline">\(\mathcal{L}_{\text{MLM}}\)</span> et <span
class="math inline">\(\mathcal{L}_{\text{NSP}}\)</span> sont calculées
correctement.</p>
<p>La perte de prédiction de mots masqués <span
class="math inline">\(\mathcal{L}_{\text{MLM}}\)</span> est définie par
:</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{MLM}} = -\sum_{i=1}^m \log p(s_i | S)
\end{equation}\]</span></p>
<p>où <span class="math inline">\(p(s_i | S)\)</span> est la probabilité
de prédire le token masqué <span class="math inline">\(s_i\)</span>
donné la séquence <span class="math inline">\(S\)</span>.</p>
<p>La perte de prédiction de phrases suivantes <span
class="math inline">\(\mathcal{L}_{\text{NSP}}\)</span> est définie par
:</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{NSP}} = -\log p(\text{Next Sentence} | S)
\end{equation}\]</span></p>
<p>où <span class="math inline">\(p(\text{Next Sentence} | S)\)</span>
est la probabilité que la phrase suivante soit correcte.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-de-la-représentation-contextuelle">Propriété de la
Représentation Contextuelle</h2>
<p>L’encodage BERT génère des représentations contextuelles riches et
précises.</p>
<p><strong>Propriété</strong> : Soit <span class="math inline">\(S =
\{s_1, s_2, \ldots, s_m\}\)</span> une séquence de tokens. L’encodage
BERT <span class="math inline">\(E(S) = \{e_1, e_2, \ldots,
e_m\}\)</span> capture le contexte bidirectionnel de chaque token.</p>
<h2 class="unnumbered" id="corollaire-de-la-fine-tuning">Corollaire de
la Fine-Tuning</h2>
<p>Le fine-tuning de BERT permet d’adapter le modèle à des tâches
spécifiques.</p>
<p><strong>Corollaire</strong> : Soit <span
class="math inline">\(T\)</span> une tâche spécifique. Le fine-tuning de
BERT sur <span class="math inline">\(T\)</span> améliore les
performances du modèle sur cette tâche.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage BERT a révolutionné le domaine du traitement automatique
du langage naturel en introduisant des représentations contextuelles
bidirectionnelles riches et précises. Grâce à son mécanisme d’attention
auto-supervisée et ses tâches de pré-entraînement innovantes, BERT a
permis des avancées significatives dans diverses applications de
NLP.</p>
</body>
</html>
{% include "footer.html" %}


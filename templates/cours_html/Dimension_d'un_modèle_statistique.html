{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Dimension d’un modèle statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Dimension d’un modèle statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La notion de dimension d’un modèle statistique émerge naturellement
dans le cadre de l’analyse des données et de la modélisation
probabiliste. Historiquement, cette idée est liée à la théorie des
modèles linéaires et à l’analyse en composantes principales. La
dimension d’un modèle statistique est un concept fondamental qui permet
de quantifier la complexité d’un modèle, c’est-à-dire le nombre de
paramètres indépendants nécessaires pour décrire un phénomène
observé.</p>
<p>Pourquoi cette notion est-elle indispensable ? Dans un contexte où
les données sont de plus en plus volumineuses et complexes, il est
crucial de pouvoir évaluer la capacité d’un modèle à capturer
l’information pertinente sans surajustement (overfitting). La dimension
d’un modèle statistique joue un rôle clé dans cette évaluation,
permettant de trouver un équilibre optimal entre la précision du modèle
et sa généralisabilité.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la notion de dimension d’un modèle statistique,
commençons par comprendre ce que nous cherchons à mesurer. Imaginons que
nous avons un ensemble de données et que nous souhaitons modéliser ces
données à l’aide d’un modèle probabiliste. Nous voulons savoir combien
de paramètres indépendants sont nécessaires pour décrire ce modèle.</p>
<p>Formellement, la dimension d’un modèle statistique peut être définie
de plusieurs manières. Supposons que nous avons un espace paramétrique
<span class="math inline">\(\Theta \subset \mathbb{R}^d\)</span>. La
dimension du modèle est alors le nombre minimal de paramètres
indépendants nécessaires pour décrire complètement ce modèle.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{M}\)</span> un modèle
statistique paramétré par <span class="math inline">\(\theta \in \Theta
\subset \mathbb{R}^d\)</span>. La dimension de <span
class="math inline">\(\mathcal{M}\)</span>, notée <span
class="math inline">\(\dim(\mathcal{M})\)</span>, est le nombre minimal
de paramètres indépendants nécessaires pour décrire complètement <span
class="math inline">\(\mathcal{M}\)</span>.</p>
</div>
<p>Une autre manière de formuler cette définition est la suivante. Soit
<span class="math inline">\(\mathcal{M} = \{ P_\theta : \theta \in
\Theta \}\)</span> un modèle statistique. La dimension de <span
class="math inline">\(\mathcal{M}\)</span> est la dimension de l’espace
vectoriel engendré par les dérivées de Log-vraisemblance <span
class="math inline">\(\frac{\partial}{\partial \theta_i} \log
P_\theta(x)\)</span> pour <span class="math inline">\(i = 1, \dots,
d\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{M} = \{ P_\theta : \theta
\in \Theta \}\)</span> un modèle statistique. La dimension de <span
class="math inline">\(\mathcal{M}\)</span>, notée <span
class="math inline">\(\dim(\mathcal{M})\)</span>, est la dimension de
l’espace vectoriel engendré par les dérivées de Log-vraisemblance <span
class="math inline">\(\frac{\partial}{\partial \theta_i} \log
P_\theta(x)\)</span> pour <span class="math inline">\(i = 1, \dots,
d\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans le cadre de la dimension des modèles
statistiques est le théorème de Fisher, qui établit une relation entre
la dimension d’un modèle et l’information de Fisher.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{M} = \{ P_\theta : \theta
\in \Theta \}\)</span> un modèle statistique de dimension <span
class="math inline">\(d\)</span>. L’information de Fisher <span
class="math inline">\(I(\theta)\)</span> est une matrice définie
positive de taille <span class="math inline">\(d \times d\)</span>.</p>
</div>
<p>Pour comprendre ce théorème, commençons par définir l’information de
Fisher. L’information de Fisher est une mesure de la quantité
d’information que contient un échantillon sur les paramètres du modèle.
Elle est définie comme l’espérance de la dérivée seconde de la
Log-vraisemblance.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{M} = \{ P_\theta : \theta
\in \Theta \}\)</span> un modèle statistique. L’information de Fisher
<span class="math inline">\(I(\theta)\)</span> est définie par <span
class="math display">\[I(\theta) = \mathbb{E}_\theta \left[ \left(
\frac{\partial}{\partial \theta} \log P_\theta(X) \right)^2
\right].\]</span></p>
</div>
<p>Le théorème de Fisher stipule que cette matrice est définie positive
et de taille <span class="math inline">\(d \times d\)</span>, où <span
class="math inline">\(d\)</span> est la dimension du modèle. Cela
signifie que l’information de Fisher capture toute l’information
disponible sur les paramètres du modèle.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Fisher, nous devons montrer que la
matrice d’information de Fisher est définie positive. Commençons par
rappeler que pour tout vecteur <span class="math inline">\(h \in
\mathbb{R}^d\)</span>, nous avons <span class="math display">\[h^T
I(\theta) h = \mathbb{E}_\theta \left[ \left( h^T
\frac{\partial}{\partial \theta} \log P_\theta(X) \right)^2 \right] \geq
0.\]</span></p>
<p>De plus, si <span class="math inline">\(h^T I(\theta) h = 0\)</span>,
alors cela implique que <span class="math display">\[\mathbb{E}_\theta
\left[ \left( h^T \frac{\partial}{\partial \theta} \log P_\theta(X)
\right)^2 \right] = 0.\]</span></p>
<p>Cela signifie que <span class="math inline">\(h^T
\frac{\partial}{\partial \theta} \log P_\theta(X) = 0\)</span> presque
sûrement. Cependant, cela est seulement possible si <span
class="math inline">\(h = 0\)</span>, car sinon nous aurions une
contradiction avec le fait que <span
class="math inline">\(\frac{\partial}{\partial \theta} \log
P_\theta(X)\)</span> engendre un espace de dimension <span
class="math inline">\(d\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
dimension d’un modèle statistique.</p>
<ol>
<li><p>La dimension d’un modèle est invariante par transformation
bijective du paramètre. C’est-à-dire que si <span
class="math inline">\(\phi : \Theta \to \tilde{\Theta}\)</span> est une
bijection, alors le modèle <span class="math inline">\(\{
P_{\phi(\theta)} : \theta \in \Theta \}\)</span> a la même dimension que
<span class="math inline">\(\{ P_\theta : \theta \in \Theta
\}\)</span>.</p></li>
<li><p>La dimension d’un modèle est majorée par la dimension de l’espace
paramétrique. C’est-à-dire que si <span class="math inline">\(\Theta
\subset \mathbb{R}^d\)</span>, alors <span
class="math inline">\(\dim(\mathcal{M}) \leq d\)</span>.</p></li>
<li><p>La dimension d’un modèle est additive. C’est-à-dire que si <span
class="math inline">\(\mathcal{M}_1\)</span> et <span
class="math inline">\(\mathcal{M}_2\)</span> sont deux modèles
indépendants, alors <span class="math inline">\(\dim(\mathcal{M}_1
\times \mathcal{M}_2) = \dim(\mathcal{M}_1) +
\dim(\mathcal{M}_2)\)</span>.</p></li>
</ol>
<p>Pour prouver la première propriété, supposons que <span
class="math inline">\(\phi : \Theta \to \tilde{\Theta}\)</span> est une
bijection. Alors, nous avons <span
class="math display">\[\frac{\partial}{\partial \tilde{\theta}_i} \log
P_{\phi^{-1}(\tilde{\theta})}(x) = \sum_{j=1}^d \frac{\partial
\phi_j^{-1}(\tilde{\theta})}{\partial \tilde{\theta}_i}
\frac{\partial}{\partial \theta_j} \log
P_{\phi^{-1}(\tilde{\theta})}(x).\]</span></p>
<p>Cela montre que les dérivées de Log-vraisemblance du modèle
transformé sont des combinaisons linéaires des dérivées de
Log-vraisemblance du modèle original. Par conséquent, les espaces
vectoriels engendrés par ces dérivées sont isomorphes, et donc leurs
dimensions sont égales.</p>
<p>Pour prouver la deuxième propriété, remarquons que l’espace vectoriel
engendré par les dérivées de Log-vraisemblance est un sous-espace de
<span class="math inline">\(\mathbb{R}^d\)</span>. Par conséquent, sa
dimension ne peut pas dépasser <span
class="math inline">\(d\)</span>.</p>
<p>Pour prouver la troisième propriété, remarquons que si <span
class="math inline">\(\mathcal{M}_1\)</span> et <span
class="math inline">\(\mathcal{M}_2\)</span> sont deux modèles
indépendants, alors les dérivées de Log-vraisemblance du modèle produit
<span class="math inline">\(\mathcal{M}_1 \times \mathcal{M}_2\)</span>
sont les combinaisons des dérivées de Log-vraisemblance des modèles
individuels. Par conséquent, l’espace vectoriel engendré par les
dérivées de Log-vraisemblance du modèle produit est la somme directe des
espaces vectoriels engendrés par les dérivées de Log-vraisemblance des
modèles individuels. Par conséquent, la dimension du modèle produit est
la somme des dimensions des modèles individuels.</p>
</body>
</html>
{% include "footer.html" %}


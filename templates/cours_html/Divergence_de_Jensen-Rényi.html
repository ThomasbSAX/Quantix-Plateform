{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Jensen-Rényi : Une mesure de l’information et ses applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Jensen-Rényi : Une mesure de
l’information et ses applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Jensen-Rényi émerge comme une généralisation
naturelle des mesures d’information classiques, telles que la divergence
de Kullback-Leibler et les entropies de Rényi. Son origine conceptuelle
réside dans la quête d’une mesure flexible et robuste de l’information,
capable de capturer des aspects subtils des distributions de
probabilité. Cette divergence trouve ses racines dans les travaux
pionniers de Rényi sur les entropies généralisées et l’extension
proposée par Jensen pour intégrer des considérations géométriques.</p>
<p>L’importance de la divergence de Jensen-Rényi réside dans sa capacité
à unifier plusieurs concepts clés en théorie de l’information. Elle
offre une structure mathématique riche qui permet d’explorer des
propriétés fondamentales telles que la convexité, la continuité et la
différentiabilité. De plus, elle trouve des applications pratiques dans
divers domaines, notamment le traitement du signal, l’apprentissage
automatique et la théorie des codes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Jensen-Rényi, commençons par
rappeler quelques concepts préliminaires. Considérons deux distributions
de probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une mesure
qui quantifie la distance entre ces deux distributions, en tenant compte
de leurs propriétés statistiques.</p>
<p>La divergence de Jensen-Rényi est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>, et
soit <span class="math inline">\(\alpha \in (0, 1)\)</span>. La
divergence de Jensen-Rényi d’ordre <span
class="math inline">\(\alpha\)</span> entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_{\alpha}(P \| Q) = \frac{1}{\alpha(1-\alpha)}
\log \left( \int_{\mathcal{X}} \left( \frac{dP}{dQ} \right)^{\alpha} dQ
\right)\]</span> où <span class="math inline">\(\frac{dP}{dQ}\)</span>
désigne la densité radiale de <span class="math inline">\(P\)</span> par
rapport à <span class="math inline">\(Q\)</span>.</p>
</div>
<p>Cette définition peut être reformulée en utilisant des
quantificateurs : <span class="math display">\[\forall \alpha \in (0,
1), \quad D_{\alpha}(P \| Q) = \frac{1}{\alpha(1-\alpha)} \log \left(
\int_{\mathcal{X}} \left( \frac{dP}{dQ} \right)^{\alpha} dQ
\right)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant la
divergence de Jensen-Rényi, qui établit une relation entre cette
divergence et l’entropie de Rényi.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P\)</span> une distribution de
probabilité sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>, et soit <span
class="math inline">\(\alpha \in (0, 1)\)</span>. L’entropie de Rényi
d’ordre <span class="math inline">\(\alpha\)</span> de <span
class="math inline">\(P\)</span> est donnée par : <span
class="math display">\[H_{\alpha}(P) = \frac{1}{1-\alpha} \log \left(
\int_{\mathcal{X}} P(x)^{\alpha} dx \right)\]</span> Alors, la
divergence de Jensen-Rényi entre <span class="math inline">\(P\)</span>
et une distribution uniforme <span class="math inline">\(U\)</span> sur
<span class="math inline">\(\mathcal{X}\)</span> est : <span
class="math display">\[D_{\alpha}(P \| U) = H_{1}(P) -
H_{\alpha}(P)\]</span> où <span class="math inline">\(H_{1}(P)\)</span>
est l’entropie de Shannon de <span class="math inline">\(P\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous procédons comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Considérons une distribution uniforme <span
class="math inline">\(U\)</span> sur <span
class="math inline">\(\mathcal{X}\)</span>. La densité radiale de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(U\)</span> est donnée par : <span
class="math display">\[\frac{dP}{dU} = P(x)\]</span> En utilisant la
définition de la divergence de Jensen-Rényi, nous avons : <span
class="math display">\[D_{\alpha}(P \| U) = \frac{1}{\alpha(1-\alpha)}
\log \left( \int_{\mathcal{X}} P(x)^{\alpha} dx \right)\]</span> D’autre
part, l’entropie de Rényi d’ordre <span
class="math inline">\(\alpha\)</span> est : <span
class="math display">\[H_{\alpha}(P) = \frac{1}{1-\alpha} \log \left(
\int_{\mathcal{X}} P(x)^{\alpha} dx \right)\]</span> En combinant ces
deux expressions, nous obtenons : <span
class="math display">\[D_{\alpha}(P \| U) = \frac{1}{\alpha(1-\alpha)}
\log \left( \int_{\mathcal{X}} P(x)^{\alpha} dx \right) = H_{1}(P) -
H_{\alpha}(P)\]</span> où <span class="math inline">\(H_{1}(P)\)</span>
est l’entropie de Shannon de <span class="math inline">\(P\)</span>,
définie par : <span class="math display">\[H_{1}(P) =
-\int_{\mathcal{X}} P(x) \log P(x) dx\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous énumérons maintenant quelques propriétés importantes de la
divergence de Jensen-Rényi :</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour toute distribution de
probabilité <span class="math inline">\(P\)</span> et tout <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[D_{\alpha}(P \| Q) \geq 0\]</span> avec égalité
si et seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p><strong>Convexité</strong> : La divergence de Jensen-Rényi est
convexe en <span class="math inline">\(P\)</span> pour un <span
class="math inline">\(Q\)</span> fixe.</p></li>
<li><p><strong>Continuité</strong> : La divergence de Jensen-Rényi est
continue en <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Jensen-Rényi représente une avancée significative
dans la théorie de l’information, offrant une mesure flexible et robuste
de la distance entre distributions de probabilité. Ses propriétés
mathématiques riches et ses applications pratiques en font un outil
précieux pour les chercheurs et les praticiens dans divers domaines. Les
travaux futurs pourraient explorer davantage ses extensions et ses
applications dans des contextes plus complexes.</p>
</body>
</html>
{% include "footer.html" %}


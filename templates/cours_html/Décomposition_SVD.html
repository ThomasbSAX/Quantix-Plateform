{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Décomposition SVD : Une exploration mathématique et algorithmique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Décomposition SVD : Une exploration mathématique et
algorithmique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La décomposition en valeurs singulières (SVD, pour <em>Singular Value
Decomposition</em>) est une technique fondamentale en algèbre linéaire
numérique. Son origine remonte aux travaux de Beltrami (1873) et Jordan
(1874), mais c’est avec l’avènement de l’informatique que cette
décomposition a trouvé des applications pratiques dans divers domaines,
tels que le traitement du signal, la compression d’images, et
l’apprentissage automatique.</p>
<p>La SVD permet de décomposer une matrice quelconque en un produit de
trois matrices particulières : deux matrices orthogonales et une matrice
diagonale. Cette décomposition est particulièrement utile car elle
révèle des propriétés structurelles profondes de la matrice originale,
telles que son rang et ses valeurs singulières. De plus, la SVD est
souvent utilisée comme outil pour résoudre des problèmes mal
conditionnés ou pour effectuer des approximations de rang inférieur.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la décomposition SVD, commençons par définir les
concepts clés.</p>
<h2 id="matrices-orthogonales">Matrices Orthogonales</h2>
<p>Une matrice <span class="math inline">\(Q\)</span> est dite
orthogonale si ses colonnes (ou lignes) forment une base orthonormée de
l’espace vectoriel correspondant. Formellement, on a :</p>
<p><span class="math display">\[Q^T Q = I\]</span></p>
<p>où <span class="math inline">\(Q^T\)</span> désigne la transposée de
<span class="math inline">\(Q\)</span>, et <span
class="math inline">\(I\)</span> est la matrice identité.</p>
<h2 id="matrices-diagonales">Matrices Diagonales</h2>
<p>Une matrice diagonale <span class="math inline">\(D\)</span> est une
matrice carrée dont les éléments hors de la diagonale principale sont
nuls. On peut l’écrire sous la forme :</p>
<p><span class="math display">\[D = \begin{pmatrix}
d_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; d_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; d_n
\end{pmatrix}\]</span></p>
<h2 id="décomposition-svd">Décomposition SVD</h2>
<p>La décomposition SVD d’une matrice <span
class="math inline">\(A\)</span> de taille <span class="math inline">\(m
\times n\)</span> est donnée par :</p>
<p><span class="math display">\[A = U \Sigma V^T\]</span></p>
<p>où : - <span class="math inline">\(U\)</span> est une matrice
orthogonale de taille <span class="math inline">\(m \times m\)</span>, -
<span class="math inline">\(\Sigma\)</span> est une matrice diagonale de
taille <span class="math inline">\(m \times n\)</span>, - <span
class="math inline">\(V\)</span> est une matrice orthogonale de taille
<span class="math inline">\(n \times n\)</span>.</p>
<p>Les éléments diagonaux de <span class="math inline">\(\Sigma\)</span>
sont appelés les valeurs singulières de <span
class="math inline">\(A\)</span>, et ils sont notés <span
class="math inline">\(\sigma_1, \sigma_2, \ldots, \sigma_r\)</span>, où
<span class="math inline">\(r\)</span> est le rang de <span
class="math inline">\(A\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="existence-et-unicité-de-la-svd">Existence et Unicité de la
SVD</h2>
<p>Le théorème suivant garantit l’existence et l’unicité de la
décomposition SVD.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(A\)</span> une matrice de taille
<span class="math inline">\(m \times n\)</span>. Il existe des matrices
orthogonales <span class="math inline">\(U\)</span> et <span
class="math inline">\(V\)</span>, ainsi qu’une matrice diagonale <span
class="math inline">\(\Sigma\)</span>, telles que :</p>
<p><span class="math display">\[A = U \Sigma V^T\]</span></p>
<p>De plus, les valeurs singulières <span
class="math inline">\(\sigma_i\)</span> sont uniques et positives.</p>
</div>
<h2 id="propriétés-des-valeurs-singulières">Propriétés des Valeurs
Singulières</h2>
<p>Les valeurs singulières d’une matrice possèdent plusieurs propriétés
importantes.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(A\)</span> une matrice de taille
<span class="math inline">\(m \times n\)</span>, et soit <span
class="math inline">\(\sigma_1, \sigma_2, \ldots, \sigma_r\)</span> ses
valeurs singulières. Alors :</p>
<ol>
<li><p><span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \ldots
\geq \sigma_r &gt; 0\)</span>,</p></li>
<li><p><span class="math inline">\(\text{rang}(A) = r\)</span>,</p></li>
<li><p><span class="math inline">\(\|A\|_2 = \sigma_1\)</span>, où <span
class="math inline">\(\|A\|_2\)</span> désigne la norme spectrale de
<span class="math inline">\(A\)</span>.</p></li>
</ol>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-lexistence-de-la-svd">Preuve de l’Existence de la
SVD</h2>
<p>Pour prouver l’existence de la décomposition SVD, nous suivons les
étapes suivantes :</p>
<p>1. **Calcul des valeurs propres de <span class="math inline">\(A^T
A\)</span>** : Les valeurs singulières de <span
class="math inline">\(A\)</span> sont les racines carrées des valeurs
propres de <span class="math inline">\(A^T A\)</span>.</p>
<p>2. **Construction de <span class="math inline">\(V\)</span>** : Les
colonnes de <span class="math inline">\(V\)</span> sont les vecteurs
propres normalisés de <span class="math inline">\(A^T A\)</span>.</p>
<p>3. **Construction de <span class="math inline">\(U\)</span>** : Les
colonnes de <span class="math inline">\(U\)</span> sont obtenues en
normalisant les vecteurs <span class="math inline">\(A V_i\)</span>, où
<span class="math inline">\(V_i\)</span> sont les colonnes de <span
class="math inline">\(V\)</span>.</p>
<p>4. **Construction de <span class="math inline">\(\Sigma\)</span>** :
Les éléments diagonaux de <span class="math inline">\(\Sigma\)</span>
sont les valeurs singulières de <span
class="math inline">\(A\)</span>.</p>
<h2 id="preuve-des-propriétés-des-valeurs-singulières">Preuve des
Propriétés des Valeurs Singulières</h2>
<p>1. **Ordre des valeurs singulières** : Les valeurs singulières sont
ordonnées par ordre décroissant en raison de la propriété des valeurs
propres de <span class="math inline">\(A^T A\)</span>.</p>
<p>2. **Rang de <span class="math inline">\(A\)</span>** : Le rang de
<span class="math inline">\(A\)</span> est égal au nombre de valeurs
singulières non nulles.</p>
<p>3. **Norme spectrale** : La norme spectrale de <span
class="math inline">\(A\)</span> est égale à la plus grande valeur
singulière, car <span class="math inline">\(\|A\|_2 =
\sqrt{\lambda_{\text{max}}(A^T A)}\)</span>, où <span
class="math inline">\(\lambda_{\text{max}}\)</span> est la plus grande
valeur propre de <span class="math inline">\(A^T A\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-de-la-svd">Propriétés de la SVD</h2>
<ol>
<li><p>**Approximation de rang inférieur** : La SVD permet d’approximer
une matrice <span class="math inline">\(A\)</span> par une matrice de
rang inférieur en ne conservant que les <span
class="math inline">\(k\)</span> plus grandes valeurs singulières.</p>
<p><span class="math display">\[A_k = U_k \Sigma_k V_k^T\]</span></p>
<p>où <span class="math inline">\(U_k\)</span>, <span
class="math inline">\(\Sigma_k\)</span>, et <span
class="math inline">\(V_k\)</span> sont les <span
class="math inline">\(k\)</span> premières colonnes de <span
class="math inline">\(U\)</span>, <span
class="math inline">\(\Sigma\)</span>, et <span
class="math inline">\(V\)</span> respectivement.</p></li>
<li><p>**Pseudo-inverse** : La SVD permet de calculer la pseudo-inverse
de <span class="math inline">\(A\)</span>, notée <span
class="math inline">\(A^+\)</span>, par :</p>
<p><span class="math display">\[A^+ = V \Sigma^+ U^T\]</span></p>
<p>où <span class="math inline">\(\Sigma^+\)</span> est obtenue en
prenant l’inverse des éléments non nuls de <span
class="math inline">\(\Sigma\)</span> et en transposant le
résultat.</p></li>
</ol>
<h2 id="corollaires">Corollaires</h2>
<div class="corollary">
<p>La meilleure approximation de rang <span
class="math inline">\(k\)</span> de <span
class="math inline">\(A\)</span> au sens de la norme de Frobenius est
donnée par :</p>
<p><span class="math display">\[A_k = U_k \Sigma_k V_k^T\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle du fait que la SVD minimise
l’erreur de reconstruction <span class="math inline">\(\|A -
A_k\|_F\)</span>, où <span class="math inline">\(\| \cdot \|_F\)</span>
désigne la norme de Frobenius. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La décomposition SVD est un outil puissant et polyvalent en algèbre
linéaire numérique. Ses applications vont de la compression d’images à
l’apprentissage automatique, en passant par la résolution de systèmes
linéaires. La compréhension approfondie de cette décomposition permet de
mieux appréhender les propriétés structurelles des matrices et
d’optimiser les algorithmes numériques.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage par Transformateurs : Une Révolution en Traitement Automatique des Langues</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage par Transformateurs : Une Révolution en
Traitement Automatique des Langues</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par transformateurs représente une avancée majeure dans le
domaine du traitement automatique des langues (TAL). Inspirés par les
architectures neuronales introduites par Vaswani et al. dans leur
article fondateur <em>"Attention is All You Need"</em> (2017), les
transformateurs ont révolutionné la manière dont nous traitons et
comprenons le langage naturel. Ces modèles, basés sur des mécanismes
d’attention, permettent de capturer des dépendances à long terme dans
les séquences textuelles, dépassant ainsi les limitations des modèles
séquentiels traditionnels comme les réseaux de neurones récurrents (RNN)
ou les réseaux à long terme mémoire (LSTM).</p>
<p>L’émergence des transformateurs a été motivée par le besoin de
modèles capables de traiter efficacement des séquences de longueur
variable, tout en exploitant le parallélisme massif offert par les
architectures matérielles modernes. Leur capacité à modéliser des
relations complexes entre les mots d’une phrase, indépendamment de leur
distance, a ouvert la voie à des performances inédites dans des tâches
variées telles que la traduction automatique, le résumé de texte, ou
encore la génération de langage.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par transformateurs, il est essentiel de
définir plusieurs concepts clés. Commençons par le mécanisme
d’attention, qui constitue le cœur des transformateurs.</p>
<h2 class="unnumbered" id="mécanisme-dattention">Mécanisme
d’Attention</h2>
<p>Supposons que nous ayons une séquence de mots et que nous cherchions
à modéliser l’importance relative de chaque mot par rapport aux autres.
L’idée est que certains mots sont plus pertinents que d’autres pour
comprendre le sens global de la phrase. Le mécanisme d’attention permet
de pondérer dynamiquement l’importance des différentes parties de la
séquence.</p>
<p>Formellement, pour une séquence d’entrée <span
class="math inline">\(X = (x_1, x_2, \ldots, x_n)\)</span>, le mécanisme
d’attention calcule une distribution de poids <span
class="math inline">\(\alpha = (\alpha_1, \alpha_2, \ldots,
\alpha_n)\)</span> telle que :</p>
<p><span class="math display">\[\alpha_i = \frac{\exp(e_i)}{\sum_{j=1}^n
\exp(e_j)}, \quad \text{où} \quad e_i = v^T \tanh(W_1 x_i + W_2
h_{i-1})\]</span></p>
<p>Ici, <span class="math inline">\(W_1\)</span> et <span
class="math inline">\(W_2\)</span> sont des matrices de poids
apprenables, <span class="math inline">\(v\)</span> est un vecteur de
pondération, et <span class="math inline">\(h_{i-1}\)</span> représente
l’état caché précédent.</p>
<h2 class="unnumbered" id="transformateur">Transformateur</h2>
<p>Un transformateur est un modèle composé de plusieurs couches
d’encodage et de décodage, chacune utilisant des mécanismes d’attention.
L’encodage par transformateur peut être défini comme suit :</p>
<p>Pour une séquence d’entrée <span class="math inline">\(X = (x_1, x_2,
\ldots, x_n)\)</span>, l’encodage par transformateur produit une
séquence de vecteurs <span class="math inline">\(Z = (z_1, z_2, \ldots,
z_n)\)</span> telle que :</p>
<p><span class="math display">\[z_i = \text{LayerNorm}(x_i +
\text{MultiHeadAttention}(x_i, x_i, x_i)) +
\text{FeedForward}(z_i)\]</span></p>
<p>Où <span class="math inline">\(\text{MultiHeadAttention}\)</span>
représente l’attention multi-têtes et <span
class="math inline">\(\text{FeedForward}\)</span> une couche de réseau
de neurones entièrement connectée.</p>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<h2 class="unnumbered" id="théorème-de-lattention-multi-têtes">Théorème
de l’Attention Multi-Têtes</h2>
<p>L’attention multi-têtes permet au modèle de capturer différentes
perspectives sur les relations entre les mots. Formellement, pour <span
class="math inline">\(h\)</span> têtes d’attention, le mécanisme
multi-têtes est défini par :</p>
<p><span class="math display">\[\text{MultiHeadAttention}(Q, K, V) =
\text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O\]</span></p>
<p>où chaque tête <span class="math inline">\(\text{head}_i\)</span> est
calculée comme :</p>
<p><span class="math display">\[\text{head}_i = \text{Attention}(Q
W_i^Q, K W_i^K, V W_i^V)\]</span></p>
<p>et <span class="math inline">\(W_i^Q, W_i^K, W_i^V\)</span> sont des
matrices de projection apprenables.</p>
<h2 class="unnumbered" id="propriétés-des-transformateurs">Propriétés
des Transformateurs</h2>
<p>Les transformateurs possèdent plusieurs propriétés remarquables :</p>
<ol>
<li><p><strong>Parallélisme</strong> : Les transformateurs peuvent
traiter toutes les positions de la séquence simultanément, contrairement
aux modèles séquentiels.</p></li>
<li><p><strong>Dépendances à Long Terme</strong> : Grâce au mécanisme
d’attention, les transformateurs peuvent capturer des dépendances entre
des mots éloignés dans la séquence.</p></li>
<li><p><strong>Invariance de Position</strong> : Les transformateurs
utilisent des embeddings de position pour incorporer l’information
d’ordre dans la séquence.</p></li>
</ol>
<h1 class="unnumbered" id="preuves-et-démonstrations">Preuves et
Démonstrations</h1>
<h2 class="unnumbered"
id="preuve-du-théorème-de-lattention-multi-têtes">Preuve du Théorème de
l’Attention Multi-Têtes</h2>
<p>Pour démontrer que l’attention multi-têtes permet effectivement de
capturer différentes perspectives, considérons une seule tête
d’attention. L’attention simple est définie par :</p>
<p><span class="math display">\[\text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]</span></p>
<p>où <span class="math inline">\(Q, K, V\)</span> sont les matrices de
requêtes, clés et valeurs respectivement, et <span
class="math inline">\(d_k\)</span> est la dimension des clés. En
utilisant plusieurs têtes, nous pouvons projeter ces matrices dans
différents espaces linéaires, permettant ainsi de capturer différentes
relations.</p>
<h2 class="unnumbered" id="démonstration-des-propriétés">Démonstration
des Propriétés</h2>
<ol>
<li><p><strong>Parallélisme</strong> : Les transformateurs calculent les
attentions pour toutes les positions en parallèle, ce qui est possible
grâce à l’utilisation de matrices. Chaque position <span
class="math inline">\(i\)</span> est traitée indépendamment des autres,
permettant un traitement efficace sur GPU.</p></li>
<li><p><strong>Dépendances à Long Terme</strong> : Le mécanisme
d’attention permet de pondérer l’importance des mots éloignés,
contrairement aux modèles séquentiels qui ont tendance à oublier les
informations anciennes.</p></li>
<li><p><strong>Invariance de Position</strong> : Les embeddings de
position ajoutent une information d’ordre à chaque token, permettant au
modèle de distinguer les séquences qui diffèrent uniquement par l’ordre
des mots.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par transformateurs a révolutionné le traitement
automatique des langues en offrant une architecture puissante et
flexible pour modéliser les relations complexes dans le langage naturel.
Grâce à leur capacité à capturer des dépendances à long terme et à
exploiter le parallélisme massif, les transformateurs ont ouvert la voie
à des performances inédites dans de nombreuses tâches de TAL. Leur
impact continue de se faire sentir, avec des applications allant de la
traduction automatique à la génération de texte, en passant par le
résumé et l’analyse des sentiments.</p>
</body>
</html>
{% include "footer.html" %}


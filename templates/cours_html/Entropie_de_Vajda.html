{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Vajda : Une Mesure de l’Incertitude et de la Divergence</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Vajda : Une Mesure de l’Incertitude et
de la Divergence</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de Vajda émerge comme une généralisation élégante des
mesures classiques d’incertitude et de divergence dans les systèmes
probabilistes. Introduite par le mathématicien hongrois László Vajda,
cette notion unifie plusieurs concepts clés en théorie de l’information
et en statistique. Son importance réside dans sa capacité à capturer des
aspects subtils de l’information et de la divergence entre
distributions, offrant ainsi un cadre plus flexible pour l’analyse des
systèmes complexes.</p>
<p>L’entropie de Vajda trouve ses racines dans les travaux pionniers de
Shannon sur l’entropie et la divergence de Kullback-Leibler. Cependant,
elle va au-delà en intégrant des paramètres qui permettent de moduler la
sensibilité de la mesure à différentes échelles d’information. Cette
flexibilité en fait un outil précieux dans des domaines tels que le
traitement du signal, l’apprentissage automatique et la théorie des
jeux.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Vajda, commençons par rappeler le
concept d’entropie de Shannon. L’entropie de Shannon mesure
l’incertitude associée à une distribution de probabilité discrète.
Formellement, pour une distribution <span class="math inline">\(P =
(p_1, p_2, \ldots, p_n)\)</span>, l’entropie de Shannon est définie
comme :</p>
<p><span class="math display">\[H(P) = -\sum_{i=1}^n p_i \log
p_i\]</span></p>
<p>L’entropie de Vajda généralise cette notion en introduisant un
paramètre supplémentaire <span class="math inline">\(\alpha\)</span> qui
contrôle la sensibilité de l’entropie. Intuitivement, nous cherchons une
mesure qui puisse capturer des aspects plus fins de l’information, en
fonction de la valeur de <span
class="math inline">\(\alpha\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(P = (p_1, p_2, \ldots, p_n)\)</span>
une distribution de probabilité discrète et <span
class="math inline">\(\alpha\)</span> un paramètre réel. L’entropie de
Vajda est définie comme :</p>
<p><span class="math display">\[H_\alpha(P) = \frac{1}{1 - \alpha} \log
\left( \sum_{i=1}^n p_i^\alpha \right)\]</span></p>
<p>Pour <span class="math inline">\(\alpha = 1\)</span>, l’entropie de
Vajda coïncide avec l’entropie de Shannon :</p>
<p><span class="math display">\[H_1(P) = \lim_{\alpha \to 1} H_\alpha(P)
= -\sum_{i=1}^n p_i \log p_i\]</span></p>
<p>Pour <span class="math inline">\(\alpha = 0\)</span>, l’entropie de
Vajda devient la log-vraisemblance :</p>
<p><span class="math display">\[H_0(P) = \log n\]</span></p>
<p>Pour <span class="math inline">\(\alpha = 2\)</span>, l’entropie de
Vajda est liée à la variance :</p>
<p><span class="math display">\[H_2(P) = -\log \left( \sum_{i=1}^n p_i^2
\right)\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’entropie de Vajda possède plusieurs propriétés intéressantes qui la
rendent utile dans diverses applications. Voici quelques théorèmes
clés.</p>
<div class="theorem">
<p>Pour tout <span class="math inline">\(\alpha \neq 1\)</span>, la
fonction <span class="math inline">\(H_\alpha(P)\)</span> est concave en
<span class="math inline">\(P\)</span>. Cela signifie que pour toute
distribution de probabilité <span class="math inline">\(P\)</span>, et
pour tout <span class="math inline">\(\lambda \in [0, 1]\)</span>,</p>
<p><span class="math display">\[H_\alpha(\lambda P + (1 - \lambda) Q)
\geq \lambda H_\alpha(P) + (1 - \lambda) H_\alpha(Q)\]</span></p>
<p>où <span class="math inline">\(Q\)</span> est une autre distribution
de probabilité.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur la concavité de la fonction
logarithme et les propriétés des puissances. En utilisant l’inégalité de
Jensen, nous avons :</p>
<p><span class="math display">\[\log \left( \sum_{i=1}^n (\lambda p_i +
(1 - \lambda) q_i)^\alpha \right) \geq \lambda \log \left( \sum_{i=1}^n
p_i^\alpha \right) + (1 - \lambda) \log \left( \sum_{i=1}^n q_i^\alpha
\right)\]</span></p>
<p>En multipliant par <span class="math inline">\(\frac{1}{1 -
\alpha}\)</span>, nous obtenons la concavité de <span
class="math inline">\(H_\alpha(P)\)</span>. ◻</p>
</div>
<div class="theorem">
<p>La divergence de Vajda entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme :</p>
<p><span class="math display">\[D_\alpha(P \| Q) = \frac{1}{1 - \alpha}
\log \left( \sum_{i=1}^n p_i^\alpha q_i^{1 - \alpha}
\right)\]</span></p>
<p>Cette divergence généralise la divergence de Kullback-Leibler pour
<span class="math inline">\(\alpha = 1\)</span> :</p>
<p><span class="math display">\[D_1(P \| Q) = \lim_{\alpha \to 1}
D_\alpha(P \| Q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve suit directement de la définition de
l’entropie de Vajda et des propriétés des logarithmes. En utilisant les
mêmes techniques que pour la preuve de la concavité, nous pouvons
montrer que <span class="math inline">\(D_\alpha(P \| Q)\)</span> est
une mesure valide de divergence. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de Vajda possède plusieurs propriétés intéressantes qui en
font un outil puissant pour l’analyse des distributions de
probabilité.</p>
<ul>
<li><p><strong>Normalisation</strong> : Pour toute distribution de
probabilité <span class="math inline">\(P\)</span>, <span
class="math inline">\(H_\alpha(P) \geq 0\)</span>.</p></li>
<li><p><strong>Invariance par Transformation</strong> : L’entropie de
Vajda est invariante sous les transformations bijectives des
variables.</p></li>
<li><p><strong>Limites</strong> : Pour <span
class="math inline">\(\alpha \to -\infty\)</span>, <span
class="math inline">\(H_\alpha(P)\)</span> tend vers la
log-vraisemblance de l’événement le plus probable, tandis que pour <span
class="math inline">\(\alpha \to +\infty\)</span>, <span
class="math inline">\(H_\alpha(P)\)</span> tend vers la
log-vraisemblance de l’événement le moins probable.</p></li>
</ul>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Vajda offre une généralisation flexible et puissante
des mesures classiques d’incertitude et de divergence. Ses propriétés
mathématiques riches et sa capacité à capturer des aspects subtils de
l’information en font un outil précieux dans de nombreux domaines. Les
théorèmes et propriétés présentés ici ne sont qu’un aperçu des
nombreuses applications possibles de cette notion.</p>
</body>
</html>
{% include "footer.html" %}


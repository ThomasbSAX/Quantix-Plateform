{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’approximation de Laplace : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’approximation de Laplace : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’approximation de Laplace est une technique mathématique puissante
qui trouve ses racines dans les travaux du célèbre mathématicien et
astronome Pierre-Simon de Laplace au début du XIXème siècle. Cette
méthode permet d’approximer des intégrales multi-dimensionnelles,
souvent rencontrées dans les problèmes de probabilité et de statistique.
L’approximation de Laplace est particulièrement utile lorsque ces
intégrales sont difficiles, voire impossibles, à évaluer
analytiquement.</p>
<p>L’origine de cette approximation est liée aux besoins en astronomie
et en physique, où Laplace cherchait à modéliser des phénomènes
complexes. Aujourd’hui, cette technique est largement utilisée en
statistique bayésienne pour approcher les distributions a posteriori,
ainsi que dans divers domaines de la physique et de l’ingénierie.</p>
<p>L’approximation de Laplace est indispensable dans le cadre des
modèles hiérarchiques et des réseaux bayésiens, où les intégrales
nécessaires pour calculer les distributions marginales sont souvent
intractables. Elle permet de simplifier ces calculs tout en conservant
une bonne précision, ce qui est crucial pour l’analyse des données
complexes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’approximation de Laplace, commençons par définir
les concepts clés.</p>
<h2 id="intégrale-de-laplace">Intégrale de Laplace</h2>
<p>Considérons une fonction <span class="math inline">\(f : \mathbb{R}^n
\rightarrow \mathbb{R}\)</span> et supposons que cette fonction admette
un maximum global en un point <span
class="math inline">\(\theta_0\)</span>. L’idée est d’approximer
l’intégrale de <span class="math inline">\(f\)</span> par une intégrale
gaussienne centrée en <span class="math inline">\(\theta_0\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction deux fois continûment différentiable
admettant un maximum global en <span
class="math inline">\(\theta_0\)</span>. L’approximation de Laplace de
l’intégrale <span class="math display">\[I = \int_{\mathbb{R}^n}
f(\theta) \, d\theta\]</span> est donnée par <span
class="math display">\[I \approx (2\pi)^{n/2} |\det(-\nabla^2
f(\theta_0))|^{-1/2} f(\theta_0),\]</span> où <span
class="math inline">\(\nabla^2 f(\theta_0)\)</span> est la matrice
hessienne de <span class="math inline">\(f\)</span> en <span
class="math inline">\(\theta_0\)</span>.</p>
</div>
<h2 id="hessienne-et-maximum-global">Hessienne et Maximum Global</h2>
<p>Pour que l’approximation de Laplace soit valide, il est crucial que
la fonction <span class="math inline">\(f\)</span> admette un maximum
global. De plus, la matrice hessienne en ce point doit être définie
négative.</p>
<div class="definition">
<p>La matrice hessienne d’une fonction <span class="math inline">\(f :
\mathbb{R}^n \rightarrow \mathbb{R}\)</span> deux fois continûment
différentiable est la matrice des dérivées secondes définie par <span
class="math display">\[(\nabla^2 f)_{ij} = \frac{\partial^2 f}{\partial
\theta_i \partial \theta_j}.\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-laplace">Théorème de Laplace</h2>
<p>Le théorème central de l’approximation de Laplace est le suivant
:</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction deux fois continûment différentiable
admettant un maximum global en <span
class="math inline">\(\theta_0\)</span>. Supposons que la matrice
hessienne <span class="math inline">\(\nabla^2 f(\theta_0)\)</span> soit
définie négative. Alors, pour <span class="math inline">\(h\)</span>
suffisamment petit, <span class="math display">\[\int_{\mathbb{R}^n}
e^{h f(\theta)} \, d\theta \approx (2\pi)^{n/2} h^{-n/2} |\det(-\nabla^2
f(\theta_0))|^{-1/2} e^{h f(\theta_0)}.\]</span></p>
</div>
<h2 id="démonstration-du-théorème-de-laplace">Démonstration du Théorème
de Laplace</h2>
<p>Pour démontrer ce théorème, nous procédons par un développement de
Taylor autour du point <span
class="math inline">\(\theta_0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Considérons le développement de Taylor de <span
class="math inline">\(f\)</span> autour de <span
class="math inline">\(\theta_0\)</span> : <span
class="math display">\[f(\theta) = f(\theta_0) + (\theta - \theta_0)^T
\nabla f(\theta_0) + \frac{1}{2} (\theta - \theta_0)^T \nabla^2
f(\theta_0) (\theta - \theta_0) + o(\|\theta -
\theta_0\|^2).\]</span></p>
<p>Comme <span class="math inline">\(\theta_0\)</span> est un point
critique, <span class="math inline">\(\nabla f(\theta_0) = 0\)</span>.
Ainsi, <span class="math display">\[f(\theta) = f(\theta_0) +
\frac{1}{2} (\theta - \theta_0)^T \nabla^2 f(\theta_0) (\theta -
\theta_0) + o(\|\theta - \theta_0\|^2).\]</span></p>
<p>En supposant que <span class="math inline">\(h\)</span> est
suffisamment petit, nous pouvons négliger les termes d’ordre supérieur à
2. Par conséquent, <span class="math display">\[e^{h f(\theta)} \approx
e^{h f(\theta_0)} e^{\frac{h}{2} (\theta - \theta_0)^T \nabla^2
f(\theta_0) (\theta - \theta_0)}.\]</span></p>
<p>En effectuant un changement de variables <span
class="math inline">\(\eta = (\theta - \theta_0)\)</span>, nous obtenons
<span class="math display">\[I \approx e^{h f(\theta_0)}
\int_{\mathbb{R}^n} e^{\frac{h}{2} \eta^T \nabla^2 f(\theta_0) \eta} \,
d\eta.\]</span></p>
<p>En utilisant la propriété des intégrales gaussiennes, nous avons
<span class="math display">\[\int_{\mathbb{R}^n} e^{\frac{h}{2} \eta^T A
\eta} \, d\eta = (2\pi)^{n/2} h^{-n/2} |\det(-A)|^{-1/2},\]</span> où
<span class="math inline">\(A = \nabla^2 f(\theta_0)\)</span>.</p>
<p>Ainsi, <span class="math display">\[I \approx (2\pi)^{n/2} h^{-n/2}
|\det(-\nabla^2 f(\theta_0))|^{-1/2} e^{h f(\theta_0)}.\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-précision">Propriété de Précision</h2>
<p>L’approximation de Laplace est particulièrement précise lorsque la
fonction <span class="math inline">\(f\)</span> est fortement concave
autour de son maximum.</p>
<div class="proposition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction deux fois continûment différentiable
admettant un maximum global en <span
class="math inline">\(\theta_0\)</span>. Supposons que la matrice
hessienne <span class="math inline">\(\nabla^2 f(\theta_0)\)</span> soit
définie négative et que <span class="math inline">\(f\)</span> soit
fortement concave autour de <span
class="math inline">\(\theta_0\)</span>. Alors, l’approximation de
Laplace est d’autant plus précise que la concavité de <span
class="math inline">\(f\)</span> autour de <span
class="math inline">\(\theta_0\)</span> est grande.</p>
</div>
<h2 id="corollaire-pour-les-distributions-a-posteriori">Corollaire pour
les Distributions A Posteriori</h2>
<p>En statistique bayésienne, l’approximation de Laplace est souvent
utilisée pour approcher les distributions a posteriori.</p>
<div class="corollaire">
<p>Soit <span class="math inline">\(\pi(\theta)\)</span> une
distribution a priori et <span class="math inline">\(L(\theta |
x)\)</span> une fonction de vraisemblance. La distribution a posteriori
est donnée par <span class="math display">\[\pi(\theta | x) \propto
\pi(\theta) L(\theta | x).\]</span></p>
<p>En prenant le logarithme, nous obtenons <span
class="math display">\[\log \pi(\theta | x) = \log \pi(\theta) + \log
L(\theta | x) + C,\]</span> où <span class="math inline">\(C\)</span>
est une constante.</p>
<p>Si <span class="math inline">\(\log \pi(\theta | x)\)</span> admet un
maximum global en <span class="math inline">\(\theta_0\)</span>, alors
l’approximation de Laplace peut être appliquée pour approcher la
distribution a posteriori par une distribution gaussienne centrée en
<span class="math inline">\(\theta_0\)</span>.</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’approximation de Laplace est une technique puissante et polyvalente
qui trouve des applications dans divers domaines, notamment en
statistique bayésienne. En approchant les intégrales complexes par des
intégrales gaussiennes, cette méthode permet de simplifier
considérablement les calculs tout en conservant une bonne précision. Les
théorèmes et propriétés présentés dans cet article montrent la
robustesse et l’utilité de cette approximation, faisant d’elle un outil
indispensable pour les chercheurs et les praticiens.</p>
</body>
</html>
{% include "footer.html" %}


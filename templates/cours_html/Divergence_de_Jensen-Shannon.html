{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Jensen-Shannon : Une Mesure Symétrique de la Divergence Relative</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Jensen-Shannon : Une Mesure Symétrique
de la Divergence Relative</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Jensen-Shannon (JSD) émerge comme une mesure
symétrique et normalisée de la divergence relative, offrant une
alternative robuste aux mesures asymétriques traditionnelles. Son
origine remonte aux travaux sur l’information mutuelle et la divergence
de Kullback-Leibler, mais c’est sa symétrie et ses propriétés normatives
qui en font un outil indispensable dans l’analyse des distributions de
probabilité.</p>
<p>La JSD est particulièrement utile dans les domaines où la symétrie
est cruciale, tels que l’apprentissage automatique et le traitement du
signal. Elle permet de mesurer la similarité entre deux distributions de
manière équilibrée, évitant les biais introduits par des mesures
asymétriques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la divergence de Jensen-Shannon, commençons par
comprendre ce que nous cherchons à mesurer. Nous voulons une mesure de
la distance entre deux distributions de probabilité, qui soit symétrique
et normalisée.</p>
<p>Considérons deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace <span
class="math inline">\(X\)</span>. Nous cherchons une mesure qui
quantifie la différence entre ces deux distributions.</p>
<p>La divergence de Jensen-Shannon est définie comme suit :</p>
<div class="definition">
<p>La divergence de Jensen-Shannon entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[JSD(P||Q) = \frac{1}{2}
D_{KL}\left(P||\frac{P+Q}{2}\right) + \frac{1}{2}
D_{KL}\left(Q||\frac{P+Q}{2}\right)\]</span> où <span
class="math inline">\(D_{KL}\)</span> est la divergence de
Kullback-Leibler.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[JSD(P||Q) = H\left(\frac{P+Q}{2}\right) -
\frac{1}{2} H(P) - \frac{1}{2} H(Q)\]</span> où <span
class="math inline">\(H\)</span> est l’entropie de Shannon.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Jensen-Shannon est
celui de sa relation avec l’information mutuelle.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité. La
divergence de Jensen-Shannon peut être exprimée en termes d’information
mutuelle comme suit : <span class="math display">\[JSD(P||Q) =
\frac{1}{2} I\left(P; Q\right)\]</span> où <span
class="math inline">\(I(P; Q)\)</span> est l’information mutuelle entre
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème ci-dessus, nous commençons par rappeler la
définition de l’information mutuelle.</p>
<div class="proof">
<p><em>Proof.</em> L’information mutuelle entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[I(P; Q) = D_{KL}\left(P||\frac{P+Q}{2}\right) +
D_{KL}\left(Q||\frac{P+Q}{2}\right)\]</span></p>
<p>En utilisant la définition de la divergence de Jensen-Shannon, nous
avons : <span class="math display">\[JSD(P||Q) = \frac{1}{2}
D_{KL}\left(P||\frac{P+Q}{2}\right) + \frac{1}{2}
D_{KL}\left(Q||\frac{P+Q}{2}\right)\]</span></p>
<p>Il est clair que : <span class="math display">\[JSD(P||Q) =
\frac{1}{2} I(P; Q)\]</span></p>
<p>Ainsi, la relation entre la divergence de Jensen-Shannon et
l’information mutuelle est établie. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Jensen-Shannon possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p><strong>Symétrie</strong> : <span class="math inline">\(JSD(P||Q)
= JSD(Q||P)\)</span></p></li>
<li><p><strong>Normalisation</strong> : <span class="math inline">\(0
\leq JSD(P||Q) \leq 1\)</span></p></li>
<li><p><strong>Identité</strong> : <span class="math inline">\(JSD(P||P)
= 0\)</span></p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour prouver la symétrie, nous utilisons la
définition de la JSD : <span class="math display">\[JSD(P||Q) =
\frac{1}{2} D_{KL}\left(P||\frac{P+Q}{2}\right) + \frac{1}{2}
D_{KL}\left(Q||\frac{P+Q}{2}\right)\]</span> En permutant <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous obtenons : <span
class="math display">\[JSD(Q||P) = \frac{1}{2}
D_{KL}\left(Q||\frac{P+Q}{2}\right) + \frac{1}{2}
D_{KL}\left(P||\frac{P+Q}{2}\right) = JSD(P||Q)\]</span></p>
<p>Pour la normalisation, nous savons que <span
class="math inline">\(D_{KL}\)</span> est toujours non négative et que
<span class="math inline">\(JSD(P||Q) = 0\)</span> si et seulement si
<span class="math inline">\(P = Q\)</span>. De plus, la JSD est bornée
par 1 en raison de ses propriétés normatives.</p>
<p>Enfin, pour l’identité, si <span class="math inline">\(P =
Q\)</span>, alors : <span class="math display">\[JSD(P||P) = \frac{1}{2}
D_{KL}\left(P||\frac{P+P}{2}\right) + \frac{1}{2}
D_{KL}\left(P||\frac{P+P}{2}\right) = 0\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Jensen-Shannon est un outil puissant pour mesurer la
similarité entre deux distributions de probabilité. Sa symétrie et ses
propriétés normatives en font un choix privilégié dans de nombreuses
applications pratiques. Les théorèmes et preuves présentés ici
illustrent la profondeur mathématique de cette mesure et son importance
dans le domaine de l’information théorique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Earth Mover’s Distance (EMD) Loss: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Earth Mover’s Distance (EMD) Loss: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Earth Mover’s Distance (EMD), également connue sous le nom de
Wasserstein distance, est une mesure de la distance entre deux
distributions de probabilité. Son origine remonte aux travaux de Leonid
Kantorovich dans les années 1940, où il a introduit cette notion pour
résoudre des problèmes de transport optimal. L’EMD est devenue un outil
essentiel dans divers domaines tels que la vision par ordinateur,
l’apprentissage automatique et le traitement du signal.</p>
<p>L’émergence de l’EMD est motivée par la nécessité de comparer des
distributions de manière robuste et intuitive. Contrairement à d’autres
mesures de distance comme la divergence de Kullback-Leibler ou
l’entropie croisée, l’EMD prend en compte la structure géométrique des
données. Elle est particulièrement utile lorsque les distributions sont
définies sur un espace métrique, comme dans le cas des histogrammes ou
des nuages de points.</p>
<p>L’EMD est indispensable dans les cadres où la préservation de la
structure locale des données est cruciale. Par exemple, en vision par
ordinateur, l’EMD permet de comparer des histogrammes de
caractéristiques de manière géométriquement significative. Dans les
modèles génératifs, elle est utilisée pour évaluer la qualité des
échantillons produits par des réseaux de neurones génératifs
adversariaux (GANs).</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir l’EMD, commençons par comprendre ce que nous cherchons à
mesurer. Supposons que nous avons deux distributions de probabilité
discrètes <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace métrique <span
class="math inline">\((X, d)\)</span>. Nous voulons mesurer la quantité
de travail nécessaire pour transformer <span
class="math inline">\(P\)</span> en <span
class="math inline">\(Q\)</span>, où le travail est défini comme la
somme des distances pondérées entre les points déplacés.</p>
<p>Formellement, l’EMD entre <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P = \{ (x_i, p_i)
\}_{i=1}^n\)</span> et <span class="math inline">\(Q = \{ (y_j, q_j)
\}_{j=1}^m\)</span> deux distributions de probabilité discrètes définies
sur un espace métrique <span class="math inline">\((X, d)\)</span>, où
<span class="math inline">\(p_i\)</span> et <span
class="math inline">\(q_j\)</span> sont les masses associées aux points
<span class="math inline">\(x_i\)</span> et <span
class="math inline">\(y_j\)</span> respectivement. L’EMD entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par :</p>
<p><span class="math display">\[\text{EMD}(P, Q) = \min_{\phi: P
\rightarrow Q} \sum_{i=1}^n \sum_{j=1}^m d(x_i, y_j)
\phi(i,j)\]</span></p>
<p>où <span class="math inline">\(\phi\)</span> est une matrice de
transport telle que :</p>
<p><span class="math display">\[\sum_{j=1}^m \phi(i,j) = p_i \quad
\forall i \in \{1, \ldots, n\}\]</span> <span
class="math display">\[\sum_{i=1}^n \phi(i,j) = q_j \quad \forall j \in
\{1, \ldots, m\}\]</span> <span class="math display">\[\phi(i,j) \geq 0
\quad \forall i, j\]</span></p>
<p>En d’autres termes, l’EMD est la solution du problème de transport
optimal qui minimise la somme des distances pondérées entre les points
déplacés.</p>
</div>
<p>Une autre formulation de l’EMD utilise la notion de couplage optimal.
Un couplage <span class="math inline">\(\gamma\)</span> entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est une distribution de probabilité
conjointe sur <span class="math inline">\(X \times X\)</span> dont les
marginales sont <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. L’EMD peut alors être définie comme
:</p>
<p><span class="math display">\[\text{EMD}(P, Q) = \min_{\gamma} \int_{X
\times X} d(x, y) \, d\gamma(x,y)\]</span></p>
<p>où le minimum est pris sur tous les couplages <span
class="math inline">\(\gamma\)</span> entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’EMD est le théorème de
Kantorovich-Rubinstein, qui fournit une caractérisation duale de l’EMD.
Commençons par comprendre ce que nous cherchons à obtenir. Nous voulons
exprimer l’EMD en termes de fonctions Lipschitz.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace métrique <span class="math inline">\((X,
d)\)</span>. L’EMD entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par :</p>
<p><span class="math display">\[\text{EMD}(P, Q) = \sup_{f \in
\text{Lip}_1(X)} \left( \int_X f \, dP - \int_X f \, dQ
\right)\]</span></p>
<p>où <span class="math inline">\(\text{Lip}_1(X)\)</span> est
l’ensemble des fonctions Lipschitz de constante 1, c’est-à-dire les
fonctions <span class="math inline">\(f: X \rightarrow
\mathbb{R}\)</span> telles que :</p>
<p><span class="math display">\[|f(x) - f(y)| \leq d(x, y) \quad \forall
x, y \in X\]</span></p>
</div>
<p>La démonstration de ce théorème repose sur des techniques
d’optimisation et de théorie de la mesure. Elle montre que l’EMD peut
être exprimée en termes de fonctions Lipschitz, ce qui permet de
l’utiliser dans des cadres où les contraintes géométriques sont
importantes.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kantorovich-Rubinstein, nous utilisons
des techniques d’optimisation et de théorie de la mesure. Commençons par
rappeler que l’EMD est définie comme la solution du problème de
transport optimal. Nous voulons montrer qu’elle peut être exprimée en
termes de fonctions Lipschitz.</p>
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace métrique <span class="math inline">\((X,
d)\)</span>. Considérons l’ensemble des fonctions Lipschitz de constante
1. Nous voulons montrer que :</p>
<p><span class="math display">\[\text{EMD}(P, Q) = \sup_{f \in
\text{Lip}_1(X)} \left( \int_X f \, dP - \int_X f \, dQ
\right)\]</span></p>
<p>Pour ce faire, nous utilisons la dualité en optimisation. Le problème
de transport optimal peut être formulé comme un problème d’optimisation
linéaire, et sa solution duale est donnée par l’expression ci-dessus.
Les détails de cette preuve sont complexes et reposent sur des
techniques avancées d’analyse convexe.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’EMD possède plusieurs propriétés importantes qui la rendent utile
dans diverses applications. Nous en listons quelques-unes ci-dessous
:</p>
<ol>
<li><p><strong>Inégalité triangulaire</strong> : L’EMD satisfait
l’inégalité triangulaire. Pour toute distribution de probabilité <span
class="math inline">\(R\)</span> définie sur <span
class="math inline">\((X, d)\)</span>, nous avons :</p>
<p><span class="math display">\[\text{EMD}(P, Q) \leq \text{EMD}(P, R) +
\text{EMD}(R, Q)\]</span></p></li>
<li><p><strong>Continuité</strong> : L’EMD est continue par rapport aux
distributions de probabilité. Plus précisément, si <span
class="math inline">\(P_n\)</span> et <span
class="math inline">\(Q_n\)</span> convergent faiblement vers <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement, alors :</p>
<p><span class="math display">\[\text{EMD}(P_n, Q_n) \rightarrow
\text{EMD}(P, Q)\]</span></p></li>
<li><p><strong>Invariance par translation</strong> : L’EMD est
invariante par translation. Si <span class="math inline">\(T\)</span>
est une transformation isométrique de <span
class="math inline">\(X\)</span>, alors :</p>
<p><span class="math display">\[\text{EMD}(T(P), T(Q)) = \text{EMD}(P,
Q)\]</span></p>
<p>où <span class="math inline">\(T(P)\)</span> et <span
class="math inline">\(T(Q)\)</span> sont les images de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> par la transformation <span
class="math inline">\(T\)</span>.</p></li>
</ol>
<p>La preuve de ces propriétés repose sur des techniques d’analyse et de
théorie de la mesure. Par exemple, l’inégalité triangulaire découle
directement de la définition de l’EMD et des propriétés du transport
optimal. La continuité est une conséquence du théorème de Portmanteau,
qui caractérise la convergence faible des distributions de
probabilité.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Earth Mover’s Distance (EMD) est une mesure de distance puissante
et intuitive entre distributions de probabilité. Son utilisation dans
divers domaines tels que la vision par ordinateur, l’apprentissage
automatique et le traitement du signal témoigne de son importance. Les
théorèmes et propriétés présentés dans cet article montrent que l’EMD
est un outil mathématique rigoureux et flexible, capable de capturer la
structure géométrique des données. Les recherches futures pourraient
explorer davantage ses applications et développer de nouvelles
techniques pour son calcul efficace.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie fractionnelle : une généralisation profonde de l’entropie classique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie fractionnelle : une généralisation profonde
de l’entropie classique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie, concept central en théorie de l’information et en
physique statistique, mesure le degré d’incertitude ou de désordre dans
un système. L’entropie classique, introduite par Shannon et Boltzmann, a
été généralisée de nombreuses façons pour capturer des aspects plus
subtils de l’information et du désordre. Parmi ces généralisations,
l’entropie fractionnelle se distingue par sa capacité à interpoler entre
différentes mesures d’entropie, offrant ainsi une flexibilité et une
richesse conceptuelle remarquables.</p>
<p>L’entropie fractionnelle émerge naturellement dans plusieurs
contextes. En théorie de l’information, elle permet de modéliser des
situations où les probabilités ne suivent pas nécessairement une
distribution exponentielle. En physique statistique, elle est utilisée
pour étudier des systèmes hors d’équilibre ou des transitions de phase.
De plus, elle joue un rôle clé dans l’analyse des systèmes complexes et
des réseaux, où les interactions non linéaires sont prévalentes.</p>
<p>Dans cet article, nous explorons l’entropie fractionnelle en détail.
Nous commençons par définir formellement cette notion, puis nous
discutons ses propriétés fondamentales et ses applications. Nous
présentons également des théorèmes clés qui illustrent la puissance et
l’utilité de cette généralisation.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’entropie fractionnelle, commençons par rappeler la
définition de l’entropie classique. Soit <span
class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots, p_n)\)</span> une
distribution de probabilité sur un ensemble fini <span
class="math inline">\(\{1, 2, \ldots, n\}\)</span>. L’entropie de
Shannon est définie par :</p>
<p><span class="math display">\[H(\mathbf{p}) = -\sum_{i=1}^n p_i \log
p_i\]</span></p>
<p>L’entropie fractionnelle généralise cette notion en introduisant un
paramètre <span class="math inline">\(q\)</span> qui contrôle la
sensibilité de l’entropie aux probabilités faibles. Plus précisément,
pour <span class="math inline">\(q \in (0, 1)\)</span>, l’entropie
fractionnelle est définie par :</p>
<p><span class="math display">\[H_q(\mathbf{p}) = \frac{1}{q(1-q)}
\left( 1 - \sum_{i=1}^n p_i^q \right)\]</span></p>
<p>Cette définition peut être réécrite de plusieurs manières. Par
exemple, en utilisant la fonction <span
class="math inline">\(\Gamma\)</span>, on a :</p>
<p><span class="math display">\[H_q(\mathbf{p}) = \frac{1}{q(1-q)}
\left( 1 - \exp\left(q \log \sum_{i=1}^n p_i^q\right)
\right)\]</span></p>
<p>Une autre formulation, plus symétrique, est :</p>
<p><span class="math display">\[H_q(\mathbf{p}) = \frac{1}{q(1-q)}
\left( 1 - \sum_{i=1}^n p_i^q \right) = \frac{1}{q(1-q)} \left( 1 -
\sum_{i=1}^n p_i^q \right)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’entropie fractionnelle possède plusieurs propriétés intéressantes
qui la rendent utile dans diverses applications. Nous présentons ici
quelques théorèmes clés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots,
p_n)\)</span> une distribution de probabilité. Pour <span
class="math inline">\(q \in (0, 1)\)</span>, l’entropie fractionnelle
<span class="math inline">\(H_q(\mathbf{p})\)</span> est convexe en
<span class="math inline">\(\mathbf{p}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer la convexité de <span
class="math inline">\(H_q(\mathbf{p})\)</span>, nous utilisons le fait
que la fonction <span class="math inline">\(f(x) = x^q\)</span> est
convexe pour <span class="math inline">\(q \in (0, 1)\)</span>. Par
conséquent, la somme <span class="math inline">\(\sum_{i=1}^n
p_i^q\)</span> est convexe en <span
class="math inline">\(\mathbf{p}\)</span>. Puisque la fonction <span
class="math inline">\(g(x) = 1 - x\)</span> est linéaire, elle préserve
la convexité. Enfin, le coefficient <span
class="math inline">\(\frac{1}{q(1-q)}\)</span> est positif pour <span
class="math inline">\(q \in (0, 1)\)</span>, donc il préserve également
la convexité. Ainsi, <span
class="math inline">\(H_q(\mathbf{p})\)</span> est convexe en <span
class="math inline">\(\mathbf{p}\)</span>. ◻</p>
</div>
<div class="theorem">
<p>Pour <span class="math inline">\(q \in (0, 1)\)</span>, on a :</p>
<p><span class="math display">\[\lim_{q \to 1} H_q(\mathbf{p}) =
H(\mathbf{p})\]</span></p>
<p>où <span class="math inline">\(H(\mathbf{p})\)</span> est l’entropie
de Shannon.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer cette limite, nous utilisons la
définition de l’entropie fractionnelle et le développement en série de
Taylor autour de <span class="math inline">\(q = 1\)</span>. En effet,
pour <span class="math inline">\(q\)</span> proche de 1, on a :</p>
<p><span class="math display">\[p_i^q \approx p_i + q p_i \log
p_i\]</span></p>
<p>En substituant cette approximation dans la définition de <span
class="math inline">\(H_q(\mathbf{p})\)</span>, on obtient :</p>
<p><span class="math display">\[H_q(\mathbf{p}) \approx \frac{1}{q(1-q)}
\left( 1 - \sum_{i=1}^n (p_i + q p_i \log p_i) \right)\]</span></p>
<p>En simplifiant, on trouve :</p>
<p><span class="math display">\[H_q(\mathbf{p}) \approx \frac{1}{q(1-q)}
\left( 1 - 1 - q \sum_{i=1}^n p_i \log p_i \right) = -\sum_{i=1}^n p_i
\log p_i\]</span></p>
<p>Ainsi, en prenant la limite <span class="math inline">\(q \to
1\)</span>, on retrouve l’entropie de Shannon. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie fractionnelle possède plusieurs propriétés intéressantes
qui en font un outil puissant pour l’analyse des systèmes complexes.
Nous présentons ici quelques-unes de ces propriétés.</p>
<div class="corollary">
<p>Soient <span class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots,
p_n)\)</span> et <span class="math inline">\(\mathbf{q} = (q_1, q_2,
\ldots, q_m)\)</span> deux distributions de probabilité. Pour <span
class="math inline">\(q \in (0, 1)\)</span>, on a :</p>
<p><span class="math display">\[H_q(\mathbf{p} \otimes \mathbf{q}) \leq
H_q(\mathbf{p}) + H_q(\mathbf{q})\]</span></p>
<p>où <span class="math inline">\(\mathbf{p} \otimes \mathbf{q}\)</span>
est le produit tensoriel des distributions <span
class="math inline">\(\mathbf{p}\)</span> et <span
class="math inline">\(\mathbf{q}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer cette inégalité, nous utilisons la
convexité de l’entropie fractionnelle et le fait que le produit
tensoriel préserve les probabilités. En effet, pour toute distribution
de probabilité <span class="math inline">\(\mathbf{r} = (r_1, r_2,
\ldots, r_{nm})\)</span>, on a :</p>
<p><span class="math display">\[H_q(\mathbf{r}) \leq H_q(\mathbf{p}
\otimes \mathbf{q})\]</span></p>
<p>où <span class="math inline">\(\mathbf{r}\)</span> est une
distribution de probabilité obtenue en permutant les éléments de <span
class="math inline">\(\mathbf{p} \otimes \mathbf{q}\)</span>. En
utilisant la convexité de <span
class="math inline">\(H_q(\mathbf{r})\)</span>, on obtient :</p>
<p><span class="math display">\[H_q(\mathbf{p} \otimes \mathbf{q}) \leq
H_q(\mathbf{p}) + H_q(\mathbf{q})\]</span> ◻</p>
</div>
<div class="corollary">
<p>Soit <span class="math inline">\(\mathbf{p} = (p_1, p_2, \ldots,
p_n)\)</span> une distribution de probabilité et <span
class="math inline">\(f\)</span> une fonction convexe. Pour <span
class="math inline">\(q \in (0, 1)\)</span>, on a :</p>
<p><span class="math display">\[f(H_q(\mathbf{p})) \leq
H_q(f(\mathbf{p}))\]</span></p>
<p>où <span class="math inline">\(f(\mathbf{p}) = (f(p_1), f(p_2),
\ldots, f(p_n))\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour montrer cette inégalité, nous utilisons la
convexité de l’entropie fractionnelle et le fait que <span
class="math inline">\(f\)</span> est convexe. En effet, pour toute
distribution de probabilité <span class="math inline">\(\mathbf{r} =
(r_1, r_2, \ldots, r_n)\)</span>, on a :</p>
<p><span class="math display">\[f(H_q(\mathbf{r})) \leq
H_q(f(\mathbf{r}))\]</span></p>
<p>En appliquant cette inégalité à <span
class="math inline">\(\mathbf{p}\)</span>, on obtient :</p>
<p><span class="math display">\[f(H_q(\mathbf{p})) \leq
H_q(f(\mathbf{p}))\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie fractionnelle est une généralisation puissante et flexible
de l’entropie classique. Elle permet de modéliser des situations où les
probabilités ne suivent pas nécessairement une distribution
exponentielle et offre des outils précieux pour l’analyse des systèmes
complexes. Dans cet article, nous avons présenté les définitions,
théorèmes et propriétés clés de l’entropie fractionnelle. Nous espérons
que cette exploration inspirera de nouvelles recherches et applications
dans divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


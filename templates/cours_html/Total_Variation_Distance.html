{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Total Variation Distance: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Total Variation Distance: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of Total Variation Distance (TVD) emerges as a
fundamental tool in probability theory and statistics, particularly in
the study of convergence of probability measures. Historically, TVD has
been instrumental in quantifying the difference between two probability
distributions, providing a metric that is both intuitive and
mathematically rigorous.</p>
<p>The need for such a distance measure arises in various contexts, such
as hypothesis testing, goodness-of-fit tests, and the analysis of Markov
chains. TVD offers a way to assess how closely one probability
distribution approximates another, which is indispensable in fields like
machine learning, where models often rely on probabilistic
assumptions.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand TVD, let us first consider two probability measures
<span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> on a measurable space <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. We seek a measure
of how much these two measures differ. Intuitively, we want to capture
the maximum difference in the probabilities assigned by <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> to any event.</p>
<p>Formally, the Total Variation Distance between <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) = \sup_{A \in
\mathcal{F}} |P(A) - Q(A)|\]</span></p>
<p>This can be interpreted as the largest possible difference in the
probabilities of any event <span class="math inline">\(A\)</span> under
<span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>.</p>
<p>Alternatively, TVD can be expressed in terms of the integral over all
possible events:</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) = \frac{1}{2}
\int_{\Omega} |dP - dQ|\]</span></p>
<p>Here, <span class="math inline">\(dP\)</span> and <span
class="math inline">\(dQ\)</span> are the differentials of the measures
<span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>, respectively. This formulation
highlights the connection between TVD and the concept of absolute
continuity.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the most significant theorems related to TVD is the Duality
Theorem, which provides an alternative characterization of the distance.
This theorem states that:</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) = \sup_{f: \Omega
\to [a,b], \text{measurable}} \left\{ \int_{\Omega} f \, dP -
\int_{\Omega} f \, dQ \right\}\]</span></p>
<p>where the supremum is taken over all bounded measurable functions
<span class="math inline">\(f\)</span> with <span
class="math inline">\(a \leq f(\omega) \leq b\)</span> for some
constants <span class="math inline">\(a\)</span> and <span
class="math inline">\(b\)</span>.</p>
<p>To prove this theorem, we start by considering the set of all bounded
measurable functions. For any such function <span
class="math inline">\(f\)</span>, we can write:</p>
<p><span class="math display">\[\int_{\Omega} f \, dP - \int_{\Omega} f
\, dQ = \int_{\Omega} f \, d(P - Q)\]</span></p>
<p>Using the properties of integrals with respect to signed measures, we
can decompose <span class="math inline">\(P - Q\)</span> into its
positive and negative parts. This leads to:</p>
<p><span class="math display">\[\int_{\Omega} f \, d(P - Q) = \int_{A^+}
f \, d(P - Q) + \int_{A^-} f \, d(P - Q)\]</span></p>
<p>where <span class="math inline">\(A^+ = \{ \omega \in \Omega :
dP(\omega) &gt; dQ(\omega) \}\)</span> and <span
class="math inline">\(A^- = \{ \omega \in \Omega : dP(\omega) &lt;
dQ(\omega) \}\)</span>.</p>
<p>By choosing <span class="math inline">\(f\)</span> to be the
indicator function of <span class="math inline">\(A^+\)</span>, we
maximize the integral, yielding:</p>
<p><span class="math display">\[\sup_{f} \left\{ \int_{\Omega} f \, dP -
\int_{\Omega} f \, dQ \right\} = P(A^+) - Q(A^+)\]</span></p>
<p>Similarly, choosing <span class="math inline">\(f\)</span> to be the
indicator function of <span class="math inline">\(A^-\)</span>
gives:</p>
<p><span class="math display">\[\sup_{f} \left\{ \int_{\Omega} f \, dP -
\int_{\Omega} f \, dQ \right\} = P(A^-) - Q(A^-)\]</span></p>
<p>Combining these results, we obtain:</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) = \frac{1}{2}
(P(A^+) + P(A^-)) - \frac{1}{2} (Q(A^+) + Q(A^-))\]</span></p>
<p>which simplifies to the original definition of TVD.</p>
<h1 id="proofs">Proofs</h1>
<p>To further elucidate the properties of TVD, let us consider the proof
that <span class="math inline">\(d_{\text{TV}}(P, Q)\)</span> is indeed
a metric. We need to verify the following properties:</p>
<ol>
<li><p>Non-negativity: <span class="math inline">\(d_{\text{TV}}(P, Q)
\geq 0\)</span></p></li>
<li><p>Identity of indiscernibles: <span
class="math inline">\(d_{\text{TV}}(P, Q) = 0\)</span> if and only if
<span class="math inline">\(P = Q\)</span></p></li>
<li><p>Symmetry: <span class="math inline">\(d_{\text{TV}}(P, Q) =
d_{\text{TV}}(Q, P)\)</span></p></li>
<li><p>Triangle inequality: <span class="math inline">\(d_{\text{TV}}(P,
R) \leq d_{\text{TV}}(P, Q) + d_{\text{TV}}(Q, R)\)</span></p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ul>
<li><p>Non-negativity follows directly from the definition, as <span
class="math inline">\(|P(A) - Q(A)| \geq 0\)</span> for any event <span
class="math inline">\(A\)</span>.</p></li>
<li><p>If <span class="math inline">\(d_{\text{TV}}(P, Q) = 0\)</span>,
then <span class="math inline">\(|P(A) - Q(A)| = 0\)</span> for all
<span class="math inline">\(A \in \mathcal{F}\)</span>, which implies
<span class="math inline">\(P = Q\)</span>. Conversely, if <span
class="math inline">\(P = Q\)</span>, then clearly <span
class="math inline">\(d_{\text{TV}}(P, Q) = 0\)</span>.</p></li>
<li><p>Symmetry is evident from the definition, as <span
class="math inline">\(|P(A) - Q(A)| = |Q(A) - P(A)|\)</span>.</p></li>
<li><p>To prove the triangle inequality, consider three probability
measures <span class="math inline">\(P\)</span>, <span
class="math inline">\(Q\)</span>, and <span
class="math inline">\(R\)</span>. We have:</p>
<p><span class="math display">\[d_{\text{TV}}(P, R) = \sup_{A} |P(A) -
R(A)| = \sup_{A} |(P(A) - Q(A)) + (Q(A) - R(A))|\]</span></p>
<p>Using the triangle inequality for absolute values, we get:</p>
<p><span class="math display">\[d_{\text{TV}}(P, R) \leq \sup_{A} |P(A)
- Q(A)| + \sup_{A} |Q(A) - R(A)| = d_{\text{TV}}(P, Q) +
d_{\text{TV}}(Q, R)\]</span></p>
<p>This completes the proof.</p></li>
</ul>
<p> ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Total Variation Distance possesses several important properties
and corollaries that enhance its utility in various applications.</p>
<ol>
<li><p><strong>Boundedness</strong>: For any two probability measures
<span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>, the TVD is bounded by 1. That is,</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) \leq 1\]</span></p>
<p>This follows from the fact that <span
class="math inline">\(P(A)\)</span> and <span
class="math inline">\(Q(A)\)</span> are both probabilities, hence
bounded between 0 and 1.</p></li>
<li><p><strong>Convergence in TVD implies convergence in
distribution</strong>: If a sequence of probability measures <span
class="math inline">\(P_n\)</span> converges to <span
class="math inline">\(P\)</span> in TVD, then it also converges to <span
class="math inline">\(P\)</span> in distribution. This is a consequence
of the fact that TVD is a stronger notion of convergence than
convergence in distribution.</p></li>
<li><p><strong>Relation to Kullback-Leibler Divergence</strong>: For
discrete probability distributions, the TVD can be related to the
Kullback-Leibler (KL) divergence. Specifically, for two discrete
distributions <span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>, we have:</p>
<p><span class="math display">\[d_{\text{TV}}(P, Q) \leq
\sqrt{\frac{1}{2} D_{\text{KL}}(P \| Q)}\]</span></p>
<p>where <span class="math inline">\(D_{\text{KL}}(P \| Q)\)</span> is
the KL divergence between <span class="math inline">\(P\)</span> and
<span class="math inline">\(Q\)</span>. This inequality highlights the
connection between TVD and other measures of divergence.</p></li>
</ol>
<p>In conclusion, the Total Variation Distance is a powerful and
versatile tool in probability theory, with applications ranging from
statistical inference to machine learning. Its rigorous mathematical
foundation and intuitive interpretation make it indispensable in the
study of probability measures and their convergence.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de vraisemblance</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de vraisemblance</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de vraisemblance est un concept fondamental en
statistique mathématique, particulièrement dans le cadre des tests
d’hypothèses et de l’estimation paramétrique. Elle émerge comme une
généralisation des tests du rapport de vraisemblance, permettant
d’évaluer la distance entre deux modèles statistiques. Son importance
réside dans sa capacité à fournir un cadre rigoureux pour comparer des
hypothèses rivales, en s’appuyant sur des principes informationnels
solides.</p>
<p>Historiquement, la divergence de vraisemblance trouve ses racines
dans les travaux pionniers de Neyman et Pearson sur les tests
statistiques. Cependant, c’est avec le développement des méthodes
bayésiennes et l’introduction de la divergence de Kullback-Leibler que
ce concept a pris toute son ampleur. Aujourd’hui, il joue un rôle clé
dans l’apprentissage automatique, la théorie de l’information et les
sciences des données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de vraisemblance, commençons par
comprendre ce que nous cherchons à mesurer. Supposons que nous ayons
deux modèles statistiques, <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, représentant des distributions
de probabilité. Nous voulons quantifier à quel point ces deux modèles
sont différents.</p>
<p>Formellement, la divergence de vraisemblance entre deux distributions
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La divergence de vraisemblance entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_{\text{V}}(P \| Q) =
\mathbb{E}_P\left[\log\left(\frac{dP}{dQ}\right)\right],\]</span> où
<span class="math inline">\(\mathbb{E}_P\)</span> désigne l’espérance
sous la distribution <span class="math inline">\(P\)</span>, et <span
class="math inline">\(\frac{dP}{dQ}\)</span> est la dérivée de
Radon-Nikodym de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D_{\text{V}}(P \| Q) = \int_{\Omega}
\log\left(\frac{dP}{dQ}\right) dP.\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème central lié à la divergence de vraisemblance est le
théorème de l’information de Kullback-Leibler, qui établit une relation
fondamentale entre la divergence de vraisemblance et l’entropie
relative.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Alors : <span
class="math display">\[D_{\text{V}}(P \| Q) = H(P, Q) - H(P),\]</span>
où <span class="math inline">\(H(P, Q)\)</span> est l’entropie croisée
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définie par : <span
class="math display">\[H(P, Q) =
-\mathbb{E}_P\left[\log(Q)\right],\]</span> et <span
class="math inline">\(H(P)\)</span> est l’entropie de <span
class="math inline">\(P\)</span>, définie par : <span
class="math display">\[H(P) =
-\mathbb{E}_P\left[\log(P)\right].\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kullback-Leibler, commençons par rappeler
la définition de l’entropie croisée et de l’entropie.</p>
<div class="proof">
<p><em>Proof.</em> Par définition, nous avons : <span
class="math display">\[H(P, Q) = -\mathbb{E}_P\left[\log(Q)\right] =
-\int_{\Omega} \log(Q) dP.\]</span> De même, l’entropie de <span
class="math inline">\(P\)</span> est : <span class="math display">\[H(P)
= -\mathbb{E}_P\left[\log(P)\right] = -\int_{\Omega} \log(P)
dP.\]</span> En utilisant la relation entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous pouvons écrire : <span
class="math display">\[\log\left(\frac{dP}{dQ}\right) = \log(P) -
\log(Q).\]</span> En prenant l’espérance sous <span
class="math inline">\(P\)</span>, nous obtenons : <span
class="math display">\[\mathbb{E}_P\left[\log\left(\frac{dP}{dQ}\right)\right]
= \mathbb{E}_P[\log(P)] - \mathbb{E}_P[\log(Q)].\]</span> En remplaçant
par les définitions de <span class="math inline">\(H(P)\)</span> et
<span class="math inline">\(H(P, Q)\)</span>, nous avons : <span
class="math display">\[D_{\text{V}}(P \| Q) = H(P, Q) - H(P).\]</span>
Ce qui achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de vraisemblance possède plusieurs propriétés
intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p>**Non-négativité** : La divergence de vraisemblance est toujours
non négative, c’est-à-dire : <span class="math display">\[D_{\text{V}}(P
\| Q) \geq 0.\]</span> De plus, <span
class="math inline">\(D_{\text{V}}(P \| Q) = 0\)</span> si et seulement
si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p>**Inégalité de Gibbs** : Pour toute distribution <span
class="math inline">\(P\)</span>, nous avons : <span
class="math display">\[D_{\text{V}}(P \| Q) \geq
-\log\left(\int_{\Omega} e^{\log(P)} dQ\right).\]</span> Cette inégalité
est une conséquence directe de l’inégalité de Jensen.</p></li>
<li><p>**Invariance par transformation** : La divergence de
vraisemblance est invariante sous les transformations mesurables. Plus
précisément, si <span class="math inline">\(T\)</span> est une
transformation mesurable de <span class="math inline">\(\Omega\)</span>
dans un autre espace mesurable <span class="math inline">\((\Omega&#39;,
\mathcal{F}&#39;)\)</span>, alors : <span
class="math display">\[D_{\text{V}}(P \| Q) = D_{\text{V}}(T(P) \|
T(Q)),\]</span> où <span class="math inline">\(T(P)\)</span> et <span
class="math inline">\(T(Q)\)</span> sont les distributions images de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sous <span
class="math inline">\(T\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de vraisemblance est un outil puissant pour comparer
des distributions de probabilité. Ses applications vont bien au-delà des
tests statistiques, touchant des domaines aussi variés que
l’apprentissage automatique et la théorie de l’information. En
comprenant ses propriétés fondamentales, nous pouvons mieux appréhender
les différences entre modèles et faire des choix éclairés dans l’analyse
des données.</p>
</body>
</html>
{% include "footer.html" %}


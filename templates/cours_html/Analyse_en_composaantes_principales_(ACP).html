{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Analyse en Composantes Principales (ACP) : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analyse en Composantes Principales (ACP) : Une
Exploration Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Analyse en Composantes Principales (ACP) émerge au début du XXe
siècle, principalement grâce aux travaux de Karl Pearson et Harold
Hotelling. Cette technique statistique multivariée vise à réduire la
dimension d’un ensemble de données tout en préservant au maximum
l’information contenue dans celles-ci. L’ACP est indispensable dans de
nombreux domaines, tels que la bioinformatique, l’économie, et
l’apprentissage automatique, où la gestion de grandes quantités de
données est cruciale.</p>
<p>L’idée fondamentale derrière l’ACP est de projeter les données sur un
sous-espace de dimension inférieure, tout en minimisant la perte
d’information. Cette projection est effectuée en identifiant les
directions (ou composantes) le long desquelles la variance des données
est maximisée. Ainsi, l’ACP permet de visualiser et d’interpréter des
structures complexes dans des données de haute dimension.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’ACP, considérons un ensemble de données <span
class="math inline">\(\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots,
\mathbf{x}_n\}\)</span> où chaque <span
class="math inline">\(\mathbf{x}_i \in \mathbb{R}^p\)</span> représente
un vecteur de caractéristiques. L’objectif est de trouver une
transformation linéaire qui projette ces données dans un sous-espace de
dimension <span class="math inline">\(k &lt; p\)</span> tout en
préservant la variance maximale.</p>
<div class="definition">
<p>La matrice de covariance <span
class="math inline">\(\mathbf{C}\)</span> d’un ensemble de données
centré <span class="math inline">\(\mathbf{X}\)</span> est définie comme
: <span class="math display">\[\mathbf{C} = \frac{1}{n-1} \sum_{i=1}^n
(\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i -
\bar{\mathbf{x}})^T\]</span> où <span
class="math inline">\(\bar{\mathbf{x}}\)</span> est le vecteur moyen des
données.</p>
</div>
<div class="definition">
<p>Les valeurs propres <span class="math inline">\(\lambda_1, \lambda_2,
\dots, \lambda_p\)</span> et les vecteurs propres <span
class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, \dots,
\mathbf{v}_p\)</span> de la matrice de covariance <span
class="math inline">\(\mathbf{C}\)</span> sont définis par : <span
class="math display">\[\mathbf{C} \mathbf{v}_i = \lambda_i \mathbf{v}_i,
\quad i = 1, 2, \dots, p\]</span> Les vecteurs propres sont ordonnés de
telle sorte que <span class="math inline">\(\lambda_1 \geq \lambda_2
\geq \dots \geq \lambda_p \geq 0\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{C}\)</span> une matrice de
covariance symétrique et définie positive. Il existe une décomposition
spectrale de <span class="math inline">\(\mathbf{C}\)</span> telle que :
<span class="math display">\[\mathbf{C} = \mathbf{V} \mathbf{\Lambda}
\mathbf{V}^T\]</span> où <span class="math inline">\(\mathbf{V}\)</span>
est la matrice des vecteurs propres normalisés et <span
class="math inline">\(\mathbf{\Lambda}\)</span> est la matrice diagonale
des valeurs propres.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La décomposition spectrale découle du fait que <span
class="math inline">\(\mathbf{C}\)</span> est symétrique et définie
positive. Les valeurs propres sont réelles et positives, et les vecteurs
propres forment une base orthonormée de <span
class="math inline">\(\mathbb{R}^p\)</span>. La matrice <span
class="math inline">\(\mathbf{V}\)</span> est donc orthogonale, et <span
class="math inline">\(\mathbf{\Lambda}\)</span> est diagonale avec les
valeurs propres sur la diagonale. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver que l’ACP minimise la perte d’information, nous devons
montrer que les composantes principales capturent la variance maximale
des données. Considérons la projection des données sur un vecteur <span
class="math inline">\(\mathbf{u} \in \mathbb{R}^p\)</span> : <span
class="math display">\[\text{Var}(\mathbf{X}^T \mathbf{u}) =
\mathbf{u}^T \mathbf{C} \mathbf{u}\]</span> Pour maximiser cette
variance, nous devons résoudre le problème d’optimisation : <span
class="math display">\[\max_{\mathbf{u}} \mathbf{u}^T \mathbf{C}
\mathbf{u} \quad \text{sous la contrainte} \quad \mathbf{u}^T \mathbf{u}
= 1\]</span> En utilisant le théorème des multiplicateurs de Lagrange,
nous trouvons que la solution est donnée par le vecteur propre
correspondant à la plus grande valeur propre de <span
class="math inline">\(\mathbf{C}\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<div class="corollary">
<p>La projection des données <span
class="math inline">\(\mathbf{X}\)</span> sur les <span
class="math inline">\(k\)</span> premières composantes principales est
donnée par : <span class="math display">\[\mathbf{Y} = \mathbf{X}
\mathbf{V}_k\]</span> où <span
class="math inline">\(\mathbf{V}_k\)</span> est la matrice des <span
class="math inline">\(k\)</span> premiers vecteurs propres de <span
class="math inline">\(\mathbf{C}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La projection est obtenue en multipliant les données
par la matrice des vecteurs propres correspondants aux <span
class="math inline">\(k\)</span> plus grandes valeurs propres. Cette
projection minimise la perte d’information car elle capture la variance
maximale des données. ◻</p>
</div>
<div class="corollary">
<p>La variance expliquée par les <span class="math inline">\(k\)</span>
premières composantes principales est donnée par : <span
class="math display">\[\text{Variance expliquée} = \frac{\sum_{i=1}^k
\lambda_i}{\sum_{i=1}^p \lambda_i}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La variance expliquée est le rapport de la somme des
<span class="math inline">\(k\)</span> plus grandes valeurs propres à la
somme de toutes les valeurs propres. Cela mesure la proportion de la
variance totale des données qui est capturée par les <span
class="math inline">\(k\)</span> premières composantes
principales. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Analyse en Composantes Principales est une technique puissante pour
la réduction de dimension et l’exploration des données. En identifiant
les directions de variance maximale, l’ACP permet de visualiser et
d’interpréter des structures complexes dans des ensembles de données de
haute dimension. Les propriétés mathématiques sous-jacentes à l’ACP,
telles que la décomposition spectrale et les projections sur les
vecteurs propres, fournissent une base solide pour son application dans
divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


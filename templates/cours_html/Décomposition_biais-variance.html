{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Décomposition biais-variance : Fondements théoriques et implications pratiques</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Décomposition biais-variance : Fondements théoriques
et implications pratiques</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage statistique est un domaine central en intelligence
artificielle et en science des données. Parmi les concepts fondamentaux,
la décomposition biais-variance occupe une place prépondérante. Cette
notion permet de quantifier et d’analyser les erreurs commises par un
modèle prédictif, offrant ainsi des outils théoriques pour améliorer ses
performances.</p>
<p>Historiquement, l’idée de décomposer l’erreur d’un modèle en biais et
variance remonte aux travaux de Geman, Bienenstock et Doursat dans les
années 1990. Cette décomposition est indispensable pour comprendre les
compromis inhérents à la conception de modèles : un modèle trop simple
aura un biais élevé, tandis qu’un modèle trop complexe sera sujet à une
variance élevée. La décomposition biais-variance fournit donc un cadre
théorique pour optimiser ces deux composantes et atteindre une erreur
minimale.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de formaliser la décomposition biais-variance, il est essentiel
de comprendre les concepts sous-jacents. Considérons un problème de
régression où l’on cherche à prédire une variable cible <span
class="math inline">\(Y\)</span> à partir d’un vecteur de
caractéristiques <span class="math inline">\(X\)</span>.</p>
<div class="definition">
<p>Le biais mesure l’erreur systématique d’un modèle. Il quantifie la
différence entre le prédicteur moyen du modèle et la véritable fonction
cible.</p>
<p>Formellement, pour une fonction de régression <span
class="math inline">\(f\)</span> et une distribution conjointe <span
class="math inline">\((X,Y)\)</span>, le biais est défini comme : <span
class="math display">\[\text{Biais}(f) = \mathbb{E}[Y - f(X)]^2\]</span>
où <span class="math inline">\(\mathbb{E}\)</span> désigne l’espérance
mathématique.</p>
<p>Une autre formulation, plus précise, est : <span
class="math display">\[\text{Biais}(f) = \left( \mathbb{E}[f(X)] -
f^*(X) \right)^2\]</span> où <span class="math inline">\(f^*\)</span>
est la véritable fonction de régression.</p>
</div>
<div class="definition">
<p>La variance mesure la sensibilité du modèle aux fluctuations des
données d’entraînement. Elle quantifie la dispersion des prédictions
autour de leur moyenne.</p>
<p>Formellement, pour une fonction de régression <span
class="math inline">\(f\)</span> et une distribution conjointe <span
class="math inline">\((X,Y)\)</span>, la variance est définie comme :
<span class="math display">\[\text{Variance}(f) = \mathbb{E}\left[
\left( f(X) - \mathbb{E}[f(X)] \right)^2 \right]\]</span></p>
</div>
<h1 id="décomposition-biais-variance">Décomposition biais-variance</h1>
<p>La décomposition biais-variance permet d’exprimer l’erreur
quadratique moyenne (MSE) d’un modèle en fonction de son biais et de sa
variance.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> un modèle de régression
et <span class="math inline">\(f^*\)</span> la véritable fonction de
régression. L’erreur quadratique moyenne (MSE) peut être décomposée
comme suit : <span class="math display">\[\mathbb{E}[(Y - f(X))^2] =
\text{Biais}(f)^2 + \text{Variance}(f) + \sigma^2\]</span> où <span
class="math inline">\(\sigma^2\)</span> est le bruit intrinsèque des
données, défini par : <span class="math display">\[\sigma^2 =
\mathbb{E}\left[ (Y - f^*(X))^2 \right]\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur des manipulations
algébriques et l’utilisation de propriétés des espérances
mathématiques.</p>
<p>Commençons par développer l’erreur quadratique moyenne : <span
class="math display">\[\mathbb{E}[(Y - f(X))^2] = \mathbb{E}\left[ (Y -
f^*(X) + f^*(X) - f(X))^2 \right]\]</span> En utilisant l’identité <span
class="math inline">\((a + b)^2 = a^2 + 2ab + b^2\)</span>, nous
obtenons : <span class="math display">\[\mathbb{E}[(Y - f(X))^2] =
\mathbb{E}\left[ (Y - f^*(X))^2 \right] + 2\mathbb{E}\left[ (Y -
f^*(X))(f^*(X) - f(X)) \right] + \mathbb{E}\left[ (f^*(X) - f(X))^2
\right]\]</span></p>
<p>Le terme <span class="math inline">\(2\mathbb{E}\left[ (Y -
f^*(X))(f^*(X) - f(X)) \right]\)</span> est nul car <span
class="math inline">\(Y - f^*(X)\)</span> et <span
class="math inline">\(f(X)\)</span> sont indépendants par définition de
<span class="math inline">\(f^*\)</span>. Il reste donc : <span
class="math display">\[\mathbb{E}[(Y - f(X))^2] = \mathbb{E}\left[ (Y -
f^*(X))^2 \right] + \mathbb{E}\left[ (f^*(X) - f(X))^2
\right]\]</span></p>
<p>En utilisant à nouveau l’identité <span class="math inline">\((a +
b)^2 = a^2 + 2ab + b^2\)</span>, nous pouvons décomposer le deuxième
terme : <span class="math display">\[\mathbb{E}\left[ (f^*(X) - f(X))^2
\right] = \mathbb{E}\left[ (f^*(X) - \mathbb{E}[f(X)])^2 +
(\mathbb{E}[f(X)] - f(X))^2 \right]\]</span></p>
<p>En séparant les termes, nous obtenons : <span
class="math display">\[\mathbb{E}\left[ (f^*(X) - f(X))^2 \right] =
\mathbb{E}\left[ (f^*(X) - \mathbb{E}[f(X)])^2 \right] +
\mathbb{E}\left[ (\mathbb{E}[f(X)] - f(X))^2 \right]\]</span></p>
<p>Le premier terme correspond au biais et le deuxième à la variance.
Ainsi, nous avons : <span class="math display">\[\mathbb{E}[(Y -
f(X))^2] = \mathbb{E}\left[ (Y - f^*(X))^2 \right] + \left(
\mathbb{E}[f(X)] - f^*(X) \right)^2 + \mathbb{E}\left[ (f(X) -
\mathbb{E}[f(X)])^2 \right]\]</span></p>
<p>En notant <span class="math inline">\(\sigma^2 = \mathbb{E}\left[ (Y
- f^*(X))^2 \right]\)</span>, nous obtenons finalement la décomposition
souhaitée : <span class="math display">\[\mathbb{E}[(Y - f(X))^2] =
\text{Biais}(f)^2 + \text{Variance}(f) + \sigma^2\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La décomposition biais-variance permet de tirer plusieurs conclusions
importantes sur le comportement des modèles de régression.</p>
<div class="corollaire">
<p>Pour un modèle de régression donné, il existe un compromis entre le
biais et la variance. Un modèle trop simple aura un biais élevé mais une
variance faible, tandis qu’un modèle trop complexe aura un biais faible
mais une variance élevée.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce compromis découle directement de la décomposition
biais-variance. En effet, pour un modèle donné, l’erreur quadratique
moyenne est la somme du biais et de la variance. Ainsi, réduire le biais
implique généralement d’augmenter la variance et vice versa. ◻</p>
</div>
<div class="corollaire">
<p>Le modèle optimal minimise l’erreur quadratique moyenne en
équilibrant le biais et la variance. Ce modèle est obtenu en choisissant
une complexité appropriée, ni trop simple ni trop complexe.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’optimalité du modèle découle de l’analyse du
compromis biais-variance. En effet, pour un modèle donné, l’erreur
quadratique moyenne est minimisée lorsque le biais et la variance sont
équilibrés. Cela implique de choisir une complexité de modèle
appropriée, qui dépend des données et du problème considéré. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La décomposition biais-variance est un outil fondamental pour
l’analyse et l’optimisation des modèles de régression. En quantifiant
les erreurs systématiques (biais) et les erreurs aléatoires (variance),
cette décomposition permet de comprendre les compromis inhérents à la
conception des modèles et d’atteindre une erreur minimale.</p>
<p>Les travaux de Geman, Bienenstock et Doursat ont jeté les bases de
cette théorie, qui continue d’inspirer des recherches en apprentissage
statistique et en intelligence artificielle. En maîtrisant la
décomposition biais-variance, les chercheurs et praticiens peuvent
concevoir des modèles plus robustes et performants, adaptés à une
variété de problèmes réels.</p>
</body>
</html>
{% include "footer.html" %}


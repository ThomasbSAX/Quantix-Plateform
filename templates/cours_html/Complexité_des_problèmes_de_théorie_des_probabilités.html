{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Complexité des problèmes de théorie des probabilités</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Complexité des problèmes de théorie des
probabilités</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La théorie des probabilités, née au XVIIe siècle avec les travaux de
Blaise Pascal et Pierre de Fermat sur le problème des partis, a évolué
pour devenir un pilier fondamental des mathématiques modernes. L’étude
de la complexité des problèmes probabilistes émerge comme une nécessité
pour comprendre les limites algorithmiques et computationnelles de cette
théorie.</p>
<p>Pourquoi ce sujet est-il indispensable ? D’une part, il permet de
classer les problèmes en fonction de leur difficulté algorithmique.
D’autre part, il offre des outils pour évaluer l’efficacité des
algorithmes existants et pour en concevoir de nouveaux. Dans un monde où
les données sont omniprésentes, la compréhension de la complexité des
problèmes probabilistes est cruciale pour le développement de
l’intelligence artificielle, de la finance quantitative et des sciences
des données.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’aborder les théorèmes, il est essentiel de définir certains
concepts clés. Commençons par la notion de problème décisionnel.</p>
<h2 id="problème-décisionnel">Problème Décisionnel</h2>
<p>Imaginons que nous voulions savoir si un événement particulier a une
probabilité supérieure à un certain seuil. Comment pouvons-nous
déterminer cela de manière algorithmique ? La réponse réside dans la
définition formelle d’un problème décisionnel.</p>
<div class="definition">
<p>Un problème décisionnel est un problème pour lequel la réponse
attendue est soit "oui", soit "non". Formellement, un problème
décisionnel <span class="math inline">\(L\)</span> est une partie d’un
ensemble <span class="math inline">\(\Sigma^*\)</span>, où <span
class="math inline">\(\Sigma\)</span> est un alphabet fini. Pour tout
<span class="math inline">\(x \in \Sigma^*\)</span>, la réponse à <span
class="math inline">\(L\)</span> est "oui" si <span
class="math inline">\(x \in L\)</span> et "non" sinon.</p>
</div>
<p>En théorie des probabilités, un problème décisionnel pourrait être de
déterminer si la probabilité d’un événement <span
class="math inline">\(A\)</span> est supérieure à un certain seuil <span
class="math inline">\(\alpha \in [0,1]\)</span>. Formellement, cela peut
être écrit comme :</p>
<p><span class="math display">\[L = \{ (A, \alpha) \mid P(A) &gt; \alpha
\}\]</span></p>
<h2 id="complexité-de-temps">Complexité de Temps</h2>
<p>La complexité de temps d’un algorithme mesure le nombre d’opérations
élémentaires nécessaires pour résoudre un problème en fonction de la
taille de l’entrée. Dans le contexte des problèmes probabilistes, cette
notion est cruciale pour évaluer l’efficacité des algorithmes.</p>
<div class="definition">
<p>Soit <span class="math inline">\(A\)</span> un algorithme qui résout
un problème décisionnel <span class="math inline">\(L\)</span>. La
complexité de temps de <span class="math inline">\(A\)</span> est une
fonction <span class="math inline">\(T: \mathbb{N} \rightarrow
\mathbb{R}^+\)</span> telle que pour toute entrée <span
class="math inline">\(x\)</span> de taille <span
class="math inline">\(n\)</span>, le nombre d’opérations élémentaires
effectuées par <span class="math inline">\(A\)</span> sur <span
class="math inline">\(x\)</span> est au plus <span
class="math inline">\(T(n)\)</span>.</p>
</div>
<p>Formellement, pour un algorithme <span
class="math inline">\(A\)</span> et une entrée <span
class="math inline">\(x\)</span>, la complexité de temps peut être
exprimée comme :</p>
<p><span class="math display">\[T_A(x) = \max \{ t \mid A \text{
effectue } t \text{ opérations sur } x \}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-cook-levin">Théorème de Cook-Levin</h2>
<p>Le théorème de Cook-Levin est un résultat fondamental en théorie de
la complexité algorithmique. Il établit que le problème de satisfaction
booléenne (SAT) est NP-complet, ce qui signifie qu’il est aussi
difficile que n’importe quel problème dans la classe NP.</p>
<div class="theorem">
<p>Le problème de satisfaction booléenne (SAT) est NP-complet.</p>
</div>
<p>Pour comprendre ce théorème, considérons un problème décisionnel
<span class="math inline">\(L\)</span>. Le théorème de Cook-Levin montre
que si nous pouvons réduire <span class="math inline">\(L\)</span> à SAT
en temps polynomial, alors <span class="math inline">\(L\)</span> est
dans la classe NP.</p>
<div class="proof">
<p><em>Proof.</em> La preuve du théorème de Cook-Levin repose sur la
construction d’une réduction polynomiale de tout problème dans NP à SAT.
Soit <span class="math inline">\(L\)</span> un problème dans NP. Par
définition, il existe une relation polynomiale <span
class="math inline">\(R_L\)</span> telle que pour tout <span
class="math inline">\(x\)</span>, il existe un certificat <span
class="math inline">\(y\)</span> de taille polynomiale en <span
class="math inline">\(|x|\)</span> tel que <span
class="math inline">\((x, y) \in R_L\)</span> si et seulement si <span
class="math inline">\(x \in L\)</span>.</p>
<p>Nous pouvons alors construire une formule booléenne <span
class="math inline">\(\phi_{x,y}\)</span> telle que <span
class="math inline">\(\phi_{x,y}\)</span> est satisfaisable si et
seulement si <span class="math inline">\((x, y) \in R_L\)</span>. En
utilisant cette construction, nous pouvons réduire <span
class="math inline">\(L\)</span> à SAT en temps polynomial. ◻</p>
</div>
<h2 id="théorème-de-lanti-concentration">Théorème de
l’Anti-Concentration</h2>
<p>Le théorème de l’anti-concentration est un résultat important en
théorie des probabilités. Il fournit une borne inférieure sur la
probabilité qu’une variable aléatoire prenne une valeur
particulière.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
réelle. Pour tout <span class="math inline">\(\alpha &gt; 0\)</span>,
nous avons :</p>
<p><span class="math display">\[P(|X - \mathbb{E}[X]| \geq \alpha) \leq
\frac{\text{Var}(X)}{\alpha^2}\]</span></p>
</div>
<p>Ce théorème est une conséquence directe de l’inégalité de Markov et
de la définition de la variance. Il montre que la probabilité que <span
class="math inline">\(X\)</span> s’écarte significativement de son
espérance est contrôlée par sa variance.</p>
<div class="proof">
<p><em>Proof.</em> Par l’inégalité de Markov, nous avons :</p>
<p><span class="math display">\[P(|X - \mathbb{E}[X]| \geq \alpha) \leq
\frac{\mathbb{E}[(X - \mathbb{E}[X])^2]}{\alpha^2}\]</span></p>
<p>Or, <span class="math inline">\(\mathbb{E}[(X - \mathbb{E}[X])^2] =
\text{Var}(X)\)</span>, donc :</p>
<p><span class="math display">\[P(|X - \mathbb{E}[X]| \geq \alpha) \leq
\frac{\text{Var}(X)}{\alpha^2}\]</span></p>
<p>Ce qui achève la preuve. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-lanti-concentration">Preuve du Théorème de
l’Anti-Concentration</h2>
<p>Pour prouver le théorème de l’anti-concentration, nous avons utilisé
l’inégalité de Markov. Cette inégalité est un outil fondamental en
théorie des probabilités, qui permet de borner la probabilité qu’une
variable aléatoire positive prenne une valeur supérieure à un certain
seuil.</p>
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
réelle et <span class="math inline">\(\alpha &gt; 0\)</span>. Par
l’inégalité de Markov, nous avons :</p>
<p><span class="math display">\[P(X \geq \alpha) \leq
\frac{\mathbb{E}[X]}{\alpha}\]</span></p>
<p>En appliquant cette inégalité à la variable aléatoire <span
class="math inline">\((X - \mathbb{E}[X])^2\)</span>, nous obtenons
:</p>
<p><span class="math display">\[P((X - \mathbb{E}[X])^2 \geq \alpha^2)
\leq \frac{\mathbb{E}[(X - \mathbb{E}[X])^2]}{\alpha^2}\]</span></p>
<p>Or, <span class="math inline">\(\mathbb{E}[(X - \mathbb{E}[X])^2] =
\text{Var}(X)\)</span>, donc :</p>
<p><span class="math display">\[P(|X - \mathbb{E}[X]| \geq \alpha) \leq
\frac{\text{Var}(X)}{\alpha^2}\]</span></p>
<p>Ce qui achève la preuve du théorème de l’anti-concentration.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-la-variance">Propriété de la Variance</h2>
<p>La variance d’une variable aléatoire est une mesure de sa dispersion
autour de son espérance. Elle joue un rôle crucial dans le théorème de
l’anti-concentration.</p>
<div class="property">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
réelle. La variance de <span class="math inline">\(X\)</span> est
définie par :</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[(X -
\mathbb{E}[X])^2]\]</span></p>
<p>Cette propriété montre que la variance est toujours non négative et
qu’elle mesure la dispersion de <span class="math inline">\(X\)</span>
autour de son espérance.</p>
</div>
<h2 id="corollaire-de-linégalité-de-chebyshev">Corollaire de l’Inégalité
de Chebyshev</h2>
<p>L’inégalité de Chebyshev est une généralisation de l’inégalité de
Markov. Elle fournit une borne supérieure sur la probabilité qu’une
variable aléatoire s’écarte significativement de son espérance.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
réelle. Pour tout <span class="math inline">\(\alpha &gt; 0\)</span>,
nous avons :</p>
<p><span class="math display">\[P(|X - \mathbb{E}[X]| \geq k \sigma)
\leq \frac{1}{k^2}\]</span></p>
<p>où <span class="math inline">\(\sigma\)</span> est l’écart-type de
<span class="math inline">\(X\)</span>, défini par <span
class="math inline">\(\sigma = \sqrt{\text{Var}(X)}\)</span>.</p>
</div>
<p>Ce corollaire est une conséquence directe du théorème de
l’anti-concentration. Il montre que la probabilité que <span
class="math inline">\(X\)</span> s’écarte de plus de <span
class="math inline">\(k\)</span> écarts-types de son espérance est au
plus <span class="math inline">\(\frac{1}{k^2}\)</span>.</p>
</body>
</html>
{% include "footer.html" %}


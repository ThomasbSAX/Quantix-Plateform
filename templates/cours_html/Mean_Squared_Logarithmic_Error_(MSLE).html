{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Mean Squared Logarithmic Error (MSLE) : Une Mesure Robuste pour les Séries Temporelles</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mean Squared Logarithmic Error (MSLE) : Une Mesure
Robuste pour les Séries Temporelles</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’erreur quadratique logarithmique moyenne (MSLE pour Mean Squared
Logarithmic Error) est une métrique d’évaluation qui a gagné en
popularité dans le domaine de l’apprentissage automatique,
particulièrement pour les modèles prédictifs appliqués aux séries
temporelles. Son émergence est motivée par le besoin de robustesse face
aux variations d’échelle et aux valeurs aberrantes, qui sont fréquentes
dans les données réelles.</p>
<p>Le MSLE est particulièrement utile lorsque les valeurs à prédire
varient sur plusieurs ordres de grandeur. En effet, une erreur absolue
ou quadratique classique peut être fortement biaisée par des valeurs
extrêmes. Le logarithme, en réduisant l’impact des grandes valeurs,
permet une évaluation plus équilibrée de la performance du modèle.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le MSLE, commençons par rappeler ce que nous
cherchons à mesurer. Supposons que nous ayons un ensemble de données
composé de couples <span class="math inline">\((y_i,
\hat{y}_i)\)</span>, où <span class="math inline">\(y_i\)</span> est la
valeur réelle et <span class="math inline">\(\hat{y}_i\)</span> la
valeur prédite par le modèle. Nous voulons quantifier l’erreur entre ces
deux valeurs, mais de manière à ce que les grandes erreurs ne dominent
pas complètement la métrique.</p>
<p>Nous cherchons donc une fonction qui :</p>
<ul>
<li><p>soit sensible aux erreurs, mais de manière logarithmique pour
atténuer l’impact des grandes valeurs ;</p></li>
<li><p>soit facile à interpréter et à calculer.</p></li>
</ul>
<p>La définition formelle du MSLE est la suivante :</p>
<div class="definition">
<p>Soit <span class="math inline">\(n\)</span> le nombre de points dans
l’ensemble de données, et <span class="math inline">\((y_i,
\hat{y}_i)\)</span> le <span class="math inline">\(i\)</span>-ème couple
de valeurs réelles et prédites. Le MSLE est défini comme : <span
class="math display">\[\text{MSLE} = \frac{1}{n} \sum_{i=1}^{n}
(\log(y_i + 1) - \log(\hat{y}_i + 1))^2\]</span></p>
</div>
<p>Remarquons que nous ajoutons 1 aux valeurs <span
class="math inline">\(y_i\)</span> et <span
class="math inline">\(\hat{y}_i\)</span> pour éviter les problèmes avec
le logarithme des nombres négatifs ou nuls.</p>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<p>Le MSLE possède plusieurs propriétés intéressantes qui le rendent
adapté à certaines applications. Nous en présentons quelques-unes
ci-dessous.</p>
<div class="theorem">
<p>Le MSLE est moins sensible aux valeurs aberrantes que l’erreur
quadratique moyenne (MSE). Plus précisément, pour deux ensembles de
données <span class="math inline">\((y_i, \hat{y}_i)\)</span> et <span
class="math inline">\((y&#39;_i, \hat{y}&#39;_i)\)</span> où les valeurs
aberrantes sont remplacées par des valeurs plus petites, le MSLE diminue
moins rapidement que le MSE.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Considérons deux ensembles de données <span
class="math inline">\((y_i, \hat{y}_i)\)</span> et <span
class="math inline">\((y&#39;_i, \hat{y}&#39;_i)\)</span> tels que <span
class="math inline">\(y&#39;_i = y_i / k\)</span> et <span
class="math inline">\(\hat{y}&#39;_i = \hat{y}_i / k\)</span> pour un
certain <span class="math inline">\(k &gt; 1\)</span>. Le MSE pour les
deux ensembles est donné par : <span class="math display">\[\text{MSE} =
\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> <span
class="math display">\[\text{MSE}&#39; = \frac{1}{n} \sum_{i=1}^{n}
(y&#39;_i - \hat{y}&#39;_i)^2 = \frac{1}{n} \sum_{i=1}^{n}
\left(\frac{y_i}{k} - \frac{\hat{y}_i}{k}\right)^2 = \frac{1}{k^2}
\text{MSE}\]</span> Le MSLE pour les deux ensembles est donné par :
<span class="math display">\[\text{MSLE} = \frac{1}{n} \sum_{i=1}^{n}
(\log(y_i + 1) - \log(\hat{y}_i + 1))^2\]</span> <span
class="math display">\[\text{MSLE}&#39; = \frac{1}{n} \sum_{i=1}^{n}
(\log(y&#39;_i + 1) - \log(\hat{y}&#39;_i + 1))^2 = \frac{1}{n}
\sum_{i=1}^{n} (\log(y_i / k + 1) - \log(\hat{y}_i / k + 1))^2\]</span>
En utilisant l’inégalité des logarithmes, nous avons : <span
class="math display">\[\log(y_i / k + 1) - \log(\hat{y}_i / k + 1) =
\log\left(\frac{y_i / k + 1}{\hat{y}_i / k + 1}\right) \leq
\log\left(\frac{y_i}{\hat{y}_i}\right)\]</span> Ainsi, <span
class="math inline">\(\text{MSLE}&#39; \leq \text{MSLE}\)</span>, mais
la diminution est moins rapide que pour le MSE. ◻</p>
</div>
<h1 id="preuves-et-développements">Preuves et Développements</h1>
<p>Pour illustrer l’utilité du MSLE, considérons un exemple simple.
Supposons que nous ayons deux modèles, A et B, qui prédisent les valeurs
d’une série temporelle. Le modèle A a une erreur quadratique moyenne de
10, tandis que le modèle B a une erreur quadratique logarithmique
moyenne de 0.5.</p>
<p>Pour comparer les deux modèles, nous devons comprendre ce que ces
valeurs signifient. Le MSE de 10 signifie que l’erreur moyenne entre les
valeurs prédites et les valeurs réelles est de <span
class="math inline">\(\sqrt{10} \approx 3.16\)</span>. Le MSLE de 0.5
signifie que l’erreur logarithmique moyenne est de <span
class="math inline">\(\sqrt{0.5} \approx 0.71\)</span>.</p>
<p>Cependant, le MSLE est plus difficile à interpréter directement. Pour
mieux comprendre, nous pouvons calculer l’erreur relative moyenne :
<span class="math display">\[\text{Erreur relative} = \frac{1}{n}
\sum_{i=1}^{n} \left|\frac{y_i - \hat{y}_i}{y_i}\right|\]</span> En
utilisant le MSLE, nous pouvons estimer cette erreur relative en prenant
l’exponentielle de l’erreur logarithmique moyenne : <span
class="math display">\[\text{Erreur relative} \approx
e^{\sqrt{\text{MSLE}}} - 1\]</span> Pour le modèle B, cela donne : <span
class="math display">\[\text{Erreur relative} \approx e^{0.71} - 1
\approx 1.02 - 1 = 0.02\]</span> Cela signifie que l’erreur relative
moyenne du modèle B est d’environ 2%, ce qui est bien meilleur que le
modèle A.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le MSLE possède plusieurs propriétés intéressantes qui le rendent
adapté à certaines applications. Nous en présentons quelques-unes
ci-dessous.</p>
<ul>
<li><p>Le MSLE est toujours non négatif. En effet, pour tout <span
class="math inline">\(x \in \mathbb{R}\)</span>, nous avons <span
class="math inline">\((\log(x + 1))^2 \geq 0\)</span>.</p></li>
<li><p>Le MSLE est invariant par translation logarithmique. Plus
précisément, si nous ajoutons une constante <span
class="math inline">\(c\)</span> à toutes les valeurs réelles et
prédites avant de prendre le logarithme, le MSLE reste inchangé. Cela
signifie que : <span class="math display">\[\text{MSLE}(y_i + c,
\hat{y}_i + c) = \text{MSLE}(y_i, \hat{y}_i)\]</span></p></li>
<li><p>Le MSLE est sensible aux erreurs de proportion. Plus précisément,
si nous multiplions toutes les valeurs réelles et prédites par une
constante <span class="math inline">\(k &gt; 0\)</span>, le MSLE reste
inchangé. Cela signifie que : <span class="math display">\[\text{MSLE}(k
y_i, k \hat{y}_i) = \text{MSLE}(y_i, \hat{y}_i)\]</span></p></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>Le MSLE est une métrique d’évaluation puissante et robuste,
particulièrement adaptée aux séries temporelles où les valeurs varient
sur plusieurs ordres de grandeur. Son utilisation permet une évaluation
plus équilibrée des performances des modèles prédictifs, en atténuant
l’impact des valeurs aberrantes.</p>
<p>Cependant, le MSLE n’est pas une solution universelle. Il est
important de comprendre les forces et les faiblesses de chaque métrique
d’évaluation et de choisir celle qui convient le mieux à l’application
spécifique. Dans certains cas, une combinaison de plusieurs métriques
peut être nécessaire pour obtenir une évaluation complète et précise des
performances du modèle.</p>
</body>
</html>
{% include "footer.html" %}


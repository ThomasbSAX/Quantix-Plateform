{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Approximation variationnelle bayésienne : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Approximation variationnelle bayésienne : Fondements
et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’approximation variationnelle bayésienne (AVB) émerge comme une
réponse élégante aux défis posés par l’inférence bayésienne dans des
modèles complexes. Historiquement, l’inférence bayésienne a été limitée
par la nécessité de calculer des intégrales intraitables, notamment dans
le cadre de modèles hiérarchiques ou de réseaux bayésiens. L’AVB propose
une alternative en approximant la distribution a posteriori par une
famille de distributions plus simples, tout en minimisant une mesure de
divergence.</p>
<p>Cette méthode est indispensable dans des domaines tels que le machine
learning, où les modèles bayésiens sont souvent trop coûteux à évaluer
directement. Elle permet de tirer parti des avantages de la modélisation
bayésienne tout en rendant les calculs pratiques. L’AVB est également
cruciale pour l’apprentissage automatique non supervisé, où elle
facilite l’estimation de paramètres dans des modèles comme les mélanges
gaussiens ou les modèles de topic.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’AVB, commençons par définir les concepts clés.
Supposons que nous avons un modèle bayésien défini par une distribution
a priori <span class="math inline">\(p(\theta)\)</span> et une
vraisemblance <span class="math inline">\(p(x|\theta)\)</span>.
L’objectif est d’estimer la distribution a posteriori <span
class="math inline">\(p(\theta|x)\)</span>, qui est souvent
intraitable.</p>
<p>Nous cherchons une distribution <span
class="math inline">\(q(\theta)\)</span> qui approxime <span
class="math inline">\(p(\theta|x)\)</span>. Cette distribution doit
appartenir à une famille paramétrique simple, par exemple les
distributions gaussiennes ou exponentielles. L’AVB repose sur l’idée de
minimiser la divergence de Kullback-Leibler (KL) entre <span
class="math inline">\(q(\theta)\)</span> et <span
class="math inline">\(p(\theta|x)\)</span>.</p>
<div class="definition">
<p>La divergence de KL entre deux distributions <span
class="math inline">\(q\)</span> et <span
class="math inline">\(p\)</span> est définie par : <span
class="math display">\[D_{\text{KL}}(q || p) = \int q(\theta) \log
\left( \frac{q(\theta)}{p(\theta|x)} \right) d\theta\]</span></p>
</div>
<p>L’objectif est donc de minimiser <span
class="math inline">\(D_{\text{KL}}(q(\theta) || p(\theta|x))\)</span>.
En utilisant la formule de Bayes, nous pouvons réécrire cette divergence
comme : <span class="math display">\[D_{\text{KL}}(q(\theta) ||
p(\theta|x)) = \mathbb{E}_{q(\theta)}[\log q(\theta)] -
\mathbb{E}_{q(\theta)}[\log p(x, \theta)]\]</span> où <span
class="math inline">\(p(x, \theta) = p(x|\theta) p(\theta)\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central en AVB est le théorème de la forme libre, qui
permet de décomposer l’énergie libre variationnelle en termes
calculables.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(q(\theta)\)</span> une distribution
d’approximation et <span class="math inline">\(p(x, \theta)\)</span> la
distribution jointe. L’énergie libre variationnelle est donnée par :
<span class="math display">\[\mathcal{F}(q, x) =
\mathbb{E}_{q(\theta)}[\log p(x, \theta)] - D_{\text{KL}}(q(\theta) ||
p(\theta))\]</span></p>
</div>
<p>Pour minimiser <span class="math inline">\(D_{\text{KL}}(q(\theta) ||
p(\theta|x))\)</span>, il suffit de maximiser <span
class="math inline">\(\mathcal{F}(q, x)\)</span>. En effet, nous avons :
<span class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) =
\log p(x) - \mathcal{F}(q, x)\]</span> où <span
class="math inline">\(\log p(x)\)</span> est une constante par rapport à
<span class="math inline">\(q(\theta)\)</span>.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la forme libre, commençons par exprimer
<span class="math inline">\(D_{\text{KL}}(q(\theta) ||
p(\theta|x))\)</span> en utilisant la formule de Bayes : <span
class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) = \int
q(\theta) \log \left( \frac{q(\theta)}{p(\theta|x)} \right)
d\theta\]</span> En utilisant la définition de <span
class="math inline">\(p(\theta|x)\)</span>, nous avons : <span
class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) = \int
q(\theta) \log \left( \frac{q(\theta) p(x)}{p(x, \theta)} \right)
d\theta\]</span> En séparant les termes, nous obtenons : <span
class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) = \int
q(\theta) \log q(\theta) d\theta - \int q(\theta) \log p(x, \theta)
d\theta + \log p(x)\]</span> En reconnaissant les espérances, nous avons
: <span class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) =
\mathbb{E}_{q(\theta)}[\log q(\theta)] - \mathbb{E}_{q(\theta)}[\log
p(x, \theta)] + \log p(x)\]</span> En réarrangeant les termes, nous
obtenons : <span class="math display">\[D_{\text{KL}}(q(\theta) ||
p(\theta|x)) = \log p(x) - \left( \mathbb{E}_{q(\theta)}[\log p(x,
\theta)] - \mathbb{E}_{q(\theta)}[\log q(\theta)] \right)\]</span> En
reconnaissant <span class="math inline">\(D_{\text{KL}}(q(\theta) ||
p(\theta))\)</span>, nous avons : <span
class="math display">\[D_{\text{KL}}(q(\theta) || p(\theta|x)) = \log
p(x) - \mathcal{F}(q, x)\]</span> où <span
class="math inline">\(\mathcal{F}(q, x) = \mathbb{E}_{q(\theta)}[\log
p(x, \theta)] - D_{\text{KL}}(q(\theta) || p(\theta))\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’AVB possède plusieurs propriétés intéressantes qui en font une
méthode puissante pour l’inférence bayésienne.</p>
<ol>
<li><p>L’énergie libre variationnelle <span
class="math inline">\(\mathcal{F}(q, x)\)</span> est une borne
inférieure de <span class="math inline">\(\log p(x)\)</span>. Cela
signifie que maximiser <span class="math inline">\(\mathcal{F}(q,
x)\)</span> conduit à une approximation de la distribution a posteriori
qui est optimale au sens de la divergence de KL.</p></li>
<li><p>L’AVB peut être étendue à des modèles hiérarchiques en utilisant
des approximations factorisées. Cela permet de traiter des modèles
complexes avec un grand nombre de paramètres.</p></li>
<li><p>L’AVB est particulièrement efficace pour les modèles où la
vraisemblance <span class="math inline">\(p(x|\theta)\)</span> et la
distribution a priori <span class="math inline">\(p(\theta)\)</span>
appartiennent à des familles exponentielles. Dans ce cas, les calculs
peuvent souvent être effectués de manière analytique.</p></li>
</ol>
<p>Pour la propriété (i), nous avons vu que <span
class="math inline">\(D_{\text{KL}}(q(\theta) || p(\theta|x)) \geq
0\)</span>, ce qui implique que <span
class="math inline">\(\mathcal{F}(q, x) \leq \log p(x)\)</span>. La
maximisation de <span class="math inline">\(\mathcal{F}(q, x)\)</span>
conduit donc à une approximation optimale.</p>
<p>Pour la propriété (ii), supposons que <span
class="math inline">\(q(\theta)\)</span> est factorisé en <span
class="math inline">\(q(\theta) = \prod_{i=1}^n q_i(\theta_i)\)</span>.
L’énergie libre variationnelle peut alors être décomposée en termes
calculables pour chaque composante <span
class="math inline">\(q_i(\theta_i)\)</span>.</p>
<p>Pour la propriété (iii), si <span
class="math inline">\(p(x|\theta)\)</span> et <span
class="math inline">\(p(\theta)\)</span> sont des distributions
exponentielles, alors <span class="math inline">\(\log p(x,
\theta)\)</span> est une somme de termes linéaires dans les paramètres.
Cela permet de calculer <span
class="math inline">\(\mathbb{E}_{q(\theta)}[\log p(x, \theta)]\)</span>
de manière analytique.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’approximation variationnelle bayésienne est une méthode puissante
pour l’inférence bayésienne dans des modèles complexes. Elle permet de
tirer parti des avantages de la modélisation bayésienne tout en rendant
les calculs pratiques. Les propriétés et théorèmes présentés dans cet
article montrent que l’AVB est une méthode robuste et flexible,
applicable à une large gamme de problèmes en machine learning et en
statistique.</p>
</body>
</html>
{% include "footer.html" %}


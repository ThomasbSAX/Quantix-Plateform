{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Lyapunov : Une avancée fondamentale en théorie des probabilités</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Lyapunov : Une avancée fondamentale en
théorie des probabilités</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Lyapunov, nommée en l’honneur du mathématicien russe
Alexandre Lyapunov, constitue une pierre angulaire dans le domaine des
probabilités et de l’analyse stochastique. Son émergence historique
s’inscrit dans un contexte où les mathématiciens cherchaient à
généraliser et à approfondir les résultats obtenus par Markov sur les
chaînes de Markov. Lyapunov, dans sa quête pour comprendre le
comportement asymptotique des processus stochastiques, a introduit une
inégalité qui permet de borner la probabilité que la somme de variables
aléatoires centrées dépasse un certain seuil. Cette notion est
indispensable dans l’étude des processus stochastiques, notamment pour
établir des résultats de convergence et de stabilité.</p>
<p>L’inégalité de Lyapunov est particulièrement utile dans le cadre des
martingales, où elle permet de contrôler les écarts par rapport à la
moyenne. Elle trouve des applications dans divers domaines, allant de la
finance mathématique à la physique statistique, en passant par les
sciences des données. Son importance réside dans sa capacité à fournir
des bornes explicites, ce qui est crucial pour les applications
pratiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégalité de Lyapunov, il est essentiel de définir
les concepts clés qui y sont associés. Commençons par les variables
aléatoires centrées et la notion de martingale.</p>
<h2 class="unnumbered" id="variables-aléatoires-centrées">Variables
Aléatoires Centrées</h2>
<p>Considérons un ensemble de variables aléatoires <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span> définies sur un
espace probabilisé <span class="math inline">\((\Omega, \mathcal{F},
P)\)</span>. On dit que ces variables sont centrées si leur espérance
mathématique est nulle, c’est-à-dire :</p>
<p><span class="math display">\[\mathbb{E}[X_i] = 0 \quad \text{pour
tout } i = 1, 2, \ldots, n.\]</span></p>
<p>Cette centration est souvent obtenue en soustrayant l’espérance de
chaque variable aléatoire :</p>
<p><span class="math display">\[X_i&#39; = X_i -
\mathbb{E}[X_i].\]</span></p>
<h2 class="unnumbered" id="martingales">Martingales</h2>
<p>Une martingale est une suite de variables aléatoires <span
class="math inline">\(\{X_n\}_{n \geq 0}\)</span> telle que, pour tout
<span class="math inline">\(n\)</span>, l’espérance conditionnelle de
<span class="math inline">\(X_{n+1}\)</span> sachant les valeurs passées
est égale à <span class="math inline">\(X_n\)</span>. Formellement, on a
:</p>
<p><span class="math display">\[\mathbb{E}[X_{n+1} | X_0, X_1, \ldots,
X_n] = X_n.\]</span></p>
<p>Les martingales jouent un rôle central dans l’analyse stochastique et
sont utilisées pour modéliser des processus où les incréments futurs ont
une espérance nulle conditionnellement aux informations passées.</p>
<h2 class="unnumbered" id="inégalité-de-lyapunov">Inégalité de
Lyapunov</h2>
<p>L’inégalité de Lyapunov permet de borner la probabilité que la somme
des carrés des variables aléatoires centrées dépasse un certain seuil.
Pour énoncer cette inégalité, nous avons besoin de quelques notations
supplémentaires.</p>
<p>Soit <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span> la
somme des variables aléatoires centrées <span class="math inline">\(X_1,
X_2, \ldots, X_n\)</span>. L’inégalité de Lyapunov s’énonce comme suit
:</p>
<p><span class="math display">\[\mathbb{P}\left( |S_n| \geq \epsilon
\right) \leq \frac{\mathbb{E}[S_n^2]}{\epsilon^2}.\]</span></p>
<p>Cette inégalité montre que la probabilité que <span
class="math inline">\(S_n\)</span> dépasse un seuil <span
class="math inline">\(\epsilon\)</span> est contrôlée par l’espérance de
<span class="math inline">\(S_n^2\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’inégalité de Lyapunov est un résultat fondamental en théorie des
probabilités. Voici une formulation plus générale et détaillée de ce
théorème.</p>
<h2 class="unnumbered" id="théorème-de-lyapunov">Théorème de
Lyapunov</h2>
<p>Soit <span class="math inline">\(\{X_n\}_{n \geq 1}\)</span> une
suite de variables aléatoires centrées, c’est-à-dire que <span
class="math inline">\(\mathbb{E}[X_n] = 0\)</span> pour tout <span
class="math inline">\(n \geq 1\)</span>. Supposons de plus que les
variables aléatoires sont indépendantes et que leurs variances sont
bornées. Alors, pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, on a :</p>
<p><span class="math display">\[\mathbb{P}\left( \left| \sum_{i=1}^n X_i
\right| \geq \epsilon \right) \leq \frac{\sum_{i=1}^n
\mathbb{E}[X_i^2]}{\epsilon^2}.\]</span></p>
<p>Ce théorème est une généralisation de l’inégalité de Markov, qui
s’applique à des variables aléatoires non centrées. La centration des
variables permet d’obtenir une borne plus fine, ce qui est crucial pour
les applications pratiques.</p>
<h2 class="unnumbered" id="preuve-du-théorème-de-lyapunov">Preuve du
Théorème de Lyapunov</h2>
<p>La preuve de l’inégalité de Lyapunov repose sur l’utilisation de
l’inégalité de Markov et des propriétés des espérances conditionnelles.
Voici une démonstration détaillée.</p>
<div class="proof">
<p><em>Proof.</em> Commençons par rappeler l’inégalité de Markov, qui
s’énonce comme suit :</p>
<p>Pour toute variable aléatoire <span class="math inline">\(Y\)</span>
à valeurs positives et pour tout <span class="math inline">\(a &gt;
0\)</span>, on a :</p>
<p><span class="math display">\[\mathbb{P}(Y \geq a) \leq
\frac{\mathbb{E}[Y]}{a}.\]</span></p>
<p>Appliquons cette inégalité à la variable aléatoire <span
class="math inline">\(Y = \left( \sum_{i=1}^n X_i \right)^2\)</span> et
au seuil <span class="math inline">\(a = \epsilon^2\)</span>. On obtient
:</p>
<p><span class="math display">\[\mathbb{P}\left( \left| \sum_{i=1}^n X_i
\right| \geq \epsilon \right) = \mathbb{P}\left( \left( \sum_{i=1}^n X_i
\right)^2 \geq \epsilon^2 \right) \leq \frac{\mathbb{E}\left[ \left(
\sum_{i=1}^n X_i \right)^2 \right]}{\epsilon^2}.\]</span></p>
<p>Il reste à calculer l’espérance <span
class="math inline">\(\mathbb{E}\left[ \left( \sum_{i=1}^n X_i \right)^2
\right]\)</span>. Développons cette expression :</p>
<p><span class="math display">\[\mathbb{E}\left[ \left( \sum_{i=1}^n X_i
\right)^2 \right] = \mathbb{E}\left[ \sum_{i=1}^n X_i^2 + 2 \sum_{1 \leq
i &lt; j \leq n} X_i X_j \right].\]</span></p>
<p>En utilisant l’indépendance des variables aléatoires <span
class="math inline">\(X_i\)</span> et le fait qu’elles sont centrées, on
a :</p>
<p><span class="math display">\[\mathbb{E}[X_i X_j] = \mathbb{E}[X_i]
\mathbb{E}[X_j] = 0 \quad \text{pour } i \neq j.\]</span></p>
<p>Ainsi, l’espérance se simplifie en :</p>
<p><span class="math display">\[\mathbb{E}\left[ \left( \sum_{i=1}^n X_i
\right)^2 \right] = \sum_{i=1}^n \mathbb{E}[X_i^2].\]</span></p>
<p>En substituant cette expression dans l’inégalité de Markov, on
obtient finalement :</p>
<p><span class="math display">\[\mathbb{P}\left( \left| \sum_{i=1}^n X_i
\right| \geq \epsilon \right) \leq \frac{\sum_{i=1}^n
\mathbb{E}[X_i^2]}{\epsilon^2}.\]</span></p>
<p>Cela achève la preuve de l’inégalité de Lyapunov. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’inégalité de Lyapunov possède plusieurs propriétés intéressantes et
corollaires qui en découlent. Voici quelques-uns des plus
importants.</p>
<h2 class="unnumbered" id="propriété-de-sous-additivité">Propriété de
Sous-Additivité</h2>
<p>L’inégalité de Lyapunov est sous-additive, ce qui signifie que pour
toute partition <span class="math inline">\(\{A_k\}_{k=1}^m\)</span> de
l’ensemble des indices <span class="math inline">\(\{1, 2, \ldots,
n\}\)</span>, on a :</p>
<p><span class="math display">\[\mathbb{P}\left( \left| \sum_{i=1}^n X_i
\right| \geq \epsilon \right) \leq \sum_{k=1}^m \frac{\sum_{i \in A_k}
\mathbb{E}[X_i^2]}{\epsilon^2}.\]</span></p>
<p>Cette propriété est utile pour décomposer la somme des variables
aléatoires en sous-sommes et pour appliquer l’inégalité de Lyapunov à
chaque sous-somme.</p>
<h2 class="unnumbered" id="corollaire-pour-les-martingales">Corollaire
pour les Martingales</h2>
<p>L’inégalité de Lyapunov peut être étendue aux martingales. Soit <span
class="math inline">\(\{M_n\}_{n \geq 0}\)</span> une martingale telle
que les différences <span class="math inline">\(D_i = M_i -
M_{i-1}\)</span> (avec <span class="math inline">\(M_0 = 0\)</span>)
sont indépendantes et centrées. Alors, pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, on a :</p>
<p><span class="math display">\[\mathbb{P}\left( |M_n| \geq \epsilon
\right) \leq \frac{\sum_{i=1}^n
\mathbb{E}[D_i^2]}{\epsilon^2}.\]</span></p>
<p>Ce corollaire est particulièrement utile dans l’étude des martingales
et de leur comportement asymptotique.</p>
<h2 class="unnumbered"
id="corollaire-pour-les-chaînes-de-markov">Corollaire pour les Chaînes
de Markov</h2>
<p>L’inégalité de Lyapunov peut également être appliquée aux chaînes de
Markov. Soit <span class="math inline">\(\{X_n\}_{n \geq 0}\)</span> une
chaîne de Markov irréductible et apériodique avec une distribution
stationnaire <span class="math inline">\(\pi\)</span>. Supposons que la
chaîne soit centrée par rapport à <span
class="math inline">\(\pi\)</span>, c’est-à-dire que :</p>
<p><span class="math display">\[\mathbb{E}[X_n | X_{n-1} = x] = 0 \quad
\text{pour tout } x.\]</span></p>
<p>Alors, pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, on a :</p>
<p><span class="math display">\[\mathbb{P}\left( \left| \sum_{i=1}^n X_i
\right| \geq \epsilon \right) \leq \frac{\sum_{i=1}^n
\mathbb{E}[X_i^2]}{\epsilon^2}.\]</span></p>
<p>Ce corollaire est utile pour étudier la convergence des chaînes de
Markov vers leur distribution stationnaire.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Lyapunov est un résultat fondamental en théorie des
probabilités, avec des applications dans divers domaines tels que la
finance mathématique, la physique statistique et les sciences des
données. Son importance réside dans sa capacité à fournir des bornes
explicites pour la probabilité que la somme de variables aléatoires
centrées dépasse un certain seuil. Les propriétés et corollaires
associés à cette inégalité en font un outil puissant pour l’analyse
stochastique.</p>
<p>En conclusion, l’inégalité de Lyapunov est un exemple remarquable de
la manière dont les mathématiques peuvent fournir des outils puissants
pour comprendre et modéliser des phénomènes complexes. Son étude
continue de susciter un vif intérêt dans la communauté mathématique et
ses applications pratiques ne cessent de se diversifier.</p>
</body>
</html>
{% include "footer.html" %}


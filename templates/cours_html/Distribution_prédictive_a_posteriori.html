{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distribution prédictive a posteriori : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distribution prédictive a posteriori : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse bayésienne offre un cadre rigoureux pour l’inférence
statistique, permettant d’incorporer des informations a priori dans le
processus de modélisation. Au cœur de cette approche se trouve la
distribution prédictive a posteriori, un outil puissant pour générer des
prévisions à partir de données observées et d’informations préalables.
Cette notion émerge naturellement dans le contexte de la prise de
décision sous incertitude, où il est crucial de quantifier non seulement
les paramètres d’un modèle mais aussi les valeurs futures
potentielles.</p>
<p>La distribution prédictive a posteriori résout le problème
fondamental de la prévision en combinant les données observées avec des
informations a priori, fournissant ainsi une distribution de probabilité
pour des observations futures. Ce cadre est indispensable dans divers
domaines, allant de la finance à la médecine, en passant par
l’ingénierie et les sciences sociales.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la distribution prédictive a posteriori, considérons
un modèle statistique paramétré par un vecteur <span
class="math inline">\(\theta \in \Theta\)</span>. Supposons que nous
disposons de données observées <span class="math inline">\(X = (x_1,
\ldots, x_n)\)</span> et d’une distribution a priori <span
class="math inline">\(\pi(\theta)\)</span>. La distribution prédictive a
posteriori est une distribution de probabilité pour une nouvelle
observation <span class="math inline">\(x_{n+1}\)</span>
conditionnellement aux données observées.</p>
<p>Formellement, la distribution prédictive a posteriori est définie
comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = (x_1, \ldots, x_n)\)</span> un
échantillon observé et <span class="math inline">\(\theta\)</span> le
paramètre d’un modèle statistique. La distribution prédictive a
posteriori pour une nouvelle observation <span
class="math inline">\(x_{n+1}\)</span> est donnée par : <span
class="math display">\[p(x_{n+1} \mid X) = \int_{\Theta} p(x_{n+1} \mid
\theta) \, p(\theta \mid X) \, d\theta\]</span> où <span
class="math inline">\(p(x_{n+1} \mid \theta)\)</span> est la
distribution de vraisemblance et <span class="math inline">\(p(\theta
\mid X)\)</span> est la distribution a posteriori.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[p(x_{n+1} \mid X) = \frac{1}{p(X)} \int_{\Theta}
p(x_{n+1}, x_1, \ldots, x_n \mid \theta) \, \pi(\theta) \,
d\theta\]</span> où <span class="math inline">\(p(X)\)</span> est la
probabilité des données observées et <span
class="math inline">\(\pi(\theta)\)</span> est la distribution a
priori.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en analyse bayésienne est le théorème de
Bayes, qui relie la distribution a posteriori à la distribution a priori
et à la vraisemblance.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un échantillon observé et
<span class="math inline">\(\theta\)</span> le paramètre d’un modèle
statistique. La distribution a posteriori est donnée par : <span
class="math display">\[p(\theta \mid X) = \frac{p(X \mid \theta) \,
\pi(\theta)}{p(X)}\]</span> où <span class="math inline">\(p(X \mid
\theta)\)</span> est la vraisemblance, <span
class="math inline">\(\pi(\theta)\)</span> est la distribution a priori
et <span class="math inline">\(p(X)\)</span> est la probabilité des
données observées.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Bayes, nous commençons par la définition
de la distribution a posteriori. La distribution a posteriori est
proportionnelle à la vraisemblance multipliée par la distribution a
priori : <span class="math display">\[p(\theta \mid X) \propto p(X \mid
\theta) \, \pi(\theta)\]</span></p>
<p>Pour normaliser cette distribution, nous introduisons la constante de
normalisation <span class="math inline">\(p(X)\)</span> : <span
class="math display">\[p(\theta \mid X) = \frac{p(X \mid \theta) \,
\pi(\theta)}{p(X)}\]</span></p>
<p>La constante de normalisation <span
class="math inline">\(p(X)\)</span> est calculée comme suit : <span
class="math display">\[p(X) = \int_{\Theta} p(X \mid \theta) \,
\pi(\theta) \, d\theta\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distribution prédictive a posteriori possède plusieurs propriétés
importantes :</p>
<ol>
<li><p>La distribution prédictive a posteriori est une moyenne pondérée
des distributions de vraisemblance, pondérées par la distribution a
posteriori : <span class="math display">\[p(x_{n+1} \mid X) =
\mathbb{E}_{\theta \mid X}[p(x_{n+1} \mid \theta)]\]</span></p></li>
<li><p>Si la distribution a priori est une distribution conjuguée, la
distribution prédictive a posteriori peut être calculée
analytiquement.</p></li>
<li><p>La distribution prédictive a posteriori permet de générer des
intervalles de crédibilité pour les observations futures.</p></li>
</ol>
<p>Pour prouver la propriété (i), nous utilisons la définition de
l’espérance conditionnelle : <span
class="math display">\[\mathbb{E}_{\theta \mid X}[p(x_{n+1} \mid
\theta)] = \int_{\Theta} p(x_{n+1} \mid \theta) \, p(\theta \mid X) \,
d\theta = p(x_{n+1} \mid X)\]</span></p>
</body>
</html>
{% include "footer.html" %}


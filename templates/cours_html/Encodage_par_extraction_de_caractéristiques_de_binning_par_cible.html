{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par cible</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par cible</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques de binning par cible
(Target Encoding) est une technique puissante en apprentissage
automatique, particulièrement utile pour traiter les variables
catégorielles. Cette méthode permet de transformer des variables
catégorielles en variables numériques en utilisant la valeur cible
moyenne (ou une autre agrégation) de chaque catégorie. L’origine de
cette technique remonte aux premières méthodes d’encodage, où les
variables catégorielles étaient souvent converties en variables
numériques par des méthodes simples comme le one-hot encoding.
Cependant, ces méthodes peuvent entraîner une explosion dimensionnelle
et ne capturent pas toujours les relations sous-jacentes entre les
variables catégorielles et la cible.</p>
<p>Le binning par cible émerge comme une solution pour résoudre ces
problèmes. Il permet de regrouper les catégories en fonction de leur
similarité par rapport à la cible, réduisant ainsi la dimensionnalité
tout en préservant les informations pertinentes. Cette technique est
indispensable dans les cadres où les variables catégorielles sont
nombreuses et où leur traitement direct peut nuire aux performances des
modèles.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
binning par cible, commençons par définir les concepts clés.</p>
<h2 class="unnumbered" id="variable-catégorielle">Variable
Catégorielle</h2>
<p>Une variable catégorielle est une variable qui prend un nombre fini
de valeurs distinctes, appelées catégories. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle prenant ses
valeurs dans un ensemble fini <span class="math inline">\(C = \{c_1,
c_2, \dots, c_k\}\)</span>.</p>
<h2 class="unnumbered" id="binning-par-cible">Binning par Cible</h2>
<p>Le binning par cible consiste à regrouper les catégories en fonction
de leur similarité par rapport à une variable cible <span
class="math inline">\(Y\)</span>. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle et <span
class="math inline">\(Y\)</span> une variable cible. Nous voulons
regrouper les catégories de <span class="math inline">\(X\)</span> en
<span class="math inline">\(m\)</span> bins <span
class="math inline">\(B_1, B_2, \dots, B_m\)</span> tels que les
catégories dans chaque bin <span class="math inline">\(B_i\)</span>
aient des valeurs similaires de <span
class="math inline">\(Y\)</span>.</p>
<h2 class="unnumbered"
id="encodage-par-extraction-de-caractéristiques">Encodage par Extraction
de Caractéristiques</h2>
<p>L’encodage par extraction de caractéristiques consiste à transformer
une variable catégorielle <span class="math inline">\(X\)</span> en une
ou plusieurs variables numériques en utilisant des statistiques de la
variable cible <span class="math inline">\(Y\)</span>. Formellement,
pour chaque catégorie <span class="math inline">\(c_j\)</span> de <span
class="math inline">\(X\)</span>, nous calculons une statistique <span
class="math inline">\(s_j\)</span> de <span
class="math inline">\(Y\)</span> pour les observations où <span
class="math inline">\(X = c_j\)</span>. La statistique la plus
couramment utilisée est la moyenne, mais d’autres statistiques comme la
médiane ou l’écart-type peuvent également être utilisées.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="théorème-de-la-réduction-de-dimension">Théorème de la Réduction de
Dimension</h2>
<p>Le binning par cible permet de réduire la dimensionnalité tout en
préservant les informations pertinentes. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle avec <span
class="math inline">\(k\)</span> catégories et <span
class="math inline">\(Y\)</span> une variable cible. Si nous regroupons
les catégories de <span class="math inline">\(X\)</span> en <span
class="math inline">\(m\)</span> bins <span class="math inline">\(B_1,
B_2, \dots, B_m\)</span> tels que les catégories dans chaque bin <span
class="math inline">\(B_i\)</span> aient des valeurs similaires de <span
class="math inline">\(Y\)</span>, alors la dimensionnalité est réduite
de <span class="math inline">\(k\)</span> à <span
class="math inline">\(m\)</span>.</p>
<h2 class="unnumbered" id="théorème-de-la-précision">Théorème de la
Précision</h2>
<p>L’encodage par extraction de caractéristiques de binning par cible
améliore la précision des modèles prédictifs. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle et <span
class="math inline">\(Y\)</span> une variable cible. Si nous utilisons
l’encodage par extraction de caractéristiques de binning par cible pour
transformer <span class="math inline">\(X\)</span> en une variable
numérique, alors la précision du modèle prédictif utilisant cette
variable transformée est supérieure à celle du modèle utilisant une
méthode d’encodage simple comme le one-hot encoding.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered"
id="preuve-du-théorème-de-la-réduction-de-dimension">Preuve du Théorème
de la Réduction de Dimension</h2>
<p>Pour prouver le théorème de la réduction de dimension, nous devons
montrer que le binning par cible réduit la dimensionnalité tout en
préservant les informations pertinentes.</p>
<p>1. Soit <span class="math inline">\(X\)</span> une variable
catégorielle avec <span class="math inline">\(k\)</span> catégories et
<span class="math inline">\(Y\)</span> une variable cible. 2. Nous
regroupons les catégories de <span class="math inline">\(X\)</span> en
<span class="math inline">\(m\)</span> bins <span
class="math inline">\(B_1, B_2, \dots, B_m\)</span> tels que les
catégories dans chaque bin <span class="math inline">\(B_i\)</span>
aient des valeurs similaires de <span class="math inline">\(Y\)</span>.
3. La dimensionnalité est réduite de <span
class="math inline">\(k\)</span> à <span
class="math inline">\(m\)</span>, car nous avons maintenant <span
class="math inline">\(m\)</span> bins au lieu de <span
class="math inline">\(k\)</span> catégories. 4. Les informations
pertinentes sont préservées, car les catégories dans chaque bin <span
class="math inline">\(B_i\)</span> ont des valeurs similaires de <span
class="math inline">\(Y\)</span>.</p>
<h2 class="unnumbered" id="preuve-du-théorème-de-la-précision">Preuve du
Théorème de la Précision</h2>
<p>Pour prouver le théorème de la précision, nous devons montrer que
l’encodage par extraction de caractéristiques de binning par cible
améliore la précision des modèles prédictifs.</p>
<p>1. Soit <span class="math inline">\(X\)</span> une variable
catégorielle et <span class="math inline">\(Y\)</span> une variable
cible. 2. Nous utilisons l’encodage par extraction de caractéristiques
de binning par cible pour transformer <span
class="math inline">\(X\)</span> en une variable numérique. 3. La
précision du modèle prédictif utilisant cette variable transformée est
supérieure à celle du modèle utilisant une méthode d’encodage simple
comme le one-hot encoding, car l’encodage par extraction de
caractéristiques de binning par cible capture les relations
sous-jacentes entre <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-de-la-réduction-de-dimension">Propriété de la Réduction de
Dimension</h2>
<p>Le binning par cible permet de réduire la dimensionnalité tout en
préservant les informations pertinentes.</p>
<h2 class="unnumbered" id="propriété-de-la-précision">Propriété de la
Précision</h2>
<p>L’encodage par extraction de caractéristiques de binning par cible
améliore la précision des modèles prédictifs.</p>
<h2 class="unnumbered" id="corollaire-de-lefficacité">Corollaire de
l’Efficacité</h2>
<p>L’encodage par extraction de caractéristiques de binning par cible
est efficace en termes de temps de calcul et de mémoire.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par cible
est une technique puissante pour traiter les variables catégorielles en
apprentissage automatique. Elle permet de transformer des variables
catégorielles en variables numériques tout en réduisant la
dimensionnalité et en améliorant la précision des modèles prédictifs.
Cette technique est indispensable dans les cadres où les variables
catégorielles sont nombreuses et où leur traitement direct peut nuire
aux performances des modèles.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La Divergence de Jeffreys : Une Mesure d’Information Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La Divergence de Jeffreys : Une Mesure d’Information
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Jeffreys émerge dans le cadre de la théorie de
l’information et des statistiques, comme une mesure symétrique de la
distance entre deux distributions de probabilité. Introduite par le
statisticien Harold Jeffreys en 1946, cette notion trouve ses racines
dans les travaux de Kullback et Leibler sur la divergence relative. La
divergence de Jeffreys est indispensable pour comparer des modèles
statistiques, évaluer l’incertitude et mesurer la similarité entre
distributions. Elle joue un rôle clé dans les méthodes bayésiennes,
l’apprentissage automatique et la théorie des graphes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la divergence de Jeffreys, commençons par comprendre ce
que nous cherchons à mesurer. Supposons que nous ayons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>. Nous voulons quantifier la
distance entre ces deux distributions de manière symétrique,
c’est-à-dire que la distance de <span class="math inline">\(P\)</span> à
<span class="math inline">\(Q\)</span> doit être égale à la distance de
<span class="math inline">\(Q\)</span> à <span
class="math inline">\(P\)</span>.</p>
<p>La divergence de Jeffreys est une mesure symétrique de la distance
entre deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Elle est définie comme la somme des
divergences de Kullback-Leibler de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span> et de <span
class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes ou continues sur un espace mesurable <span
class="math inline">\(\Omega\)</span>. La divergence de Jeffreys entre
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[J(P \| Q) = D_{KL}(P \| Q) + D_{KL}(Q \|
P)\]</span> où <span class="math inline">\(D_{KL}(P \| Q)\)</span> est
la divergence de Kullback-Leibler de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span>, définie par : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right)\]</span> pour des distributions
discrètes, et <span class="math display">\[D_{KL}(P \| Q) =
\int_{\Omega} P(x) \log\left(\frac{P(x)}{Q(x)}\right) dx\]</span> pour
des distributions continues.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la divergence de Jeffreys est celui de
l’inégalité de Pinsker, qui relie la divergence de Kullback-Leibler à la
distance totale variationnelle.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. Alors,
on a : <span class="math display">\[D_{KL}(P \| Q) \geq \frac{1}{2} \|P
- Q\|_1^2\]</span> où <span class="math inline">\(\|P - Q\|_1\)</span>
est la distance totale variationnelle entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définie par : <span
class="math display">\[\|P - Q\|_1 = \sup_{A \subseteq \Omega} |P(A) -
Q(A)|\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Pinsker, nous utilisons la convexité de
la fonction <span class="math inline">\(\phi(x) = x \log x\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(\phi(x) = x \log x\)</span>. Cette fonction est
convexe, ce qui signifie que pour tout <span class="math inline">\(x, y
\geq 0\)</span> et <span class="math inline">\(\lambda \in
[0,1]\)</span>, on a : <span class="math display">\[\phi(\lambda x +
(1-\lambda) y) \leq \lambda \phi(x) + (1-\lambda) \phi(y)\]</span> En
utilisant cette propriété, nous pouvons écrire : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \Omega} P(x)
\log\left(\frac{P(x)}{Q(x)}\right) = \sum_{x \in \Omega} P(x)
\log\left(P(x)\right) - \sum_{x \in \Omega} P(x)
\log\left(Q(x)\right)\]</span> En appliquant l’inégalité de convexité,
nous obtenons : <span class="math display">\[D_{KL}(P \| Q) \geq
\frac{1}{2} \left( \sum_{x \in \Omega} (P(x) - Q(x))^2 \right)\]</span>
En utilisant l’inégalité de Cauchy-Schwarz, nous avons : <span
class="math display">\[\sum_{x \in \Omega} (P(x) - Q(x))^2 \geq \left(
\sum_{x \in \Omega} |P(x) - Q(x)| \right)^2 = \|P - Q\|_1^2\]</span>
Ainsi, nous obtenons : <span class="math display">\[D_{KL}(P \| Q) \geq
\frac{1}{2} \|P - Q\|_1^2\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Jeffreys possède plusieurs propriétés importantes
:</p>
<ol>
<li><p>Symétrie : <span class="math inline">\(J(P \| Q) = J(Q \|
P)\)</span></p></li>
<li><p>Non-négativité : <span class="math inline">\(J(P \| Q) \geq
0\)</span>, avec égalité si et seulement si <span
class="math inline">\(P = Q\)</span></p></li>
<li><p>Invariance par transformation : Si <span
class="math inline">\(T\)</span> est une transformation bijective et
mesurable, alors <span class="math inline">\(J(P \| Q) = J(T_{\#}P \|
T_{\#}Q)\)</span>, où <span class="math inline">\(T_{\#}P\)</span> est
la distribution de probabilité image de <span
class="math inline">\(P\)</span> par <span
class="math inline">\(T\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ul>
<li><p>La symétrie découle directement de la définition : <span
class="math display">\[J(P \| Q) = D_{KL}(P \| Q) + D_{KL}(Q \| P) =
D_{KL}(Q \| P) + D_{KL}(P \| Q) = J(Q \| P)\]</span></p></li>
<li><p>La non-négativité découle de la non-négativité de la divergence
de Kullback-Leibler : <span class="math display">\[D_{KL}(P \| Q) \geq 0
\quad \text{et} \quad D_{KL}(Q \| P) \geq 0\]</span> avec égalité si et
seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p>L’invariance par transformation découle du fait que la divergence
de Kullback-Leibler est invariante par transformation bijective et
mesurable : <span class="math display">\[D_{KL}(T_{\#}P \| T_{\#}Q) =
D_{KL}(P \| Q)\]</span></p></li>
</ul>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Jeffreys est une mesure puissante et élégante pour
comparer des distributions de probabilité. Ses propriétés de symétrie,
non-négativité et invariance par transformation en font un outil
indispensable dans de nombreux domaines des mathématiques appliquées et
des statistiques. Les théorèmes et propriétés associés à la divergence
de Jeffreys ouvrent des perspectives riches pour l’analyse des données,
l’apprentissage automatique et la théorie de l’information.</p>
</body>
</html>
{% include "footer.html" %}


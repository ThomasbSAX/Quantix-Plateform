{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Minimum Kernel : A Fundamental Concept in Optimization and Machine Learning</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Minimum Kernel : A Fundamental Concept in Optimization
and Machine Learning</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>The concept of the <em>minimum kernel</em> emerges from the
intersection of optimization theory and machine learning, particularly
in the context of support vector machines (SVMs) and kernel methods. The
minimum kernel is not just a mathematical abstraction but a powerful
tool that has revolutionized the way we approach classification and
regression problems. Historically, the need for such a kernel arose from
the desire to find the simplest possible model that can separate data
points with maximal margin, a principle that underpins the success of
SVMs.</p>
<p>The minimum kernel is indispensable in scenarios where we need to
ensure that our models are not only accurate but also interpretable and
computationally efficient. By minimizing the complexity of the kernel,
we avoid overfitting and ensure that our models generalize well to
unseen data. This concept is particularly crucial in high-dimensional
spaces where the curse of dimensionality can make traditional methods
infeasible.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>To understand the minimum kernel, we first need to grasp the concept
of a <em>kernel function</em>. A kernel function is a function that
computes the inner product of two vectors in some (possibly
high-dimensional) feature space. Formally, a kernel is defined as
follows:</p>
<div class="definition">
<p>Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty
set. A kernel function is a function <span class="math inline">\(K:
\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> such that
there exists a feature map <span class="math inline">\(\phi: \mathcal{X}
\rightarrow \mathcal{H}\)</span> into some (possibly
infinite-dimensional) Hilbert space <span
class="math inline">\(\mathcal{H}\)</span> with the property that: <span
class="math display">\[K(x, y) = \langle \phi(x), \phi(y)
\rangle_{\mathcal{H}}\]</span> for all <span class="math inline">\(x, y
\in \mathcal{X}\)</span>.</p>
</div>
<p>Now, the minimum kernel can be defined as the kernel with the
smallest possible complexity that still achieves a desired level of
accuracy. This can be formalized using the concept of the <em>Rademacher
complexity</em> or other measures of model complexity.</p>
<div class="definition">
<p>Let <span class="math inline">\(\mathcal{K}\)</span> be a set of
kernel functions on <span class="math inline">\(\mathcal{X} \times
\mathcal{X}\)</span>, and let <span
class="math inline">\(\mathcal{D}\)</span> be a distribution over <span
class="math inline">\(\mathcal{X}\)</span>. The minimum kernel <span
class="math inline">\(K^* \in \mathcal{K}\)</span> is defined as the
kernel that minimizes the Rademacher complexity <span
class="math inline">\(\mathfrak{R}_n(\mathcal{K})\)</span> subject to a
constraint on the empirical risk: <span class="math display">\[K^* =
\arg\min_{K \in \mathcal{K}} \mathfrak{R}_n(\mathcal{K})\]</span> such
that: <span class="math display">\[\frac{1}{n} \sum_{i=1}^n L(y_i,
f(x_i)) \leq \epsilon\]</span> for some loss function <span
class="math inline">\(L\)</span> and accuracy parameter <span
class="math inline">\(\epsilon &gt; 0\)</span>, where <span
class="math inline">\(f\)</span> is the decision function induced by the
kernel <span class="math inline">\(K\)</span>.</p>
</div>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>One of the fundamental theorems related to the minimum kernel is the
<em>Representer Theorem</em>, which ensures that the solution to a
regularized empirical risk minimization problem can be expressed as a
linear combination of the kernel functions evaluated at the training
points.</p>
<div class="theorem">
<p>Let <span class="math inline">\(\mathcal{H}\)</span> be a reproducing
kernel Hilbert space (RKHS) with kernel <span
class="math inline">\(K\)</span>, and let <span
class="math inline">\(\Omega: \mathcal{H} \rightarrow
\mathbb{R}^+\)</span> be a strictly convex, continuous functional. Then,
the minimizer <span class="math inline">\(f^*\)</span> of the
regularized empirical risk: <span class="math display">\[f^* =
\arg\min_{f \in \mathcal{H}} \left\{ \frac{1}{n} \sum_{i=1}^n L(y_i,
f(x_i)) + \lambda \Omega(f) \right\}\]</span> admits a representation of
the form: <span class="math display">\[f^* = \sum_{i=1}^n \alpha_i
K(x_i, \cdot)\]</span> for some coefficients <span
class="math inline">\(\alpha_i \in \mathbb{R}\)</span>, where <span
class="math inline">\(\lambda &gt; 0\)</span> is a regularization
parameter.</p>
</div>
<p>This theorem is crucial because it shows that the solution to our
optimization problem can be expressed in terms of the kernel functions
evaluated at the training points, which is a key property that allows us
to work with kernels efficiently.</p>
<h1 class="unnumbered" id="proofs">Proofs</h1>
<p>The proof of the Representer Theorem relies on the properties of
reproducing kernel Hilbert spaces and convex optimization. Here, we
provide a sketch of the proof:</p>
<div class="proof">
<p><em>Proof.</em> By the definition of an RKHS, for any <span
class="math inline">\(f \in \mathcal{H}\)</span>, we can write: <span
class="math display">\[f = \sum_{i=1}^\infty \langle f, K(x_i, \cdot)
\rangle_{\mathcal{H}} K(x_i, \cdot)\]</span> for some orthonormal basis
<span class="math inline">\(\{K(x_i, \cdot)\}_{i=1}^\infty\)</span>.</p>
<p>Now, consider the regularized empirical risk minimization problem:
<span class="math display">\[f^* = \arg\min_{f \in \mathcal{H}} \left\{
\frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i)) + \lambda \Omega(f)
\right\}\]</span></p>
<p>By the strict convexity of <span
class="math inline">\(\Omega\)</span>, the functional <span
class="math inline">\(f \mapsto \frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i))
+ \lambda \Omega(f)\)</span> is also strictly convex. Therefore, the
minimizer <span class="math inline">\(f^*\)</span> is unique.</p>
<p>Now, consider any function <span class="math inline">\(g \in
\mathcal{H}\)</span>. By the reproducing property of the kernel, we
have: <span class="math display">\[g = f^* + \sum_{i=1}^n \alpha_i
(K(x_i, \cdot) - K(f^*(x_i), \cdot))\]</span> for some coefficients
<span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>.</p>
<p>Substituting this into the regularized empirical risk, we get: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n L(y_i, g(x_i)) + \lambda
\Omega(g) = \frac{1}{n} \sum_{i=1}^n L\left(y_i, f^*(x_i) + \sum_{j=1}^n
\alpha_j (K(x_j, x_i) - K(f^*(x_j), x_i))\right) + \lambda
\Omega\left(f^* + \sum_{j=1}^n \alpha_j (K(x_j, \cdot) - K(f^*(x_j),
\cdot))\right)\]</span></p>
<p>By the convexity of <span class="math inline">\(L\)</span> and <span
class="math inline">\(\Omega\)</span>, we have: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n L(y_i, g(x_i)) + \lambda
\Omega(g) \geq \frac{1}{n} \sum_{i=1}^n L(y_i, f^*(x_i)) + \lambda
\Omega(f^*)\]</span></p>
<p>This shows that <span class="math inline">\(f^*\)</span> is indeed
the minimizer, and it can be expressed as a linear combination of the
kernel functions evaluated at the training points. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>The minimum kernel has several important properties that make it a
powerful tool in machine learning and optimization. Here, we list some
of these properties along with their proofs.</p>
<ol>
<li><p>The minimum kernel is unique under certain conditions.
Specifically, if the set of kernels <span
class="math inline">\(\mathcal{K}\)</span> is convex and the Rademacher
complexity <span
class="math inline">\(\mathfrak{R}_n(\mathcal{K})\)</span> is strictly
convex, then the minimum kernel <span class="math inline">\(K^*\)</span>
is unique.</p>
<div class="proof">
<p><em>Proof.</em> The uniqueness of the minimum kernel follows from the
strict convexity of the Rademacher complexity. If there were two
distinct kernels <span class="math inline">\(K_1\)</span> and <span
class="math inline">\(K_2\)</span> that both minimize the Rademacher
complexity, then by convexity, any convex combination <span
class="math inline">\(K_\lambda = \lambda K_1 + (1 - \lambda)
K_2\)</span> for <span class="math inline">\(\lambda \in (0, 1)\)</span>
would also minimize the Rademacher complexity. However, by strict
convexity, we have: <span
class="math display">\[\mathfrak{R}_n(K_\lambda) &lt; \lambda
\mathfrak{R}_n(K_1) + (1 - \lambda) \mathfrak{R}_n(K_2)\]</span> which
contradicts the assumption that <span class="math inline">\(K_1\)</span>
and <span class="math inline">\(K_2\)</span> both minimize the
Rademacher complexity. Therefore, the minimum kernel must be
unique. ◻</p>
</div></li>
<li><p>The minimum kernel achieves the best possible trade-off between
accuracy and complexity. This means that it minimizes the empirical risk
subject to a constraint on the Rademacher complexity.</p>
<div class="proof">
<p><em>Proof.</em> By definition, the minimum kernel <span
class="math inline">\(K^*\)</span> is the kernel that minimizes the
Rademacher complexity subject to a constraint on the empirical risk.
This means that it achieves the best possible trade-off between accuracy
(as measured by the empirical risk) and complexity (as measured by the
Rademacher complexity). ◻</p>
</div></li>
<li><p>The minimum kernel can be computed efficiently using convex
optimization techniques. This is because the Rademacher complexity is a
convex functional of the kernel, and the empirical risk constraint is
also convex.</p>
<div class="proof">
<p><em>Proof.</em> The computation of the minimum kernel can be
formulated as a convex optimization problem: <span
class="math display">\[\min_{K \in \mathcal{K}}
\mathfrak{R}_n(\mathcal{K})\]</span> subject to: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i)) \leq
\epsilon\]</span> where <span class="math inline">\(f\)</span> is the
decision function induced by the kernel <span
class="math inline">\(K\)</span>. This problem can be solved efficiently
using standard convex optimization techniques, such as gradient descent
or interior-point methods. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>The concept of the minimum kernel is a powerful and fundamental tool
in optimization and machine learning. It allows us to find the simplest
possible model that achieves a desired level of accuracy, ensuring that
our models are not only accurate but also interpretable and
computationally efficient. The minimum kernel has a rich theoretical
foundation, as evidenced by the Representer Theorem and its properties,
and it can be computed efficiently using convex optimization techniques.
As such, it is an indispensable tool in the modern data scientist’s
arsenal.</p>
</body>
</html>
{% include "footer.html" %}


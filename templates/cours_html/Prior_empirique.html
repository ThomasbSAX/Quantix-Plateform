{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Prior empirique : Une approche bayésienne pour l’apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Prior empirique : Une approche bayésienne pour
l’apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage statistique est un domaine en pleine expansion, où
l’on cherche à extraire des informations pertinentes à partir de
données. Une des approches les plus prometteuses est l’approche
bayésienne, qui permet d’incorporer des informations a priori dans le
processus d’apprentissage. Parmi les différentes méthodes bayésiennes,
la notion de <em>prior empirique</em> se distingue par son efficacité et
sa simplicité.</p>
<p>L’idée fondamentale derrière le prior empirique est d’utiliser les
données elles-mêmes pour construire une distribution a priori. Cela
permet de tirer parti des informations contenues dans les données tout
en conservant la rigueur mathématique de l’approche bayésienne. Cette
méthode est particulièrement utile lorsque les données sont abondantes
mais que les informations a priori sont limitées ou inexistantes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le prior empirique, commençons par rappeler quelques
notions fondamentales. Supposons que nous avons un ensemble de données
<span class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots,
x_n\}\)</span> et que nous voulons estimer un paramètre <span
class="math inline">\(\theta\)</span> d’un modèle statistique. En
approche bayésienne, nous cherchons à calculer la distribution a
posteriori de <span class="math inline">\(\theta\)</span> donnée les
données <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>La distribution a posteriori est donnée par la formule de Bayes :
<span class="math display">\[p(\theta | \mathcal{D}) =
\frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})}\]</span></p>
<p>Ici, <span class="math inline">\(p(\theta)\)</span> est la
distribution a priori de <span class="math inline">\(\theta\)</span>,
que nous appelons le <em>prior</em>. Le prior empirique est une méthode
pour choisir ce prior en fonction des données <span
class="math inline">\(\mathcal{D}\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble de données et <span
class="math inline">\(\theta\)</span> un paramètre d’un modèle
statistique. Le prior empirique est défini comme la distribution a
priori <span class="math inline">\(p(\theta)\)</span> qui maximise
l’espérance de la log-vraisemblance empirique : <span
class="math display">\[p^*(\theta) = \argmax_{p(\theta)} \mathbb{E}_{x
\sim \hat{p}_\mathcal{D}} [\log p(x | \theta)]\]</span> où <span
class="math inline">\(\hat{p}_\mathcal{D}\)</span> est la distribution
empirique des données, définie par : <span
class="math display">\[\hat{p}_\mathcal{D}(x) = \frac{1}{n} \sum_{i=1}^n
\delta(x - x_i)\]</span> et <span class="math inline">\(\delta\)</span>
est la fonction delta de Dirac.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des résultats fondamentaux concernant le prior empirique est le
théorème suivant, qui montre que le prior empirique est asymptotiquement
optimal.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D}_n = \{x_1, x_2, \dots,
x_n\}\)</span> un échantillon i.i.d. de taille <span
class="math inline">\(n\)</span> d’une distribution inconnue <span
class="math inline">\(p^*\)</span>. Supposons que le modèle statistique
est bien spécifié, c’est-à-dire que <span class="math inline">\(p^*(x |
\theta)\)</span> appartient à la famille de modèles paramétriques pour
une certaine valeur de <span class="math inline">\(\theta^*\)</span>.
Alors, le prior empirique <span
class="math inline">\(p_n^*(\theta)\)</span> converge vers la
distribution de Dirac en <span class="math inline">\(\theta^*\)</span>
lorsque <span class="math inline">\(n \to \infty\)</span> : <span
class="math display">\[p_n^*(\theta) \xrightarrow[n \to \infty]{}
\delta(\theta - \theta^*)\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous avons besoin de quelques lemmes
intermédiaires.</p>
<div class="lemma">
<p>Sous les hypothèses du théorème précédent, la distribution empirique
<span class="math inline">\(\hat{p}_{\mathcal{D}_n}\)</span> converge
faiblement vers <span class="math inline">\(p^*\)</span> lorsque <span
class="math inline">\(n \to \infty\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce lemme est une conséquence directe du théorème de
Glivenko-Cantelli. ◻</p>
</div>
<div class="lemma">
<p>Sous les hypothèses du théorème précédent, pour toute fonction
continue et bornée <span class="math inline">\(f\)</span>, l’espérance
empirique converge vers l’espérance théorique : <span
class="math display">\[\mathbb{E}_{x \sim \hat{p}_{\mathcal{D}_n}}
[f(x)] \xrightarrow[n \to \infty]{} \mathbb{E}_{x \sim p^*}
[f(x)]\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce lemme découle du lemme précédent et de la
définition de la convergence faible. ◻</p>
</div>
<p>Nous pouvons maintenant prouver le théorème principal.</p>
<div class="proof">
<p><em>Preuve du théorème d’optimalité asymptotique.</em> Soit <span
class="math inline">\(\epsilon &gt; 0\)</span>. Par définition du prior
empirique, nous avons : <span class="math display">\[\mathbb{E}_{x \sim
\hat{p}_{\mathcal{D}_n}} [\log p_n^*(x | \theta)] \geq \mathbb{E}_{x
\sim \hat{p}_{\mathcal{D}_n}} [\log p^*(x | \theta + \epsilon)] -
o(1)\]</span> où <span class="math inline">\(o(1)\)</span> est un terme
qui tend vers zéro lorsque <span class="math inline">\(n \to
\infty\)</span>.</p>
<p>En utilisant le lemme de convergence de l’espérance empirique, nous
obtenons : <span class="math display">\[\mathbb{E}_{x \sim p^*} [\log
p_n^*(x | \theta)] \geq \mathbb{E}_{x \sim p^*} [\log p^*(x | \theta +
\epsilon)] - o(1)\]</span></p>
<p>En prenant la limite lorsque <span class="math inline">\(n \to
\infty\)</span>, nous avons : <span class="math display">\[\mathbb{E}_{x
\sim p^*} [\log p^*(\theta)(x | \theta)] \geq \mathbb{E}_{x \sim p^*}
[\log p^*(\theta)(x | \theta + \epsilon)]\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(p^*\)</span> est
la vraie distribution, nous pouvons simplifier cette inégalité : <span
class="math display">\[\log p^*(\theta) \geq \log p^*(\theta +
\epsilon)\]</span></p>
<p>En prenant l’exponentielle des deux côtés, nous obtenons : <span
class="math display">\[p^*(\theta) \geq p^*(\theta +
\epsilon)\]</span></p>
<p>En prenant la limite lorsque <span class="math inline">\(\epsilon \to
0\)</span>, nous obtenons : <span class="math display">\[p^*(\theta)
\geq \lim_{\epsilon \to 0} p^*(\theta + \epsilon)\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(p^*\)</span> est
continue, nous obtenons : <span class="math display">\[p^*(\theta) \geq
p^*(\theta)\]</span></p>
<p>Ce qui montre que <span class="math inline">\(p^*\)</span> est une
distribution de Dirac en <span
class="math inline">\(\theta^*\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
corollaires</h1>
<p>Le prior empirique possède plusieurs propriétés intéressantes.</p>
<ul>
<li><p><strong>Consistance</strong> : Le prior empirique est consistant,
c’est-à-dire que lorsque la taille de l’échantillon tend vers l’infini,
l’estimateur bayésien converge vers la vraie valeur du
paramètre.</p></li>
<li><p><strong>Efficacité</strong> : Le prior empirique est efficace,
c’est-à-dire qu’il atteint la borne de Cramér-Rao lorsque le modèle est
bien spécifié.</p></li>
<li><p><strong>Robustesse</strong> : Le prior empirique est robuste aux
erreurs de spécification du modèle, c’est-à-dire qu’il peut encore
fournir de bonnes estimations même lorsque le modèle n’est pas
parfaitement spécifié.</p></li>
</ul>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le prior empirique est une méthode puissante et flexible pour
l’apprentissage statistique bayésien. Il permet de tirer parti des
informations contenues dans les données tout en conservant la rigueur
mathématique de l’approche bayésienne. Les résultats théoriques
présentés dans cet article montrent que le prior empirique est
asymptotiquement optimal et possède de nombreuses propriétés
intéressantes.</p>
</body>
</html>
{% include "footer.html" %}


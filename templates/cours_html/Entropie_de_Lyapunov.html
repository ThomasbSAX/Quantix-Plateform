{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Lyapunov : Un Pont entre le Chaos et l’Ordre</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Lyapunov : Un Pont entre le Chaos et
l’Ordre</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de Lyapunov émerge comme une notion fondamentale dans
l’étude des systèmes dynamiques, particulièrement ceux qui présentent un
comportement chaotique. Introduite pour la première fois par le
mathématicien russe Aleksandr Lyapunov à la fin du XIXe siècle, cette
notion a révolutionné notre compréhension des systèmes complexes.
L’entropie de Lyapunov mesure la sensibilité d’un système dynamique aux
conditions initiales, un concept clé pour distinguer les systèmes
chaotiques des systèmes ordonnés.</p>
<p>Dans le cadre des systèmes dynamiques, l’entropie de Lyapunov permet
de quantifier la complexité du comportement d’un système. Elle est
indispensable pour comprendre les phénomènes chaotiques, où de petites
variations des conditions initiales peuvent conduire à des comportements
radicalement différents. Cette notion est également cruciale dans
l’étude de la stabilité des systèmes, car elle permet de déterminer si
un système est stable ou instable.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Lyapunov, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons un système dynamique où une
petite perturbation des conditions initiales peut entraîner une
divergence exponentielle des trajectoires. Nous voulons quantifier cette
divergence, c’est-à-dire mesurer à quel point le système est sensible
aux conditions initiales.</p>
<p>Formellement, considérons un système dynamique défini par une
application <span class="math inline">\(f: M \to M\)</span>, où <span
class="math inline">\(M\)</span> est un espace de phase. Pour un point
<span class="math inline">\(x_0 \in M\)</span>, la trajectoire du
système est donnée par l’itération de <span
class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[x_n = f^n(x_0)\]</span></p>
<p>où <span class="math inline">\(f^n\)</span> désigne la <span
class="math inline">\(n\)</span>-ième itération de <span
class="math inline">\(f\)</span>.</p>
<p>L’entropie de Lyapunov est définie en termes des exposants de
Lyapunov, qui mesurent la moyenne exponentielle du taux de divergence ou
de convergence des trajectoires voisines. Pour un point <span
class="math inline">\(x_0\)</span>, les exposants de Lyapunov sont
définis comme suit:</p>
<p><span class="math display">\[\lambda_i(x_0) = \lim_{n \to \infty}
\frac{1}{n} \log \| Df^n(x_0) v_i \|\]</span></p>
<p>où <span class="math inline">\(v_i\)</span> sont les vecteurs propres
de la matrice jacobienne <span class="math inline">\(Df(x_0)\)</span>,
et <span class="math inline">\(\| \cdot \|\)</span> désigne une norme
sur l’espace tangent à <span class="math inline">\(M\)</span>.</p>
<p>L’entropie de Lyapunov est alors définie comme la somme des exposants
de Lyapunov positifs:</p>
<p><span class="math display">\[h_{\text{Lyap}}(f) = \sum_{i: \lambda_i
&gt; 0} \lambda_i\]</span></p>
<p>Cette définition peut être généralisée à des ensembles mesurables
<span class="math inline">\(A \subset M\)</span> en intégrant les
exposants de Lyapunov sur <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[h_{\text{Lyap}}(f, A) = \int_A \sum_{i:
\lambda_i &gt; 0} \lambda_i \, d\mu\]</span></p>
<p>où <span class="math inline">\(\mu\)</span> est une mesure invariante
sur <span class="math inline">\(M\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux concernant l’entropie de Lyapunov est
le théorème de Pesin, qui relie l’entropie métrique d’un système
dynamique à son entropie de Lyapunov. Ce théorème est crucial pour
comprendre la structure fine des systèmes dynamiques chaotiques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: M \to M\)</span> un
difféomorphisme d’un variété riemannienne compacte <span
class="math inline">\(M\)</span>, et soit <span
class="math inline">\(\mu\)</span> une mesure invariante de <span
class="math inline">\(f\)</span>. Supposons que <span
class="math inline">\(\mu\)</span>-presque partout, les exposants de
Lyapunov sont non nuls. Alors l’entropie métrique <span
class="math inline">\(h_\mu(f)\)</span> est égale à l’intégrale des
exposants de Lyapunov positifs:</p>
<p><span class="math display">\[h_\mu(f) = \int_M \sum_{i: \lambda_i
&gt; 0} \lambda_i \, d\mu\]</span></p>
</div>
<p>La preuve de ce théorème repose sur plusieurs lemmes et propriétés
importantes, notamment le théorème ergodique de Birkhoff et la théorie
des exposants de Lyapunov. Nous allons maintenant développer cette
preuve en détail.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Pesin, nous commençons par rappeler le
théorème ergodique de Birkhoff, qui affirme que pour une fonction
intégrable <span class="math inline">\(\phi: M \to \mathbb{R}\)</span>,
la moyenne temporelle converge presque partout vers l’intégrale de <span
class="math inline">\(\phi\)</span> par rapport à une mesure
invariante:</p>
<p><span class="math display">\[\lim_{n \to \infty} \frac{1}{n}
\sum_{k=0}^{n-1} \phi(f^k(x)) = \int_M \phi \, d\mu\]</span></p>
<p>En appliquant ce théorème aux fonctions <span
class="math inline">\(\phi_i(x) = \log \| Df(x) v_i \|\)</span>, nous
obtenons:</p>
<p><span class="math display">\[\lim_{n \to \infty} \frac{1}{n}
\sum_{k=0}^{n-1} \log \| Df(f^k(x)) v_i \| = \int_M \log \| Df(x) v_i \|
\, d\mu\]</span></p>
<p>En utilisant la définition des exposants de Lyapunov, nous avons:</p>
<p><span class="math display">\[\lambda_i(x) = \lim_{n \to \infty}
\frac{1}{n} \log \| Df^n(x) v_i \| = \lim_{n \to \infty} \frac{1}{n}
\sum_{k=0}^{n-1} \log \| Df(f^k(x)) v_i \| = \int_M \log \| Df(x) v_i \|
\, d\mu\]</span></p>
<p>Ainsi, les exposants de Lyapunov sont égaux aux intégrales des
fonctions logarithmiques des normes des dérivées.</p>
<p>Ensuite, nous utilisons la théorie des exposants de Lyapunov pour
montrer que l’entropie métrique est égale à la somme des exposants de
Lyapunov positifs. En particulier, nous utilisons le fait que l’entropie
métrique peut être exprimée en termes des exposants de Lyapunov via une
formule due à Ruelle:</p>
<p><span class="math display">\[h_\mu(f) = \sum_{i: \lambda_i &gt; 0}
\lambda_i\]</span></p>
<p>Cette formule est une conséquence directe de la définition de
l’entropie métrique et des propriétés des exposants de Lyapunov.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous allons maintenant énoncer quelques propriétés importantes de
l’entropie de Lyapunov et en donner des preuves détaillées.</p>
<div class="corollary">
<p>L’entropie de Lyapunov est invariante par conjugaison. C’est-à-dire,
si <span class="math inline">\(f: M \to M\)</span> et <span
class="math inline">\(g: N \to N\)</span> sont conjugués par un
homéomorphisme <span class="math inline">\(h: M \to N\)</span>,
alors:</p>
<p><span class="math display">\[h_{\text{Lyap}}(f) =
h_{\text{Lyap}}(g)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété repose sur le fait que
la conjugaison préserve les exposants de Lyapunov. En effet, si <span
class="math inline">\(g = h \circ f \circ h^{-1}\)</span>, alors les
dérivées de <span class="math inline">\(g\)</span> et <span
class="math inline">\(f\)</span> sont liées par:</p>
<p><span class="math display">\[Dg(y) = Dh(f^{-1}(y)) \circ
Df(h^{-1}(y)) \circ (Dh^{-1}(y))^{-1}\]</span></p>
<p>En utilisant cette relation, on peut montrer que les exposants de
Lyapunov de <span class="math inline">\(g\)</span> sont égaux à ceux de
<span class="math inline">\(f\)</span>, et donc que leurs entropies de
Lyapunov sont égales. ◻</p>
</div>
<div class="corollary">
<p>L’entropie de Lyapunov est stable sous les petites perturbations.
C’est-à-dire, si <span class="math inline">\(f_t: M \to M\)</span> est
une famille à un paramètre de difféomorphismes, alors l’entropie de
Lyapunov <span class="math inline">\(h_{\text{Lyap}}(f_t)\)</span> est
une fonction continue de <span class="math inline">\(t\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété repose sur le fait que
les exposants de Lyapunov varient continûment avec <span
class="math inline">\(t\)</span>. En effet, en utilisant le théorème des
fonctions implicites, on peut montrer que les valeurs propres de la
matrice jacobienne <span class="math inline">\(Df_t(x)\)</span> varient
continûment avec <span class="math inline">\(t\)</span>, et donc que les
exposants de Lyapunov varient également continûment. Par conséquent,
l’entropie de Lyapunov est une fonction continue de <span
class="math inline">\(t\)</span>. ◻</p>
</div>
<p>En conclusion, l’entropie de Lyapunov est une notion fondamentale
dans l’étude des systèmes dynamiques chaotiques. Elle permet de
quantifier la complexité du comportement d’un système et de comprendre
les phénomènes chaotiques. Les théorèmes et propriétés présentés dans
cet article montrent l’importance de cette notion et ouvrent la voie à
de nombreuses applications dans divers domaines des mathématiques et des
sciences.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Mahalanobis généralisée</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Mahalanobis généralisée</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Mahalanobis généralisée émerge comme une extension
naturelle de la distance de Mahalanobis classique, introduite par le
statisticien indien P. C. Mahalanobis en 1936. Cette notion trouve son
origine dans le besoin de mesurer la similarité entre des points dans un
espace métrique, en tenant compte de la structure de covariance des
données. La distance de Mahalanobis classique est définie pour des
ensembles de données multivariées et prend en compte la corrélation
entre les variables.</p>
<p>Cependant, dans de nombreux contextes modernes, notamment en
apprentissage automatique et en analyse de données complexes, les
données peuvent être représentées sous forme de matrices ou de tenseurs.
La distance de Mahalanobis généralisée étend cette notion aux espaces de
matrices et de tenseurs, permettant ainsi une analyse plus fine et plus
précise des données. Cette généralisation est indispensable pour traiter
des problèmes où les données ne peuvent pas être simplement représentées
par des vecteurs, mais nécessitent une représentation plus riche et plus
complexe.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la distance de Mahalanobis généralisée, commençons
par rappeler ce que nous cherchons à mesurer. Supposons que nous avons
un ensemble de données représentées par des matrices ou des tenseurs.
Nous voulons mesurer la distance entre deux points en tenant compte de
la structure de covariance des données. La distance de Mahalanobis
généralisée doit capturer cette information de manière à ce que deux
points soient considérés comme proches s’ils sont similaires en tenant
compte des corrélations entre les variables.</p>
<p>Formellement, soit <span class="math inline">\(\mathbf{X}\)</span> et
<span class="math inline">\(\mathbf{Y}\)</span> deux matrices de taille
<span class="math inline">\(m \times n\)</span>. La distance de
Mahalanobis généralisée entre <span
class="math inline">\(\mathbf{X}\)</span> et <span
class="math inline">\(\mathbf{Y}\)</span> est définie comme suit:</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Y}) =
\sqrt{\text{tr}\left((\mathbf{X} - \mathbf{Y})^T \mathbf{C}^{-1}
(\mathbf{X} - \mathbf{Y})\right)}\]</span></p>
<p>où <span class="math inline">\(\mathbf{C}\)</span> est la matrice de
covariance des données.</p>
<p>Une autre formulation équivalente est:</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Y}) =
\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} (\mathbf{X}_{ij} -
\mathbf{Y}_{ij})^2 \mathbf{C}^{-1}_{ij}}\]</span></p>
<p>où <span class="math inline">\(\mathbf{C}^{-1}_{ij}\)</span> est
l’élément <span class="math inline">\((i,j)\)</span>-ème de la matrice
inverse de covariance.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Considérons maintenant un théorème important lié à la distance de
Mahalanobis généralisée. Supposons que nous avons un ensemble de données
représentées par des matrices <span class="math inline">\(\mathbf{X}_1,
\mathbf{X}_2, \ldots, \mathbf{X}_k\)</span> de taille <span
class="math inline">\(m \times n\)</span>. Nous voulons montrer que la
distance de Mahalanobis généralisée satisfait les propriétés d’une
distance métrique.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{X}, \mathbf{Y},
\mathbf{Z}\)</span> des matrices de taille <span class="math inline">\(m
\times n\)</span>. Alors la distance de Mahalanobis généralisée <span
class="math inline">\(d(\mathbf{X}, \mathbf{Y})\)</span> satisfait les
propriétés suivantes:</p>
<ol>
<li><p><span class="math inline">\(d(\mathbf{X}, \mathbf{Y}) \geq
0\)</span></p></li>
<li><p><span class="math inline">\(d(\mathbf{X}, \mathbf{Y}) =
0\)</span> si et seulement si <span class="math inline">\(\mathbf{X} =
\mathbf{Y}\)</span></p></li>
<li><p><span class="math inline">\(d(\mathbf{X}, \mathbf{Y}) =
d(\mathbf{Y}, \mathbf{X})\)</span></p></li>
<li><p><span class="math inline">\(d(\mathbf{X}, \mathbf{Z}) \leq
d(\mathbf{X}, \mathbf{Y}) + d(\mathbf{Y}, \mathbf{Z})\)</span></p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> Nous allons démontrer chaque propriété une par
une.</p>
<ol>
<li><p>La non-négativité de la distance est évidente car le trace d’une
matrice semi-définie positive est toujours non négatif.</p></li>
<li><p>La distance est nulle si et seulement si <span
class="math inline">\(\mathbf{X} = \mathbf{Y}\)</span>. En effet, <span
class="math inline">\(d(\mathbf{X}, \mathbf{Y}) = 0\)</span> implique
que <span class="math inline">\((\mathbf{X} - \mathbf{Y})^T
\mathbf{C}^{-1} (\mathbf{X} - \mathbf{Y}) = 0\)</span>, ce qui signifie
que <span class="math inline">\(\mathbf{X} - \mathbf{Y} = 0\)</span> car
<span class="math inline">\(\mathbf{C}^{-1}\)</span> est définie
positive.</p></li>
<li><p>La symétrie de la distance découle directement de la symétrie du
produit matriciel et de la trace.</p></li>
<li><p>L’inégalité triangulaire peut être démontrée en utilisant
l’inégalité de Cauchy-Schwarz pour les matrices. Soit <span
class="math inline">\(\mathbf{A} = \mathbf{X} - \mathbf{Y}\)</span> et
<span class="math inline">\(\mathbf{B} = \mathbf{Y} -
\mathbf{Z}\)</span>. Alors,</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Z}) =
\sqrt{\text{tr}(\mathbf{A}^T \mathbf{C}^{-1} \mathbf{A} + \mathbf{B}^T
\mathbf{C}^{-1} \mathbf{B} + 2 \mathbf{A}^T \mathbf{C}^{-1}
\mathbf{B})}\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons:</p>
<p><span class="math display">\[\text{tr}(\mathbf{A}^T \mathbf{C}^{-1}
\mathbf{B}) \leq \sqrt{\text{tr}(\mathbf{A}^T \mathbf{C}^{-1}
\mathbf{A})} \sqrt{\text{tr}(\mathbf{B}^T \mathbf{C}^{-1}
\mathbf{B})}\]</span></p>
<p>Ce qui implique que:</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Z}) \leq
d(\mathbf{X}, \mathbf{Y}) + d(\mathbf{Y}, \mathbf{Z})\]</span></p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour justifier l’inégalité triangulaire, nous avons utilisé
l’inégalité de Cauchy-Schwarz pour les matrices. Cette inégalité est une
généralisation de l’inégalité de Cauchy-Schwarz pour les vecteurs et
peut être démontrée en utilisant la décomposition spectrale de la
matrice <span class="math inline">\(\mathbf{C}^{-1}\)</span>. En
particulier, nous pouvons écrire <span
class="math inline">\(\mathbf{C}^{-1}\)</span> comme une somme de
matrices de rang un, ce qui permet d’appliquer l’inégalité de
Cauchy-Schwarz pour chaque terme de la somme.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
distance de Mahalanobis généralisée.</p>
<ol>
<li><p>La distance de Mahalanobis généralisée est invariante sous les
transformations linéaires. Plus précisément, si <span
class="math inline">\(\mathbf{T}\)</span> est une matrice inversible de
taille <span class="math inline">\(m \times m\)</span>, alors:</p>
<p><span class="math display">\[d(\mathbf{T}\mathbf{X},
\mathbf{T}\mathbf{Y}) = d(\mathbf{X}, \mathbf{Y})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle du fait que la matrice de covariance
<span class="math inline">\(\mathbf{C}\)</span> est invariante sous les
transformations linéaires. En effet, si <span
class="math inline">\(\mathbf{T}\)</span> est une matrice inversible,
alors:</p>
<p><span class="math display">\[(\mathbf{T}\mathbf{X} -
\mathbf{T}\mathbf{Y})^T \mathbf{C}^{-1} (\mathbf{T}\mathbf{X} -
\mathbf{T}\mathbf{Y}) = (\mathbf{X} - \mathbf{Y})^T \mathbf{T}^T
\mathbf{C}^{-1} \mathbf{T} (\mathbf{X} - \mathbf{Y}) = (\mathbf{X} -
\mathbf{Y})^T \mathbf{C}^{-1} (\mathbf{X} - \mathbf{Y})\]</span></p>
<p>car <span class="math inline">\(\mathbf{T}^T \mathbf{C}^{-1}
\mathbf{T} = \mathbf{C}^{-1}\)</span> par définition de la matrice de
covariance. ◻</p>
</div></li>
<li><p>La distance de Mahalanobis généralisée est compatible avec la
norme euclidienne. Plus précisément, si <span
class="math inline">\(\mathbf{C} = \mathbf{I}\)</span>, la matrice
identité, alors:</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Y}) =
\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} (\mathbf{X}_{ij} -
\mathbf{Y}_{ij})^2}\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle directement de la définition de la
distance de Mahalanobis généralisée. Si <span
class="math inline">\(\mathbf{C} = \mathbf{I}\)</span>, alors <span
class="math inline">\(\mathbf{C}^{-1} = \mathbf{I}\)</span>, et la
distance devient:</p>
<p><span class="math display">\[d(\mathbf{X}, \mathbf{Y}) =
\sqrt{\text{tr}\left((\mathbf{X} - \mathbf{Y})^T (\mathbf{X} -
\mathbf{Y})\right)} = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}
(\mathbf{X}_{ij} - \mathbf{Y}_{ij})^2}\]</span> ◻</p>
</div></li>
<li><p>La distance de Mahalanobis généralisée est robuste aux
changements d’échelle. Plus précisément, si <span
class="math inline">\(\mathbf{X}\)</span> et <span
class="math inline">\(\mathbf{Y}\)</span> sont multipliées par un
scalaire <span class="math inline">\(a\)</span>, alors:</p>
<p><span class="math display">\[d(a\mathbf{X}, a\mathbf{Y}) = |a|
d(\mathbf{X}, \mathbf{Y})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle du fait que la distance de Mahalanobis
généralisée est homogène de degré un. En effet, si <span
class="math inline">\(\mathbf{X}\)</span> et <span
class="math inline">\(\mathbf{Y}\)</span> sont multipliées par un
scalaire <span class="math inline">\(a\)</span>, alors:</p>
<p><span class="math display">\[d(a\mathbf{X}, a\mathbf{Y}) =
\sqrt{\text{tr}\left((a\mathbf{X} - a\mathbf{Y})^T \mathbf{C}^{-1}
(a\mathbf{X} - a\mathbf{Y})\right)} = |a|
\sqrt{\text{tr}\left((\mathbf{X} - \mathbf{Y})^T \mathbf{C}^{-1}
(\mathbf{X} - \mathbf{Y})\right)} = |a| d(\mathbf{X},
\mathbf{Y})\]</span> ◻</p>
</div></li>
</ol>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage par extraction de caractéristiques de percentile : une approche robuste pour l’apprentissage automatique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage par extraction de caractéristiques de
percentile : une approche robuste pour l’apprentissage automatique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique moderne repose souvent sur la capacité à
extraire des caractéristiques pertinentes (features) à partir de données
brutes. Parmi les nombreuses techniques disponibles, l’encodage par
extraction de caractéristiques de percentile se distingue par sa
simplicité et son efficacité, particulièrement dans les contextes où la
robustesse aux valeurs aberrantes est cruciale.</p>
<p>L’idée sous-jacente à cette méthode est de transformer des données
continues en caractéristiques discrètes basées sur les percentiles.
Cette approche permet non seulement de réduire la dimension des données,
mais aussi de capturer des informations statistiques essentielles tout
en atténuant l’impact des valeurs extrêmes.</p>
<p>Dans cet article, nous explorerons les fondements théoriques de cette
technique, ses applications pratiques, et son intégration dans des
pipelines d’apprentissage automatique modernes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
percentile, commençons par définir les concepts clés.</p>
<h2 id="percentile">Percentile</h2>
<p>Considérons un ensemble de données <span class="math inline">\(X =
\{x_1, x_2, \dots, x_n\}\)</span> où chaque <span
class="math inline">\(x_i\)</span> est une observation réelle. Nous
cherchons à diviser cet ensemble en segments basés sur des
percentiles.</p>
<div class="definition">
<p>Soit <span class="math inline">\(p \in [0, 1]\)</span> un percentile.
Le <span class="math inline">\(p\)</span>-percentile d’un ensemble de
données <span class="math inline">\(X\)</span> est une valeur <span
class="math inline">\(v_p\)</span> telle que : <span
class="math display">\[\mathbb{P}(x_i \leq v_p) = p\]</span> ou, de
manière équivalente, <span class="math display">\[\exists k \in \{1, 2,
\dots, n\} \text{ tel que } v_p = x_{(k)} \text{ où } k = \lceil pn
\rceil\]</span></p>
</div>
<h2
id="encodage-par-extraction-de-caractéristiques-de-percentile">Encodage
par Extraction de Caractéristiques de Percentile</h2>
<p>L’encodage par extraction de caractéristiques de percentile consiste
à transformer chaque observation <span
class="math inline">\(x_i\)</span> en un vecteur de caractéristiques
basé sur sa position relative par rapport aux percentiles
prédéfinis.</p>
<div class="definition">
<p>Soit <span class="math inline">\(P = \{p_1, p_2, \dots,
p_m\}\)</span> un ensemble de percentiles, avec <span
class="math inline">\(0 &lt; p_1 &lt; p_2 &lt; \dots &lt; p_m &lt;
1\)</span>. L’encodage par extraction de caractéristiques de percentile
d’une observation <span class="math inline">\(x_i\)</span> est un
vecteur <span class="math inline">\(\mathbf{f}_i = (f_{i1}, f_{i2},
\dots, f_{im})\)</span> où : <span class="math display">\[f_{ij} =
\begin{cases}
1 &amp; \text{si } x_i \leq v_{p_j} \\
0 &amp; \text{sinon}
\end{cases}\]</span> pour chaque <span class="math inline">\(j = 1, 2,
\dots, m\)</span>, et où <span class="math inline">\(v_{p_j}\)</span>
est le <span class="math inline">\(p_j\)</span>-percentile de l’ensemble
de données <span class="math inline">\(X\)</span>.</p>
</div>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<p>Dans cette section, nous explorons les propriétés théoriques de
l’encodage par extraction de caractéristiques de percentile.</p>
<h2 id="conservation-de-lordre">Conservation de l’Ordre</h2>
<p>L’une des propriétés fondamentales de cette technique est la
conservation de l’ordre relatif des observations.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> deux observations telles que <span
class="math inline">\(x_i \leq x_j\)</span>. Alors, pour tout percentile
<span class="math inline">\(p_k \in P\)</span>, nous avons : <span
class="math display">\[f_{ik} \geq f_{jk}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(x_i \leq
x_j\)</span>. Par définition de l’encodage, si <span
class="math inline">\(x_i \leq v_{p_k}\)</span>, alors <span
class="math inline">\(f_{ik} = 1\)</span>. Comme <span
class="math inline">\(x_j \geq x_i\)</span>, il s’ensuit que si <span
class="math inline">\(f_{ik} = 1\)</span>, alors <span
class="math inline">\(x_j \leq v_{p_k}\)</span> et donc <span
class="math inline">\(f_{jk} = 1\)</span>. Ainsi, nous avons <span
class="math inline">\(f_{ik} \geq f_{jk}\)</span>. ◻</p>
</div>
<h2 id="robustesse-aux-valeurs-aberrantes">Robustesse aux Valeurs
Aberrantes</h2>
<p>L’encodage par extraction de caractéristiques de percentile est
robuste aux valeurs aberrantes en raison de sa nature discrète.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(x_{\text{out}}\)</span> une valeur
aberrante dans l’ensemble de données <span
class="math inline">\(X\)</span>. L’encodage par extraction de
caractéristiques de percentile est invariant à l’ajout ou à la
suppression de <span class="math inline">\(x_{\text{out}}\)</span> si
<span class="math inline">\(x_{\text{out}}\)</span> est un percentile
extrême.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span
class="math inline">\(x_{\text{out}}\)</span> soit le maximum de
l’ensemble de données. L’ajout ou la suppression de <span
class="math inline">\(x_{\text{out}}\)</span> n’affectera pas les
percentiles inférieurs, car ceux-ci sont déterminés par des quantités
relatives de données. Par conséquent, l’encodage des autres observations
restera inchangé. ◻</p>
</div>
<h1 id="applications-pratiques">Applications Pratiques</h1>
<p>L’encodage par extraction de caractéristiques de percentile trouve
des applications dans divers domaines de l’apprentissage
automatique.</p>
<h2 id="réduction-de-dimension">Réduction de Dimension</h2>
<p>En transformant les données continues en caractéristiques binaires,
cette technique permet de réduire la dimension des données tout en
préservant les informations statistiques essentielles.</p>
<h2 id="classification">Classification</h2>
<p>Les caractéristiques binaires obtenues peuvent être utilisées comme
entrées pour des algorithmes de classification, améliorant souvent la
performance et la robustesse du modèle.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de percentile est une
technique puissante et flexible pour l’apprentissage automatique. Sa
simplicité, sa robustesse aux valeurs aberrantes, et ses applications
pratiques en font un outil précieux pour les chercheurs et les
praticiens.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de Wiener : Une Mesure de la Complexité des Systèmes Dynamiques</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de Wiener : Une Mesure de la Complexité des
Systèmes Dynamiques</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie de Wiener, également connue sous le nom d’entropie
métrique ou entropie de Kolmogorov-Sinai, est un concept fondamental en
théorie ergodique et en systèmes dynamiques. Introduite par Norbert
Wiener dans les années 1940, cette notion permet de quantifier la
complexité et le désordre d’un système dynamique. L’entropie de Wiener
émerge comme une réponse à la question de mesure du degré
d’imprévisibilité dans les systèmes dynamiques.</p>
<p>L’importance de l’entropie de Wiener réside dans sa capacité à
distinguer entre les systèmes dynamiques chaotiques et ceux qui sont
plus prévisibles. Elle est indispensable dans l’étude des systèmes
dynamiques, notamment dans les domaines de la physique théorique, de la
théorie du chaos et de l’analyse des séries temporelles.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Wiener, il est essentiel de commencer
par définir les concepts de base nécessaires.</p>
<h2 id="partitions-et-entropie">Partitions et Entropie</h2>
<p>Considérons un espace mesurable <span class="math inline">\((X,
\mathcal{A}, \mu)\)</span> où <span class="math inline">\(\mu\)</span>
est une mesure de probabilité. Une partition d’un ensemble <span
class="math inline">\(X\)</span> est une collection finie ou dénombrable
de sous-ensembles disjoints dont l’union recouvre <span
class="math inline">\(X\)</span>.</p>
<p>Soit <span class="math inline">\(\alpha = \{A_1, A_2, \ldots,
A_n\}\)</span> une partition finie de <span
class="math inline">\(X\)</span>. L’entropie d’une partition <span
class="math inline">\(\alpha\)</span> est définie comme :</p>
<p><span class="math display">\[H(\alpha) = -\sum_{i=1}^n \mu(A_i) \log
\mu(A_i)\]</span></p>
<p>Cette quantité mesure l’incertitude ou l’information moyenne associée
à la partition <span class="math inline">\(\alpha\)</span>.</p>
<h2 id="entropie-de-wiener">Entropie de Wiener</h2>
<p>Soit <span class="math inline">\(T: X \rightarrow X\)</span> un
endomorphisme mesurable préservant la mesure. Pour une partition <span
class="math inline">\(\alpha\)</span>, nous définissons l’entropie de
Wiener comme :</p>
<p><span class="math display">\[h(T, \alpha) = \lim_{n \to \infty}
\frac{1}{n} H\left(\bigvee_{k=0}^{n-1}
T^{-k}(\alpha)\right)\]</span></p>
<p>où <span class="math inline">\(\bigvee_{k=0}^{n-1}
T^{-k}(\alpha)\)</span> représente la partition engendrée par les images
inverses de <span class="math inline">\(\alpha\)</span> sous l’action de
<span class="math inline">\(T\)</span>.</p>
<p>L’entropie de Wiener du système dynamique <span
class="math inline">\((X, \mathcal{A}, \mu, T)\)</span> est alors
définie comme :</p>
<p><span class="math display">\[h(T) = \sup_{\alpha} h(T,
\alpha)\]</span></p>
<p>où la supériorité est prise sur toutes les partitions finies <span
class="math inline">\(\alpha\)</span> de <span
class="math inline">\(X\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-kolmogorov-sinai">Théorème de Kolmogorov-Sinai</h2>
<p>Un résultat fondamental en théorie ergodique est le théorème de
Kolmogorov-Sinai, qui établit que l’entropie de Wiener d’un système
dynamique est déterminée par une partition génératrice.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X, \mathcal{A}, \mu, T)\)</span> un
système dynamique et <span class="math inline">\(\alpha\)</span> une
partition génératrice de <span class="math inline">\(X\)</span>.
Alors,</p>
<p><span class="math display">\[h(T) = h(T, \alpha)\]</span></p>
</div>
<h2 id="démonstration-du-théorème-de-kolmogorov-sinai">Démonstration du
Théorème de Kolmogorov-Sinai</h2>
<p>La démonstration repose sur plusieurs étapes clés :</p>
<p>1. **Partition Génératrice** : Une partition <span
class="math inline">\(\alpha\)</span> est dite génératrice si les
partitions <span class="math inline">\(\bigvee_{k=-\infty}^{\infty}
T^{-k}(\alpha)\)</span> séparent les points de <span
class="math inline">\(X\)</span>.</p>
<p>2. **Entropie Conditionnelle** : Pour deux partitions <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>, l’entropie conditionnelle de <span
class="math inline">\(\alpha\)</span> sachant <span
class="math inline">\(\beta\)</span> est définie comme :</p>
<p><span class="math display">\[H(\alpha|\beta) = \sum_{B \in \beta}
\mu(B) H(\alpha|B)\]</span></p>
<p>où <span class="math inline">\(H(\alpha|B)\)</span> est l’entropie de
la restriction de <span class="math inline">\(\alpha\)</span> à <span
class="math inline">\(B\)</span>.</p>
<p>3. **Inégalité de Subadditivité** : Pour toute partition <span
class="math inline">\(\alpha\)</span>, on a :</p>
<p><span class="math display">\[H\left(\bigvee_{k=0}^{n-1}
T^{-k}(\alpha)\right) \leq \sum_{k=0}^{n-1}
H(T^{-k}(\alpha))\]</span></p>
<p>4. **Convergence de l’Entropie** : En utilisant les propriétés
ci-dessus, on peut montrer que pour une partition génératrice <span
class="math inline">\(\alpha\)</span>,</p>
<p><span class="math display">\[h(T) = \lim_{n \to \infty} \frac{1}{n}
H\left(\bigvee_{k=0}^{n-1} T^{-k}(\alpha)\right) = h(T,
\alpha)\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-linégalité-de-subadditivité">Preuve de l’Inégalité de
Subadditivité</h2>
<p>Considérons deux partitions <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>. L’entropie conjointe de <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span> est donnée par :</p>
<p><span class="math display">\[H(\alpha \vee \beta) = H(\alpha) +
H(\beta|\alpha)\]</span></p>
<p>En utilisant la définition de l’entropie conditionnelle, on obtient
:</p>
<p><span class="math display">\[H(\alpha \vee \beta) = H(\alpha) +
\sum_{A \in \alpha} \mu(A) H(\beta|A)\]</span></p>
<p>Pour montrer l’inégalité de subadditivité, nous utilisons le fait que
<span class="math inline">\(H(\beta|A) \leq H(\beta)\)</span> pour tout
<span class="math inline">\(A \in \alpha\)</span>. Ainsi,</p>
<p><span class="math display">\[H(\alpha \vee \beta) \leq H(\alpha) +
H(\beta)\]</span></p>
<p>En itérant ce processus pour plusieurs partitions, on obtient
l’inégalité de subadditivité.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-de-lentropie-de-wiener">Propriétés de l’Entropie de
Wiener</h2>
<ol>
<li><p>**Invariance par Isomorphisme** : Si deux systèmes dynamiques
<span class="math inline">\((X, \mathcal{A}, \mu, T)\)</span> et <span
class="math inline">\((Y, \mathcal{B}, \nu, S)\)</span> sont isomorphes,
alors <span class="math inline">\(h(T) = h(S)\)</span>.</p></li>
<li><p>**Monotonie** : Si <span class="math inline">\(\alpha\)</span> et
<span class="math inline">\(\beta\)</span> sont des partitions telles
que <span class="math inline">\(\alpha \preceq \beta\)</span>, alors
<span class="math inline">\(h(T, \alpha) \leq h(T,
\beta)\)</span>.</p></li>
<li><p>**Additivité** : Pour deux systèmes dynamiques indépendants <span
class="math inline">\((X_1, \mathcal{A}_1, \mu_1, T_1)\)</span> et <span
class="math inline">\((X_2, \mathcal{A}_2, \mu_2, T_2)\)</span>, on a
:</p>
<p><span class="math display">\[h(T_1 \times T_2) = h(T_1) +
h(T_2)\]</span></p></li>
</ol>
<h2 id="démonstration-des-propriétés">Démonstration des Propriétés</h2>
<p>1. **Invariance par Isomorphisme** : Un isomorphisme entre deux
systèmes dynamiques préserve les propriétés mesurables, y compris
l’entropie.</p>
<p>2. **Monotonie** : Si <span class="math inline">\(\alpha \preceq
\beta\)</span>, alors chaque élément de <span
class="math inline">\(\alpha\)</span> est contenu dans un élément de
<span class="math inline">\(\beta\)</span>. Par conséquent,
l’information fournie par <span class="math inline">\(\alpha\)</span>
est inférieure ou égale à celle fournie par <span
class="math inline">\(\beta\)</span>, ce qui implique <span
class="math inline">\(h(T, \alpha) \leq h(T, \beta)\)</span>.</p>
<p>3. **Additivité** : L’entropie conjointe de deux systèmes dynamiques
indépendants est la somme de leurs entropies individuelles, ce qui
résulte directement de la définition de l’entropie.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie de Wiener est un outil puissant pour analyser la
complexité des systèmes dynamiques. Son introduction a permis de mieux
comprendre les propriétés des systèmes chaotiques et a ouvert la voie à
de nombreuses avancées en théorie ergodique. Les théorèmes et propriétés
associés à l’entropie de Wiener continuent d’être explorés, offrant de
nouvelles perspectives dans l’étude des systèmes dynamiques.</p>
</body>
</html>
{% include "footer.html" %}


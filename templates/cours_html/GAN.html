{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Generative Adversarial Networks: A Mathematical Journey</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Generative Adversarial Networks: A Mathematical
Journey</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>Generative Adversarial Networks (GANs) have emerged as a
groundbreaking paradigm in the field of machine learning, particularly
in generative modeling. The concept was first introduced by Ian
Goodfellow and his colleagues in 2014, revolutionizing the way we
approach data generation. GANs are indispensable in various domains such
as image synthesis, style transfer, and even drug discovery. The beauty
of GANs lies in their adversarial nature, where two neural networks—the
generator and the discriminator—engage in a minimax game, pushing each
other to improve continually.</p>
<h1 id="definitions">Definitions</h1>
<p>Before delving into the formal definition of GANs, let’s understand
what we aim to achieve. Imagine you have a dataset of images, and you
want to create a model that can generate new images indistinguishable
from the real ones. The generator network takes random noise as input
and produces an image, while the discriminator network tries to
distinguish between real images from the dataset and fake ones generated
by the generator. The goal is to have the generator produce images so
realistic that the discriminator cannot tell them apart from real
images.</p>
<div class="definition">
<p>A Generative Adversarial Network consists of two neural networks:</p>
<ul>
<li><p>A generator <span class="math inline">\(G: \mathcal{Z}
\rightarrow \mathcal{X}\)</span>, where <span
class="math inline">\(\mathcal{Z}\)</span> is the latent space and <span
class="math inline">\(\mathcal{X}\)</span> is the data space.</p></li>
<li><p>A discriminator <span class="math inline">\(D: \mathcal{X}
\rightarrow [0,1]\)</span>, which outputs the probability that a given
input is from the real data distribution.</p></li>
</ul>
<p>The generator <span class="math inline">\(G\)</span> takes a random
noise vector <span class="math inline">\(z \sim p_z(z)\)</span> and
generates a sample <span class="math inline">\(G(z)\)</span>. The
discriminator <span class="math inline">\(D\)</span> then evaluates the
probability that this sample is real.</p>
<p>Formally, the GAN framework can be described as a minimax game with
the following value function: <span class="math display">\[\min_G \max_D
V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z
\sim p_z(z)}[\log (1 - D(G(z)))]\]</span> Here, <span
class="math inline">\(p_{data}(x)\)</span> is the real data distribution
and <span class="math inline">\(p_z(z)\)</span> is the noise
distribution.</p>
</div>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems in the theory of GANs is the
convergence theorem, which states that under certain conditions, the
generator can learn to model the real data distribution.</p>
<div class="theorem">
<p>Assume that both the generator <span class="math inline">\(G\)</span>
and the discriminator <span class="math inline">\(D\)</span> are capable
of representing any probability distribution. Then, the optimal solution
for the discriminator <span class="math inline">\(D^*\)</span> is given
by: <span class="math display">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x)
+ p_g(x)}\]</span> where <span class="math inline">\(p_g(x)\)</span> is
the distribution of generated samples.</p>
<p>The optimal generator <span class="math inline">\(G^*\)</span> will
make the discriminator’s output uniform, i.e., <span
class="math inline">\(D^*(G(z)) = 0.5\)</span> for all <span
class="math inline">\(z \sim p_z(z)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> To prove this theorem, we start by finding the
optimal discriminator <span class="math inline">\(D^*\)</span> for a
fixed generator <span class="math inline">\(G\)</span>. The value
function <span class="math inline">\(V(D, G)\)</span> is maximized when
the discriminator’s output is as close to 1 for real samples and as
close to 0 for generated samples as possible.</p>
<p>The optimal discriminator can be derived by setting the gradient of
<span class="math inline">\(V(D, G)\)</span> with respect to <span
class="math inline">\(D\)</span> to zero. This leads to: <span
class="math display">\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) +
p_g(x)}\]</span></p>
<p>Next, we find the optimal generator <span
class="math inline">\(G^*\)</span> for this fixed discriminator. The
value function <span class="math inline">\(V(D^*, G)\)</span> is
minimized when the generated samples <span
class="math inline">\(G(z)\)</span> are indistinguishable from real
samples. This occurs when: <span class="math display">\[D^*(G(z)) =
0.5\]</span></p>
<p>Substituting the expression for <span
class="math inline">\(D^*\)</span>, we get: <span
class="math display">\[\frac{p_{data}(G(z))}{p_{data}(G(z)) + p_g(G(z))}
= 0.5\]</span></p>
<p>This implies that <span class="math inline">\(p_{data}(x) =
p_g(x)\)</span>, meaning the generator has successfully modeled the real
data distribution. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<div class="corollary">
<p>The training of GANs is highly sensitive to the architecture and
hyperparameters. Small changes can lead to mode collapse, where the
generator produces limited varieties of samples.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The stability of GAN training can be analyzed through
the lens of game theory. The minimax game between the generator and
discriminator is not guaranteed to converge to a stable equilibrium. In
practice, the discriminator might become too strong and reject all
generated samples, causing the generator to fail in learning meaningful
patterns. This phenomenon is known as mode collapse. ◻</p>
</div>
<div class="corollary">
<p>Different loss functions can be used in the GAN framework, such as
the Wasserstein distance, which provides a more stable training
process.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The Wasserstein GAN (WGAN) uses the Earth Mover’s
distance as the loss function, which is more stable and provides better
gradient signals for training. The value function for WGAN is given by:
<span class="math display">\[\min_G \max_D V(D, G) = \mathbb{E}_{x \sim
p_{data}(x)}[D(x)] - \mathbb{E}_{z \sim p_z(z)}[D(G(z))]\]</span> This
formulation ensures that the gradients do not vanish, leading to more
stable training. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Generative Adversarial Networks have opened up new avenues in
generative modeling, with applications ranging from image synthesis to
drug discovery. The adversarial training process, coupled with the
convergence theorem, provides a robust framework for learning complex
data distributions. However, challenges such as training stability and
mode collapse remain areas of active research.</p>
</body>
</html>
{% include "footer.html" %}


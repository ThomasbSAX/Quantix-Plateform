{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Jensen-Shannon Divergence: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Jensen-Shannon Divergence: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The Jensen-Shannon Divergence (JSD) is a popular method for measuring
the similarity between two probability distributions. It is based on the
well-known Kullback-Leibler divergence, but it offers several
advantages, such as symmetry and boundedness. The JSD has found
applications in various fields, including machine learning,
bioinformatics, and natural language processing.</p>
<p>The emergence of the JSD can be traced back to the need for a more
robust and interpretable measure of divergence between probability
distributions. The Kullback-Leibler divergence, while fundamental,
suffers from asymmetry and the lack of a finite bound. The JSD addresses
these issues by averaging the Kullback-Leibler divergences between each
distribution and their mixture.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the JSD, we first need to recall some fundamental
concepts. Let <span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> be two probability distributions over a
finite set <span class="math inline">\(\mathcal{X}\)</span>. The
Kullback-Leibler divergence between <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span> is defined as:</p>
<p><span class="math display">\[D_{KL}(P \parallel Q) = \sum_{x \in
\mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</span></p>
<p>However, the Kullback-Leibler divergence is not symmetric and can be
infinite. To overcome these limitations, we consider the average of the
Kullback-Leibler divergences between each distribution and their
mixture. The mixture distribution <span class="math inline">\(M\)</span>
is given by:</p>
<p><span class="math display">\[M = \frac{P + Q}{2}\]</span></p>
<p>The Jensen-Shannon Divergence is then defined as:</p>
<p><span class="math display">\[D_{JS}(P \parallel Q) = \frac{1}{2}
D_{KL}\left(P \parallel M\right) + \frac{1}{2} D_{KL}\left(Q \parallel
M\right)\]</span></p>
<p>Alternatively, the JSD can be expressed in terms of the entropy of
the distributions:</p>
<p><span class="math display">\[D_{JS}(P \parallel Q) = H\left(\frac{P +
Q}{2}\right) - \frac{H(P) + H(Q)}{2}\]</span></p>
<p>where <span class="math inline">\(H\)</span> denotes the Shannon
entropy.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key properties of the JSD is its boundedness. The
following theorem establishes an upper bound for the JSD.</p>
<div class="theorem">
<p>For any two probability distributions <span
class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>, the Jensen-Shannon Divergence
satisfies:</p>
<p><span class="math display">\[0 \leq D_{JS}(P \parallel Q) \leq
1\]</span></p>
<p>Moreover, <span class="math inline">\(D_{JS}(P \parallel Q) =
0\)</span> if and only if <span class="math inline">\(P =
Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The non-negativity of the JSD follows from the
non-negativity of the Kullback-Leibler divergence. To establish the
upper bound, we use the fact that the Kullback-Leibler divergence is
maximized when one of the distributions is a point mass and the other is
uniform.</p>
<p>Consider the case where <span class="math inline">\(P\)</span> is a
point mass at some <span class="math inline">\(x_0 \in
\mathcal{X}\)</span> and <span class="math inline">\(Q\)</span> is the
uniform distribution over <span
class="math inline">\(\mathcal{X}\)</span>. Then, the mixture
distribution <span class="math inline">\(M\)</span> is given by:</p>
<p><span class="math display">\[M(x) = \frac{1}{2} \delta_{x_0}(x) +
\frac{1}{2} \frac{1}{|\mathcal{X}|}\]</span></p>
<p>where <span class="math inline">\(\delta_{x_0}\)</span> is the Dirac
delta function centered at <span class="math inline">\(x_0\)</span>. The
Kullback-Leibler divergences are then:</p>
<p><span class="math display">\[D_{KL}(P \parallel M) =
\log(2|\mathcal{X}|)\]</span></p>
<p><span class="math display">\[D_{KL}(Q \parallel M) =
\log(|\mathcal{X}|)\]</span></p>
<p>Thus, the JSD is:</p>
<p><span class="math display">\[D_{JS}(P \parallel Q) = \frac{1}{2}
\log(2|\mathcal{X}|) + \frac{1}{2} \log(|\mathcal{X}|) =
\log(|\mathcal{X}|) + \frac{1}{2} \log(2)\]</span></p>
<p>As <span class="math inline">\(|\mathcal{X}|\)</span> increases, the
JSD approaches 1. Therefore, the maximum value of the JSD is 1.</p>
<p>To see that <span class="math inline">\(D_{JS}(P \parallel Q) =
0\)</span> if and only if <span class="math inline">\(P = Q\)</span>,
note that the JSD is zero if and only if both Kullback-Leibler
divergences are zero. This occurs precisely when <span
class="math inline">\(P = Q = M\)</span>. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Jensen-Shannon Divergence enjoys several important properties,
which we list and prove below.</p>
<ol>
<li><p><strong>Symmetry:</strong> The JSD is symmetric in its arguments,
i.e., <span class="math inline">\(D_{JS}(P \parallel Q) = D_{JS}(Q
\parallel P)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> The symmetry of the JSD follows from the symmetry of
the mixture distribution <span class="math inline">\(M\)</span> and the
fact that the Kullback-Leibler divergence is not symmetric, but the
average of <span class="math inline">\(D_{KL}(P \parallel M)\)</span>
and <span class="math inline">\(D_{KL}(Q \parallel M)\)</span> is
symmetric. ◻</p>
</div></li>
<li><p><strong>Joint Convexity:</strong> The JSD is jointly convex in
its arguments, i.e., for any two pairs of distributions <span
class="math inline">\((P_1, Q_1)\)</span> and <span
class="math inline">\((P_2, Q_2)\)</span>, and any <span
class="math inline">\(\lambda \in [0, 1]\)</span>, we have:</p>
<p><span class="math display">\[D_{JS}((1 - \lambda)P_1 + \lambda P_2
\parallel (1 - \lambda)Q_1 + \lambda Q_2) \leq (1 - \lambda) D_{JS}(P_1
\parallel Q_1) + \lambda D_{JS}(P_2 \parallel Q_2)\]</span></p>
<div class="proof">
<p><em>Proof.</em> The joint convexity of the JSD follows from the
convexity of the Kullback-Leibler divergence and the fact that the
average of convex functions is convex. ◻</p>
</div></li>
<li><p><strong>Metric Property:</strong> The squared JSD satisfies the
properties of a metric, i.e., it is non-negative, symmetric, satisfies
the triangle inequality, and is zero if and only if the distributions
are equal.</p>
<div class="proof">
<p><em>Proof.</em> The non-negativity and symmetry of the squared JSD
follow from the corresponding properties of the JSD. To establish the
triangle inequality, we use the fact that the Kullback-Leibler
divergence satisfies a version of the triangle inequality. The zero
condition follows from the corresponding property of the JSD. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>The Jensen-Shannon Divergence is a powerful and versatile tool for
measuring the similarity between probability distributions. Its
symmetry, boundedness, and convexity make it particularly attractive for
applications in machine learning and other fields. The theoretical
properties of the JSD, as well as its practical advantages, have
contributed to its widespread adoption and continued study.</p>
</body>
</html>
{% include "footer.html" %}


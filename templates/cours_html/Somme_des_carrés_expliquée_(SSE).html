{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Somme des Carrés Expliquée (SSE)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Somme des Carrés Expliquée (SSE)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La somme des carrés expliquée (SSE), également connue sous le nom de
somme résiduelle, est une mesure fondamentale en statistique et en
analyse des données. Elle émerge dans le contexte de la régression
linéaire, où elle quantifie l’écart entre les valeurs observées et les
valeurs prédites par un modèle. La SSE est indispensable pour évaluer la
qualité d’un modèle de régression, en fournissant une mesure objective
de l’erreur commise par le modèle.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la SSE, commençons par considérer un ensemble de
données <span class="math inline">\((x_i, y_i)\)</span> pour <span
class="math inline">\(i = 1, \ldots, n\)</span>, où <span
class="math inline">\(x_i\)</span> sont les variables indépendantes et
<span class="math inline">\(y_i\)</span> sont les variables dépendantes.
Supposons que nous ayons un modèle de régression linéaire <span
class="math inline">\(\hat{y}_i = \beta_0 + \beta_1 x_i\)</span>, où
<span class="math inline">\(\beta_0\)</span> et <span
class="math inline">\(\beta_1\)</span> sont les coefficients du
modèle.</p>
<p>Nous cherchons à mesurer l’écart entre les valeurs observées <span
class="math inline">\(y_i\)</span> et les valeurs prédites <span
class="math inline">\(\hat{y}_i\)</span>. Cet écart est appelé résidu,
défini comme <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>.
La somme des carrés des résidus (SSE) est alors définie comme la somme
des carrés de ces écarts.</p>
<div class="definition">
<p>La somme des carrés expliquée (SSE) est donnée par: <span
class="math display">\[\text{SSE} = \sum_{i=1}^n (y_i -
\hat{y}_i)^2\]</span> où <span class="math inline">\(y_i\)</span> sont
les valeurs observées, <span class="math inline">\(\hat{y}_i\)</span>
sont les valeurs prédites par le modèle de régression, et <span
class="math inline">\(n\)</span> est le nombre total d’observations.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la SSE est le théorème de Gauss-Markov,
qui fournit des conditions sous lesquelles les estimateurs des moindres
carrés sont les meilleurs estimateurs linéaires non biaisés.</p>
<div class="theorem">
<p>Supposons que le modèle de régression linéaire soit donné par <span
class="math inline">\(y = X\beta + \epsilon\)</span>, où <span
class="math inline">\(X\)</span> est la matrice de conception, <span
class="math inline">\(\beta\)</span> est le vecteur des coefficients, et
<span class="math inline">\(\epsilon\)</span> est le vecteur des
erreurs. Si les hypothèses suivantes sont satisfaites:</p>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[\epsilon] =
0\)</span>,</p></li>
<li><p><span class="math inline">\(\text{Var}(\epsilon) = \sigma^2
I_n\)</span>,</p></li>
</ul>
<p>alors les estimateurs des moindres carrés <span
class="math inline">\(\hat{\beta} = (X^T X)^{-1} X^T y\)</span> sont les
meilleurs estimateurs linéaires non biaisés (BLUE).</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Gauss-Markov, nous devons montrer que les
estimateurs des moindres carrés minimisent la SSE sous les hypothèses
données.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un autre estimateur linéaire non biaisé
<span class="math inline">\(b = Cy\)</span> pour une matrice <span
class="math inline">\(C\)</span>. Nous voulons montrer que <span
class="math inline">\(\text{Var}(\hat{\beta}) \leq
\text{Var}(b)\)</span>.</p>
<p>La variance de <span class="math inline">\(b\)</span> est donnée par:
<span class="math display">\[\text{Var}(b) = C \text{Var}(y) C^T = C X
\sigma^2 I_n X^T C^T = \sigma^2 CX X^T C^T\]</span></p>
<p>La variance de <span class="math inline">\(\hat{\beta}\)</span> est
donnée par: <span class="math display">\[\text{Var}(\hat{\beta}) =
\sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1} = \sigma^2 (X^T
X)^{-1}\]</span></p>
<p>Pour montrer que <span class="math inline">\(\text{Var}(\hat{\beta})
\leq \text{Var}(b)\)</span>, nous devons montrer que: <span
class="math display">\[(X^T X)^{-1} \leq CX X^T C^T\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(C = (X^T X)^{-1}
X^T\)</span> pour les estimateurs des moindres carrés, nous avons: <span
class="math display">\[CX X^T C^T = (X^T X)^{-1} X^T X (X^T X)^{-1} =
(X^T X)^{-1}\]</span></p>
<p>Ainsi, <span class="math inline">\(\text{Var}(\hat{\beta}) \leq
\text{Var}(b)\)</span>, ce qui prouve que les estimateurs des moindres
carrés sont les meilleurs estimateurs linéaires non biaisés. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La SSE possède plusieurs propriétés importantes qui en font un outil
puissant pour l’analyse des données.</p>
<ul>
<li><p>La SSE est toujours non négative, car elle est la somme des
carrés des résidus.</p></li>
<li><p>La SSE peut être utilisée pour calculer le coefficient de
détermination <span class="math inline">\(R^2\)</span>, qui mesure la
proportion de la variance des données expliquée par le modèle.</p></li>
<li><p>La SSE est minimisée par les estimateurs des moindres carrés sous
les hypothèses du théorème de Gauss-Markov.</p></li>
</ul>
<div class="corollary">
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> est donné par: <span
class="math display">\[R^2 = 1 - \frac{\text{SSE}}{\text{SST}}\]</span>
où <span class="math inline">\(\text{SST}\)</span> est la somme totale
des carrés, définie par: <span class="math display">\[\text{SST} =
\sum_{i=1}^n (y_i - \bar{y})^2\]</span> et <span
class="math inline">\(\bar{y}\)</span> est la moyenne des valeurs
observées.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver ce corollaire, nous devons montrer que:
<span class="math display">\[R^2 = 1 -
\frac{\text{SSE}}{\text{SST}}\]</span></p>
<p>Commençons par exprimer <span
class="math inline">\(\text{SST}\)</span> en termes de <span
class="math inline">\(\text{SSE}\)</span> et de la somme des carrés
expliquée (SSM): <span class="math display">\[\text{SST} = \text{SSE} +
\text{SSM}\]</span></p>
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> est défini comme la proportion de la
variance expliquée par le modèle: <span class="math display">\[R^2 =
\frac{\text{SSM}}{\text{SST}}\]</span></p>
<p>En substituant <span class="math inline">\(\text{SSM} = \text{SST} -
\text{SSE}\)</span> dans l’expression de <span
class="math inline">\(R^2\)</span>, nous obtenons: <span
class="math display">\[R^2 = \frac{\text{SST} - \text{SSE}}{\text{SST}}
= 1 - \frac{\text{SSE}}{\text{SST}}\]</span></p>
<p>Ainsi, le corollaire est prouvé. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La somme des carrés expliquée (SSE) est un outil fondamental en
statistique et en analyse des données. Elle permet de quantifier
l’erreur commise par un modèle de régression et de mesurer la qualité du
modèle. Grâce au théorème de Gauss-Markov, nous savons que les
estimateurs des moindres carrés minimisent la SSE sous certaines
hypothèses, ce qui en fait les meilleurs estimateurs linéaires non
biaisés. Les propriétés et corollaires de la SSE en font un outil
puissant pour l’analyse des données, permettant de calculer des mesures
telles que le coefficient de détermination <span
class="math inline">\(R^2\)</span>.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Fano : Un outil fondamental en théorie de l’information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Fano : Un outil fondamental en théorie
de l’information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Fano, nommée en l’honneur du mathématicien italien
Roberto Mario Fano, est un résultat central en théorie de l’information.
Elle émerge dans le cadre de la transmission d’information fiable à
travers des canaux bruités. L’origine conceptuelle de cette inégalité
remonte aux années 1940, avec les travaux pionniers de Claude Shannon
sur la théorie de l’information. L’inégalité de Fano permet de
quantifier la relation entre l’incertitude sur un message et la
probabilité d’erreur dans sa transmission. Elle est indispensable pour
établir des bornes sur les performances des systèmes de communication et
pour concevoir des codes correcteurs d’erreurs efficaces.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Considérons un système de communication où nous avons un message
<span class="math inline">\(X\)</span> pris dans un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span> et transmis à travers un
canal bruité. Le récepteur reçoit un signal <span
class="math inline">\(Y\)</span> pris dans un ensemble fini <span
class="math inline">\(\mathcal{Y}\)</span>. L’objectif est de
reconstruire le message original <span class="math inline">\(X\)</span>
à partir du signal reçu <span class="math inline">\(Y\)</span>. Nous
définissons une fonction de décodage <span class="math inline">\(f:
\mathcal{Y} \rightarrow \mathcal{X}\)</span> qui associe à chaque signal
reçu <span class="math inline">\(y \in \mathcal{Y}\)</span> un message
estimé <span class="math inline">\(\hat{x} = f(y)\)</span>.</p>
<p>Nous cherchons à quantifier la probabilité d’erreur de décodage,
c’est-à-dire la probabilité que le message estimé <span
class="math inline">\(\hat{X}\)</span> diffère du message original <span
class="math inline">\(X\)</span>. Cette probabilité est donnée par :
<span class="math display">\[P_e = \mathbb{P}(\hat{X} \neq
X)\]</span></p>
<p>L’incertitude sur le message <span class="math inline">\(X\)</span>
peut être quantifiée par l’entropie de <span
class="math inline">\(X\)</span>, définie comme : <span
class="math display">\[H(X) = -\sum_{x \in \mathcal{X}} P_X(x) \log
P_X(x)\]</span></p>
<p>L’incertitude résiduelle sur le message <span
class="math inline">\(X\)</span> après avoir observé le signal <span
class="math inline">\(Y\)</span> est donnée par l’entropie
conditionnelle de <span class="math inline">\(X\)</span> sachant <span
class="math inline">\(Y\)</span>, définie comme : <span
class="math display">\[H(X|Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in
\mathcal{Y}} P_{X,Y}(x,y) \log P_{X|Y}(x|y)\]</span></p>
<p>Formellement, l’inégalité de Fano s’énonce comme suit :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un message pris dans un
ensemble fini <span class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(Y\)</span> le signal reçu après transmission à
travers un canal bruité. Soit <span class="math inline">\(f: \mathcal{Y}
\rightarrow \mathcal{X}\)</span> une fonction de décodage et <span
class="math inline">\(\hat{X} = f(Y)\)</span> le message estimé. Alors,
l’inégitude de Fano s’écrit : <span class="math display">\[H(X|\hat{X})
\leq H(P_e) + P_e \log(|\mathcal{X}| - 1)\]</span> où <span
class="math inline">\(H(P_e) = -P_e \log P_e - (1 - P_e) \log(1 -
P_e)\)</span> est l’entropie binaire et <span
class="math inline">\(|\mathcal{X}|\)</span> est le cardinal de
l’ensemble <span class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuve-de-linégalité-de-fano">Preuve de
l’inégalité de Fano</h1>
<p>Pour prouver l’inégalité de Fano, nous commençons par exprimer
l’entropie conditionnelle <span
class="math inline">\(H(X|\hat{X})\)</span> en utilisant la définition
de l’entropie conditionnelle : <span class="math display">\[H(X|\hat{X})
= \sum_{\hat{x} \in \mathcal{X}} P_{\hat{X}}(\hat{x})
H(X|\hat{X}=\hat{x})\]</span></p>
<p>Nous pouvons décomposer l’ensemble <span
class="math inline">\(\mathcal{X}\)</span> en deux sous-ensembles
disjoints : l’ensemble des messages correctement décodés <span
class="math inline">\(\mathcal{C} = \{x \in \mathcal{X} :
P(\hat{X}=X|X=x) &gt; 0\}\)</span> et l’ensemble des messages
incorrectement décodés <span class="math inline">\(\mathcal{E} = \{x \in
\mathcal{X} : P(\hat{X}=X|X=x) = 0\}\)</span>.</p>
<p>Pour chaque message <span class="math inline">\(x \in
\mathcal{C}\)</span>, nous avons : <span
class="math display">\[H(X|\hat{X}=\hat{x}) = -\sum_{x \in \mathcal{C}}
P_{X|\hat{X}}(x|\hat{x}) \log P_{X|\hat{X}}(x|\hat{x})\]</span></p>
<p>En utilisant l’inégalité de Gibbs, nous obtenons : <span
class="math display">\[H(X|\hat{X}=\hat{x}) \leq -\log
P_{X|\hat{X}}(x=\hat{x}|\hat{x}) = -\log
P_{\hat{X}|X}(\hat{x}=\hat{x}|x=\hat{x})\]</span></p>
<p>En sommant sur tous les messages <span class="math inline">\(x \in
\mathcal{C}\)</span>, nous obtenons : <span
class="math display">\[H(X|\hat{X}) \leq -\sum_{\hat{x} \in \mathcal{C}}
P_{\hat{X}}(\hat{x}) \log P_{\hat{X}|X}(\hat{x}|\hat{x})\]</span></p>
<p>Nous pouvons maintenant utiliser l’inégalité de Markov pour borner la
probabilité <span
class="math inline">\(P_{\hat{X}|X}(\hat{x}|\hat{x})\)</span> : <span
class="math display">\[P_{\hat{X}|X}(\hat{x}|\hat{x}) = 1 - \sum_{x \in
\mathcal{E}} P_{\hat{X}|X}(\hat{x}|x)\]</span></p>
<p>En utilisant l’inégalité de Markov, nous obtenons : <span
class="math display">\[P_{\hat{X}|X}(\hat{x}|\hat{x}) \geq 1 -
(|\mathcal{E}|) \max_{x \in \mathcal{E}}
P_{\hat{X}|X}(\hat{x}|x)\]</span></p>
<p>En prenant le maximum sur tous les messages <span
class="math inline">\(x \in \mathcal{E}\)</span>, nous obtenons : <span
class="math display">\[P_{\hat{X}|X}(\hat{x}|\hat{x}) \geq 1 -
(|\mathcal{E}|) P_e\]</span></p>
<p>En utilisant l’inégalité de Fano, nous obtenons finalement : <span
class="math display">\[H(X|\hat{X}) \leq -\sum_{\hat{x} \in \mathcal{C}}
P_{\hat{X}}(\hat{x}) \log (1 - (|\mathcal{E}|) P_e)\]</span></p>
<p>En utilisant l’inégalité de Gibbs, nous obtenons : <span
class="math display">\[H(X|\hat{X}) \leq -\log (1 - P_e)\]</span></p>
<p>En utilisant l’inégalité de Fano, nous obtenons finalement : <span
class="math display">\[H(X|\hat{X}) \leq H(P_e) + P_e \log(|\mathcal{X}|
- 1)\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<ol>
<li><p>L’inégalité de Fano montre que l’entropie conditionnelle <span
class="math inline">\(H(X|\hat{X})\)</span> est bornée supérieurement
par une fonction de la probabilité d’erreur <span
class="math inline">\(P_e\)</span>. Plus précisément, cette borne dépend
de l’entropie binaire <span class="math inline">\(H(P_e)\)</span> et du
logarithme du nombre de messages possibles.</p></li>
<li><p>En prenant la limite lorsque <span
class="math inline">\(P_e\)</span> tend vers 0, l’inégalité de Fano
implique que <span class="math inline">\(H(X|\hat{X})\)</span> tend vers
0. Cela signifie que l’incertitude résiduelle sur le message <span
class="math inline">\(X\)</span> après décodage tend vers 0 lorsque la
probabilité d’erreur tend vers 0.</p></li>
<li><p>L’inégalité de Fano peut être utilisée pour établir des bornes
inférieures sur la probabilité d’erreur <span
class="math inline">\(P_e\)</span> en fonction de l’entropie
conditionnelle <span class="math inline">\(H(X|\hat{X})\)</span>. Plus
précisément, en inversant l’inégalité de Fano, nous obtenons : <span
class="math display">\[P_e \geq \frac{H(X|\hat{X}) -
H(P_e)}{H(|\mathcal{X}| - 1)}\]</span></p></li>
<li><p>L’inégalité de Fano peut être généralisée à des ensembles de
messages infinis en utilisant des notions d’entropie différentielle.
Dans ce cas, l’inégalité de Fano devient : <span
class="math display">\[h(X|\hat{X}) \leq h(P_e) + P_e \log(2\pi e
\sigma^2)\]</span> où <span class="math inline">\(h(X|\hat{X})\)</span>
est l’entropie différentielle conditionnelle de <span
class="math inline">\(X\)</span> sachant <span
class="math inline">\(\hat{X}\)</span>, <span
class="math inline">\(h(P_e)\)</span> est l’entropie différentielle de
la variable aléatoire <span class="math inline">\(P_e\)</span>, et <span
class="math inline">\(\sigma^2\)</span> est la variance de <span
class="math inline">\(X\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Fano est un outil fondamental en théorie de
l’information, permettant de quantifier la relation entre l’incertitude
sur un message et la probabilité d’erreur dans sa transmission. Elle
trouve des applications dans divers domaines, tels que la conception de
codes correcteurs d’erreurs, l’estimation de paramètres et la
compression de données. Les propriétés et corollaires de l’inégalité de
Fano offrent des insights précieux sur les limites fondamentales des
systèmes de communication et ouvrent la voie à de nouvelles recherches
dans ce domaine.</p>
</body>
</html>
{% include "footer.html" %}


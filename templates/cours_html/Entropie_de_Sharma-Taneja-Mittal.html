{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de Sharma-Taneja-Mittal : Une Généralisation Puissante des Mesures d’Incertitude</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de Sharma-Taneja-Mittal : Une Généralisation
Puissante des Mesures d’Incertitude</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie, concept central en théorie de l’information et en
statistique, mesure l’incertitude ou le désordre d’un système.
Introduite par Shannon en 1948, elle a été généralisée de nombreuses
fois pour capturer des aspects plus subtils de l’incertitude. Parmi ces
généralisations, l’entropie de Sharma-Taneja-Mittal (STM) se distingue
par sa flexibilité et son pouvoir descriptif.</p>
<p>L’entropie STM émerge du besoin de modéliser des situations où
l’incertitude n’est pas simplement additive, mais peut être amplifiée ou
atténuée par des paramètres supplémentaires. Elle résout le problème de
la rigidité des mesures classiques en introduisant deux paramètres,
<span class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>, qui permettent de contrôler la
sensibilité de l’entropie à différentes distributions.</p>
<p>Cette mesure est indispensable dans des domaines tels que la théorie
des probabilités, l’apprentissage automatique et la physique
statistique, où une compréhension fine de l’incertitude est
cruciale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Sharma-Taneja-Mittal, considérons
d’abord une distribution de probabilité discrète <span
class="math inline">\(P = \{p_1, p_2, \ldots, p_n\}\)</span> sur un
ensemble fini. Nous cherchons une mesure qui généralise l’entropie de
Shannon <span class="math inline">\(H(P) = -\sum_{i=1}^n p_i \log
p_i\)</span> en introduisant des paramètres qui permettent de moduler la
sensibilité de l’entropie.</p>
<p>L’entropie de Sharma-Taneja-Mittal est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(P = \{p_1, p_2, \ldots,
p_n\}\)</span> une distribution de probabilité discrète. Pour <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> avec <span
class="math inline">\(\alpha \neq 1\)</span> et <span
class="math inline">\(\beta &gt; 0\)</span>, l’entropie de
Sharma-Taneja-Mittal est donnée par : <span
class="math display">\[E_{\alpha, \beta}(P) = \begin{cases}
\frac{1}{2^{1-\beta} - 1} \left( \sum_{i=1}^n p_i^\alpha - 1 \right)
&amp; \text{si } \alpha = \beta, \\
\frac{1}{2^{1-\beta} - 1} \left( \frac{\sum_{i=1}^n p_i^\alpha -
1}{\alpha} + \frac{2^{1-\beta} - \sum_{i=1}^n p_i^\alpha}{\beta} \right)
&amp; \text{sinon.}
\end{cases}\]</span></p>
</div>
<p>Cette définition peut être réécrite de manière plus compacte en
utilisant des quantificateurs :</p>
<p><span class="math display">\[E_{\alpha, \beta}(P) =
\frac{1}{2^{1-\beta} - 1} \left( \sum_{i=1}^n p_i^\alpha - 1 +
\frac{2^{1-\beta} - \sum_{i=1}^n p_i^\alpha}{\beta} \right) \quad
\text{pour } \alpha \neq 1, \beta &gt; 0.\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant l’entropie de Sharma-Taneja-Mittal
est la généralisation du principe d’incertitude de Shannon.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P = \{p_1, p_2, \ldots,
p_n\}\)</span> une distribution de probabilité discrète. Pour <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> avec <span
class="math inline">\(\alpha \neq 1\)</span> et <span
class="math inline">\(\beta &gt; 0\)</span>, l’entropie de
Sharma-Taneja-Mittal satisfait : <span class="math display">\[E_{\alpha,
\beta}(P) \geq 0,\]</span> avec égalité si et seulement si <span
class="math inline">\(P\)</span> est une distribution dégénérée,
c’est-à-dire <span class="math inline">\(p_i = 1\)</span> pour un
certain <span class="math inline">\(i\)</span> et <span
class="math inline">\(p_j = 0\)</span> pour tout <span
class="math inline">\(j \neq i\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous procédons par étapes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons d’abord le cas où <span
class="math inline">\(\alpha = \beta\)</span>. Nous avons : <span
class="math display">\[E_{\alpha, \beta}(P) = \frac{1}{2^{1-\alpha} - 1}
\left( \sum_{i=1}^n p_i^\alpha - 1 \right).\]</span> Puisque <span
class="math inline">\(p_i \geq 0\)</span> et <span
class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>, il est clair que
<span class="math inline">\(0 \leq p_i^\alpha \leq 1\)</span> pour tout
<span class="math inline">\(i\)</span>. Par conséquent, <span
class="math inline">\(\sum_{i=1}^n p_i^\alpha \leq 1\)</span>, et donc
<span class="math inline">\(E_{\alpha, \beta}(P) \geq 0\)</span>.</p>
<p>Pour le cas général où <span class="math inline">\(\alpha \neq
\beta\)</span>, nous avons : <span class="math display">\[E_{\alpha,
\beta}(P) = \frac{1}{2^{1-\beta} - 1} \left( \frac{\sum_{i=1}^n
p_i^\alpha - 1}{\alpha} + \frac{2^{1-\beta} - \sum_{i=1}^n
p_i^\alpha}{\beta} \right).\]</span> En utilisant l’inégalité de Jensen
et les propriétés des fonctions convexes, on peut montrer que cette
expression est également non négative. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de Sharma-Taneja-Mittal possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p><strong>Continuité</strong> : Pour tout <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> avec <span
class="math inline">\(\alpha \neq 1\)</span> et <span
class="math inline">\(\beta &gt; 0\)</span>, la fonction <span
class="math inline">\(E_{\alpha, \beta}(P)\)</span> est continue en
<span class="math inline">\(P\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La continuité découle du fait que les fonctions <span
class="math inline">\(p_i^\alpha\)</span> et <span
class="math inline">\(2^{1-\beta}\)</span> sont continues. ◻</p>
</div></li>
<li><p><strong>Symétrie</strong> : L’entropie STM est symétrique,
c’est-à-dire que pour toute permutation <span
class="math inline">\(\sigma\)</span> de <span
class="math inline">\(\{1, 2, \ldots, n\}\)</span>, <span
class="math display">\[E_{\alpha, \beta}(\{p_{\sigma(1)}, p_{\sigma(2)},
\ldots, p_{\sigma(n)}\}) = E_{\alpha, \beta}(P).\]</span></p>
<div class="proof">
<p><em>Proof.</em> La symétrie résulte du fait que la somme <span
class="math inline">\(\sum_{i=1}^n p_i^\alpha\)</span> est invariante
sous les permutations. ◻</p>
</div></li>
<li><p><strong>Normalisation</strong> : Pour toute distribution
dégénérée <span class="math inline">\(P\)</span>, nous avons <span
class="math inline">\(E_{\alpha, \beta}(P) = 0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(P\)</span> est
dégénérée, alors <span class="math inline">\(\sum_{i=1}^n p_i^\alpha =
1\)</span> et <span class="math inline">\(2^{1-\beta} - \sum_{i=1}^n
p_i^\alpha = 0\)</span>, ce qui implique <span
class="math inline">\(E_{\alpha, \beta}(P) = 0\)</span>. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Sharma-Taneja-Mittal offre une généralisation puissante
et flexible des mesures d’incertitude classiques. Sa capacité à moduler
la sensibilité via les paramètres <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span> en fait un outil précieux dans de
nombreux domaines. Les propriétés et théorèmes présentés ici ne sont que
la pointe de l’iceberg, et des recherches futures pourraient explorer
davantage ses applications et ses extensions.</p>
</body>
</html>
{% include "footer.html" %}


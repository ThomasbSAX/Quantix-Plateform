{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Différentielle Maximale : Un Concept Fondamental en Théorie de l’Information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Différentielle Maximale : Un Concept
Fondamental en Théorie de l’Information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie différentielle maximale émerge comme un concept central
dans la théorie de l’information, particulièrement dans le cadre des
processus continus. Inspirée par les travaux pionniers de Claude Shannon
sur l’entropie discrète, cette notion étend les principes fondamentaux
de l’information à des variables aléatoires continues. L’entropie
différentielle maximale est indispensable pour comprendre la limite
ultime de compression des données continues et pour évaluer
l’incertitude inhérente à des phénomènes physiques modélisés par des
distributions continues.</p>
<p>Cette notion trouve ses racines dans la nécessité de quantifier
l’information contenue dans des signaux analogiques, où les valeurs
peuvent prendre une infinité de configurations. En introduisant
l’entropie différentielle maximale, nous pouvons non seulement mesurer
cette information mais aussi optimiser les systèmes de communication et
de traitement du signal.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie différentielle maximale, commençons par
comprendre ce que nous cherchons à quantifier. Imaginons une variable
aléatoire continue <span class="math inline">\(X\)</span> prenant ses
valeurs dans un ensemble continu <span
class="math inline">\(\mathcal{X}\)</span>. Nous voulons mesurer
l’incertitude ou l’information contenue dans <span
class="math inline">\(X\)</span>, c’est-à-dire à quel point les valeurs
de <span class="math inline">\(X\)</span> sont imprévisibles.</p>
<p>Formellement, l’entropie différentielle maximale d’une variable
aléatoire continue <span class="math inline">\(X\)</span> avec une
densité de probabilité <span class="math inline">\(f(x)\)</span> est
définie comme :</p>
<p><span class="math display">\[H(X) = -\int_{\mathcal{X}} f(x) \log
f(x) \, dx\]</span></p>
<p>Cette intégrale mesure l’espérance de l’information contenue dans
<span class="math inline">\(X\)</span>. Plus précisément, si <span
class="math inline">\(f(x)\)</span> est la densité de probabilité de
<span class="math inline">\(X\)</span>, alors <span
class="math inline">\(H(X)\)</span> quantifie l’incertitude moyenne
associée à <span class="math inline">\(X\)</span>.</p>
<p>Une autre manière de formuler cette définition est la suivante : pour
toute densité de probabilité <span class="math inline">\(f(x)\)</span>,
l’entropie différentielle maximale est donnée par :</p>
<p><span class="math display">\[H(X) = -\mathbb{E} \left[ \log f(X)
\right]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}\)</span> désigne
l’espérance mathématique.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie différentielle maximale est
le théorème de l’entropie maximale, qui stipule que parmi toutes les
distributions de probabilité continues avec une variance donnée, la
distribution normale maximise l’entropie différentielle. Ce théorème est
crucial pour comprendre les limites de la compression des données
continues.</p>
<p>Pour énoncer ce théorème, supposons que <span
class="math inline">\(X\)</span> est une variable aléatoire continue
avec une variance fixe <span class="math inline">\(\sigma^2\)</span>. Le
théorème de l’entropie maximale affirme que :</p>
<p><span class="math display">\[H(X) \leq \frac{1}{2} \log (2 \pi e
\sigma^2)\]</span></p>
<p>avec égalité si et seulement si <span
class="math inline">\(X\)</span> suit une distribution normale de
moyenne <span class="math inline">\(\mu\)</span> et de variance <span
class="math inline">\(\sigma^2\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de l’entropie maximale, nous utilisons des
outils de l’analyse et de la théorie des probabilités. Commençons par
rappeler que pour toute densité de probabilité <span
class="math inline">\(f(x)\)</span>, l’entropie différentielle est
donnée par :</p>
<p><span class="math display">\[H(X) = -\int_{-\infty}^{\infty} f(x)
\log f(x) \, dx\]</span></p>
<p>Nous voulons maximiser <span class="math inline">\(H(X)\)</span> sous
la contrainte que l’espérance de <span class="math inline">\(X\)</span>
est <span class="math inline">\(\mu\)</span> et que sa variance est
<span class="math inline">\(\sigma^2\)</span>. Utilisons le formalisme
des multiplicateurs de Lagrange pour résoudre ce problème
d’optimisation.</p>
<p>Soit <span class="math inline">\(\mathcal{L}\)</span> le lagrangien
défini par :</p>
<p><span class="math display">\[\mathcal{L}(f, \lambda, \gamma) =
-\int_{-\infty}^{\infty} f(x) \log f(x) \, dx - \lambda \left(
\int_{-\infty}^{\infty} x f(x) \, dx - \mu \right) - \gamma \left(
\int_{-\infty}^{\infty} (x - \mu)^2 f(x) \, dx - \sigma^2
\right)\]</span></p>
<p>Pour maximiser <span class="math inline">\(H(X)\)</span>, nous
cherchons à trouver la densité <span class="math inline">\(f(x)\)</span>
qui maximise <span class="math inline">\(\mathcal{L}\)</span>. En
prenant la dérivée fonctionnelle de <span
class="math inline">\(\mathcal{L}\)</span> par rapport à <span
class="math inline">\(f(x)\)</span> et en égalisant à zéro, nous
obtenons :</p>
<p><span class="math display">\[- \log f(x) - 1 - \lambda x - \gamma (x
- \mu)^2 = 0\]</span></p>
<p>En résolvant cette équation, nous trouvons que :</p>
<p><span class="math display">\[f(x) = \exp \left( -1 - \lambda x -
\gamma (x - \mu)^2 \right)\]</span></p>
<p>En identifiant cette forme à celle d’une distribution normale, nous
concluons que <span class="math inline">\(f(x)\)</span> est une densité
gaussienne. Ainsi, la distribution qui maximise l’entropie
différentielle sous les contraintes données est bien la distribution
normale.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie différentielle maximale possède plusieurs propriétés
intéressantes, que nous énumérons et démontrons ci-dessous :</p>
<ol>
<li><p><strong>Additivité de l’entropie</strong> : Si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont deux variables aléatoires
indépendantes, alors l’entropie différentielle de leur produit <span
class="math inline">\((X, Y)\)</span> est la somme des entropies
différentielles de <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[H(X, Y) = H(X) + H(Y)\]</span></p>
<p><strong>Preuve</strong> : Par définition de l’entropie
différentielle, nous avons :</p>
<p><span class="math display">\[H(X, Y) = -\int_{\mathcal{X} \times
\mathcal{Y}} f_{X,Y}(x,y) \log f_{X,Y}(x,y) \, dx \, dy\]</span></p>
<p>Comme <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendants, <span
class="math inline">\(f_{X,Y}(x,y) = f_X(x) f_Y(y)\)</span>. Ainsi,</p>
<p><span class="math display">\[H(X, Y) = -\int_{\mathcal{X}} f_X(x)
\log f_X(x) \, dx - \int_{\mathcal{Y}} f_Y(y) \log f_Y(y) \, dy = H(X) +
H(Y)\]</span></p></li>
<li><p><strong>Inégalité de Gibbs</strong> : Pour toute densité de
probabilité <span class="math inline">\(f(x)\)</span>, l’entropie
différentielle est toujours non négative.</p>
<p><span class="math display">\[H(X) \geq 0\]</span></p>
<p><strong>Preuve</strong> : Considérons la fonction <span
class="math inline">\(g(t) = t \log t - t + 1\)</span>. Cette fonction
est convexe et atteint son minimum en <span class="math inline">\(t =
1\)</span> avec <span class="math inline">\(g(1) = 0\)</span>. Ainsi,
pour toute densité de probabilité <span
class="math inline">\(f(x)\)</span>, nous avons :</p>
<p><span class="math display">\[g(f(x)) = f(x) \log f(x) - f(x) + 1 \geq
0\]</span></p>
<p>En intégrant sur <span class="math inline">\(\mathcal{X}\)</span>,
nous obtenons :</p>
<p><span class="math display">\[\int_{\mathcal{X}} g(f(x)) \, dx = H(X)
+ 1 - 1 \geq 0\]</span></p>
<p>donc <span class="math inline">\(H(X) \geq 0\)</span>.</p></li>
<li><p><strong>Invariance par translation</strong> : L’entropie
différentielle est invariante par translation. Si <span
class="math inline">\(Y = X + c\)</span> pour une constante <span
class="math inline">\(c\)</span>, alors <span class="math inline">\(H(Y)
= H(X)\)</span>.</p>
<p><span class="math display">\[H(X + c) = H(X)\]</span></p>
<p><strong>Preuve</strong> : Soit <span class="math inline">\(Y = X +
c\)</span>. La densité de probabilité de <span
class="math inline">\(Y\)</span> est donnée par :</p>
<p><span class="math display">\[f_Y(y) = f_X(y - c)\]</span></p>
<p>Ainsi, l’entropie différentielle de <span
class="math inline">\(Y\)</span> est :</p>
<p><span class="math display">\[H(Y) = -\int_{\mathcal{Y}} f_Y(y) \log
f_Y(y) \, dy = -\int_{\mathcal{X}} f_X(x) \log f_X(x) \, dx =
H(X)\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie différentielle maximale est un concept fondamental en
théorie de l’information, offrant des outils puissants pour quantifier
et optimiser l’information dans les systèmes continus. À travers ses
propriétés et théorèmes, elle permet de comprendre les limites de la
compression des données et de modéliser l’incertitude inhérente aux
phénomènes physiques. Les démonstrations détaillées présentées dans cet
article illustrent la rigueur mathématique sous-jacente à ce concept,
soulignant son importance dans les applications pratiques et
théoriques.</p>
</body>
</html>
{% include "footer.html" %}


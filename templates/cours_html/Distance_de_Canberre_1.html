{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La Distance de Canberra : Une Mesure Métrique des Vecteurs</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La Distance de Canberra : Une Mesure Métrique des
Vecteurs</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Canberra, introduite par Llyod dans les années 1960,
est une mesure de dissimilarité entre deux vecteurs. Elle trouve son
origine dans le domaine de l’analyse des données et plus
particulièrement dans la classification automatique. Cette distance est
particulièrement utile lorsque les données présentent des échelles
différentes ou lorsque certaines dimensions sont plus pertinentes que
d’autres.</p>
<p>L’émergence de la distance de Canberra est motivée par le besoin de
mesurer la dissimilarité entre des objets décrits par plusieurs
caractéristiques, tout en tenant compte des différences d’échelle.
Contrairement à la distance euclidienne, qui est sensible aux variations
d’échelle, la distance de Canberra normalise implicitement chaque
dimension par rapport à la somme des valeurs absolues des deux vecteurs
considérés.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la distance de Canberra, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons deux vecteurs dans un espace
multidimensionnel. Nous voulons quantifier la dissimilarité entre ces
deux vecteurs de manière à ce que chaque dimension contribue
proportionnellement à la somme des valeurs absolues des deux
vecteurs.</p>
<p>Formellement, soit <span class="math inline">\(\mathbf{x} = (x_1,
x_2, \ldots, x_n)\)</span> et <span class="math inline">\(\mathbf{y} =
(y_1, y_2, \ldots, y_n)\)</span> deux vecteurs de <span
class="math inline">\(\mathbb{R}^n\)</span>. La distance de Canberra
entre <span class="math inline">\(\mathbf{x}\)</span> et <span
class="math inline">\(\mathbf{y}\)</span> est définie comme suit :</p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{y}) =
\sum_{i=1}^n \frac{|x_i - y_i|}{|x_i| + |y_i|}\]</span></p>
<p>Cette définition peut être réécrite en utilisant des quantificateurs
:</p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{y}) =
\sum_{i=1}^n \frac{\left| x_i - y_i \right|}{\max(|x_i|, |y_i|) +
\min(|x_i|, |y_i|)}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la distance de Canberra est le suivant
:</p>
<div class="theorem">
<p>La distance de Canberra satisfait les propriétés suivantes pour tout
<span class="math inline">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in
\mathbb{R}^n\)</span> :</p>
<ol>
<li><p><span class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) \geq
0\)</span> (Positivité)</p></li>
<li><p><span class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) =
0\)</span> si et seulement si <span class="math inline">\(\mathbf{x} =
\mathbf{y}\)</span> (Identité des indiscernables)</p></li>
<li><p><span class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) =
d_C(\mathbf{y}, \mathbf{x})\)</span> (Symétrie)</p></li>
<li><p><span class="math inline">\(d_C(\mathbf{x}, \mathbf{z}) \leq
d_C(\mathbf{x}, \mathbf{y}) + d_C(\mathbf{y}, \mathbf{z})\)</span>
(Inégalité triangulaire)</p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<div class="proof">
<p><em>Preuve de la Propriété Métrique.</em> Nous allons démontrer
chaque propriété une par une.</p>
<ol>
<li><p>Positivité : Par définition, <span class="math inline">\(|x_i -
y_i| \geq 0\)</span> et <span class="math inline">\(|x_i| + |y_i| &gt;
0\)</span> pour tout <span class="math inline">\(i\)</span>. Donc, <span
class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) \geq
0\)</span>.</p></li>
<li><p>Identité des indiscernables : Si <span
class="math inline">\(\mathbf{x} = \mathbf{y}\)</span>, alors <span
class="math inline">\(x_i = y_i\)</span> pour tout <span
class="math inline">\(i\)</span>, ce qui implique <span
class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) = 0\)</span>.
Réciproquement, si <span class="math inline">\(d_C(\mathbf{x},
\mathbf{y}) = 0\)</span>, alors <span class="math inline">\(|x_i - y_i|
= 0\)</span> pour tout <span class="math inline">\(i\)</span>, ce qui
implique <span class="math inline">\(x_i = y_i\)</span> pour tout <span
class="math inline">\(i\)</span>.</p></li>
<li><p>Symétrie : Par définition, <span
class="math inline">\(d_C(\mathbf{x}, \mathbf{y}) = d_C(\mathbf{y},
\mathbf{x})\)</span> car <span class="math inline">\(|x_i - y_i| = |y_i
- x_i|\)</span>.</p></li>
<li><p>Inégalité triangulaire : Nous devons montrer que <span
class="math inline">\(d_C(\mathbf{x}, \mathbf{z}) \leq d_C(\mathbf{x},
\mathbf{y}) + d_C(\mathbf{y}, \mathbf{z})\)</span>. Considérons chaque
terme de la somme :</p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{z}) =
\sum_{i=1}^n \frac{|x_i - z_i|}{|x_i| + |z_i|}\]</span></p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{y}) +
d_C(\mathbf{y}, \mathbf{z}) = \sum_{i=1}^n \left( \frac{|x_i -
y_i|}{|x_i| + |y_i|} + \frac{|y_i - z_i|}{|y_i| + |z_i|}
\right)\]</span></p>
<p>Il suffit de montrer que pour chaque <span
class="math inline">\(i\)</span>, <span class="math inline">\(\frac{|x_i
- z_i|}{|x_i| + |z_i|} \leq \frac{|x_i - y_i|}{|x_i| + |y_i|} +
\frac{|y_i - z_i|}{|y_i| + |z_i|}\)</span>. Cette inégalité découle de
l’inégalité triangulaire pour les nombres réels et des propriétés des
fractions.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<ol>
<li><p>Normalisation : La distance de Canberra est normalisée par la
somme des valeurs absolues des deux vecteurs, ce qui permet de comparer
des dimensions sur des échelles différentes.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\mathbf{x} = (x_1,
x_2, \ldots, x_n)\)</span> et <span class="math inline">\(\mathbf{y} =
(y_1, y_2, \ldots, y_n)\)</span>. La distance de Canberra est définie
comme :</p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{y}) =
\sum_{i=1}^n \frac{|x_i - y_i|}{|x_i| + |y_i|}\]</span></p>
<p>Chaque terme <span class="math inline">\(\frac{|x_i - y_i|}{|x_i| +
|y_i|}\)</span> est compris entre 0 et 1, ce qui garantit que la
distance globale est également comprise entre 0 et <span
class="math inline">\(n\)</span>. ◻</p>
</div></li>
<li><p>Sensibilité aux Échelles : La distance de Canberra est moins
sensible aux variations d’échelle que la distance euclidienne, ce qui la
rend particulièrement utile pour les données hétérogènes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux vecteurs <span
class="math inline">\(\mathbf{x}\)</span> et <span
class="math inline">\(\mathbf{y}\)</span> tels que <span
class="math inline">\(x_i = k y_i\)</span> pour une constante <span
class="math inline">\(k &gt; 0\)</span>. La distance de Canberra devient
:</p>
<p><span class="math display">\[d_C(\mathbf{x}, \mathbf{y}) =
\sum_{i=1}^n \frac{|k y_i - y_i|}{|k y_i| + |y_i|} = \sum_{i=1}^n
\frac{|k - 1|}{|k| + 1} |y_i|\]</span></p>
<p>Cette expression montre que la distance de Canberra est
proportionnelle à <span class="math inline">\(|k - 1|\)</span>, ce qui
indique une sensibilité modérée aux variations d’échelle. ◻</p>
</div></li>
<li><p>Robustesse aux Valeurs Aberrantes : La distance de Canberra est
robuste aux valeurs aberrantes car elle normalise chaque dimension par
la somme des valeurs absolues.</p>
<div class="proof">
<p><em>Proof.</em> Supposons qu’une dimension <span
class="math inline">\(i\)</span> présente une valeur aberrante,
c’est-à-dire que <span class="math inline">\(x_i\)</span> ou <span
class="math inline">\(y_i\)</span> est très grand par rapport aux autres
dimensions. Dans ce cas, le terme <span class="math inline">\(\frac{|x_i
- y_i|}{|x_i| + |y_i|}\)</span> sera proche de 0 ou 1, mais ne dominera
pas la somme globale car il est normalisé par <span
class="math inline">\(|x_i| + |y_i|\)</span>. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de Canberra est une mesure métrique puissante et flexible
pour quantifier la dissimilarité entre des vecteurs multidimensionnels.
Ses propriétés de normalisation, de sensibilité modérée aux échelles et
de robustesse aux valeurs aberrantes en font un outil précieux dans de
nombreuses applications, notamment en analyse des données et en
classification automatique. Bien que moins connue que la distance
euclidienne, elle offre des avantages significatifs dans les contextes
où les données présentent des échelles différentes ou des valeurs
aberrantes.</p>
</body>
</html>
{% include "footer.html" %}


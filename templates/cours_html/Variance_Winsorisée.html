{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Variance Winsorisée : Une Approche Robuste en Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Variance Winsorisée : Une Approche Robuste en
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La variance winsorisée émerge comme une alternative robuste à la
variance classique dans le contexte des données statistiques contenant
des valeurs aberrantes. L’idée fondamentale derrière cette notion est de
limiter l’impact des extrêmes en les remplaçant par des valeurs moins
extrêmes, tout en préservant l’information centrale. Cette technique est
particulièrement utile dans les domaines où la présence de valeurs
aberrantes peut fausser les analyses statistiques traditionnelles.</p>
<p>L’origine historique de la variance winsorisée remonte aux travaux de
Charles P. Winsor en 1932, qui a introduit cette méthode pour traiter
les données économiques. Depuis lors, la variance winsorisée a trouvé
des applications dans divers champs disciplinaires, notamment en
économétrie, en biostatistique et en sciences sociales.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la variance winsorisée, commençons par rappeler ce
qu’est une valeur aberrante. Une valeur aberrante est un point de
données qui s’écarte significativement des autres observations dans un
ensemble de données. La variance winsorisée vise à atténuer l’impact de
ces valeurs aberrantes.</p>
<p>Supposons que nous ayons un échantillon de données <span
class="math inline">\(X = \{x_1, x_2, \ldots, x_n\}\)</span>. Pour
winsoriser ces données, nous choisissons un pourcentage <span
class="math inline">\(\alpha\)</span> (généralement entre 5% et 20%) et
remplaçons les <span class="math inline">\(\alpha \times n\)</span>
valeurs les plus petites par la valeur minimale non aberrante et les
<span class="math inline">\(\alpha \times n\)</span> valeurs les plus
grandes par la valeur maximale non aberrante.</p>
<p>Formellement, la variance winsorisée <span
class="math inline">\(s^2_W\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_n\}\)</span> un échantillon de données et <span
class="math inline">\(\alpha \in (0, 1)\)</span> un pourcentage de
winsorisation. Les données winsorisées <span class="math inline">\(X_W =
\{x_{W1}, x_{W2}, \ldots, x_{Wn}\}\)</span> sont obtenues par : <span
class="math display">\[x_{Wi} =
\begin{cases}
x_{(k)} &amp; \text{si } i \leq k, \\
x_i &amp; \text{si } k &lt; i &lt; n - k + 1, \\
x_{(n - k + 1)} &amp; \text{si } i \geq n - k + 1,
\end{cases}\]</span> où <span class="math inline">\(k = \lfloor \alpha n
\rfloor\)</span> et <span class="math inline">\(x_{(1)} \leq x_{(2)}
\leq \ldots \leq x_{(n)}\)</span> sont les données triées.</p>
<p>La variance winsorisée est alors donnée par : <span
class="math display">\[s^2_W = \frac{1}{n} \sum_{i=1}^n (x_{Wi} -
\bar{x}_W)^2,\]</span> où <span class="math inline">\(\bar{x}_W =
\frac{1}{n} \sum_{i=1}^n x_{Wi}\)</span> est la moyenne des données
winsorisées.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la variance winsorisée est le
suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un échantillon aléatoire
issu d’une distribution avec une variance finie. La variance winsorisée
<span class="math inline">\(s^2_W\)</span> est un estimateur consistant
de la vraie variance de la distribution.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous utilisons le fait
que la winsorisation est une opération de troncature qui ne modifie pas
asymptotiquement les propriétés de l’estimateur. En effet, lorsque <span
class="math inline">\(n \to \infty\)</span>, la proportion des données
winsorisées tend vers zéro. Ainsi, par le théorème central limite et les
propriétés de la variance échantillonnaire, nous avons : <span
class="math display">\[s^2_W \xrightarrow{P} \sigma^2,\]</span> où <span
class="math inline">\(\sigma^2\)</span> est la vraie variance de la
distribution. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer les propriétés de la variance winsorisée, considérons
un exemple simple. Supposons que nous ayons un échantillon <span
class="math inline">\(X = \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}\)</span> et
que nous souhaitons winsoriser les données à <span
class="math inline">\(\alpha = 10\%\)</span>. Les valeurs aberrantes
sont alors les deux plus petites et les deux plus grandes valeurs.</p>
<p>Les données winsorisées sont : <span class="math display">\[X_W =
\{2, 2, 3, 4, 5, 6, 7, 8, 9, 9\}.\]</span></p>
<p>La moyenne des données winsorisées est : <span
class="math display">\[\bar{x}_W = \frac{2 + 2 + 3 + 4 + 5 + 6 + 7 + 8 +
9 + 9}{10} = 5.4.\]</span></p>
<p>La variance winsorisée est : <span class="math display">\[s^2_W =
\frac{(2-5.4)^2 + (2-5.4)^2 + (3-5.4)^2 + \ldots + (9-5.4)^2}{10} =
6.72.\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La variance winsorisée possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p>Robustesse : La variance winsorisée est moins sensible aux
valeurs aberrantes que la variance classique.</p></li>
<li><p>Continuité : La fonction de winsorisation est continue, ce qui
signifie que de petites modifications des données entraînent de petites
modifications de la variance winsorisée.</p></li>
<li><p>Invariance par translation : La variance winsorisée est
invariante par ajout d’une constante à toutes les observations.</p></li>
</ol>
<p>Pour démontrer la robustesse, considérons un échantillon avec une
valeur aberrante. La variance classique sera fortement influencée par
cette valeur, tandis que la variance winsorisée la remplacera par une
valeur moins extrême, réduisant ainsi son impact.</p>
<p>Pour démontrer la continuité, supposons que nous avons deux
échantillons <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> très proches. Les données winsorisées
correspondantes seront également très proches, et par conséquent, les
variances winsorisées le seront aussi.</p>
<p>Pour démontrer l’invariance par translation, supposons que nous
ajoutons une constante <span class="math inline">\(c\)</span> à toutes
les observations. Les données winsorisées deviennent <span
class="math inline">\(x_{Wi} + c\)</span>, et la moyenne des données
winsorisées devient <span class="math inline">\(\bar{x}_W + c\)</span>.
La variance winsorisée reste inchangée : <span
class="math display">\[s^2_W = \frac{1}{n} \sum_{i=1}^n (x_{Wi} + c -
(\bar{x}_W + c))^2 = \frac{1}{n} \sum_{i=1}^n (x_{Wi} -
\bar{x}_W)^2.\]</span></p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Méthode du sous-gradient : Une approche pour l’optimisation non-lisse</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Méthode du sous-gradient : Une approche pour
l’optimisation non-lisse</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’optimisation non-lisse est un domaine crucial en mathématiques
appliquées, notamment dans les problèmes d’optimisation où la fonction
objectif n’est pas différentiable partout. Parmi les méthodes les plus
efficaces pour traiter ces problèmes, la méthode du sous-gradient se
distingue par sa simplicité et son efficacité. Cette méthode a été
introduite pour la première fois par H. Frisch dans les années 1950 et a
depuis été largement étudiée et améliorée.</p>
<p>L’émergence de la méthode du sous-gradient est motivée par le besoin
de résoudre des problèmes d’optimisation où les fonctions objectives
sont non différentiables, comme dans les problèmes de minimax ou
d’optimisation convexe. Cette méthode est particulièrement utile lorsque
la fonction objectif est convexe mais non différentiable, ce qui est
souvent le cas dans les applications pratiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir la méthode du sous-gradient, il est essentiel de
comprendre ce qu’est un sous-gradient. Considérons une fonction convexe
<span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>. Pour un point <span class="math inline">\(x_0 \in
\mathbb{R}^n\)</span>, un sous-gradient de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span> est un vecteur <span
class="math inline">\(g \in \mathbb{R}^n\)</span> tel que pour tout
<span class="math inline">\(x \in \mathbb{R}^n\)</span>, l’inégalité
suivante soit satisfaite :</p>
<p><span class="math display">\[f(x) \geq f(x_0) + g^T (x -
x_0).\]</span></p>
<p>Cette inégalité est connue sous le nom d’inégalité de sous-gradient.
Elle généralise l’idée du gradient pour les fonctions différentiables,
où le gradient fournit une approximation linéaire de la fonction autour
du point <span class="math inline">\(x_0\)</span>.</p>
<p>Formellement, un sous-gradient de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span> est défini comme suit :</p>
<p><span class="math display">\[g \in \partial f(x_0),\]</span></p>
<p>où <span class="math inline">\(\partial f(x_0)\)</span> est le
sous-différentiel de <span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span>, défini par :</p>
<p><span class="math display">\[\partial f(x_0) = \{ g \in \mathbb{R}^n
\mid \forall x \in \mathbb{R}^n, f(x) \geq f(x_0) + g^T (x - x_0)
\}.\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux concernant la méthode du sous-gradient
est le théorème de convergence. Ce théorème garantit que, sous certaines
conditions, la méthode du sous-gradient converge vers une solution
optimale.</p>
<p>Supposons que <span class="math inline">\(f\)</span> soit une
fonction convexe et coercive, et que <span
class="math inline">\(x^*\)</span> soit un point optimal de <span
class="math inline">\(f\)</span>. Alors, pour toute suite <span
class="math inline">\(\{x^k\}\)</span> générée par la méthode du
sous-gradient avec un pas de descente approprié, on a :</p>
<p><span class="math display">\[\lim_{k \rightarrow \infty} f(x^k) =
f(x^*).\]</span></p>
<p>Ce théorème est crucial car il assure que la méthode du sous-gradient
peut effectivement trouver une solution optimale pour des problèmes
d’optimisation non-lisses.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de convergence, nous devons d’abord établir
quelques propriétés fondamentales de la méthode du sous-gradient.
Considérons l’algorithme de la méthode du sous-gradient :</p>
<p><span class="math display">\[x^{k+1} = x^k - \alpha_k
g^k,\]</span></p>
<p>où <span class="math inline">\(g^k\)</span> est un sous-gradient de
<span class="math inline">\(f\)</span> en <span
class="math inline">\(x^k\)</span>, et <span
class="math inline">\(\alpha_k\)</span> est un pas de descente.</p>
<p>Pour que cet algorithme converge, il est essentiel de choisir un pas
de descente approprié. Une condition couramment utilisée est que <span
class="math inline">\(\alpha_k\)</span> soit une suite de nombres
positifs tels que :</p>
<p><span class="math display">\[\sum_{k=0}^{\infty} \alpha_k = \infty
\quad \text{et} \quad \sum_{k=0}^{\infty} \alpha_k^2 &lt;
\infty.\]</span></p>
<p>Cette condition garantit que le pas de descente est suffisamment
grand pour explorer l’espace des solutions, mais suffisamment petit pour
assurer la convergence.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La méthode du sous-gradient possède plusieurs propriétés
intéressantes qui en font une méthode puissante pour l’optimisation
non-lisse. Voici quelques-unes de ces propriétés :</p>
<ol>
<li><p>**Convergence vers un point optimal** : Sous les conditions
appropriées, la méthode du sous-gradient converge vers un point optimal
de la fonction objectif.</p></li>
<li><p>**Robustesse** : La méthode du sous-gradient est robuste aux
choix des paramètres, ce qui la rend adaptable à une large gamme de
problèmes d’optimisation.</p></li>
<li><p>**Simplicité** : La méthode du sous-gradient est relativement
simple à implémenter, ce qui en fait une méthode accessible pour les
problèmes d’optimisation non-lisses.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
principes fondamentaux de l’optimisation convexe et les théorèmes de
convergence des méthodes itératives.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La méthode du sous-gradient est une approche puissante et efficace
pour l’optimisation non-lisse. Son efficacité, sa simplicité et sa
robustesse en font une méthode de choix pour résoudre des problèmes
d’optimisation où la fonction objectif est convexe mais non
différentiable. Les théorèmes de convergence et les propriétés de la
méthode du sous-gradient garantissent qu’elle peut effectivement trouver
des solutions optimales pour une large gamme de problèmes
d’optimisation.</p>
</body>
</html>
{% include "footer.html" %}


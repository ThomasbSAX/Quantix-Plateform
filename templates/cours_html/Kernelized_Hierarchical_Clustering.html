{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Hierarchical Clustering: An In-Depth Exploration</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Hierarchical Clustering: An In-Depth
Exploration</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Hierarchical clustering is a fundamental technique in unsupervised
learning, offering a rich and interpretable way to organize data into
nested clusters. Traditionally, hierarchical clustering methods operate
on the raw feature space of the data, which can be limiting when dealing
with complex, non-linear relationships. The introduction of kernel
methods has revolutionized this field by enabling the application of
hierarchical clustering in high-dimensional, non-linear spaces through
the use of kernel functions.</p>
<p>Kernelized Hierarchical Clustering (KHC) combines the strengths of
kernel methods and hierarchical clustering, allowing for the discovery
of intricate patterns in data that would otherwise remain hidden. This
approach is particularly indispensable in domains such as
bioinformatics, image analysis, and natural language processing, where
data often exhibits non-linear structures.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Before delving into the formal definitions, let us consider what we
aim to achieve with Kernelized Hierarchical Clustering. We want a method
that can capture the inherent structure of data in a high-dimensional
space, even when the relationships between data points are non-linear.
This necessitates a way to measure similarities or distances between
data points that goes beyond simple Euclidean distance.</p>
<h2 class="unnumbered" id="kernel-function">Kernel Function</h2>
<p>A kernel function is a mathematical tool that computes the inner
product of two vectors in some (possibly high-dimensional) feature
space. Formally, a kernel function <span
class="math inline">\(k\)</span> is defined as:</p>
<p><span class="math display">\[k: \mathcal{X} \times \mathcal{X}
\rightarrow \mathbb{R}\]</span></p>
<p>such that for any <span class="math inline">\(x, y \in
\mathcal{X}\)</span>, there exists a feature map <span
class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span>
into a Hilbert space <span class="math inline">\(\mathcal{H}\)</span>,
such that:</p>
<p><span class="math display">\[k(x, y) = \langle \phi(x), \phi(y)
\rangle_{\mathcal{H}}\]</span></p>
<p>This means that the kernel function implicitly maps the data into a
higher-dimensional space where linear methods can be applied.</p>
<h2 class="unnumbered" id="hierarchical-clustering">Hierarchical
Clustering</h2>
<p>Hierarchical clustering is a method of cluster analysis which seeks
to build a hierarchy of clusters. There are two main types:
agglomerative (bottom-up) and divisive (top-down). In agglomerative
hierarchical clustering, each data point starts in its own cluster, and
pairs of clusters are merged as one moves up the hierarchy.</p>
<p>Formally, given a set of data points <span
class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span>, hierarchical
clustering produces a dendrogram <span class="math inline">\(T\)</span>
which is a tree such that:</p>
<p><span class="math display">\[\forall i, j \in \{1, 2, \ldots, n\},
\quad d(x_i, x_j) = h(T(i), T(j))\]</span></p>
<p>where <span class="math inline">\(d\)</span> is a distance metric and
<span class="math inline">\(h\)</span> is the height of the least common
ancestor of <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span> in <span
class="math inline">\(T\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="kernelized-hierarchical-clustering-algorithm">Kernelized
Hierarchical Clustering Algorithm</h2>
<p>The kernelized hierarchical clustering algorithm can be outlined as
follows. Given a set of data points <span class="math inline">\(\{x_1,
x_2, \ldots, x_n\}\)</span> and a kernel function <span
class="math inline">\(k\)</span>, the algorithm proceeds by:</p>
<p>1. Computing the Gram matrix <span class="math inline">\(K\)</span>
where <span class="math inline">\(K_{ij} = k(x_i, x_j)\)</span>. 2.
Converting the Gram matrix into a distance matrix <span
class="math inline">\(D\)</span> using an appropriate transformation,
such as <span class="math inline">\(D_{ij} = \sqrt{k(x_i, x_i) + k(x_j,
x_j) - 2k(x_i, x_j)}\)</span>. 3. Applying a standard hierarchical
clustering algorithm (e.g., single linkage, complete linkage, or average
linkage) to the distance matrix <span
class="math inline">\(D\)</span>.</p>
<p>The key theorem here is that the resulting dendrogram captures the
structure of the data in the feature space induced by the kernel
function.</p>
<h2 class="unnumbered"
id="theorem-consistency-of-kernelized-hierarchical-clustering">Theorem:
Consistency of Kernelized Hierarchical Clustering</h2>
<p>Assume that the kernel function <span
class="math inline">\(k\)</span> is continuous and positive definite.
Then, the kernelized hierarchical clustering algorithm is consistent in
the sense that as the number of data points <span
class="math inline">\(n\)</span> goes to infinity, the resulting
dendrogram converges to the true hierarchy of the data in the feature
space.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<h2 class="unnumbered" id="proof-of-consistency-theorem">Proof of
Consistency Theorem</h2>
<p>To prove the consistency of kernelized hierarchical clustering, we
need to show that the distance matrix <span
class="math inline">\(D\)</span> derived from the Gram matrix <span
class="math inline">\(K\)</span> accurately reflects the distances in
the feature space.</p>
<p>1. **Gram Matrix Construction**: The Gram matrix <span
class="math inline">\(K\)</span> is constructed such that <span
class="math inline">\(K_{ij} = k(x_i, x_j) = \langle \phi(x_i),
\phi(x_j) \rangle_{\mathcal{H}}\)</span>. This matrix is positive
semi-definite by construction.</p>
<p>2. **Distance Matrix Transformation**: The distance matrix <span
class="math inline">\(D\)</span> is derived from <span
class="math inline">\(K\)</span> using the transformation <span
class="math inline">\(D_{ij} = \sqrt{k(x_i, x_i) + k(x_j, x_j) - 2k(x_i,
x_j)}\)</span>. This transformation ensures that <span
class="math inline">\(D\)</span> is a valid distance matrix, as it
satisfies the properties of non-negativity, identity of indiscernibles,
symmetry, and the triangle inequality.</p>
<p>3. **Convergence of Dendrogram**: As <span
class="math inline">\(n\)</span> increases, the Gram matrix <span
class="math inline">\(K\)</span> becomes a better approximation of the
true inner products in the feature space. Consequently, the distance
matrix <span class="math inline">\(D\)</span> becomes a better
approximation of the true distances in the feature space. By the
consistency of hierarchical clustering algorithms, the resulting
dendrogram converges to the true hierarchy.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-i-robustness-to-non-linearity">Propriété (i): Robustness
to Non-Linearity</h2>
<p>Kernelized hierarchical clustering is robust to non-linear
relationships in the data. This is because the kernel function
implicitly maps the data into a higher-dimensional space where linear
relationships can be captured.</p>
<h2 class="unnumbered" id="propriété-ii-interpretability">Propriété
(ii): Interpretability</h2>
<p>The dendrogram produced by hierarchical clustering provides an
interpretable visualization of the data structure. This interpretability
is preserved in the kernelized version, making it a powerful tool for
exploratory data analysis.</p>
<h2 class="unnumbered" id="propriété-iii-scalability">Propriété (iii):
Scalability</h2>
<p>While hierarchical clustering can be computationally intensive,
kernelized versions can be optimized using techniques such as low-rank
approximations of the Gram matrix. This makes the method scalable to
large datasets.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Kernelized Hierarchical Clustering represents a powerful fusion of
kernel methods and hierarchical clustering, enabling the discovery of
complex patterns in high-dimensional data. Its theoretical foundations
are solid, and its practical applications are vast. As the field of
machine learning continues to evolve, KHC will undoubtedly play a
crucial role in unraveling the intricacies of real-world datasets.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Courbe de précision-rappel : Une analyse approfondie</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Courbe de précision-rappel : Une analyse
approfondie</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’évaluation des modèles de classification est une tâche cruciale en
apprentissage automatique. Parmi les métriques les plus utilisées, la
précision et le rappel jouent un rôle central. La courbe de
précision-rappel est un outil graphique qui permet de visualiser le
compromis entre ces deux métriques. Cette courbe est particulièrement
utile dans les cas où la classe positive est rare, c’est-à-dire lorsque
le déséquilibre des classes est important.</p>
<p>La précision et le rappel sont deux métriques qui mesurent la
performance d’un modèle de classification binaire. La précision est
définie comme le rapport entre le nombre de vrais positifs et le nombre
total de prédictions positives. Le rappel, quant à lui, est défini comme
le rapport entre le nombre de vrais positifs et le nombre total
d’exemples positifs. Ces deux métriques sont souvent en conflit :
améliorer l’une peut entraîner une dégradation de l’autre.</p>
<p>La courbe de précision-rappel permet de visualiser ce compromis. Elle
est tracée en fonction du seuil de classification, qui détermine quelle
prédiction est considérée comme positive. En variant ce seuil, on
obtient différentes valeurs de précision et de rappel, qui sont ensuite
tracées sur un graphique.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement la courbe de précision-rappel, il est
important de comprendre les concepts de base.</p>
<h2 id="précision-et-rappel">Précision et Rappel</h2>
<p>Considérons un modèle de classification binaire qui prédit une classe
positive ou négative pour chaque exemple. Soient :</p>
<ul>
<li><p><span class="math inline">\(TP\)</span> le nombre de vrais
positifs,</p></li>
<li><p><span class="math inline">\(FP\)</span> le nombre de faux
positifs,</p></li>
<li><p><span class="math inline">\(FN\)</span> le nombre de faux
négatifs.</p></li>
</ul>
<p>La précision est définie comme le rapport entre le nombre de vrais
positifs et le nombre total de prédictions positives : <span
class="math display">\[\text{Précision} = \frac{TP}{TP +
FP}\]</span></p>
<p>Le rappel est défini comme le rapport entre le nombre de vrais
positifs et le nombre total d’exemples positifs : <span
class="math display">\[\text{Rappel} = \frac{TP}{TP + FN}\]</span></p>
<h2 id="courbe-de-précision-rappel">Courbe de Précision-Rappel</h2>
<p>La courbe de précision-rappel est tracée en fonction du seuil de
classification. Pour chaque seuil <span
class="math inline">\(t\)</span>, on calcule la précision et le rappel
correspondants.</p>
<p>Formellement, soit <span class="math inline">\(\mathcal{D}\)</span>
un ensemble de données de test, et <span
class="math inline">\(f\)</span> une fonction de score qui assigne à
chaque exemple un score réel. On définit la courbe de précision-rappel
comme suit : <span class="math display">\[\text{Précision}(t) =
\frac{\sum_{i=1}^{N} \mathbb{I}(f(x_i) \geq t \text{ et } y_i =
1)}{\sum_{i=1}^{N} \mathbb{I}(f(x_i) \geq t)}\]</span> <span
class="math display">\[\text{Rappel}(t) = \frac{\sum_{i=1}^{N}
\mathbb{I}(f(x_i) \geq t \text{ et } y_i = 1)}{\sum_{i=1}^{N}
\mathbb{I}(y_i = 1)}\]</span> où <span
class="math inline">\(\mathbb{I}\)</span> est la fonction indicatrice,
<span class="math inline">\(N\)</span> est le nombre total d’exemples
dans <span class="math inline">\(\mathcal{D}\)</span>, <span
class="math inline">\(x_i\)</span> est le <span
class="math inline">\(i\)</span>-ème exemple, et <span
class="math inline">\(y_i\)</span> est la vraie classe de l’exemple
<span class="math inline">\(x_i\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-courbe-de-précision-rappel">Théorème de la Courbe
de Précision-Rappel</h2>
<p>Un théorème important concernant la courbe de précision-rappel est le
suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D}\)</span> un ensemble de
données de test, et <span class="math inline">\(f\)</span> une fonction
de score. La courbe de précision-rappel est une fonction décroissante du
seuil <span class="math inline">\(t\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver ce théorème, nous devons montrer que
pour tout <span class="math inline">\(t_1 &lt; t_2\)</span>, on a <span
class="math inline">\(\text{Précision}(t_1) \leq
\text{Précision}(t_2)\)</span> et <span
class="math inline">\(\text{Rappel}(t_1) \geq
\text{Rappel}(t_2)\)</span>.</p>
<p>Considérons deux seuils <span class="math inline">\(t_1\)</span> et
<span class="math inline">\(t_2\)</span> tels que <span
class="math inline">\(t_1 &lt; t_2\)</span>. Pour tout exemple <span
class="math inline">\(x_i\)</span>, si <span
class="math inline">\(f(x_i) \geq t_2\)</span>, alors <span
class="math inline">\(f(x_i) \geq t_1\)</span>. Par conséquent,
l’ensemble des exemples pour lesquels <span class="math inline">\(f(x_i)
\geq t_2\)</span> est un sous-ensemble de l’ensemble des exemples pour
lesquels <span class="math inline">\(f(x_i) \geq t_1\)</span>.</p>
<p>Cela implique que le nombre de vrais positifs pour <span
class="math inline">\(t_2\)</span> est inférieur ou égal au nombre de
vrais positifs pour <span class="math inline">\(t_1\)</span>, et que le
nombre total de prédictions positives pour <span
class="math inline">\(t_2\)</span> est inférieur ou égal au nombre total
de prédictions positives pour <span
class="math inline">\(t_1\)</span>.</p>
<p>Par conséquent, on a : <span
class="math display">\[\text{Précision}(t_1) = \frac{TP(t_1)}{TP(t_1) +
FP(t_1)} \leq \frac{TP(t_2)}{TP(t_2) + FP(t_2)} =
\text{Précision}(t_2)\]</span> et <span
class="math display">\[\text{Rappel}(t_1) = \frac{TP(t_1)}{TP(t_1) +
FN(t_1)} \geq \frac{TP(t_2)}{TP(t_2) + FN(t_2)} =
\text{Rappel}(t_2)\]</span> ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-la-courbe-de-précision-rappel">Preuve du
Théorème de la Courbe de Précision-Rappel</h2>
<p>La preuve du théorème de la courbe de précision-rappel repose sur
l’idée que lorsque le seuil augmente, l’ensemble des exemples considérés
comme positifs se réduit. Cela entraîne une diminution du nombre de faux
positifs, ce qui améliore la précision, mais aussi une diminution du
nombre de vrais positifs, ce qui réduit le rappel.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-de-la-courbe-de-précision-rappel">Propriétés de la
Courbe de Précision-Rappel</h2>
<ol>
<li><p>La courbe de précision-rappel est toujours située entre les axes
de précision et de rappel.</p></li>
<li><p>La courbe de précision-rappel est une fonction décroissante du
seuil <span class="math inline">\(t\)</span>.</p></li>
<li><p>La surface sous la courbe de précision-rappel (AUC-PR) est une
métrique globale qui permet de comparer les performances de différents
modèles.</p></li>
</ol>
<h2 id="corollaires">Corollaires</h2>
<div class="corollary">
<p>La surface sous la courbe de précision-rappel (AUC-PR) est une
métrique robuste pour évaluer les performances des modèles de
classification binaire, en particulier dans les cas où la classe
positive est rare.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La surface sous la courbe de précision-rappel
(AUC-PR) est une métrique globale qui prend en compte toutes les valeurs
de précision et de rappel pour différents seuils. Elle est
particulièrement utile dans les cas où la classe positive est rare, car
elle donne plus de poids aux performances du modèle sur les exemples
positifs.</p>
<p>Pour prouver ce corollaire, nous devons montrer que l’AUC-PR est une
métrique robuste pour évaluer les performances des modèles de
classification binaire. Cela peut être fait en montrant que l’AUC-PR est
corrélée avec d’autres métriques de performance, telles que la précision
et le rappel, et en montrant qu’elle est insensible aux déséquilibres
des classes. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La courbe de précision-rappel est un outil puissant pour évaluer les
performances des modèles de classification binaire. Elle permet de
visualiser le compromis entre la précision et le rappel, et de comparer
les performances de différents modèles. La surface sous la courbe de
précision-rappel (AUC-PR) est une métrique globale qui permet de résumer
les performances du modèle en un seul nombre.</p>
</body>
</html>
{% include "footer.html" %}


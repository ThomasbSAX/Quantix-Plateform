{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage par Extraction de Caractéristiques de Kurtosis</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage par Extraction de Caractéristiques de
Kurtosis</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de kurtosis est une
technique avancée en traitement du signal et analyse statistique qui
permet d’extraire des informations pertinentes à partir de données
souvent bruitées ou complexes. Le kurtosis, ou l’aplatissement, est une
mesure de la forme de la distribution d’une variable aléatoire. Il
quantifie la présence de queues lourdes par rapport à une distribution
normale.</p>
<p>Cette méthode trouve son origine dans le besoin croissant de traiter
des données de plus en plus volumineuses et complexes, où les techniques
traditionnelles d’analyse peuvent s’avérer insuffisantes. Le kurtosis
offre une approche robuste pour identifier des structures cachées dans
les données, en particulier lorsque celles-ci présentent des
comportements non gaussiens.</p>
<p>L’importance de cette technique réside dans sa capacité à détecter
des anomalies, des événements rares ou des structures sous-jacentes qui
pourraient être masquées par le bruit. Elle est particulièrement utile
dans des domaines tels que la finance, la biologie et l’ingénierie, où
la compréhension des distributions de données est cruciale.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails de l’encodage par extraction de
caractéristiques de kurtosis, il est essentiel de comprendre ce qu’est
le kurtosis et comment il peut être utilisé pour extraire des
informations.</p>
<h2 id="le-kurtosis">Le Kurtosis</h2>
<p>Considérons une variable aléatoire <span
class="math inline">\(X\)</span> avec une espérance <span
class="math inline">\(\mu\)</span> et une variance <span
class="math inline">\(\sigma^2\)</span>. Nous cherchons à mesurer la
forme de sa distribution, en particulier la présence de queues lourdes
par rapport à une distribution normale.</p>
<h3 id="définition-formelle">Définition Formelle</h3>
<p>Le kurtosis d’une variable aléatoire <span
class="math inline">\(X\)</span> est défini comme le quatrième moment
centré normalisé :</p>
<p><span class="math display">\[\text{Kurtosis}(X) = \frac{\mathbb{E}[(X
- \mu)^4]}{(\sigma^2)^2} - 3\]</span></p>
<p>Cette définition peut être réécrite en utilisant des quantificateurs
:</p>
<p><span class="math display">\[\text{Kurtosis}(X) =
\frac{\int_{-\infty}^{\infty} (x - \mu)^4 f_X(x) \, dx}{\left(
\int_{-\infty}^{\infty} (x - \mu)^2 f_X(x) \, dx \right)^2} -
3\]</span></p>
<p>où <span class="math inline">\(f_X(x)\)</span> est la fonction de
densité de probabilité de <span class="math inline">\(X\)</span>.</p>
<h3 id="interprétation">Interprétation</h3>
<p>Le kurtosis mesure la présence de queues lourdes dans la distribution
de <span class="math inline">\(X\)</span>. Une valeur positive indique
une distribution avec des queues plus lourdes que celles d’une
distribution normale, tandis qu’une valeur négative indique une
distribution avec des queues plus légères.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-limite-centrale-pour-le-kurtosis">Théorème de la
Limite Centrale pour le Kurtosis</h2>
<p>Le théorème de la limite centrale nous dit que la somme de variables
aléatoires indépendantes et identiquement distribuées (i.i.d.) tend vers
une distribution normale lorsque le nombre de variables tend vers
l’infini. Cependant, ce théorème ne s’applique pas directement au
kurtosis.</p>
<h3 id="formulation">Formulation</h3>
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des
variables aléatoires i.i.d. avec un kurtosis <span
class="math inline">\(\kappa\)</span>. Alors, le kurtosis de la somme
<span class="math inline">\(S_n = X_1 + X_2 + \ldots + X_n\)</span> est
donné par :</p>
<p><span class="math display">\[\text{Kurtosis}(S_n) = \frac{\kappa}{n}
+ 3 \left(1 - \frac{1}{n}\right)\]</span></p>
<h3 id="démonstration">Démonstration</h3>
<p>La démonstration de ce théorème repose sur le calcul des moments
centrés de la somme <span class="math inline">\(S_n\)</span>. En
utilisant les propriétés des moments et des quantificateurs, nous
pouvons montrer que :</p>
<p><span class="math display">\[\mathbb{E}[(S_n - \mu)^4] = n^3
\mathbb{E}[(X_1 - \mu)^4] + 3n^2(n-1)(\mathbb{E}[(X_1 -
\mu)^2])^2\]</span></p>
<p>En normalisant par la variance au carré, nous obtenons :</p>
<p><span class="math display">\[\text{Kurtosis}(S_n) = \frac{n^3
\mathbb{E}[(X_1 - \mu)^4]}{n^2 (\mathbb{E}[(X_1 - \mu)^2])^2} + 3
\frac{n^2(n-1)}{n^2}\]</span></p>
<p>En simplifiant, nous trouvons :</p>
<p><span class="math display">\[\text{Kurtosis}(S_n) = \frac{\kappa}{n}
+ 3 \left(1 - \frac{1}{n}\right)\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2
id="preuve-du-théorème-de-la-limite-centrale-pour-le-kurtosis">Preuve du
Théorème de la Limite Centrale pour le Kurtosis</h2>
<p>Pour prouver ce théorème, nous devons calculer les moments centrés de
la somme <span class="math inline">\(S_n\)</span>. En utilisant les
propriétés des moments, nous avons :</p>
<p><span class="math display">\[\mathbb{E}[(S_n - \mu)^4] = n^3
\mathbb{E}[(X_1 - \mu)^4] + 3n^2(n-1)(\mathbb{E}[(X_1 -
\mu)^2])^2\]</span></p>
<p>En normalisant par la variance au carré, nous obtenons :</p>
<p><span class="math display">\[\text{Kurtosis}(S_n) = \frac{n^3
\mathbb{E}[(X_1 - \mu)^4]}{n^2 (\mathbb{E}[(X_1 - \mu)^2])^2} + 3
\frac{n^2(n-1)}{n^2}\]</span></p>
<p>En simplifiant, nous trouvons :</p>
<p><span class="math display">\[\text{Kurtosis}(S_n) = \frac{\kappa}{n}
+ 3 \left(1 - \frac{1}{n}\right)\]</span></p>
<p>Cette preuve montre que le kurtosis de la somme <span
class="math inline">\(S_n\)</span> dépend du kurtosis des variables
individuelles et du nombre de variables.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-1-kurtosis-des-variables-indépendantes">Propriété 1:
Kurtosis des Variables Indépendantes</h2>
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires
indépendantes. Alors, le kurtosis de la somme <span
class="math inline">\(X + Y\)</span> est donné par :</p>
<p><span class="math display">\[\text{Kurtosis}(X + Y) =
\frac{\text{Kurtosis}(X) (\sigma_Y^2)^2 + \text{Kurtosis}(Y)
(\sigma_X^2)^2}{(\sigma_X^2 + \sigma_Y^2)^2}\]</span></p>
<h3 id="preuve">Preuve</h3>
<p>La preuve de cette propriété repose sur le calcul des moments centrés
de la somme <span class="math inline">\(X + Y\)</span>. En utilisant les
propriétés des moments et des quantificateurs, nous pouvons montrer que
:</p>
<p><span class="math display">\[\mathbb{E}[(X + Y - \mu_{X+Y})^4] =
\mathbb{E}[(X - \mu_X)^4] + \mathbb{E}[(Y - \mu_Y)^4] + 3(\sigma_X^2 +
\sigma_Y^2)^2\]</span></p>
<p>En normalisant par la variance au carré, nous obtenons :</p>
<p><span class="math display">\[\text{Kurtosis}(X + Y) =
\frac{\text{Kurtosis}(X) (\sigma_Y^2)^2 + \text{Kurtosis}(Y)
(\sigma_X^2)^2}{(\sigma_X^2 + \sigma_Y^2)^2}\]</span></p>
<h2
id="propriété-2-kurtosis-des-variables-linéairement-transformées">Propriété
2: Kurtosis des Variables Linéairement Transformées</h2>
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire et
<span class="math inline">\(a, b\)</span> des constantes réelles. Alors,
le kurtosis de la variable transformée <span class="math inline">\(aX +
b\)</span> est donné par :</p>
<p><span class="math display">\[\text{Kurtosis}(aX + b) =
\text{Kurtosis}(X)\]</span></p>
<h3 id="preuve-1">Preuve</h3>
<p>La preuve de cette propriété repose sur le fait que les
transformations linéaires ne changent pas la forme de la distribution.
En utilisant les propriétés des moments et des quantificateurs, nous
pouvons montrer que :</p>
<p><span class="math display">\[\mathbb{E}[(aX + b - \mu_{aX+b})^4] =
a^4 \mathbb{E}[(X - \mu_X)^4]\]</span></p>
<p>En normalisant par la variance au carré, nous obtenons :</p>
<p><span class="math display">\[\text{Kurtosis}(aX + b) =
\text{Kurtosis}(X)\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de kurtosis est une
technique puissante pour analyser des données complexes et bruitées. En
comprenant le kurtosis et ses propriétés, nous pouvons extraire des
informations pertinentes et détecter des structures cachées dans les
données. Les théorèmes et propriétés présentés dans cet article
fournissent une base solide pour l’application de cette technique dans
divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


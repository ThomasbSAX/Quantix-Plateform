{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie d’échantillon : Une mesure de la complexité des séries temporelles</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie d’échantillon : Une mesure de la complexité
des séries temporelles</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie d’échantillon (Sample Entropy, ou SampEn) est une mesure
de la complexité des séries temporelles qui a été introduite par Richard
C. Richards en 2000. Cette notion émerge dans le contexte de l’analyse
des systèmes dynamiques et de la théorie du signal, où il est crucial de
quantifier le degré d’irrégularité ou de prévisibilité d’une série
temporelle.</p>
<p>L’entropie d’échantillon est particulièrement utile dans des domaines
tels que la biomédecine, où elle permet de distinguer entre des signaux
sains et pathologiques. Par exemple, l’analyse de
l’électroencéphalogramme (EEG) ou de l’électrocardiogramme (ECG) peut
bénéficier de cette mesure pour identifier des anomalies ou des
changements dans l’état du patient.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir l’entropie d’échantillon, nous devons d’abord comprendre
ce que nous cherchons à mesurer. Imaginons une série temporelle <span
class="math inline">\(X = \{x_1, x_2, \ldots, x_N\}\)</span>. Nous
voulons quantifier à quel point cette série est prévisible ou
répétitive. Plus précisément, nous cherchons une mesure qui capture la
probabilité que des motifs similaires se répètent dans la série.</p>
<p>Formellement, l’entropie d’échantillon est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_N\}\)</span> une série temporelle de longueur <span
class="math inline">\(N\)</span>, et soit <span
class="math inline">\(m\)</span> un entier positif représentant la
longueur des motifs à considérer. Soit également <span
class="math inline">\(r &gt; 0\)</span> un seuil de tolérance.</p>
<p>Pour chaque <span class="math inline">\(i\)</span> et <span
class="math inline">\(j\)</span> tels que <span class="math inline">\(1
\leq i, j \leq N - m\)</span>, nous définissons la distance entre les
motifs <span class="math inline">\(X_i^m = \{x_i, x_{i+1}, \ldots,
x_{i+m-1}\}\)</span> et <span class="math inline">\(X_j^m = \{x_j,
x_{j+1}, \ldots, x_{j+m-1}\}\)</span> comme la distance maximale entre
leurs composantes :</p>
<p><span class="math display">\[d[X_i^m, X_j^m] = \max_{0 \leq k &lt; m}
(|x_{i+k} - x_{j+k}|)\]</span></p>
<p>Nous disons que <span class="math inline">\(X_i^m\)</span> et <span
class="math inline">\(X_j^m\)</span> sont similaires si <span
class="math inline">\(d[X_i^m, X_j^m] &lt; r\)</span>.</p>
</div>
<p>L’entropie d’échantillon est alors définie comme :</p>
<p><span class="math display">\[\text{SampEn}(m, r, N) = -\ln \left(
\frac{A^m(r)}{B^{m-1}(r)} \right)\]</span></p>
<p>où <span class="math inline">\(A^m(r)\)</span> est le nombre de
paires de motifs similaires de longueur <span
class="math inline">\(m\)</span>, et <span
class="math inline">\(B^{m-1}(r)\)</span> est le nombre de paires de
motifs similaires de longueur <span
class="math inline">\(m-1\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour mieux comprendre l’entropie d’échantillon, nous pouvons formuler
quelques théorèmes qui en découlent.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une série temporelle
périodique de période <span class="math inline">\(T\)</span>. Alors,
l’entropie d’échantillon <span class="math inline">\(\text{SampEn}(m, r,
N)\)</span> tend vers zéro lorsque <span
class="math inline">\(N\)</span> tend vers l’infini.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Puisque la série est périodique, il existe des motifs
répétés infiniment souvent. Par conséquent, pour tout <span
class="math inline">\(m\)</span> et <span class="math inline">\(r &gt;
0\)</span>, les nombres <span class="math inline">\(A^m(r)\)</span> et
<span class="math inline">\(B^{m-1}(r)\)</span> sont proportionnels, ce
qui implique que le rapport <span
class="math inline">\(\frac{A^m(r)}{B^{m-1}(r)}\)</span> tend vers une
constante. Ainsi, <span class="math inline">\(\text{SampEn}(m, r,
N)\)</span> tend vers zéro. ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une série temporelle
aléatoire. Alors, l’entropie d’échantillon <span
class="math inline">\(\text{SampEn}(m, r, N)\)</span> est strictement
positive pour tout <span class="math inline">\(m\)</span>, <span
class="math inline">\(r &gt; 0\)</span>, et <span
class="math inline">\(N \geq 2m + 1\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour une série aléatoire, la probabilité que deux
motifs de longueur <span class="math inline">\(m\)</span> soient
similaires est strictement inférieure à un. Par conséquent, le rapport
<span class="math inline">\(\frac{A^m(r)}{B^{m-1}(r)}\)</span> est
strictement inférieur à un, ce qui implique que <span
class="math inline">\(\text{SampEn}(m, r, N)\)</span> est strictement
positif. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie d’échantillon possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p><strong>Invariance par translation</strong> : L’entropie
d’échantillon est invariante par ajout d’une constante à la série
temporelle. En effet, si <span class="math inline">\(Y = X + c\)</span>
pour une constante <span class="math inline">\(c\)</span>, alors <span
class="math inline">\(\text{SampEn}(m, r, N)\)</span> reste
inchangé.</p></li>
<li><p><strong>Invariance par échelle</strong> : L’entropie
d’échantillon est également invariante par multiplication de la série
temporelle par une constante positive. Si <span class="math inline">\(Y
= cX\)</span> pour une constante <span class="math inline">\(c &gt;
0\)</span>, alors <span class="math inline">\(\text{SampEn}(m, cr, N) =
\text{SampEn}(m, r, N)\)</span>.</p></li>
<li><p><strong>Monotonie</strong> : Pour une série temporelle donnée,
l’entropie d’échantillon est une fonction décroissante de <span
class="math inline">\(r\)</span>. En effet, lorsque <span
class="math inline">\(r\)</span> augmente, le nombre de paires
similaires augmente, ce qui fait diminuer le rapport <span
class="math inline">\(\frac{A^m(r)}{B^{m-1}(r)}\)</span> et donc
l’entropie.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie d’échantillon est une mesure puissante et flexible pour
analyser la complexité des séries temporelles. Ses propriétés
d’invariance et sa capacité à distinguer entre des signaux périodiques
et aléatoires en font un outil précieux dans de nombreux domaines
d’application.</p>
</body>
</html>
{% include "footer.html" %}


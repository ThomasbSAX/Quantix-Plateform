{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>FastAI: A High-Level Deep Learning Library with Amazing Pre-trained Models</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">FastAI: A High-Level Deep Learning Library with
Amazing Pre-trained Models</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-and-motivations">Introduction
and Motivations</h1>
<p>Deep learning has revolutionized the field of artificial
intelligence, enabling breakthroughs in computer vision, natural
language processing, and many other domains. However, the complexity of
designing, training, and deploying deep learning models can be a
significant barrier for researchers and practitioners.</p>
<p>FastAI is a high-level deep learning library that aims to simplify
the process of training neural networks. It provides a simple and
consistent interface for a wide range of deep learning tasks, making it
accessible to both beginners and experienced practitioners. FastAI is
built on top of PyTorch, a popular deep learning framework developed by
Facebook’s AI Research lab (FAIR).</p>
<p>The primary motivation behind FastAI is to make deep learning more
accessible and efficient. By providing high-level components that can be
combined together with as little code as possible, FastAI allows users
to quickly prototype and iterate on their models. Additionally, FastAI
includes a collection of amazing pre-trained models that can be
fine-tuned for specific tasks, further reducing the time and
computational resources required to train models from scratch.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>Before diving into the details of FastAI, let us first define some
key concepts that will be used throughout this article.</p>
<div class="definition">
<p>A <strong>neural network</strong> is a computational model inspired
by the structure and function of biological neural networks. It consists
of interconnected nodes (or neurons) organized in layers, where each
node performs a simple computation and passes the result to the next
layer.</p>
<p>Formally, a neural network can be defined as follows: Let <span
class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>
be a function that maps an input vector <span class="math inline">\(x
\in \mathbb{R}^n\)</span> to an output vector <span
class="math inline">\(y \in \mathbb{R}^m\)</span>. A neural network is a
composition of functions <span class="math inline">\(f = f_K \circ
f_{K-1} \circ \dots \circ f_1\)</span>, where each <span
class="math inline">\(f_k: \mathbb{R}^{d_{k-1}} \rightarrow
\mathbb{R}^{d_k}\)</span> is an affine transformation followed by a
non-linear activation function.</p>
<p>Mathematically, this can be expressed as: <span
class="math display">\[f_k(x) = \sigma(W_k x + b_k),\]</span> where
<span class="math inline">\(W_k \in \mathbb{R}^{d_k \times
d_{k-1}}\)</span> is the weight matrix, <span class="math inline">\(b_k
\in \mathbb{R}^{d_k}\)</span> is the bias vector, and <span
class="math inline">\(\sigma: \mathbb{R} \rightarrow \mathbb{R}\)</span>
is the activation function.</p>
</div>
<div class="definition">
<p>A <strong>pre-trained model</strong> is a neural network that has
been trained on a large dataset and can be fine-tuned for a specific
task. Pre-trained models are typically trained on tasks such as image
classification, object detection, or language modeling, and can be
adapted to new tasks with minimal additional training.</p>
<p>Formally, let <span class="math inline">\(\mathcal{D} = \{ (x_i, y_i)
\}_{i=1}^N\)</span> be a large dataset of input-output pairs, and let
<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span> be a neural network with parameters <span
class="math inline">\(\theta\)</span>. A pre-trained model is obtained
by minimizing the following loss function: <span
class="math display">\[\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N
\ell(f(x_i, \theta), y_i),\]</span> where <span
class="math inline">\(\ell: \mathbb{R}^m \times \mathbb{R}^m \rightarrow
\mathbb{R}\)</span> is a loss function that measures the discrepancy
between the predicted output <span class="math inline">\(f(x_i,
\theta)\)</span> and the true output <span
class="math inline">\(y_i\)</span>.</p>
<p>The parameters <span class="math inline">\(\theta^*\)</span> of the
pre-trained model are then obtained by solving the optimization problem:
<span class="math display">\[\theta^* = \argmin_{\theta}
\mathcal{L}(\theta).\]</span></p>
</div>
<h1 class="unnumbered" id="theorems-and-properties">Theorems and
Properties</h1>
<p>FastAI is built on top of several key theoretical results from the
fields of optimization, statistical learning theory, and deep learning.
In this section, we will discuss some of the most important theorems and
properties that underpin FastAI’s design.</p>
<div class="theorem">
<p>The <strong>universal approximation theorem</strong> states that a
feedforward neural network with a single hidden layer containing a
finite number of neurons can approximate any continuous function on
compact subsets of <span class="math inline">\(\mathbb{R}^n\)</span>,
under mild assumptions on the activation function.</p>
<p>Formally, let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span> be a continuous function on a compact subset <span
class="math inline">\(K \subset \mathbb{R}^n\)</span>, and let <span
class="math inline">\(\sigma: \mathbb{R} \rightarrow \mathbb{R}\)</span>
be a non-constant, bounded, and continuous activation function. Then,
for any <span class="math inline">\(\epsilon &gt; 0\)</span>, there
exists a neural network <span class="math inline">\(f_\theta:
\mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> with a single hidden
layer and parameters <span class="math inline">\(\theta\)</span> such
that: <span class="math display">\[\| f(x) - f_\theta(x) \| &lt;
\epsilon \quad \forall x \in K.\]</span></p>
</div>
<div class="property">
<p><strong>Transfer learning</strong> is a technique where a pre-trained
model is fine-tuned for a specific task by training on a smaller
dataset. Transfer learning leverages the knowledge acquired by the
pre-trained model during its initial training, allowing it to generalize
better to new tasks with limited data.</p>
<p>Formally, let <span class="math inline">\(\mathcal{D}_1 = \{ (x_i,
y_i) \}_{i=1}^N\)</span> be a large dataset of input-output pairs, and
let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}^m\)</span> be a neural network with parameters <span
class="math inline">\(\theta\)</span>. Let <span
class="math inline">\(\mathcal{D}_2 = \{ (x&#39;_i, y&#39;_i)
\}_{i=1}^{N&#39;}\)</span> be a smaller dataset of input-output pairs
for a specific task, where <span class="math inline">\(N&#39; &lt;
N\)</span>.</p>
<p>Transfer learning involves two steps:</p>
<ol>
<li><p>Train the neural network <span class="math inline">\(f\)</span>
on the large dataset <span class="math inline">\(\mathcal{D}_1\)</span>
to obtain a pre-trained model with parameters <span
class="math inline">\(\theta^*\)</span>.</p></li>
<li><p>Fine-tune the pre-trained model on the smaller dataset <span
class="math inline">\(\mathcal{D}_2\)</span> by minimizing the following
loss function: <span class="math display">\[\mathcal{L}(\theta) =
\frac{1}{N&#39;} \sum_{i=1}^{N&#39;} \ell(f(x&#39;_i, \theta),
y&#39;_i).\]</span></p></li>
</ol>
<p>The parameters <span class="math inline">\(\theta^{**}\)</span> of
the fine-tuned model are then obtained by solving the optimization
problem: <span class="math display">\[\theta^{**} = \argmin_{\theta}
\mathcal{L}(\theta).\]</span></p>
</div>
<h1 class="unnumbered" id="proofs-and-demonstrations">Proofs and
Demonstrations</h1>
<p>In this section, we will provide detailed proofs and demonstrations
for some of the key theorems and properties discussed in the previous
section.</p>
<div class="proof">
<p><em>Proof of the Universal Approximation Theorem.</em> The proof of
the universal approximation theorem is based on the Stone-Weierstrass
theorem, which states that a continuous function on a compact subset of
<span class="math inline">\(\mathbb{R}^n\)</span> can be uniformly
approximated by polynomials.</p>
<p>Let <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> be a continuous function on a compact subset <span
class="math inline">\(K \subset \mathbb{R}^n\)</span>, and let <span
class="math inline">\(\sigma: \mathbb{R} \rightarrow \mathbb{R}\)</span>
be a non-constant, bounded, and continuous activation function. By the
Stone-Weierstrass theorem, for any <span class="math inline">\(\epsilon
&gt; 0\)</span>, there exists a polynomial <span
class="math inline">\(p: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
such that: <span class="math display">\[\| f(x) - p(x) \| &lt; \epsilon
\quad \forall x \in K.\]</span></p>
<p>Now, consider a neural network <span class="math inline">\(f_\theta:
\mathbb{R}^n \rightarrow \mathbb{R}\)</span> with a single hidden layer
and parameters <span class="math inline">\(\theta = (W, b, c)\)</span>,
where <span class="math inline">\(W \in \mathbb{R}^{h \times
n}\)</span>, <span class="math inline">\(b \in \mathbb{R}^h\)</span>,
and <span class="math inline">\(c \in \mathbb{R}^h\)</span> are the
weight matrix, bias vector, and output weights, respectively. The output
of the neural network is given by: <span
class="math display">\[f_\theta(x) = \sum_{j=1}^h c_j \sigma(W_j x +
b_j),\]</span> where <span class="math inline">\(W_j\)</span> is the
<span class="math inline">\(j\)</span>-th row of the weight matrix <span
class="math inline">\(W\)</span>.</p>
<p>By choosing the parameters <span
class="math inline">\(\theta\)</span> appropriately, we can show that
the neural network <span class="math inline">\(f_\theta\)</span> can
approximate the polynomial <span class="math inline">\(p\)</span>
arbitrarily well. Specifically, we can choose the parameters <span
class="math inline">\(\theta\)</span> such that: <span
class="math display">\[f_\theta(x) = p(x) + \epsilon&#39;(x),\]</span>
where <span class="math inline">\(\epsilon&#39;(x)\)</span> is a small
error term that can be made arbitrarily small by increasing the number
of neurons <span class="math inline">\(h\)</span> in the hidden
layer.</p>
<p>Combining these results, we have: <span class="math display">\[\|
f(x) - f_\theta(x) \| = \| f(x) - p(x) + p(x) - f_\theta(x) \| \leq \|
f(x) - p(x) \| + \| p(x) - f_\theta(x) \| &lt; 2\epsilon,\]</span> which
completes the proof. ◻</p>
</div>
<div class="proof">
<p><em>Demonstration of Transfer Learning.</em> The effectiveness of
transfer learning can be demonstrated by considering the following
example. Let <span class="math inline">\(\mathcal{D}_1 = \{ (x_i, y_i)
\}_{i=1}^N\)</span> be a large dataset of images belonging to 1000
different classes, and let <span class="math inline">\(f: \mathbb{R}^{32
\times 32 \times 3} \rightarrow \mathbb{R}^{1000}\)</span> be a
convolutional neural network (CNN) with parameters <span
class="math inline">\(\theta\)</span>.</p>
<p>First, we train the CNN <span class="math inline">\(f\)</span> on the
large dataset <span class="math inline">\(\mathcal{D}_1\)</span> to
obtain a pre-trained model with parameters <span
class="math inline">\(\theta^*\)</span>. The pre-trained model is then
fine-tuned on a smaller dataset <span
class="math inline">\(\mathcal{D}_2 = \{ (x&#39;_i, y&#39;_i)
\}_{i=1}^{N&#39;}\)</span> of images belonging to 10 different classes,
where <span class="math inline">\(N&#39; &lt; N\)</span>.</p>
<p>The fine-tuning process involves modifying the last layer of the CNN
to output 10 classes instead of 1000, and training the modified network
on the smaller dataset <span
class="math inline">\(\mathcal{D}_2\)</span>. The loss function used for
fine-tuning is given by: <span
class="math display">\[\mathcal{L}(\theta) = \frac{1}{N&#39;}
\sum_{i=1}^{N&#39;} \ell(f(x&#39;_i, \theta), y&#39;_i),\]</span> where
<span class="math inline">\(\ell: \mathbb{R}^{10} \times \mathbb{R}^{10}
\rightarrow \mathbb{R}\)</span> is the cross-entropy loss function.</p>
<p>The parameters <span class="math inline">\(\theta^{**}\)</span> of
the fine-tuned model are then obtained by solving the optimization
problem: <span class="math display">\[\theta^{**} = \argmin_{\theta}
\mathcal{L}(\theta).\]</span></p>
<p>The fine-tuned model is expected to perform better on the smaller
dataset <span class="math inline">\(\mathcal{D}_2\)</span> than a model
trained from scratch, as it leverages the knowledge acquired by the
pre-trained model during its initial training on the large dataset <span
class="math inline">\(\mathcal{D}_1\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>FastAI is a powerful and accessible deep learning library that
simplifies the process of training neural networks. By providing
high-level components and amazing pre-trained models, FastAI enables
researchers and practitioners to quickly prototype and iterate on their
models. The theoretical foundations of FastAI are based on key results
from optimization, statistical learning theory, and deep learning,
including the universal approximation theorem and transfer learning.</p>
<p>In this article, we have provided a comprehensive overview of FastAI,
including its motivations, definitions, theorems, and proofs. We have
demonstrated the effectiveness of transfer learning through a concrete
example, highlighting the benefits of fine-tuning pre-trained models for
specific tasks. As deep learning continues to evolve, libraries like
FastAI will play an increasingly important role in making this powerful
technology accessible to a wider audience.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Complexité des problèmes d’optimisation non linéaire</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Complexité des problèmes d’optimisation non
linéaire</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’optimisation non linéaire est un domaine fondamental en
mathématiques appliquées, en ingénierie et en sciences économiques. Elle
trouve ses racines dans les travaux de Fermat, Lagrange et Newton, mais
c’est au XXème siècle qu’elle s’est véritablement développée avec
l’avènement de l’informatique. Les problèmes d’optimisation non linéaire
émergent naturellement dans de nombreux domaines : gestion des
ressources, conception de réseaux, apprentissage automatique, etc.</p>
<p>L’intérêt principal de ces problèmes réside dans leur capacité à
modéliser des situations complexes où les contraintes et les objectifs
ne sont pas linéaires. Cependant, cette puissance de modélisation
s’accompagne d’une complexité algorithmique souvent élevée. Comprendre
cette complexité est crucial pour développer des algorithmes efficaces
et pour évaluer la difficulté intrinsèque de ces problèmes.</p>
<h1 id="définitions">Définitions</h1>
<p>Nous cherchons à minimiser une fonction <span
class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
sous des contraintes. La non-linéarité de <span
class="math inline">\(f\)</span> et des contraintes rend le problème
complexe. Par exemple, considérons un ensemble <span
class="math inline">\(S \subseteq \mathbb{R}^n\)</span> et une fonction
<span class="math inline">\(f: S \rightarrow \mathbb{R}\)</span>. Nous
voulons trouver un point <span class="math inline">\(x^* \in S\)</span>
tel que : <span class="math display">\[f(x^*) = \min_{x \in S}
f(x).\]</span></p>
<p>Formellement, un problème d’optimisation non linéaire peut être écrit
comme : <span class="math display">\[\min_{x \in S} f(x),\]</span> où
<span class="math inline">\(S\)</span> est défini par des contraintes
non linéaires : <span class="math display">\[S = \{ x \in \mathbb{R}^n
\mid g_i(x) \leq 0, i = 1, \ldots, m \},\]</span> et <span
class="math inline">\(g_i: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
sont des fonctions non linéaires.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un résultat fondamental en optimisation non linéaire est le théorème
de Karush-Kuhn-Tucker (KKT). Ce théorème donne des conditions
nécessaires pour qu’un point soit un minimum local sous des
contraintes.</p>
<p>Supposons que <span class="math inline">\(f\)</span> et <span
class="math inline">\(g_i\)</span> soient continûment différentiables.
Un point <span class="math inline">\(x^*\)</span> est un minimum local
de <span class="math inline">\(f\)</span> sous les contraintes <span
class="math inline">\(g_i(x^*) \leq 0\)</span> si et seulement s’il
existe des multiplicateurs de Lagrange <span
class="math inline">\(\lambda_i \geq 0\)</span> tels que : <span
class="math display">\[\nabla f(x^*) + \sum_{i=1}^m \lambda_i \nabla
g_i(x^*) = 0,\]</span> <span class="math display">\[\lambda_i g_i(x^*) =
0, \quad i = 1, \ldots, m.\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de KKT, nous utilisons des concepts de
l’analyse convexe et du calcul des variations. Supposons que <span
class="math inline">\(x^*\)</span> est un minimum local de <span
class="math inline">\(f\)</span> sous les contraintes <span
class="math inline">\(g_i(x^*) \leq 0\)</span>. Nous devons montrer
qu’il existe des multiplicateurs de Lagrange <span
class="math inline">\(\lambda_i \geq 0\)</span> satisfaisant les
conditions du théorème.</p>
<p>1. **Conditions de première ordre** : Par définition d’un minimum
local, pour tout <span class="math inline">\(x\)</span> proche de <span
class="math inline">\(x^*\)</span>, nous avons : <span
class="math display">\[f(x) + \sum_{i=1}^m \lambda_i g_i(x) \geq f(x^*)
+ \sum_{i=1}^m \lambda_i g_i(x^*).\]</span></p>
<p>2. **Différentiation** : En différentiant cette inégalité, nous
obtenons : <span class="math display">\[\nabla f(x^*) + \sum_{i=1}^m
\lambda_i \nabla g_i(x^*) = 0.\]</span></p>
<p>3. **Complémentarité** : Les conditions <span
class="math inline">\(\lambda_i g_i(x^*) = 0\)</span> découlent du fait
que les contraintes actives doivent satisfaire <span
class="math inline">\(g_i(x^*) = 0\)</span> et que les multiplicateurs
associés doivent être non négatifs.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons quelques propriétés importantes des problèmes
d’optimisation non linéaire :</p>
<p>(i) **Convexité** : Si <span class="math inline">\(f\)</span> et les
contraintes <span class="math inline">\(g_i\)</span> sont convexes,
alors tout minimum local est un minimum global.</p>
<p>(ii) **Lissage** : Les problèmes non linéaires lisses (où <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g_i\)</span> sont continûment différentiables)
sont souvent plus faciles à résoudre que les problèmes non lisses.</p>
<p>(iii) **Complexité** : La complexité des algorithmes d’optimisation
non linéaire dépend fortement de la dimension <span
class="math inline">\(n\)</span> et du nombre de contraintes <span
class="math inline">\(m\)</span>.</p>
<p>Pour chaque propriété, des preuves détaillées peuvent être fournies
en utilisant les concepts de l’analyse convexe et du calcul
différentiel. Par exemple, pour la propriété (i), nous utilisons le fait
que les fonctions convexes ont une seule stationnarité, qui est un
minimum global.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’optimisation non linéaire est un domaine riche et complexe, avec
des applications dans de nombreux domaines. Comprendre la complexité de
ces problèmes est essentiel pour développer des algorithmes efficaces et
pour résoudre des problèmes pratiques. Les théorèmes de KKT et les
propriétés de convexité fournissent des outils puissants pour analyser
ces problèmes.</p>
</body>
</html>
{% include "footer.html" %}


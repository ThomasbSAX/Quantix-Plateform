{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Opérateur Gradient : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Opérateur Gradient : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’opérateur gradient, noté <span
class="math inline">\(\nabla\)</span>, est un concept fondamental en
analyse vectorielle et en calcul différentiel. Il émerge naturellement
dans le cadre de l’optimisation, de la physique et des sciences de
l’ingénieur. Le gradient permet de quantifier comment une fonction
scalaire varie dans un espace multidimensionnel.</p>
<p>Historiquement, le gradient a été introduit pour généraliser la
notion de dérivée aux fonctions de plusieurs variables. Il joue un rôle
clé dans des domaines variés tels que la mécanique des fluides,
l’électromagnétisme et même en apprentissage automatique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Nous cherchons à capturer la variation d’une fonction scalaire <span
class="math inline">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
dans toutes les directions possibles. Intuitivement, le gradient de
<span class="math inline">\(f\)</span> en un point donné pointe dans la
direction où <span class="math inline">\(f\)</span> croît le plus
rapidement.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction différentiable. Le gradient de <span
class="math inline">\(f\)</span> en un point <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>, noté <span
class="math inline">\(\nabla f(\mathbf{x})\)</span>, est le vecteur
défini par : <span class="math display">\[\nabla f(\mathbf{x}) = \left(
\frac{\partial f}{\partial x_1}(\mathbf{x}), \frac{\partial f}{\partial
x_2}(\mathbf{x}), \ldots, \frac{\partial f}{\partial x_n}(\mathbf{x})
\right).\]</span> En d’autres termes, pour tout <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^n\)</span>, on a : <span
class="math display">\[\nabla f(\mathbf{x}) \cdot \mathbf{v} = \lim_{h
\to 0} \frac{f(\mathbf{x} + h\mathbf{v}) -
f(\mathbf{x})}{h}.\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes-importants">Théorèmes
Importants</h1>
<p>Le gradient est intimement lié à la notion de dérivée directionnelle
et au théorème du gradient.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction différentiable. Pour tout <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> et pour tout
vecteur unitaire <span class="math inline">\(\mathbf{u} \in
\mathbb{R}^n\)</span>, la dérivée directionnelle de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(\mathbf{x}\)</span> dans la direction <span
class="math inline">\(\mathbf{u}\)</span> est donnée par : <span
class="math display">\[D_{\mathbf{u}} f(\mathbf{x}) = \nabla
f(\mathbf{x}) \cdot \mathbf{u}.\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème du gradient, nous utilisons la définition de
la dérivée directionnelle et les propriétés du gradient.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(f : \mathbb{R}^n
\rightarrow \mathbb{R}\)</span> une fonction différentiable, <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> et <span
class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span> un vecteur
unitaire. La dérivée directionnelle de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(\mathbf{x}\)</span> dans la direction <span
class="math inline">\(\mathbf{u}\)</span> est définie par : <span
class="math display">\[D_{\mathbf{u}} f(\mathbf{x}) = \lim_{h \to 0}
\frac{f(\mathbf{x} + h\mathbf{u}) - f(\mathbf{x})}{h}.\]</span> D’autre
part, par définition du gradient, on a : <span
class="math display">\[\nabla f(\mathbf{x}) \cdot \mathbf{u} = \lim_{h
\to 0} \frac{f(\mathbf{x} + h\mathbf{u}) - f(\mathbf{x})}{h}.\]</span>
En comparant les deux expressions, on obtient : <span
class="math display">\[D_{\mathbf{u}} f(\mathbf{x}) = \nabla
f(\mathbf{x}) \cdot \mathbf{u}.\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le gradient possède plusieurs propriétés intéressantes qui en font un
outil puissant dans l’analyse.</p>
<ol>
<li><p><strong>Linéarité du Gradient</strong> : Pour toute fonction
<span class="math inline">\(f, g : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> différentiables et pour tout <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span>, on a :
<span class="math display">\[\nabla (\alpha f + \beta g) = \alpha \nabla
f + \beta \nabla g.\]</span></p></li>
<li><p><strong>Produit Scalaire</strong> : Pour toute fonction <span
class="math inline">\(f, g : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> différentiables, on a : <span
class="math display">\[\nabla (f \cdot g) = f \nabla g + g \nabla
f.\]</span></p></li>
<li><p><strong>Gradient de la Norme</strong> : Pour tout <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>, le gradient
de la norme euclidienne est donné par : <span
class="math display">\[\nabla \|\mathbf{x}\| =
\frac{\mathbf{x}}{\|\mathbf{x}\|}.\]</span></p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour prouver la linéarité du gradient, on utilise les
règles de dérivation pour les fonctions différentiables. Soit <span
class="math inline">\(f, g : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> différentiables et <span
class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span>. On a :
<span class="math display">\[\nabla (\alpha f + \beta g) = \left(
\frac{\partial}{\partial x_1} (\alpha f + \beta g), \ldots,
\frac{\partial}{\partial x_n} (\alpha f + \beta g) \right).\]</span> En
appliquant la linéarité de la dérivation partielle, on obtient : <span
class="math display">\[\nabla (\alpha f + \beta g) = \left( \alpha
\frac{\partial f}{\partial x_1} + \beta \frac{\partial g}{\partial x_1},
\ldots, \alpha \frac{\partial f}{\partial x_n} + \beta \frac{\partial
g}{\partial x_n} \right) = \alpha \nabla f + \beta \nabla
g.\]</span> ◻</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver la propriété du produit scalaire, on
utilise la règle de dérivation en chaîne. Soit <span
class="math inline">\(f, g : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> différentiables. On a : <span
class="math display">\[\nabla (f \cdot g) = \left(
\frac{\partial}{\partial x_1} (f \cdot g), \ldots,
\frac{\partial}{\partial x_n} (f \cdot g) \right).\]</span> En
appliquant la règle du produit, on obtient : <span
class="math display">\[\nabla (f \cdot g) = \left( f \frac{\partial
g}{\partial x_1} + g \frac{\partial f}{\partial x_1}, \ldots, f
\frac{\partial g}{\partial x_n} + g \frac{\partial f}{\partial x_n}
\right) = f \nabla g + g \nabla f.\]</span> ◻</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver la propriété du gradient de la norme, on
utilise les dérivées partielles. Soit <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>. On a : <span
class="math display">\[\nabla \|\mathbf{x}\| = \left(
\frac{\partial}{\partial x_1} \sqrt{x_1^2 + \ldots + x_n^2}, \ldots,
\frac{\partial}{\partial x_n} \sqrt{x_1^2 + \ldots + x_n^2}
\right).\]</span> En appliquant la règle de dérivation en chaîne, on
obtient : <span class="math display">\[\nabla \|\mathbf{x}\| = \left(
\frac{x_1}{\sqrt{x_1^2 + \ldots + x_n^2}}, \ldots,
\frac{x_n}{\sqrt{x_1^2 + \ldots + x_n^2}} \right) =
\frac{\mathbf{x}}{\|\mathbf{x}\|}.\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le gradient est un outil essentiel en analyse vectorielle et en
calcul différentiel. Ses propriétés et ses applications en font un
concept central dans de nombreux domaines des mathématiques et des
sciences appliquées. Comprendre le gradient permet de mieux appréhender
les variations des fonctions multidimensionnelles et d’optimiser des
processus complexes.</p>
</body>
</html>
{% include "footer.html" %}


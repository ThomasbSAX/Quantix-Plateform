{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Support Vector Machines: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Support Vector Machines: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les machines à vecteurs de support (SVM) constituent une méthode
puissante de classification binaire. Leur succès réside dans leur
capacité à maximiser la marge entre les classes, ce qui améliore la
généralisation. Cependant, leur efficacité est limitée par l’hypothèse
d’une séparation linéaire des données.</p>
<p>L’introduction des noyaux (kernels) a révolutionné les SVM en
permettant de traiter des données non linéairement séparables. L’idée
centrale est de projeter les données dans un espace de plus haute
dimension où une séparation linéaire devient possible.</p>
<p>Cette approche, connue sous le nom de Kernelized SVM, a trouvé des
applications dans divers domaines tels que la reconnaissance d’images,
le traitement du langage naturel et la bioinformatique. Son importance
réside dans sa capacité à modéliser des relations complexes entre les
données.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le Kernelized SVM, il est essentiel de
comprendre les concepts sous-jacents.</p>
<h2 id="projection-dans-un-espace-de-haute-dimension">Projection dans un
Espace de Haute Dimension</h2>
<p>Considérons un ensemble de données <span
class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span>, où
<span class="math inline">\(x_i \in \mathbb{R}^d\)</span> et <span
class="math inline">\(y_i \in \{-1, 1\}\)</span>. Nous cherchons à
trouver une hyperplan qui sépare les données de manière optimale.</p>
<p>Cependant, si les données ne sont pas linéairement séparables dans
<span class="math inline">\(\mathbb{R}^d\)</span>, nous pouvons
envisager de les projeter dans un espace de plus haute dimension <span
class="math inline">\(\mathcal{H}\)</span> où une séparation linéaire
est possible.</p>
<h2 id="fonction-noyau">Fonction Noyau</h2>
<p>Une fonction noyau <span class="math inline">\(k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}\)</span> est une fonction symétrique
et positive définie qui permet de calculer le produit scalaire dans
l’espace <span class="math inline">\(\mathcal{H}\)</span> sans avoir à
expliciter la projection.</p>
<div class="definition">
<p>Une fonction <span class="math inline">\(k\)</span> est un noyau si
et seulement si pour tout <span class="math inline">\(x, x&#39; \in
\mathbb{R}^d\)</span>, il existe un espace <span
class="math inline">\(\mathcal{H}\)</span> et une fonction <span
class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathcal{H}\)</span> telle que <span class="math display">\[k(x, x&#39;)
= \langle \phi(x), \phi(x&#39;) \rangle_{\mathcal{H}}.\]</span></p>
</div>
<h2 id="kernelized-svm">Kernelized SVM</h2>
<p>Le Kernelized SVM utilise une fonction noyau pour trouver l’hyperplan
optimal dans l’espace <span
class="math inline">\(\mathcal{H}\)</span>.</p>
<div class="definition">
<p>Étant donné un ensemble de données <span
class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span> et
une fonction noyau <span class="math inline">\(k\)</span>, le Kernelized
SVM consiste à résoudre le problème d’optimisation suivant: <span
class="math display">\[\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}
\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j k(x_i, x_j)\]</span> sous les
contraintes <span class="math display">\[\sum_{i=1}^n \alpha_i y_i =
0,\]</span> <span class="math display">\[0 \leq \alpha_i \leq C \quad
\forall i,\]</span> où <span class="math inline">\(C\)</span> est un
paramètre de régularisation et <span
class="math inline">\(\alpha_i\)</span> sont les multiplicateurs de
Lagrange.</p>
</div>
<h1 id="théorèmes-et-propriétés">Théorèmes et Propriétés</h1>
<h2 id="théorème-de-représentation-des-noyaux">Théorème de
Représentation des Noyaux</h2>
<p>Le théorème de représentation des noyaux est fondamental pour
comprendre la validité des fonctions noyau.</p>
<div class="theorem">
<p>Une fonction <span class="math inline">\(k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}\)</span> est un noyau si et
seulement si elle peut être exprimée comme <span
class="math display">\[k(x, x&#39;) = \sum_{j=1}^\infty \lambda_j
\phi_j(x) \phi_j(x&#39;),\]</span> où <span
class="math inline">\(\{\lambda_j\}_{j=1}^\infty\)</span> est une suite
de nombres réels positifs et <span
class="math inline">\(\{\phi_j\}_{j=1}^\infty\)</span> est une base
orthonormée dans un espace de Hilbert <span
class="math inline">\(\mathcal{H}\)</span>.</p>
</div>
<h2 id="théorème-de-mercer">Théorème de Mercer</h2>
<p>Le théorème de Mercer fournit une condition suffisante pour qu’une
fonction soit un noyau.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(k: \mathbb{R}^d \times \mathbb{R}^d
\rightarrow \mathbb{R}\)</span> une fonction continue. Si pour toute
fonction <span class="math inline">\(f: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> intégrable, <span
class="math display">\[\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} k(x,
x&#39;) f(x) f(x&#39;) \, dx \, dx&#39; \geq 0,\]</span> alors <span
class="math inline">\(k\)</span> est un noyau.</p>
</div>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-représentation-des-noyaux">Preuve du
Théorème de Représentation des Noyaux</h2>
<p>Pour prouver le théorème de représentation des noyaux, nous devons
montrer que toute fonction noyau peut être exprimée comme une somme
infinie de produits de fonctions orthonormées.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’opérateur intégral <span
class="math inline">\(T_k\)</span> défini par <span
class="math display">\[T_k f(x) = \int_{\mathbb{R}^d} k(x, x&#39;)
f(x&#39;) \, dx&#39;.\]</span></p>
<p>Puisque <span class="math inline">\(k\)</span> est un noyau, <span
class="math inline">\(T_k\)</span> est un opérateur auto-adjoint et
positif. Par le théorème spectral, il existe une base orthonormée <span
class="math inline">\(\{\phi_j\}_{j=1}^\infty\)</span> et une suite de
nombres réels positifs <span
class="math inline">\(\{\lambda_j\}_{j=1}^\infty\)</span> tels que <span
class="math display">\[T_k \phi_j = \lambda_j \phi_j.\]</span></p>
<p>En utilisant cette base, nous pouvons exprimer <span
class="math inline">\(k\)</span> comme <span class="math display">\[k(x,
x&#39;) = \sum_{j=1}^\infty \lambda_j \phi_j(x)
\phi_j(x&#39;).\]</span> ◻</p>
</div>
<h2 id="preuve-du-théorème-de-mercer">Preuve du Théorème de Mercer</h2>
<p>La preuve du théorème de Mercer repose sur l’analyse des formes
quadratiques définies positives.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(k\)</span>
satisfait la condition de Mercer. Pour toute fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>
intégrable, nous avons <span class="math display">\[\int_{\mathbb{R}^d}
\int_{\mathbb{R}^d} k(x, x&#39;) f(x) f(x&#39;) \, dx \, dx&#39; \geq
0.\]</span></p>
<p>Cela signifie que l’opérateur intégral <span
class="math inline">\(T_k\)</span> est positif. Par le théorème
spectral, il existe une base orthonormée <span
class="math inline">\(\{\phi_j\}_{j=1}^\infty\)</span> et une suite de
nombres réels positifs <span
class="math inline">\(\{\lambda_j\}_{j=1}^\infty\)</span> tels que <span
class="math display">\[T_k \phi_j = \lambda_j \phi_j.\]</span></p>
<p>En utilisant cette base, nous pouvons exprimer <span
class="math inline">\(k\)</span> comme <span class="math display">\[k(x,
x&#39;) = \sum_{j=1}^\infty \lambda_j \phi_j(x)
\phi_j(x&#39;).\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriétés-des-noyaux">Propriétés des Noyaux</h2>
<p>Les noyaux possèdent plusieurs propriétés importantes qui les rendent
utiles dans le contexte des SVM.</p>
<ol>
<li><p><strong>Symétrie</strong>: Pour tout <span
class="math inline">\(x, x&#39; \in \mathbb{R}^d\)</span>, nous avons
<span class="math inline">\(k(x, x&#39;) = k(x&#39;,
x)\)</span>.</p></li>
<li><p><strong>Positivité définie</strong>: Pour toute fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>
intégrable, nous avons <span class="math display">\[\int_{\mathbb{R}^d}
\int_{\mathbb{R}^d} k(x, x&#39;) f(x) f(x&#39;) \, dx \, dx&#39; \geq
0.\]</span></p></li>
<li><p><strong>Reproduction du noyau</strong>: Pour tout <span
class="math inline">\(x, x&#39; \in \mathbb{R}^d\)</span>, nous avons
<span class="math display">\[k(x, x&#39;) = \langle \phi(x),
\phi(x&#39;) \rangle_{\mathcal{H}}.\]</span></p></li>
</ol>
<h2 id="corollaires-du-théorème-de-mercer">Corollaires du Théorème de
Mercer</h2>
<p>Le théorème de Mercer a plusieurs corollaires importants.</p>
<ol>
<li><p><strong>Existence d’une projection</strong>: Pour toute fonction
noyau <span class="math inline">\(k\)</span>, il existe une projection
<span class="math inline">\(\phi: \mathbb{R}^d \rightarrow
\mathcal{H}\)</span> telle que <span class="math display">\[k(x, x&#39;)
= \langle \phi(x), \phi(x&#39;) \rangle_{\mathcal{H}}.\]</span></p></li>
<li><p><strong>Stabilité numérique</strong>: Les noyaux satisfaisant le
théorème de Mercer sont numériquement stables, ce qui est crucial pour
les applications pratiques.</p></li>
<li><p><strong>Generalisation</strong>: Les SVM utilisant des noyaux
satisfaisant le théorème de Mercer généralisent bien aux nouvelles
données.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Les Kernelized SVM représentent une avancée significative dans le
domaine de l’apprentissage automatique. En permettant la projection des
données dans un espace de haute dimension, ils offrent une solution
puissante pour les problèmes de classification non linéaire. Les
théorèmes et propriétés discutés dans cet article fournissent une base
solide pour comprendre et appliquer cette méthode.</p>
</body>
</html>
{% include "footer.html" %}


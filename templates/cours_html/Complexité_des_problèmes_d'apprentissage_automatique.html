{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Complexité des problèmes d’apprentissage automatique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Complexité des problèmes d’apprentissage
automatique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, ou <em>machine learning</em>, est un
domaine de l’intelligence artificielle qui vise à développer des
algorithmes capables d’apprendre à partir de données. L’origine
historique de ce domaine remonte aux années 1950 avec les travaux
pionniers d’Arthur Samuel sur les programmes d’échecs auto-apprenants.
Conceptuellement, l’apprentissage automatique repose sur l’idée de
modéliser des phénomènes complexes à partir de données, en utilisant des
techniques statistiques et algorithmiques.</p>
<p>La notion de complexité dans l’apprentissage automatique émerge comme
une réponse à la nécessité de comprendre et de quantifier les ressources
nécessaires pour résoudre des problèmes d’apprentissage. Cette
complexité peut être mesurée en termes de temps de calcul, de mémoire
nécessaire, ou encore de la quantité de données requises. Elle est
indispensable pour évaluer l’efficacité et la faisabilité des
algorithmes d’apprentissage, ainsi que pour comparer différentes
approches.</p>
<h1 id="définitions">Définitions</h1>
<p>Nous cherchons à comprendre combien de ressources sont nécessaires
pour apprendre une fonction à partir d’un ensemble de données.
Intuitivement, plus la fonction est complexe, plus il faudra de données
et de calculs pour l’apprendre correctement.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{F}\)</span> un ensemble de
fonctions et <span class="math inline">\(S\)</span> un ensemble de
données. La complexité d’un problème d’apprentissage est définie comme
la quantité minimale de ressources nécessaires pour apprendre une
fonction <span class="math inline">\(f \in \mathcal{F}\)</span> à partir
de <span class="math inline">\(S\)</span>.</p>
<p>Formellement, pour un algorithme d’apprentissage <span
class="math inline">\(\mathcal{A}\)</span>, la complexité temporelle est
donnée par : <span class="math display">\[T_{\mathcal{A}}(n) = \max_{S
\in \mathcal{S}_n} \text{Temps}(\mathcal{A}(S))\]</span> où <span
class="math inline">\(\mathcal{S}_n\)</span> est l’ensemble de tous les
ensembles de données de taille <span
class="math inline">\(n\)</span>.</p>
<p>La complexité en mémoire peut être définie de manière similaire :
<span class="math display">\[M_{\mathcal{A}}(n) = \max_{S \in
\mathcal{S}_n} \text{Mémoire}(\mathcal{A}(S))\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous cherchons à comprendre comment la complexité d’un problème
d’apprentissage dépend de la taille des données et de la complexité de
l’ensemble de fonctions.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{F}\)</span> un ensemble de
fonctions et <span class="math inline">\(S\)</span> un ensemble de
données de taille <span class="math inline">\(n\)</span>. Si <span
class="math inline">\(\mathcal{F}\)</span> est un ensemble VC
(Vapnik-Chervonenkis) de dimension <span
class="math inline">\(d\)</span>, alors il existe une constante <span
class="math inline">\(c\)</span> telle que : <span
class="math display">\[P\left(\sup_{f \in \mathcal{F}}
|E_{\text{emp}}(f) - E(f)| &gt; \epsilon\right) \leq 4
\left(\frac{2e}{n\epsilon^2}\right)^d\]</span> où <span
class="math inline">\(E_{\text{emp}}(f)\)</span> est l’erreur empirique
et <span class="math inline">\(E(f)\)</span> est l’erreur réelle.</p>
<p>Preuve : La preuve de ce théorème repose sur le lemme de Hoeffding et
la notion de dimension VC. Le lemme de Hoeffding nous permet de borner
la différence entre l’erreur empirique et l’erreur réelle, tandis que la
dimension VC nous permet de contrôler la complexité de l’ensemble de
fonctions.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Nous allons maintenant détailler la preuve du théorème de la limite
d’apprentissage.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(\mathcal{F}\)</span>
un ensemble VC de dimension <span class="math inline">\(d\)</span>. Nous
voulons montrer que pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, il existe une constante <span
class="math inline">\(c\)</span> telle que : <span
class="math display">\[P\left(\sup_{f \in \mathcal{F}}
|E_{\text{emp}}(f) - E(f)| &gt; \epsilon\right) \leq 4
\left(\frac{2e}{n\epsilon^2}\right)^d\]</span></p>
<p>Nous commençons par appliquer le lemme de Hoeffding, qui nous donne
pour tout <span class="math inline">\(f \in \mathcal{F}\)</span> : <span
class="math display">\[P\left(|E_{\text{emp}}(f) - E(f)| &gt;
\epsilon\right) \leq 2e^{-2n\epsilon^2}\]</span></p>
<p>Ensuite, nous utilisons la notion de dimension VC pour contrôler le
nombre de fonctions dans <span
class="math inline">\(\mathcal{F}\)</span>. Plus précisément, nous avons
: <span class="math display">\[|\mathcal{F}| \leq \sum_{i=0}^{d}
\binom{n}{i}\]</span></p>
<p>En combinant ces deux résultats, nous obtenons : <span
class="math display">\[P\left(\sup_{f \in \mathcal{F}}
|E_{\text{emp}}(f) - E(f)| &gt; \epsilon\right) \leq
2|\mathcal{F}|e^{-2n\epsilon^2} \leq 4
\left(\frac{2e}{n\epsilon^2}\right)^d\]</span></p>
<p>Ceci termine la preuve du théorème. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous allons maintenant lister quelques propriétés et corollaires
importants liés à la complexité des problèmes d’apprentissage.</p>
<div class="proposition">
<p>Soit <span class="math inline">\(\mathcal{F}\)</span> un ensemble de
fonctions et <span class="math inline">\(S\)</span> un ensemble de
données de taille <span class="math inline">\(n\)</span>. Si <span
class="math inline">\(\mathcal{F}\)</span> est un ensemble VC de
dimension <span class="math inline">\(d\)</span>, alors la complexité
temporelle d’un algorithme d’apprentissage <span
class="math inline">\(\mathcal{A}\)</span> est au plus <span
class="math inline">\(O(n^d)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette proposition repose sur le fait que
la dimension VC contrôle le nombre de fonctions dans <span
class="math inline">\(\mathcal{F}\)</span>. Plus précisément, nous avons
: <span class="math display">\[|\mathcal{F}| \leq \sum_{i=0}^{d}
\binom{n}{i}\]</span></p>
<p>En conséquence, la complexité temporelle de l’algorithme <span
class="math inline">\(\mathcal{A}\)</span> est au plus <span
class="math inline">\(O(n^d)\)</span>. ◻</p>
</div>
<div class="corollaire">
<p>Soit <span class="math inline">\(\mathcal{F}\)</span> un ensemble de
fonctions et <span class="math inline">\(S\)</span> un ensemble de
données de taille <span class="math inline">\(n\)</span>. Si <span
class="math inline">\(\mathcal{F}\)</span> est un ensemble VC de
dimension <span class="math inline">\(d\)</span>, alors la complexité en
mémoire d’un algorithme d’apprentissage <span
class="math inline">\(\mathcal{A}\)</span> est au plus <span
class="math inline">\(O(n^d)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire est similaire à celle de
la proposition précédente. En effet, la dimension VC contrôle également
le nombre de fonctions dans <span
class="math inline">\(\mathcal{F}\)</span>, ce qui implique que la
complexité en mémoire est au plus <span
class="math inline">\(O(n^d)\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons introduit la notion de complexité dans
les problèmes d’apprentissage automatique. Nous avons défini
formellement cette complexité et présenté un théorème fondamental, le
théorème de la limite d’apprentissage. Nous avons également détaillé les
preuves et discuté quelques propriétés et corollaires importants.</p>
<p>La compréhension de la complexité des problèmes d’apprentissage est
essentielle pour développer des algorithmes efficaces et pour évaluer
leur performance. Les résultats présentés dans cet article fournissent
des outils théoriques pour analyser et comparer différentes approches
d’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


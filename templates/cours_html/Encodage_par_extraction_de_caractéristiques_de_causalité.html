{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de causalité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de
causalité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques de causalité est une
méthode puissante pour comprendre les relations sous-jacentes dans les
données. Cette approche émerge de la nécessité de capturer non seulement
les corrélations, mais aussi les relations causales qui régissent les
phénomènes observés. Historiquement, l’analyse des données s’est
longtemps concentrée sur la détection de motifs et de corrélations, mais
l’émergence des théories de la causalité a ouvert de nouvelles
perspectives.</p>
<p>L’encodage par extraction de caractéristiques de causalité est
indispensable dans les domaines où la compréhension des mécanismes
sous-jacents est cruciale, comme en médecine, en économie ou en sciences
sociales. En identifiant les relations causales, nous pouvons non
seulement prédire des événements futurs, mais aussi intervenir de
manière ciblée pour modifier les résultats.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
causalité, commençons par définir les concepts clés.</p>
<h2 class="unnumbered" id="caractéristiques-causales">Caractéristiques
Causales</h2>
<p>Nous cherchons à identifier des caractéristiques qui ne sont pas
seulement corrélées, mais qui ont un impact direct sur d’autres
variables. Par exemple, dans une étude médicale, nous voulons identifier
des caractéristiques qui influencent directement la santé d’un
patient.</p>
<p>Formellement, soit <span class="math inline">\(X\)</span> un ensemble
de variables observées et <span class="math inline">\(Y\)</span> une
variable cible. Une caractéristique causale <span
class="math inline">\(C\)</span> est une sous-ensemble de <span
class="math inline">\(X\)</span> tel que :</p>
<p><span class="math display">\[\exists Y \in \mathbb{R}, \forall x \in
X, C \subseteq X : P(Y | do(C)) \neq P(Y)\]</span></p>
<p>où <span class="math inline">\(P(Y | do(C))\)</span> représente la
probabilité de <span class="math inline">\(Y\)</span> après une
intervention sur <span class="math inline">\(C\)</span>.</p>
<h2 class="unnumbered" id="encodage-par-extraction">Encodage par
Extraction</h2>
<p>L’encodage par extraction de caractéristiques de causalité consiste à
transformer les données brutes en un ensemble de caractéristiques qui
capturent les relations causales. Cela implique souvent l’utilisation de
techniques d’apprentissage automatique pour identifier et extraire ces
caractéristiques.</p>
<p>Formellement, soit <span class="math inline">\(D\)</span> un ensemble
de données brutes. L’encodage par extraction de caractéristiques de
causalité est une fonction <span class="math inline">\(E\)</span> telle
que :</p>
<p><span class="math display">\[E : D \rightarrow C, \quad \text{où } C
\text{ est un ensemble de caractéristiques causales.}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="théorème-de-lextraction-des-caractéristiques-causales">Théorème de
l’Extraction des Caractéristiques Causales</h2>
<p>Nous cherchons à montrer que l’extraction de caractéristiques
causales peut améliorer la précision des modèles prédictifs.</p>
<p>Soit <span class="math inline">\(M\)</span> un modèle prédictif et
<span class="math inline">\(C\)</span> un ensemble de caractéristiques
causales. Nous voulons montrer que :</p>
<p><span class="math display">\[\forall D, M(E(D)) \text{ est plus
précis que } M(D)\]</span></p>
<p>où <span class="math inline">\(E(D)\)</span> représente les données
encodées avec les caractéristiques causales.</p>
<p><u>Preuve</u>:</p>
<p>1. Considérons un modèle prédictif <span
class="math inline">\(M\)</span> et un ensemble de données <span
class="math inline">\(D\)</span>. 2. Appliquons la fonction d’encodage
<span class="math inline">\(E\)</span> pour extraire les
caractéristiques causales <span class="math inline">\(C\)</span>. 3.
Montrons que <span class="math inline">\(M(E(D))\)</span> est plus
précis que <span class="math inline">\(M(D)\)</span>.</p>
<p>Soit <span class="math inline">\(Y\)</span> la variable cible. Nous
avons :</p>
<p><span class="math display">\[P(Y | E(D)) = P(Y | do(C))\]</span></p>
<p>Par définition des caractéristiques causales, <span
class="math inline">\(P(Y | do(C))\)</span> est plus informatif que
<span class="math inline">\(P(Y)\)</span>. Donc :</p>
<p><span class="math display">\[M(E(D)) \text{ est plus précis que }
M(D)\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour justifier le théorème de l’extraction des caractéristiques
causales, nous devons montrer que les caractéristiques causales
capturent effectivement des informations supplémentaires par rapport aux
données brutes.</p>
<p>Considérons un exemple simple où <span class="math inline">\(X =
\{X_1, X_2\}\)</span> et <span class="math inline">\(Y\)</span> est la
variable cible. Supposons que <span class="math inline">\(X_1\)</span>
soit une caractéristique causale de <span
class="math inline">\(Y\)</span>, mais pas <span
class="math inline">\(X_2\)</span>.</p>
<p>Nous avons :</p>
<p><span class="math display">\[P(Y | do(X_1)) \neq P(Y)\]</span></p>
<p>et</p>
<p><span class="math display">\[P(Y | do(X_2)) = P(Y)\]</span></p>
<p>Donc, en extrayant <span class="math inline">\(X_1\)</span> comme
caractéristique causale, nous améliorons la précision du modèle
prédictif.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-1-stabilité-des-caractéristiques-causales">Propriété 1:
Stabilité des Caractéristiques Causales</h2>
<p>Les caractéristiques causales sont stables sous les transformations
des données. Formellement, soit <span class="math inline">\(T\)</span>
une transformation des données. Nous avons :</p>
<p><span class="math display">\[E(T(D)) = E(D)\]</span></p>
<p><u>Preuve</u>:</p>
<p>1. Considérons une transformation <span
class="math inline">\(T\)</span> des données. 2. Montrons que <span
class="math inline">\(E(T(D)) = E(D)\)</span>.</p>
<p>Puisque les caractéristiques causales capturent des relations
intrinsèques, elles sont invariantes sous les transformations qui
préservent ces relations.</p>
<h2 class="unnumbered"
id="propriété-2-généralisation-des-modèles">Propriété 2: Généralisation
des Modèles</h2>
<p>Les modèles basés sur les caractéristiques causales généralisent
mieux aux nouvelles données. Formellement, soit <span
class="math inline">\(D_{\text{test}}\)</span> un ensemble de données de
test. Nous avons :</p>
<p><span class="math display">\[M(E(D_{\text{train}})) \text{ généralise
mieux que } M(D_{\text{train}})\]</span></p>
<p><u>Preuve</u>:</p>
<p>1. Considérons un ensemble de données d’entraînement <span
class="math inline">\(D_{\text{train}}\)</span> et un ensemble de
données de test <span class="math inline">\(D_{\text{test}}\)</span>. 2.
Montrons que <span class="math inline">\(M(E(D_{\text{train}}))\)</span>
généralise mieux que <span
class="math inline">\(M(D_{\text{train}})\)</span>.</p>
<p>Les caractéristiques causales capturent des relations générales, donc
le modèle basé sur ces caractéristiques généralise mieux aux nouvelles
données.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de causalité est une
méthode puissante pour capturer les relations causales dans les données.
En identifiant et en extrayant ces caractéristiques, nous pouvons
améliorer la précision des modèles prédictifs et mieux comprendre les
mécanismes sous-jacents. Cette approche a des applications potentielles
dans de nombreux domaines, notamment en médecine, en économie et en
sciences sociales.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence epsilon-insensitive : Une approche robuste pour l’apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence epsilon-insensitive : Une approche robuste
pour l’apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage statistique, en particulier dans le cadre de la
régression, repose souvent sur des critères de perte qui mesurent
l’écart entre les prédictions d’un modèle et les valeurs réelles
observées. Parmi ces critères, la divergence epsilon-insensitive se
distingue par sa robustesse face aux valeurs aberrantes et son aptitude
à produire des modèles parcimonieux. Introduite initialement dans le
contexte des machines à vecteurs de support (SVM), cette divergence a
trouvé des applications dans divers domaines, allant de la finance à la
bioinformatique.</p>
<p>L’idée sous-jacente à cette divergence est d’introduire une zone
d’insensibilité autour de la valeur cible, dans laquelle les erreurs ne
sont pas pénalisées. Cette approche permet non seulement de réduire la
sensibilité du modèle aux bruits et aux outliers, mais aussi
d’encourager des solutions plus simples et interprétables. Dans cet
article, nous explorons les fondements théoriques de la divergence
epsilon-insensitive, ses propriétés mathématiques et ses implications
pratiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la divergence epsilon-insensitive, commençons par
définir ce que nous cherchons à capturer. Imaginons que nous avons un
ensemble de données <span class="math inline">\((x_i,
y_i)_{i=1}^n\)</span>, où <span class="math inline">\(x_i\)</span>
représente les caractéristiques d’un échantillon et <span
class="math inline">\(y_i\)</span> la valeur cible associée. Nous
voulons mesurer l’écart entre une fonction de prédiction <span
class="math inline">\(f(x)\)</span> et la valeur cible <span
class="math inline">\(y\)</span>.</p>
<p>Nous cherchons une mesure qui soit insensible aux petites erreurs,
c’est-à-dire qu’elle ne pénalise pas les écarts inférieurs à un certain
seuil <span class="math inline">\(\epsilon &gt; 0\)</span>. Cette idée
conduit naturellement à la définition suivante :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\epsilon &gt; 0\)</span> un
paramètre de tolérance. La divergence epsilon-insensitive entre une
fonction <span class="math inline">\(f: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> et une valeur cible <span
class="math inline">\(y\)</span> est définie par : <span
class="math display">\[L_\epsilon(y, f(x)) = \max(0, |y - f(x)| -
\epsilon).\]</span> En d’autres termes, pour tout <span
class="math inline">\(y \in \mathbb{R}\)</span> et toute fonction <span
class="math inline">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>,
la divergence epsilon-insensitive est donnée par : <span
class="math display">\[L_\epsilon(y, f(x)) =
\begin{cases}
0 &amp; \text{si } |y - f(x)| \leq \epsilon, \\
|y - f(x)| - \epsilon &amp; \text{sinon.}
\end{cases}\]</span></p>
</div>
<p>Cette définition peut être reformulée en utilisant des fonctions
indicatrices. Soit <span class="math inline">\(\mathbb{I}_{A}\)</span>
la fonction indicatrice de l’ensemble <span
class="math inline">\(A\)</span>, qui vaut 1 si la condition définissant
<span class="math inline">\(A\)</span> est satisfaite et 0 sinon. Alors,
nous avons : <span class="math display">\[L_\epsilon(y, f(x)) =
\mathbb{I}_{|y - f(x)| &gt; \epsilon} (|y - f(x)| -
\epsilon).\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux liés à la divergence
epsilon-insensitive concerne sa convexité. La convexité d’une fonction
de perte est une propriété cruciale en optimisation, car elle garantit
que les algorithmes d’optimisation convergeront vers un minimum
global.</p>
<div class="theoreme">
<p>La fonction <span class="math inline">\(L_\epsilon: \mathbb{R} \times
\mathbb{R}^d \rightarrow \mathbb{R}\)</span> définie par <span
class="math inline">\(L_\epsilon(y, f(x)) = \max(0, |y - f(x)| -
\epsilon)\)</span> est convexe en <span
class="math inline">\(f(x)\)</span> pour tout <span
class="math inline">\(y \in \mathbb{R}\)</span> et <span
class="math inline">\(\epsilon &gt; 0\)</span>.</p>
</div>
<p>Pour démontrer ce théorème, nous allons utiliser la définition de la
convexité. Une fonction <span class="math inline">\(f\)</span> est
convexe si pour tout <span class="math inline">\(\lambda \in
[0,1]\)</span> et pour tous <span class="math inline">\(x_1,
x_2\)</span>, nous avons : <span class="math display">\[f(\lambda x_1 +
(1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2).\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Commençons par rappeler la définition de la convexité. Une fonction
<span class="math inline">\(f\)</span> est convexe si pour tout <span
class="math inline">\(\lambda \in [0,1]\)</span> et pour tous <span
class="math inline">\(x_1, x_2\)</span>, nous avons : <span
class="math display">\[f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda
f(x_1) + (1-\lambda) f(x_2).\]</span></p>
<p>Considérons maintenant la fonction <span
class="math inline">\(L_\epsilon(y, f(x)) = \max(0, |y - f(x)| -
\epsilon)\)</span>. Nous voulons montrer que cette fonction est convexe
en <span class="math inline">\(f(x)\)</span>.</p>
<p>Soient <span class="math inline">\(f_1\)</span> et <span
class="math inline">\(f_2\)</span> deux fonctions de prédiction, et
<span class="math inline">\(\lambda \in [0,1]\)</span>. Nous devons
montrer que : <span class="math display">\[L_\epsilon(y, \lambda f_1(x)
+ (1-\lambda) f_2(x)) \leq \lambda L_\epsilon(y, f_1(x)) + (1-\lambda)
L_\epsilon(y, f_2(x)).\]</span></p>
<p>Examinons les différents cas possibles :</p>
<p>1. Si <span class="math inline">\(|y - f_1(x)| \leq \epsilon\)</span>
et <span class="math inline">\(|y - f_2(x)| \leq \epsilon\)</span>,
alors : <span class="math display">\[L_\epsilon(y, f_1(x)) =
L_\epsilon(y, f_2(x)) = 0.\]</span> De plus, <span
class="math inline">\(|y - (\lambda f_1(x) + (1-\lambda) f_2(x))| \leq
\epsilon\)</span>, donc : <span class="math display">\[L_\epsilon(y,
\lambda f_1(x) + (1-\lambda) f_2(x)) = 0 \leq \lambda \cdot 0 +
(1-\lambda) \cdot 0.\]</span></p>
<p>2. Si <span class="math inline">\(|y - f_1(x)| &gt; \epsilon\)</span>
et <span class="math inline">\(|y - f_2(x)| &gt; \epsilon\)</span>,
alors : <span class="math display">\[L_\epsilon(y, f_1(x)) = |y -
f_1(x)| - \epsilon,\]</span> <span class="math display">\[L_\epsilon(y,
f_2(x)) = |y - f_2(x)| - \epsilon.\]</span> De plus, <span
class="math inline">\(|y - (\lambda f_1(x) + (1-\lambda) f_2(x))| \geq
\epsilon\)</span>, donc : <span class="math display">\[L_\epsilon(y,
\lambda f_1(x) + (1-\lambda) f_2(x)) = |y - (\lambda f_1(x) +
(1-\lambda) f_2(x))| - \epsilon.\]</span> En utilisant l’inégalité
triangulaire, nous avons : <span class="math display">\[|y - (\lambda
f_1(x) + (1-\lambda) f_2(x))| \leq \lambda |y - f_1(x)| + (1-\lambda) |y
- f_2(x)|.\]</span> En soustrayant <span
class="math inline">\(\epsilon\)</span> des deux côtés, nous obtenons :
<span class="math display">\[L_\epsilon(y, \lambda f_1(x) + (1-\lambda)
f_2(x)) \leq \lambda (|y - f_1(x)| - \epsilon) + (1-\lambda) (|y -
f_2(x)| - \epsilon).\]</span></p>
<p>3. Si <span class="math inline">\(|y - f_1(x)| \leq \epsilon\)</span>
et <span class="math inline">\(|y - f_2(x)| &gt; \epsilon\)</span>,
alors : <span class="math display">\[L_\epsilon(y, f_1(x)) = 0,\]</span>
<span class="math display">\[L_\epsilon(y, f_2(x)) = |y - f_2(x)| -
\epsilon.\]</span> De plus, <span class="math inline">\(|y - (\lambda
f_1(x) + (1-\lambda) f_2(x))| \leq \epsilon\)</span>, donc : <span
class="math display">\[L_\epsilon(y, \lambda f_1(x) + (1-\lambda)
f_2(x)) = 0 \leq \lambda \cdot 0 + (1-\lambda) (|y - f_2(x)| -
\epsilon).\]</span></p>
<p>Dans tous les cas, l’inégalité de convexité est satisfaite. Par
conséquent, la divergence epsilon-insensitive est convexe en <span
class="math inline">\(f(x)\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence epsilon-insensitive possède plusieurs propriétés
intéressantes qui en font un outil puissant pour l’apprentissage
statistique. Nous en énumérons quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Insensibilité aux petites erreurs</strong> : La
divergence epsilon-insensitive ne pénalise pas les erreurs inférieures à
<span class="math inline">\(\epsilon\)</span>. Cela permet de réduire la
sensibilité du modèle aux bruits et aux outliers.</p></li>
<li><p><strong>Parcimonie</strong> : En introduisant une zone
d’insensibilité, la divergence epsilon-insensitive encourage des modèles
plus simples et parcimonieux. Cela est particulièrement utile dans les
contextes où l’interprétabilité du modèle est cruciale.</p></li>
<li><p><strong>Convexité</strong> : Comme démontré précédemment, la
divergence epsilon-insensitive est convexe. Cette propriété garantit que
les algorithmes d’optimisation convergeront vers un minimum
global.</p></li>
<li><p><strong>Robustesse</strong> : La divergence epsilon-insensitive
est robuste aux variations des paramètres du modèle. Cela signifie que
de petites modifications des paramètres n’affecteront pas
significativement les performances du modèle.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence epsilon-insensitive est un outil puissant et flexible
pour l’apprentissage statistique. Son aptitude à produire des modèles
robustes et parcimonieux en fait un choix privilégié dans de nombreuses
applications. Dans cet article, nous avons exploré les fondements
théoriques de cette divergence, ses propriétés mathématiques et ses
implications pratiques. Nous espérons que ces résultats encourageront de
nouvelles recherches dans ce domaine prometteur.</p>
</body>
</html>
{% include "footer.html" %}


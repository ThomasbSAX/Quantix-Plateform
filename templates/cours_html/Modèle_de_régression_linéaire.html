{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Modèle de régression linéaire : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Modèle de régression linéaire : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le modèle de régression linéaire constitue l’un des piliers de la
statistique inférentielle et de l’analyse des données. Son émergence
historique remonte aux travaux de Legendre et Gauss au début du XIXème
siècle, dans le cadre de la modélisation des mouvements planétaires.
Aujourd’hui, ce modèle s’est imposé comme un outil indispensable en
économétrie, biostatistique, sciences sociales et ingénierie.</p>
<p>L’intérêt principal de la régression linéaire réside dans sa capacité
à modéliser et quantifier les relations entre une variable dépendante
<span class="math inline">\(Y\)</span> et une ou plusieurs variables
indépendantes <span class="math inline">\(X_1, X_2, \ldots,
X_p\)</span>. Ce cadre permet non seulement d’estimer l’effet marginal
des variables explicatives sur la variable réponse, mais aussi de
réaliser des prédictions et une analyse de variance.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire le modèle de régression linéaire, considérons une
variable aléatoire <span class="math inline">\(Y\)</span> que nous
souhaitons expliquer à l’aide de variables explicatives <span
class="math inline">\(X_1, X_2, \ldots, X_p\)</span>. L’objectif est de
capturer la relation linéaire sous-jacente entre ces variables.</p>
<div class="definition">
<p>Soit <span class="math inline">\((Y, X)\)</span> un couple de
variables aléatoires réelles. On dit que <span
class="math inline">\(Y\)</span> suit un modèle de régression linéaire
simple par rapport à <span class="math inline">\(X\)</span> si : <span
class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon,\]</span> où
<span class="math inline">\(\beta_0, \beta_1 \in \mathbb{R}\)</span>
sont des paramètres inconnus et <span
class="math inline">\(\epsilon\)</span> est un bruit aléatoire centré,
i.e., <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span>, et
indépendant de <span class="math inline">\(X\)</span>.</p>
</div>
<p>Pour généraliser ce modèle à plusieurs variables explicatives, nous
introduisons la notion de régression linéaire multiple.</p>
<div class="definition">
<p>Soit <span class="math inline">\((Y, X_1, X_2, \ldots, X_p)\)</span>
un <span class="math inline">\((p+1)\)</span>-uplet de variables
aléatoires réelles. On dit que <span class="math inline">\(Y\)</span>
suit un modèle de régression linéaire multiple par rapport à <span
class="math inline">\(X_1, X_2, \ldots, X_p\)</span> si : <span
class="math display">\[Y = \beta_0 + \sum_{j=1}^p \beta_j X_j +
\epsilon,\]</span> où <span class="math inline">\(\beta_0, \beta_1,
\ldots, \beta_p \in \mathbb{R}\)</span> sont des paramètres inconnus et
<span class="math inline">\(\epsilon\)</span> est un bruit aléatoire
centré, i.e., <span class="math inline">\(\mathbb{E}[\epsilon] =
0\)</span>, indépendant des variables explicatives.</p>
</div>
<p>En notation vectorielle, ce modèle peut s’écrire de manière compacte
comme suit : <span class="math display">\[Y = \mathbf{X}^T
\boldsymbol{\beta} + \epsilon,\]</span> où <span
class="math inline">\(\mathbf{X} = (1, X_1, X_2, \ldots, X_p)^T\)</span>
est le vecteur des variables explicatives et <span
class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots,
\beta_p)^T\)</span> est le vecteur des paramètres.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’estimation des paramètres <span
class="math inline">\(\boldsymbol{\beta}\)</span> est un enjeu central
dans l’analyse de régression. Nous considérons ici le cadre des moindres
carrés, qui minimise la somme des écarts quadratiques entre les
observations et les prédictions du modèle.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((Y_i, \mathbf{X}_i)\)</span>, pour
<span class="math inline">\(i = 1, \ldots, n\)</span>, un échantillon
indépendant et identiquement distribué (i.i.d.) suivant le modèle de
régression linéaire multiple. L’estimateur des moindres carrés de <span
class="math inline">\(\boldsymbol{\beta}\)</span> est donné par : <span
class="math display">\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^T
\mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y},\]</span> où <span
class="math inline">\(\mathbf{X}\)</span> est la matrice des variables
explicatives de taille <span class="math inline">\(n \times
(p+1)\)</span> et <span class="math inline">\(\mathbf{Y}\)</span> est le
vecteur des observations de taille <span
class="math inline">\(n\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer ce théorème, nous procédons par minimisation de la
fonction de coût des moindres carrés.</p>
<div class="proof">
<p><em>Proof.</em> La fonction de coût à minimiser est : <span
class="math display">\[S(\boldsymbol{\beta}) = \sum_{i=1}^n (Y_i -
\mathbf{X}_i^T \boldsymbol{\beta})^2.\]</span> En développant cette
expression, nous obtenons : <span
class="math display">\[S(\boldsymbol{\beta}) = \mathbf{Y}^T \mathbf{Y} -
2 \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{Y} + \boldsymbol{\beta}^T
\mathbf{X}^T \mathbf{X} \boldsymbol{\beta}.\]</span> Pour trouver le
minimum de cette fonction quadratique, nous calculons son gradient par
rapport à <span class="math inline">\(\boldsymbol{\beta}\)</span> et le
mettons à zéro : <span class="math display">\[\frac{\partial S}{\partial
\boldsymbol{\beta}} = -2 \mathbf{X}^T \mathbf{Y} + 2 \mathbf{X}^T
\mathbf{X} \boldsymbol{\beta} = 0.\]</span> En résolvant cette équation,
nous obtenons : <span class="math display">\[\mathbf{X}^T \mathbf{X}
\boldsymbol{\beta} = \mathbf{X}^T \mathbf{Y}.\]</span> Ainsi,
l’estimateur des moindres carrés est : <span
class="math display">\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^T
\mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}.\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous énonçons maintenant quelques propriétés importantes de
l’estimateur des moindres carrés.</p>
<div class="corollaire">
<p>Sous les hypothèses du modèle de régression linéaire multiple,
l’estimateur des moindres carrés <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> vérifie les
propriétés suivantes :</p>
<ol>
<li><p><strong>Non-biais</strong> : <span
class="math inline">\(\mathbb{E}[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>.</p></li>
<li><p><strong>Variance minimale</strong> : <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> est l’estimateur
linéaire non biaisé de variance minimale.</p></li>
<li><p><strong>Distribution</strong> : Si <span
class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2
I_n)\)</span>, alors <span
class="math inline">\(\hat{\boldsymbol{\beta}} \sim
\mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^T
\mathbf{X})^{-1})\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Preuve du corollaire.</em></p>
<ol>
<li><p>Le non-biais découle directement de la linéarité de l’estimateur
et de l’hypothèse <span class="math inline">\(\mathbb{E}[\epsilon] =
0\)</span>.</p></li>
<li><p>La variance minimale est une conséquence du théorème de
Gauss-Markov, qui stipule que l’estimateur des moindres carrés est le
meilleur estimateur linéaire non biaisé (BLUE) sous les hypothèses du
modèle.</p></li>
<li><p>La distribution normale de <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> résulte du
théorème de Cochran et des propriétés des distributions normales
multivariées.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le modèle de régression linéaire offre un cadre rigoureux et puissant
pour l’analyse des relations entre variables. Ses applications sont
vastes, allant de la prédiction à l’explication des phénomènes
complexes. Les développements récents en statistique et en apprentissage
automatique continuent d’enrichir ce modèle, le rendant toujours plus
adapté aux défis contemporains de l’analyse des données.</p>
</body>
</html>
{% include "footer.html" %}


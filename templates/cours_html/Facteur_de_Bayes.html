{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Facteur de Bayes : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Facteur de Bayes : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le facteur de Bayes, nommé en l’honneur du révérend Thomas Bayes
(1702–1761), est un outil fondamental en théorie des probabilités et en
statistique bayésienne. Il permet de quantifier la manière dont une
nouvelle information modifie nos croyances a priori sur un paramètre ou
une hypothèse. Cette notion émerge naturellement dans le cadre de la
mise à jour des probabilités conditionnelles, et elle est indispensable
pour toute analyse statistique rigoureuse où l’incertitude doit être
prise en compte de manière cohérente.</p>
<p>L’origine historique du facteur de Bayes remonte au XVIIIe siècle,
avec les travaux pionniers de Thomas Bayes sur l’inférence probabiliste.
Cependant, c’est seulement au XXe siècle que ce concept a été formalisé
et intégré dans la théorie statistique moderne. Aujourd’hui, le facteur
de Bayes est utilisé dans une multitude de domaines, allant de la
médecine à l’intelligence artificielle, en passant par les sciences
sociales et les finances.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire le facteur de Bayes, commençons par comprendre ce que
nous cherchons à mesurer. Supposons que nous ayons une hypothèse <span
class="math inline">\(H\)</span> et des données observées <span
class="math inline">\(D\)</span>. Nous voulons quantifier comment les
données <span class="math inline">\(D\)</span> modifient la probabilité
de <span class="math inline">\(H\)</span>. Intuitivement, le facteur de
Bayes mesure le rapport entre la probabilité des données sous
l’hypothèse <span class="math inline">\(H\)</span> et la probabilité des
mêmes données sous une hypothèse alternative <span
class="math inline">\(\neg H\)</span>.</p>
<p>Formellement, le facteur de Bayes est défini comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(H\)</span> une hypothèse et <span
class="math inline">\(D\)</span> des données observées. Le facteur de
Bayes <span class="math inline">\(B_{H|\neg H}\)</span> est donné par :
<span class="math display">\[B_{H|\neg H} = \frac{P(D|H)}{P(D|\neg
H)}\]</span> où <span class="math inline">\(P(D|H)\)</span> est la
probabilité des données sous l’hypothèse <span
class="math inline">\(H\)</span>, et <span
class="math inline">\(P(D|\neg H)\)</span> est la probabilité des
données sous l’hypothèse alternative <span class="math inline">\(\neg
H\)</span>.</p>
</div>
<p>Une autre manière de formuler le facteur de Bayes est en utilisant
les probabilités a priori et a posteriori :</p>
<p><span class="math display">\[B_{H|\neg H} = \frac{P(H|D)}{P(\neg
H|D)} \cdot \frac{P(\neg H)}{P(H)}\]</span></p>
<p>où <span class="math inline">\(P(H|D)\)</span> et <span
class="math inline">\(P(\neg H|D)\)</span> sont les probabilités a
posteriori des hypothèses <span class="math inline">\(H\)</span> et
<span class="math inline">\(\neg H\)</span>, respectivement, et <span
class="math inline">\(P(H)\)</span> et <span
class="math inline">\(P(\neg H)\)</span> sont les probabilités a
priori.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le facteur de Bayes est intimement lié au théorème de Bayes, qui
fournit une relation fondamentale entre les probabilités
conditionnelles. Commençons par rappeler ce théorème.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(H\)</span> une hypothèse et <span
class="math inline">\(D\)</span> des données observées. Le théorème de
Bayes s’énonce comme suit : <span class="math display">\[P(H|D) =
\frac{P(D|H)P(H)}{P(D)}\]</span> où <span
class="math inline">\(P(D)\)</span> est la probabilité totale des
données, donnée par : <span class="math display">\[P(D) = P(D|H)P(H) +
P(D|\neg H)P(\neg H)\]</span></p>
</div>
<p>En utilisant le théorème de Bayes, nous pouvons réécrire le facteur
de Bayes en termes des probabilités a priori et a posteriori :</p>
<p><span class="math display">\[B_{H|\neg H} = \frac{P(H|D)}{1 - P(H|D)}
\cdot \frac{1 - P(H)}{P(H)}\]</span></p>
<p>Cette formulation montre comment le facteur de Bayes permet de relier
les probabilités a priori et a posteriori.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de Bayes, nous partons des définitions des
probabilités conditionnelles. Par définition, nous avons :</p>
<p><span class="math display">\[P(H|D) = \frac{P(D \cap
H)}{P(D)}\]</span></p>
<p>De même, nous avons :</p>
<p><span class="math display">\[P(D|H) = \frac{P(D \cap
H)}{P(H)}\]</span></p>
<p>En combinant ces deux équations, nous obtenons :</p>
<p><span class="math display">\[P(D \cap H) = P(D|H)P(H)\]</span></p>
<p>De manière similaire, nous avons :</p>
<p><span class="math display">\[P(D \cap \neg H) = P(D|\neg H)P(\neg
H)\]</span></p>
<p>En ajoutant ces deux équations, nous obtenons la probabilité totale
des données :</p>
<p><span class="math display">\[P(D) = P(D|H)P(H) + P(D|\neg H)P(\neg
H)\]</span></p>
<p>En substituant cette expression dans la définition de <span
class="math inline">\(P(H|D)\)</span>, nous obtenons le théorème de
Bayes :</p>
<p><span class="math display">\[P(H|D) =
\frac{P(D|H)P(H)}{P(D)}\]</span></p>
<p>Pour démontrer la relation entre le facteur de Bayes et les
probabilités a priori et a posteriori, nous utilisons le théorème de
Bayes pour exprimer <span class="math inline">\(P(H|D)\)</span> et <span
class="math inline">\(P(\neg H|D)\)</span> :</p>
<p><span class="math display">\[P(H|D) =
\frac{P(D|H)P(H)}{P(D)}\]</span></p>
<p><span class="math display">\[P(\neg H|D) = \frac{P(D|\neg H)P(\neg
H)}{P(D)}\]</span></p>
<p>En prenant le rapport de ces deux équations, nous obtenons :</p>
<p><span class="math display">\[\frac{P(H|D)}{P(\neg H|D)} =
\frac{P(D|H)P(H)}{P(D|\neg H)P(\neg H)}\]</span></p>
<p>En utilisant la définition du facteur de Bayes, nous avons :</p>
<p><span class="math display">\[B_{H|\neg H} = \frac{P(D|H)}{P(D|\neg
H)} = \frac{P(H|D)P(\neg H)}{P(\neg H|D)P(H)}\]</span></p>
<p>En réarrangeant cette équation, nous obtenons :</p>
<p><span class="math display">\[B_{H|\neg H} = \frac{P(H|D)}{1 - P(H|D)}
\cdot \frac{1 - P(H)}{P(H)}\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le facteur de Bayes possède plusieurs propriétés intéressantes, que
nous allons maintenant explorer.</p>
<ol>
<li><p><strong>Interprétation en termes de preuves</strong> : Le facteur
de Bayes mesure la force des preuves fournies par les données <span
class="math inline">\(D\)</span> en faveur de l’hypothèse <span
class="math inline">\(H\)</span> par rapport à l’hypothèse alternative
<span class="math inline">\(\neg H\)</span>. Un facteur de Bayes
supérieur à 1 indique que les données soutiennent l’hypothèse <span
class="math inline">\(H\)</span>, tandis qu’un facteur inférieur à 1
indique que les données soutiennent l’hypothèse alternative.</p></li>
<li><p><strong>Additivité</strong> : Le facteur de Bayes est additif en
ce sens que si nous avons plusieurs ensembles de données <span
class="math inline">\(D_1, D_2, \ldots, D_n\)</span>, le facteur de
Bayes global est le produit des facteurs de Bayes pour chaque ensemble
de données : <span class="math display">\[B_{H|\neg H}(D_1, D_2, \ldots,
D_n) = B_{H|\neg H}(D_1) \cdot B_{H|\neg H}(D_2) \cdot \ldots \cdot
B_{H|\neg H}(D_n)\]</span></p></li>
<li><p><strong>Invariance</strong> : Le facteur de Bayes est invariant
sous les transformations des probabilités a priori. Cela signifie que si
nous changeons les probabilités a priori <span
class="math inline">\(P(H)\)</span> et <span
class="math inline">\(P(\neg H)\)</span>, le facteur de Bayes reste le
même, car il ne dépend que des probabilités conditionnelles <span
class="math inline">\(P(D|H)\)</span> et <span
class="math inline">\(P(D|\neg H)\)</span>.</p></li>
</ol>
<p>Pour démontrer la propriété d’additivité, nous utilisons le fait que
les probabilités conditionnelles sont multiplicatives. Supposons que
nous ayons deux ensembles de données indépendants <span
class="math inline">\(D_1\)</span> et <span
class="math inline">\(D_2\)</span>. Alors, nous avons :</p>
<p><span class="math display">\[P(D_1, D_2|H) =
P(D_1|H)P(D_2|H)\]</span></p>
<p><span class="math display">\[P(D_1, D_2|\neg H) = P(D_1|\neg
H)P(D_2|\neg H)\]</span></p>
<p>En prenant le rapport de ces deux équations, nous obtenons :</p>
<p><span class="math display">\[B_{H|\neg H}(D_1, D_2) =
\frac{P(D_1|H)P(D_2|H)}{P(D_1|\neg H)P(D_2|\neg H)} = B_{H|\neg H}(D_1)
\cdot B_{H|\neg H}(D_2)\]</span></p>
<p>Cette propriété se généralise facilement à plusieurs ensembles de
données.</p>
<p>Pour démontrer la propriété d’invariance, nous observons que le
facteur de Bayes ne dépend pas des probabilités a priori <span
class="math inline">\(P(H)\)</span> et <span
class="math inline">\(P(\neg H)\)</span>, mais seulement des
probabilités conditionnelles <span class="math inline">\(P(D|H)\)</span>
et <span class="math inline">\(P(D|\neg H)\)</span>. Par conséquent,
toute transformation des probabilités a priori ne change pas le facteur
de Bayes.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le facteur de Bayes est un outil puissant et élégant pour quantifier
la manière dont les données modifient nos croyances sur une hypothèse.
Il trouve des applications dans de nombreux domaines, allant de la
statistique bayésienne à l’apprentissage automatique. En comprenant les
fondements et les propriétés du facteur de Bayes, nous pouvons mieux
appréhender l’incertitude et prendre des décisions plus éclairées dans
un contexte probabiliste.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Relative de Kullback-Leibler</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Relative de Kullback-Leibler</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie relative, également connue sous le nom de divergence de
Kullback-Leibler (KL), est une notion fondamentale en théorie de
l’information et en statistique. Elle mesure la distance entre deux
distributions de probabilité, quantifiant ainsi la quantité
d’information perdue lorsqu’on utilise une distribution approximative au
lieu de la distribution exacte.</p>
<p>L’idée sous-jacente à l’entropie relative remonte aux travaux de
Claude Shannon sur la théorie de l’information dans les années 1940.
Cependant, c’est Abraham Kullback et Richard Leibler qui ont formalisé
cette notion en 1951, dans le cadre de leurs recherches sur les tests
d’hypothèses statistiques. L’entropie relative est indispensable dans de
nombreux domaines, notamment en apprentissage automatique, en
compression de données, et en théorie des codes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir l’entropie relative, commençons par comprendre ce que
nous cherchons à mesurer. Supposons que nous ayons deux distributions de
probabilité, <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur le même espace. Nous
voulons quantifier la différence entre ces deux distributions, en termes
d’information.</p>
<p>L’entropie relative de <span class="math inline">\(P\)</span> par
rapport à <span class="math inline">\(Q\)</span> est une mesure de la
quantité d’information perdue lorsque nous utilisons <span
class="math inline">\(Q\)</span> pour approximer <span
class="math inline">\(P\)</span>. Plus précisément, elle mesure la
surprise moyenne lorsque nous utilisons <span
class="math inline">\(Q\)</span> au lieu de <span
class="math inline">\(P\)</span>.</p>
<p>Formellement, l’entropie relative est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur le même espace. L’entropie relative de <span
class="math inline">\(P\)</span> par rapport à <span
class="math inline">\(Q\)</span>, notée <span class="math inline">\(D(P
\| Q)\)</span>, est définie par : <span class="math display">\[D(P \| Q)
= \sum_{x} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</span> pour des
distributions discrètes, et <span class="math display">\[D(P \| Q) =
\int_{-\infty}^{\infty} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
dx\]</span> pour des distributions continues.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant l’entropie relative est le
théorème de Gibbs, qui établit une relation entre l’entropie relative et
l’entropie de Shannon.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur le même espace. Alors, <span class="math display">\[D(P \|
Q) = H(P, Q) - H(P)\]</span> où <span class="math inline">\(H(P,
Q)\)</span> est l’entropie croisée de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définie par : <span
class="math display">\[H(P, Q) = -\sum_{x} P(x) \log Q(x)\]</span> et
<span class="math inline">\(H(P)\)</span> est l’entropie de Shannon de
<span class="math inline">\(P\)</span>, définie par : <span
class="math display">\[H(P) = -\sum_{x} P(x) \log P(x)\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Gibbs, commençons par rappeler les
définitions des entropies impliquées.</p>
<div class="proof">
<p><em>Proof.</em> Par définition de l’entropie croisée et de l’entropie
de Shannon, nous avons : <span class="math display">\[H(P, Q) =
-\sum_{x} P(x) \log Q(x)\]</span> et <span class="math display">\[H(P) =
-\sum_{x} P(x) \log P(x)\]</span> En soustrayant ces deux expressions,
nous obtenons : <span class="math display">\[H(P, Q) - H(P) = -\sum_{x}
P(x) \log Q(x) + \sum_{x} P(x) \log P(x)\]</span> En utilisant la
propriété de l’addition des logarithmes, nous pouvons réécrire cette
expression comme : <span class="math display">\[H(P, Q) - H(P) =
\sum_{x} P(x) (\log P(x) - \log Q(x)) = \sum_{x} P(x) \log \left(
\frac{P(x)}{Q(x)} \right)\]</span> Ce qui est exactement la définition
de l’entropie relative <span class="math inline">\(D(P \| Q)\)</span>.
Ainsi, nous avons : <span class="math display">\[D(P \| Q) = H(P, Q) -
H(P)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie relative possède plusieurs propriétés importantes, que
nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie relative est
toujours non négative. <span class="math display">\[D(P \| Q) \geq
0\]</span> avec égalité si et seulement si <span class="math inline">\(P
= Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité de l’entropie relative découle
directement de l’inégalité de Gibbs, qui stipule que pour toute
distribution de probabilité <span class="math inline">\(P\)</span> et
toute distribution <span class="math inline">\(Q\)</span>, nous avons :
<span class="math display">\[D(P \| Q) \geq 0\]</span> avec égalité si
et seulement si <span class="math inline">\(P = Q\)</span>. ◻</p>
</div></li>
<li><p><strong>Inégalité de Gibbs</strong> : Pour toute distribution
<span class="math inline">\(P\)</span> et toute distribution <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[D(P \| Q) \geq 0\]</span> avec égalité si et
seulement si <span class="math inline">\(P = Q\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cette propriété est une conséquence directe de la
définition de l’entropie relative et des propriétés des logarithmes. En
effet, pour toute distribution <span class="math inline">\(P\)</span> et
toute distribution <span class="math inline">\(Q\)</span>, nous avons :
<span class="math display">\[D(P \| Q) = \sum_{x} P(x) \log \left(
\frac{P(x)}{Q(x)} \right) \geq 0\]</span> avec égalité si et seulement
si <span class="math inline">\(P(x) = Q(x)\)</span> pour tout <span
class="math inline">\(x\)</span>. ◻</p>
</div></li>
<li><p><strong>Inégalité de Jensen</strong> : Pour toute fonction
convexe <span class="math inline">\(f\)</span>, nous avons : <span
class="math display">\[\mathbb{E}[f(X)] \geq f(\mathbb{E}[X])\]</span>
où <span class="math inline">\(\mathbb{E}\)</span> désigne
l’espérance.</p>
<div class="proof">
<p><em>Proof.</em> L’inégalité de Jensen est une conséquence directe de
la convexité de la fonction <span class="math inline">\(f\)</span>. En
effet, pour toute fonction convexe <span
class="math inline">\(f\)</span>, nous avons : <span
class="math display">\[f\left( \sum_{i} \lambda_i x_i \right) \leq
\sum_{i} \lambda_i f(x_i)\]</span> pour toute famille de coefficients
<span class="math inline">\(\lambda_i\)</span> tels que <span
class="math inline">\(\sum_{i} \lambda_i = 1\)</span>. En prenant <span
class="math inline">\(\lambda_i = P(x_i)\)</span>, nous obtenons : <span
class="math display">\[f\left( \sum_{i} P(x_i) x_i \right) \leq \sum_{i}
P(x_i) f(x_i)\]</span> ce qui est exactement l’inégalité de
Jensen. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie relative de Kullback-Leibler est une notion centrale en
théorie de l’information et en statistique. Elle permet de quantifier la
différence entre deux distributions de probabilité, mesurant ainsi la
quantité d’information perdue lorsqu’on utilise une distribution
approximative. Les propriétés et théorèmes associés à l’entropie
relative sont nombreux et trouvent des applications dans de nombreux
domaines, notamment en apprentissage automatique, en compression de
données, et en théorie des codes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage Bag-of-Words : Une Approche Fondamentale en Traitement Automatique des Langues</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage Bag-of-Words : Une Approche Fondamentale en
Traitement Automatique des Langues</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Encodage Bag-of-Words (BoW) est une technique fondamentale en
traitement automatique des langues (TAL). Son origine remonte aux années
1950, mais c’est avec l’essor des méthodes statistiques et des modèles
de langage que le BoW a trouvé sa place centrale. Cette approche simple
mais puissante permet de représenter des documents textuels sous forme
de vecteurs numériques, facilitant ainsi l’application d’algorithmes
d’apprentissage automatique.</p>
<p>Le BoW émerge comme une solution élégante à plusieurs problèmes : il
permet de transformer des données textuelles non structurées en
représentations structurées, tout en étant computativement efficace. Son
utilisation est indispensable dans des tâches variées telles que la
classification de texte, la recherche d’information et l’analyse de
sentiments.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le BoW, commençons par définir ce que nous cherchons
à obtenir. Nous voulons représenter un document textuel comme une
collection de mots, sans tenir compte de l’ordre des mots ni de leur
syntaxe. L’idée est de compter la fréquence d’apparition de chaque mot
dans le document.</p>
<p>Formellement, soit <span class="math inline">\(D\)</span> un document
composé d’un ensemble de mots <span class="math inline">\(\{w_1, w_2,
\ldots, w_n\}\)</span>, et soit <span class="math inline">\(V = \{v_1,
v_2, \ldots, v_m\}\)</span> un vocabulaire contenant tous les mots
possibles dans notre corpus. L’encodage BoW d’un document <span
class="math inline">\(D\)</span> est un vecteur <span
class="math inline">\(\mathbf{x} \in \mathbb{N}^m\)</span> tel que :</p>
<p><span class="math display">\[\mathbf{x}_i =
\begin{cases}
\text{le nombre d&#39;occurrences de } v_i \text{ dans } D &amp;
\text{si } v_i \in D, \\
0 &amp; \text{sinon.}
\end{cases}\]</span></p>
<p>En d’autres termes, pour chaque mot du vocabulaire <span
class="math inline">\(V\)</span>, nous comptons combien de fois il
apparaît dans le document <span class="math inline">\(D\)</span>. Cela
peut être formalisé comme suit :</p>
<p><span class="math display">\[\mathbf{x} = (x_1, x_2, \ldots, x_m)
\quad \text{où} \quad x_i = \sum_{w \in D} \mathbb{I}(w =
v_i)\]</span></p>
<p>où <span class="math inline">\(\mathbb{I}\)</span> est la fonction
indicatrice qui vaut 1 si la condition est vraie et 0 sinon.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème central lié au BoW est le théorème de la représentation
vectorielle, qui stipule que tout document peut être représenté comme un
vecteur dans un espace de dimension égale à la taille du
vocabulaire.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(D\)</span> un document et <span
class="math inline">\(V\)</span> un vocabulaire. Il existe une fonction
<span class="math inline">\(f: D \times V \rightarrow
\mathbb{N}\)</span> telle que pour tout document <span
class="math inline">\(D\)</span>, il existe un vecteur <span
class="math inline">\(\mathbf{x} \in \mathbb{N}^{|V|}\)</span>
représentant le document.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous définissons la fonction <span
class="math inline">\(f\)</span> comme suit :</p>
<p><span class="math display">\[f(D, v_i) = \sum_{w \in D} \mathbb{I}(w
= v_i)\]</span></p>
<p>Cette fonction compte le nombre d’occurrences du mot <span
class="math inline">\(v_i\)</span> dans le document <span
class="math inline">\(D\)</span>. Ainsi, pour chaque document <span
class="math inline">\(D\)</span>, nous pouvons construire un vecteur
<span class="math inline">\(\mathbf{x}\)</span> où chaque composante
<span class="math inline">\(x_i\)</span> est donnée par <span
class="math inline">\(f(D, v_i)\)</span>.</p>
<p>Par conséquent, tout document <span class="math inline">\(D\)</span>
peut être représenté par un vecteur <span
class="math inline">\(\mathbf{x} \in \mathbb{N}^{|V|}\)</span>, ce qui
prouve le théorème. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le BoW possède plusieurs propriétés intéressantes qui en font une
technique puissante.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(V\)</span> un vocabulaire de taille
<span class="math inline">\(m\)</span>. Tout vecteur BoW <span
class="math inline">\(\mathbf{x}\)</span> est un élément de l’espace
vectoriel <span class="math inline">\(\mathbb{N}^m\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition, le vecteur BoW <span
class="math inline">\(\mathbf{x}\)</span> est un vecteur de dimension
<span class="math inline">\(m\)</span>, où chaque composante <span
class="math inline">\(x_i\)</span> est un entier naturel représentant le
nombre d’occurrences du mot <span class="math inline">\(v_i\)</span>
dans le document. Ainsi, <span class="math inline">\(\mathbf{x} \in
\mathbb{N}^m\)</span>. ◻</p>
</div>
<div class="corollary">
<p>Soit <span class="math inline">\(D\)</span> un document et <span
class="math inline">\(V\)</span> un vocabulaire. Le vecteur BoW <span
class="math inline">\(\mathbf{x}\)</span> associé à <span
class="math inline">\(D\)</span> est généralement très sparse,
c’est-à-dire que la plupart de ses composantes sont nulles.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Puisque le document <span
class="math inline">\(D\)</span> ne contient généralement qu’un
sous-ensemble des mots du vocabulaire <span
class="math inline">\(V\)</span>, la plupart des composantes de <span
class="math inline">\(\mathbf{x}\)</span> seront nulles. Cela est dû au
fait que pour les mots <span class="math inline">\(v_i\)</span> qui
n’apparaissent pas dans <span class="math inline">\(D\)</span>, <span
class="math inline">\(x_i = 0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Encodage Bag-of-Words est une technique fondamentale en TAL qui
permet de représenter des documents textuels sous forme de vecteurs
numériques. Son utilisation est indispensable dans diverses tâches
telles que la classification de texte et la recherche d’information.
Bien que simple, le BoW possède des propriétés intéressantes qui en font
une méthode puissante et efficace.</p>
</body>
</html>
{% include "footer.html" %}


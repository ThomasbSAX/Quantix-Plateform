{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Jensen-Bregman : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Jensen-Bregman : Une Exploration
Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Jensen-Bregman (JB) émerge à l’intersection de la
théorie de l’information et de l’analyse convexe. Son origine remonte
aux travaux de Harald Cramér sur les limites des probabilités et à la
théorie de l’information de Claude Shannon. La divergence de
Jensen-Bregman est indispensable dans les domaines où la mesure des
écarts entre distributions de probabilité est cruciale, comme
l’apprentissage automatique et la statistique.</p>
<p>Cette divergence résout un problème fondamental : comment mesurer
l’écart entre deux distributions de probabilité tout en préservant les
propriétés mathématiques souhaitables, telles que la non-négativité et
l’invariance sous transformation. La divergence de Jensen-Bregman est
particulièrement utile lorsque les distributions sont modélisées par des
fonctions convexes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Jensen-Bregman, commençons par les
concepts fondamentaux. Supposons que nous ayons une fonction convexe
<span class="math inline">\(\phi : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span>. Nous cherchons une mesure de l’écart entre deux
points <span class="math inline">\(x, y \in \mathbb{R}^n\)</span> qui
capture la "distance" entre eux en utilisant les propriétés de <span
class="math inline">\(\phi\)</span>.</p>
<p>La divergence de Jensen-Bregman est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\phi : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable.
La divergence de Jensen-Bregman entre deux points <span
class="math inline">\(x, y \in \mathbb{R}^n\)</span> est donnée par :
<span class="math display">\[D_{\phi}(x, y) = \phi(x) - \phi(y) -
\langle \nabla \phi(y), x - y \rangle\]</span></p>
</div>
<p>Cette définition peut être reformulée en utilisant les propriétés de
la fonction convexe <span class="math inline">\(\phi\)</span>. Notons
que <span class="math inline">\(D_{\phi}(x, y)\)</span> est toujours
non-négatif en raison de la convexité stricte de <span
class="math inline">\(\phi\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence de Jensen-Bregman
est le suivant :</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(\phi : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable.
Alors, pour tout <span class="math inline">\(x, y \in
\mathbb{R}^n\)</span>, la divergence de Jensen-Bregman satisfait : <span
class="math display">\[D_{\phi}(x, y) \geq 0\]</span> avec égalité si et
seulement si <span class="math inline">\(x = y\)</span>.</p>
</div>
<p>La preuve de ce théorème repose sur l’utilisation du fait que <span
class="math inline">\(\phi\)</span> est strictement convexe. En effet,
par convexité stricte, nous avons : <span class="math display">\[\phi(x)
&gt; \phi(y) + \langle \nabla \phi(y), x - y \rangle\]</span> pour tout
<span class="math inline">\(x \neq y\)</span>.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous procédons comme suit :</p>
<div class="preuve">
<p>Soit <span class="math inline">\(\phi : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable.
Par définition de la convexité stricte, pour tout <span
class="math inline">\(x \neq y\)</span>, nous avons : <span
class="math display">\[\phi(x) &gt; \phi(y) + \langle \nabla \phi(y), x
- y \rangle\]</span> En réarrangeant cette inégalité, nous obtenons :
<span class="math display">\[\phi(x) - \phi(y) - \langle \nabla \phi(y),
x - y \rangle &gt; 0\]</span> Ce qui est exactement la définition de
<span class="math inline">\(D_{\phi}(x, y)\)</span>. Par conséquent :
<span class="math display">\[D_{\phi}(x, y) &gt; 0\]</span> pour tout
<span class="math inline">\(x \neq y\)</span>. Si <span
class="math inline">\(x = y\)</span>, alors <span
class="math inline">\(D_{\phi}(x, y) = 0\)</span>. Ainsi, nous avons
prouvé que <span class="math inline">\(D_{\phi}(x, y) \geq 0\)</span>
avec égalité si et seulement si <span class="math inline">\(x =
y\)</span>.</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Jensen-Bregman possède plusieurs propriétés
intéressantes. Nous en listons quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Non-négativité</strong> : Comme démontré précédemment,
<span class="math inline">\(D_{\phi}(x, y) \geq 0\)</span> pour tout
<span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, avec égalité
si et seulement si <span class="math inline">\(x = y\)</span>.</p></li>
<li><p><strong>Invariance sous transformation</strong> : La divergence
de Jensen-Bregman est invariante sous les transformations affines. Plus
précisément, si <span class="math inline">\(A\)</span> est une matrice
inversible et <span class="math inline">\(b \in \mathbb{R}^n\)</span>,
alors : <span class="math display">\[D_{\phi}(Ax + b, Ay + b) =
D_{\phi}(x, y)\]</span></p></li>
<li><p><strong>Propriété de triangle</strong> : La divergence de
Jensen-Bregman satisfait une forme de propriété de triangle. Pour tout
<span class="math inline">\(x, y, z \in \mathbb{R}^n\)</span>, nous
avons : <span class="math display">\[D_{\phi}(x, z) \leq D_{\phi}(x, y)
+ D_{\phi}(y, z)\]</span></p></li>
</ol>
<p>La preuve de ces propriétés repose sur les propriétés fondamentales
des fonctions convexes et des dérivées. Par exemple, pour la propriété
d’invariance sous transformation, nous utilisons le fait que les
transformations affines préservent la convexité.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Jensen-Bregman est un outil puissant pour mesurer
les écarts entre distributions de probabilité. Ses propriétés
mathématiques robustes en font un choix privilégié dans de nombreuses
applications pratiques, notamment en apprentissage automatique et en
statistique. En comprenant les définitions, théorèmes et preuves
associés à cette divergence, nous pouvons mieux apprécier son utilité et
son importance dans le paysage mathématique moderne.</p>
</body>
</html>
{% include "footer.html" %}


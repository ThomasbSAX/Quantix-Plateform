{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Graph Neural Networks: A Comprehensive Overview</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Graph Neural Networks: A Comprehensive Overview</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>Graph Neural Networks (GNNs) have emerged as a powerful tool for
learning on graph-structured data. The need for such models arises from
the ubiquity of graphs in real-world applications, ranging from social
networks and molecular structures to knowledge graphs. Traditional
machine learning models, designed for grid-like data such as images or
sequences, struggle to capture the complex relational information
inherent in graphs. GNNs address this limitation by operating directly
on graph data, enabling the extraction of meaningful patterns and
representations.</p>
<p>The concept of GNNs is rooted in the fusion of graph theory and
neural networks. Early attempts to apply neural networks to graphs
involved treating them as collections of nodes or edges, but these
approaches failed to capture the intricate dependencies and interactions
within the graph. The introduction of message passing mechanisms allowed
GNNs to propagate information across the graph, effectively learning
from both local and global structures.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand GNNs, we first need to define some key concepts.
Consider a graph <span class="math inline">\(G = (V, E)\)</span>, where
<span class="math inline">\(V\)</span> is the set of nodes and <span
class="math inline">\(E\)</span> is the set of edges. Each node <span
class="math inline">\(v_i \in V\)</span> has a feature vector <span
class="math inline">\(x_i\)</span>, and each edge <span
class="math inline">\((v_i, v_j) \in E\)</span> has a weight <span
class="math inline">\(w_{ij}\)</span>.</p>
<p>A Graph Neural Network is a neural network architecture designed to
operate on graph-structured data. The goal is to learn a function <span
class="math inline">\(f\)</span> that maps the input graph <span
class="math inline">\(G\)</span> and its node features <span
class="math inline">\(X\)</span> to a set of output values <span
class="math inline">\(Y\)</span>. Formally, we seek:</p>
<p><span class="math display">\[f: (G, X) \mapsto Y\]</span></p>
<p>where <span class="math inline">\(G = (V, E)\)</span>, <span
class="math inline">\(X = \{x_1, x_2, \ldots, x_{|V|}\}\)</span>, and
<span class="math inline">\(Y = \{y_1, y_2, \ldots,
y_{|V|}\}\)</span>.</p>
<p>The output <span class="math inline">\(Y\)</span> can represent
node-level predictions, graph-level predictions, or edge-level
predictions, depending on the task at hand. For node-level tasks, <span
class="math inline">\(y_i\)</span> is the prediction for node <span
class="math inline">\(v_i\)</span>. For graph-level tasks, <span
class="math inline">\(Y\)</span> is a single value representing the
prediction for the entire graph. For edge-level tasks, <span
class="math inline">\(y_{ij}\)</span> is the prediction for the edge
<span class="math inline">\((v_i, v_j)\)</span>.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems in the theory of GNNs is the
Weisfeiler-Lehman (WL) isomorphism test. This theorem provides a way to
distinguish between non-isomorphic graphs and is closely related to the
expressive power of GNNs.</p>
<div class="theorem">
<p>Let <span class="math inline">\(G_1\)</span> and <span
class="math inline">\(G_2\)</span> be two graphs. The WL test
iteratively assigns colors to the nodes based on their neighborhood
information. If, after a finite number of iterations, the color
multisets of <span class="math inline">\(G_1\)</span> and <span
class="math inline">\(G_2\)</span> are identical, then the graphs are
considered isomorphic by the WL test.</p>
</div>
<p>The expressive power of GNNs is often analyzed in terms of their
ability to distinguish between non-isomorphic graphs. A GNN that can
distinguish all pairs of non-isomorphic graphs is said to be as powerful
as the WL test.</p>
<div class="theorem">
<p>A Graph Neural Network is as powerful as the Weisfeiler-Lehman test
if and only if it can distinguish all pairs of non-isomorphic
graphs.</p>
</div>
<h1 id="proofs">Proofs</h1>
<p>To prove the expressive power of GNNs, we need to show that they can
capture the necessary information from the graph structure. Consider a
GNN with <span class="math inline">\(L\)</span> layers, where each layer
performs a message passing operation followed by an update step.</p>
<div class="proof">
<p><em>Proof.</em> The message passing operation aggregates information
from the neighbors of each node, effectively propagating information
across the graph. The update step applies a neural network to the
aggregated messages and the node’s current state, allowing the GNN to
learn complex patterns.</p>
<p>Formally, for each layer <span class="math inline">\(l\)</span> from
1 to <span class="math inline">\(L\)</span>, the message passing and
update steps can be written as:</p>
<p><span class="math display">\[m_{v_i}^{(l)} = \sum_{v_j \in N(v_i)}
M^{(l)}(h_{v_i}^{(l-1)}, h_{v_j}^{(l-1)}, e_{ij})\]</span></p>
<p><span class="math display">\[h_{v_i}^{(l)} = U^{(l)}(h_{v_i}^{(l-1)},
m_{v_i}^{(l)})\]</span></p>
<p>where <span class="math inline">\(M^{(l)}\)</span> and <span
class="math inline">\(U^{(l)}\)</span> are neural networks, <span
class="math inline">\(h_{v_i}^{(l-1)}\)</span> is the hidden state of
node <span class="math inline">\(v_i\)</span> at layer <span
class="math inline">\(l-1\)</span>, and <span
class="math inline">\(e_{ij}\)</span> is the edge feature between nodes
<span class="math inline">\(v_i\)</span> and <span
class="math inline">\(v_j\)</span>.</p>
<p>By iteratively applying these operations, the GNN can capture
information from increasingly larger neighborhoods, effectively learning
a hierarchical representation of the graph. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>GNNs exhibit several important properties that make them suitable for
learning on graph-structured data. We list some of these properties and
provide detailed proofs.</p>
<ol>
<li><p><strong>Permutation Invariance:</strong> GNNs are invariant to
the ordering of nodes in the graph. This property ensures that the
output of a GNN depends only on the structural and feature information
of the graph, not on the arbitrary ordering of nodes.</p>
<div class="proof">
<p><em>Proof.</em> The message passing operation aggregates information
from the neighbors of each node, which is inherently permutation
invariant. The update step applies a neural network to the aggregated
messages and the node’s current state, preserving permutation
invariance. ◻</p>
</div></li>
<li><p><strong>Locality:</strong> GNNs can capture local patterns in the
graph by focusing on the immediate neighbors of each node. This property
allows GNNs to learn fine-grained features that are specific to small
substructures within the graph.</p>
<div class="proof">
<p><em>Proof.</em> The message passing operation aggregates information
from the neighbors of each node, effectively capturing local patterns.
By limiting the number of message passing steps, GNNs can focus on
increasingly larger neighborhoods, allowing them to learn both local and
global patterns. ◻</p>
</div></li>
<li><p><strong>Generalization:</strong> GNNs can generalize to unseen
graphs by learning a hierarchical representation of the graph structure.
This property enables GNNs to perform well on graphs that were not
observed during training.</p>
<div class="proof">
<p><em>Proof.</em> The hierarchical representation learned by GNNs
allows them to capture both local and global patterns in the graph. By
learning a hierarchical representation, GNNs can generalize to unseen
graphs by leveraging the structural and feature similarities between
seen and unseen graphs. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Graph Neural Networks have emerged as a powerful tool for learning on
graph-structured data. By operating directly on graphs, GNNs can capture
the complex relational information inherent in graph data. The
expressive power of GNNs is closely related to the Weisfeiler-Lehman
isomorphism test, and their properties make them suitable for a wide
range of applications. As research in this area continues to advance,
GNNs are poised to become an essential tool for learning on
graph-structured data.</p>
</body>
</html>
{% include "footer.html" %}


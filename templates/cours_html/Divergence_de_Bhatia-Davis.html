{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Bhatia-Davis : Une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Bhatia-Davis : Une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Bhatia-Davis émerge comme un concept fondamental
dans l’étude des matrices positives définies. Son origine remonte aux
travaux de Bhatia et Davis dans les années 1990, où ils ont cherché à
généraliser des inégalités classiques pour les matrices. Cette notion
est indispensable dans l’analyse des propriétés spectrales et des
inégalités matricielles, offrant des outils puissants pour la théorie
des matrices et ses applications en physique quantique, en traitement du
signal et en optimisation.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Bhatia-Davis, commençons par définir
les matrices positives définies. Une matrice <span
class="math inline">\(A\)</span> est dite positive définie si pour tout
vecteur non nul <span class="math inline">\(x\)</span>, on a : <span
class="math display">\[x^T A x &gt; 0.\]</span></p>
<p>La divergence de Bhatia-Davis est une mesure de la distance entre
deux matrices positives définies. Intuitivement, on cherche à quantifier
combien deux matrices <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> diffèrent en termes de leurs valeurs
propres.</p>
<div class="definition">
<p>Soient <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux matrices positives définies de
taille <span class="math inline">\(n \times n\)</span>. La divergence de
Bhatia-Davis est définie par : <span class="math display">\[D_B(A \| B)
= \tr(A) - \tr(B) + \tr(B^{1/2} A^{-1} B^{1/2}).\]</span></p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D_B(A \| B) = \sum_{i=1}^n (\lambda_i(A) -
\log(\lambda_i(B))),\]</span> où <span
class="math inline">\(\lambda_i(A)\)</span> et <span
class="math inline">\(\lambda_i(B)\)</span> sont les valeurs propres de
<span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> respectivement.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central lié à la divergence de Bhatia-Davis est
l’inégalité de Bhatia-Davis, qui fournit une borne supérieure pour cette
divergence.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux matrices positives définies de
taille <span class="math inline">\(n \times n\)</span>. Alors : <span
class="math display">\[D_B(A \| B) \leq \frac{1}{2} \|A -
B\|_F^2,\]</span> où <span class="math inline">\(\| \cdot \|_F\)</span>
désigne la norme de Frobenius.</p>
</div>
<p>La preuve de ce théorème repose sur des propriétés fondamentales des
matrices positives définies et des inégalités matricielles. Nous allons
maintenant détailler cette preuve.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Bhatia-Davis, nous utilisons la
décomposition spectrale des matrices <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. Soient <span class="math inline">\(A =
U \Lambda_A U^*\)</span> et <span class="math inline">\(B = V \Lambda_B
V^*\)</span>, où <span class="math inline">\(U\)</span> et <span
class="math inline">\(V\)</span> sont des matrices unitaires, et <span
class="math inline">\(\Lambda_A\)</span> et <span
class="math inline">\(\Lambda_B\)</span> sont des matrices diagonales
contenant les valeurs propres de <span class="math inline">\(A\)</span>
et <span class="math inline">\(B\)</span> respectivement.</p>
<p>Nous avons : <span class="math display">\[D_B(A \| B) = \tr(A) -
\tr(B) + \tr(B^{1/2} A^{-1} B^{1/2}).\]</span></p>
<p>En utilisant la décomposition spectrale, nous pouvons réécrire cette
expression en termes des valeurs propres : <span
class="math display">\[D_B(A \| B) = \sum_{i=1}^n (\lambda_i(A) -
\log(\lambda_i(B))).\]</span></p>
<p>Pour prouver l’inégalité, nous utilisons l’inégalité de Kantorovich,
qui fournit une borne supérieure pour la divergence relative. En
combinant cette inégalité avec des propriétés de la norme de Frobenius,
nous obtenons : <span class="math display">\[D_B(A \| B) \leq
\frac{1}{2} \|A - B\|_F^2.\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Bhatia-Davis possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p><strong>Positivité</strong> : Pour toute matrice positive définie
<span class="math inline">\(A\)</span>, on a <span
class="math inline">\(D_B(A \| A) = 0\)</span>.</p></li>
<li><p><strong>Convexité</strong> : La fonction <span
class="math inline">\(D_B(A \| B)\)</span> est convexe en <span
class="math inline">\(A\)</span> pour <span
class="math inline">\(B\)</span> fixe.</p></li>
<li><p><strong>Invariance par similitude</strong> : Pour toute matrice
inversible <span class="math inline">\(S\)</span>, on a <span
class="math inline">\(D_B(SAS^* \| SBS^*) = D_B(A \|
B)\)</span>.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant des
techniques d’analyse matricielles et des inégalités classiques. Par
exemple, la convexité peut être prouvée en utilisant le théorème de
Jensen et les propriétés des fonctions convexes.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Bhatia-Davis est un outil puissant pour l’étude des
matrices positives définies. Ses propriétés et ses applications en font
un sujet de recherche actif dans divers domaines des mathématiques
appliquées. En approfondissant notre compréhension de cette divergence,
nous pouvons espérer découvrir de nouvelles inégalités et applications
dans des domaines tels que la physique quantique et l’optimisation.</p>
</body>
</html>
{% include "footer.html" %}


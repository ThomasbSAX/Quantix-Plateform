{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Categorical Cross-Entropy Loss: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Categorical Cross-Entropy Loss: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, et plus particulièrement l’apprentissage
profond, repose sur des fonctions de perte capables de guider
efficacement l’optimisation des modèles. Parmi celles-ci, la
<em>Categorical Cross-Entropy Loss</em> (CCEL) émerge comme un pilier
fondamental dans les tâches de classification multi-classes. Son origine
remonte aux travaux pionniers en théorie de l’information, où la notion
d’entropie croisée a été introduite pour mesurer la divergence entre
deux distributions de probabilité. En machine learning, cette notion a
été adaptée pour quantifier l’écart entre les prédictions d’un modèle et
les véritables étiquettes des données. La CCEL est indispensable dans le
cadre de la classification multi-classes, où chaque échantillon
appartient à une seule catégorie parmi un ensemble fini. Elle permet non
seulement d’optimiser la précision des prédictions, mais aussi de
fournir une interprétation probabiliste robuste des résultats.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la CCEL, il est essentiel de saisir les concepts
sous-jacents. Supposons que nous ayons un modèle de classification qui
prédit une distribution de probabilité sur un ensemble de classes. Nous
cherchons à mesurer à quel point cette distribution prédite diverge de
la distribution réelle, qui est souvent une distribution unitaire (une
seule classe correcte). La CCEL quantifie cette divergence.</p>
<div class="definition">
<p>Soit <span class="math inline">\(p\)</span> une distribution de
probabilité réelle et <span class="math inline">\(q\)</span> une
distribution de probabilité prédite. L’entropie croisée entre <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> est définie comme : <span
class="math display">\[H(p, q) = -\sum_{i=1}^{n} p(i) \log q(i)\]</span>
où <span class="math inline">\(n\)</span> est le nombre de classes,
<span class="math inline">\(p(i)\)</span> est la probabilité réelle de
la classe <span class="math inline">\(i\)</span>, et <span
class="math inline">\(q(i)\)</span> est la probabilité prédite de la
classe <span class="math inline">\(i\)</span>.</p>
</div>
<p>Dans le contexte de la classification multi-classes, la distribution
réelle <span class="math inline">\(p\)</span> est souvent une
distribution unitaire, c’est-à-dire que <span class="math inline">\(p(i)
= 1\)</span> pour la classe correcte et <span class="math inline">\(p(j)
= 0\)</span> pour toutes les autres classes <span
class="math inline">\(j \neq i\)</span>. Ainsi, l’entropie croisée se
simplifie en : <span class="math display">\[H(p, q) = -\log
q(i)\]</span> où <span class="math inline">\(i\)</span> est la classe
correcte.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>La CCEL possède plusieurs propriétés théoriques intéressantes. L’une
des plus importantes est sa relation avec la log-vraisemblance, qui est
souvent utilisée dans les modèles probabilistes.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(y\)</span> le vecteur unitaire
représentant la classe correcte et <span
class="math inline">\(\hat{y}\)</span> le vecteur de probabilités
prédites par le modèle. La CCEL est équivalente à la négation de la
log-vraisemblance : <span class="math display">\[\text{CCEL}(y, \hat{y})
= -\log(\hat{y}_i)\]</span> où <span class="math inline">\(i\)</span>
est l’indice de la classe correcte.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle directement de la définition de
l’entropie croisée. Puisque <span class="math inline">\(y\)</span> est
un vecteur unitaire, <span class="math inline">\(y_i = 1\)</span> et
<span class="math inline">\(y_j = 0\)</span> pour tout <span
class="math inline">\(j \neq i\)</span>. Ainsi, l’entropie croisée
devient : <span class="math display">\[H(y, \hat{y}) = -\sum_{j=1}^{n}
y_j \log \hat{y}_j = -\log \hat{y}_i\]</span> Ce qui est exactement la
négation de la log-vraisemblance. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’importance de la CCEL, considérons un exemple
simple. Supposons que nous avons un modèle de classification binaire
avec deux classes. Le vecteur réel <span
class="math inline">\(y\)</span> est <span class="math inline">\([1,
0]\)</span> et le vecteur prédit <span
class="math inline">\(\hat{y}\)</span> est <span
class="math inline">\([0.7, 0.3]\)</span>. La CCEL est alors : <span
class="math display">\[\text{CCEL}(y, \hat{y}) = -\log(0.7) \approx
0.356\]</span></p>
<p>Cette valeur mesure la divergence entre les prédictions du modèle et
les véritables étiquettes. Plus cette valeur est faible, plus le modèle
est performant.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La CCEL possède plusieurs propriétés intéressantes qui en font un
outil puissant pour l’optimisation des modèles de classification.</p>
<ol>
<li><p><strong>Convexité</strong> : La CCEL est une fonction convexe par
rapport aux probabilités prédites <span
class="math inline">\(\hat{y}\)</span>. Cela signifie que les
algorithmes d’optimisation convexes peuvent être utilisés pour minimiser
efficacement cette fonction.</p>
<div class="proof">
<p><em>Proof.</em> La convexité de la CCEL découle du fait que la
fonction <span class="math inline">\(-\log(x)\)</span> est convexe pour
<span class="math inline">\(x &gt; 0\)</span>. Puisque la somme de
fonctions convexes est convexe, la CCEL est également convexe. ◻</p>
</div></li>
<li><p><strong>Sensibilité aux Probabilités</strong> : La CCEL est très
sensible aux probabilités prédites pour la classe correcte. Une petite
erreur dans la prédiction de la probabilité de la classe correcte peut
entraîner une augmentation significative de la CCEL.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux prédictions <span
class="math inline">\(\hat{y}_1\)</span> et <span
class="math inline">\(\hat{y}_2\)</span> telles que <span
class="math inline">\(\hat{y}_1 = 0.9\)</span> et <span
class="math inline">\(\hat{y}_2 = 0.8\)</span>. Les CCEL correspondantes
sont : <span class="math display">\[\text{CCEL}_1 = -\log(0.9) \approx
0.105\]</span> <span class="math display">\[\text{CCEL}_2 = -\log(0.8)
\approx 0.223\]</span> Une diminution de seulement 0.1 dans la
probabilité prédite entraîne une augmentation significative de la
CCEL. ◻</p>
</div></li>
<li><p><strong>Robustesse aux Classes Incorrectes</strong> : La CCEL est
relativement robuste aux probabilités prédites pour les classes
incorrectes. Une erreur dans la prédiction de la probabilité d’une
classe incorrecte a un impact moindre sur la CCEL.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux prédictions <span
class="math inline">\(\hat{y}_1\)</span> et <span
class="math inline">\(\hat{y}_2\)</span> telles que <span
class="math inline">\(\hat{y}_1 = [0.9, 0.1]\)</span> et <span
class="math inline">\(\hat{y}_2 = [0.8, 0.2]\)</span>. Les CCEL
correspondantes sont : <span class="math display">\[\text{CCEL}_1 =
-\log(0.9) \approx 0.105\]</span> <span
class="math display">\[\text{CCEL}_2 = -\log(0.8) \approx 0.223\]</span>
Une augmentation de 0.1 dans la probabilité prédite pour la classe
incorrecte entraîne une augmentation significative de la CCEL, mais
cette augmentation est moins importante que celle observée pour la
classe correcte. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La Categorical Cross-Entropy Loss est un outil fondamental dans le
domaine de l’apprentissage automatique, particulièrement pour les tâches
de classification multi-classes. Son interprétation probabiliste et ses
propriétés théoriques en font un choix privilégié pour l’optimisation
des modèles. Les preuves et propriétés présentées dans cet article
illustrent sa robustesse et son efficacité, faisant de la CCEL un pilier
essentiel dans le développement des algorithmes d’apprentissage
profond.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Méthode des Directions Alternées (ADMM)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Méthode des Directions Alternées (ADMM)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La méthode des directions alternées, connue sous le nom d’ADMM
(Alternating Direction Method of Multipliers), est une technique
algorithmique puissante pour la résolution de problèmes d’optimisation.
Son origine remonte aux années 1970, où elle a été introduite pour
traiter des problèmes de grande dimension avec des contraintes
séparables. L’ADMM est particulièrement utile dans le cadre de
l’optimisation convexe, où elle permet de décomposer un problème
complexe en sous-problèmes plus simples et plus faciles à résoudre.</p>
<p>L’émergence de l’ADMM est motivée par la nécessité de traiter des
problèmes d’optimisation qui apparaissent dans divers domaines tels que
le traitement du signal, l’apprentissage automatique, et la théorie des
jeux. Son efficacité réside dans sa capacité à exploiter la structure
des problèmes, en particulier les contraintes de séparabilité, pour
accélérer la convergence vers une solution optimale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’ADMM, il est essentiel de définir quelques concepts
clés. Considérons un problème d’optimisation général de la forme :</p>
<p><span class="math display">\[\min_{x, y} f(x) + g(y) \quad \text{tel
que} \quad Ax + By = c\]</span></p>
<p>où <span class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span> sont des fonctions convexes, et <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont des matrices. L’objectif est de
minimiser la somme des fonctions <span class="math inline">\(f\)</span>
et <span class="math inline">\(g\)</span> sous une contrainte
linéaire.</p>
<p>L’ADMM transforme ce problème en une série de sous-problèmes plus
simples. La méthode alterne entre la mise à jour des variables <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>, en utilisant un multiplicateur de
Lagrange pour garantir la satisfaction de la contrainte.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux de l’ADMM est le suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\{x^k, y^k, z^k\}\)</span> la suite
générée par l’ADMM. Si les fonctions <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span> sont fortement convexes et que la suite
est bornée, alors elle converge vers une solution optimale du problème
d’optimisation.</p>
</div>
<p>Pour démontrer ce théorème, nous devons d’abord établir quelques
lemmes intermédiaires.</p>
<div class="lemma">
<p>La suite <span class="math inline">\(\{x^k, y^k, z^k\}\)</span>
générée par l’ADMM satisfait la condition de convergence si et seulement
si les sous-problèmes en <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> sont résolus exactement.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de convergence repose sur plusieurs étapes
clés. Tout d’abord, nous devons montrer que la suite générée par l’ADMM
est bornée. Ensuite, nous utilisons le lemme de convergence pour établir
que la suite converge vers une solution optimale.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction de Lagrange associée au
problème d’optimisation :</p>
<p><span class="math display">\[\mathcal{L}(x, y, z) = f(x) + g(y) + z^T
(Ax + By - c) + \frac{\rho}{2} \|Ax + By - c\|^2\]</span></p>
<p>où <span class="math inline">\(z\)</span> est le multiplicateur de
Lagrange et <span class="math inline">\(\rho\)</span> est un paramètre
de pénalité. L’ADMM alterne entre la mise à jour des variables <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>, en utilisant une méthode de gradient
proximal pour minimiser la fonction de Lagrange.</p>
<p>En utilisant le lemme de convergence, nous pouvons montrer que si les
sous-problèmes en <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> sont résolus exactement, alors la suite
<span class="math inline">\(\{x^k, y^k, z^k\}\)</span> converge vers une
solution optimale. Cela repose sur le fait que la fonction de Lagrange
est fortement convexe, ce qui garantit l’unicité de la solution
optimale. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’ADMM possède plusieurs propriétés intéressantes qui en font une
méthode puissante pour l’optimisation.</p>
<ol>
<li><p><strong>Convergence globale</strong> : L’ADMM converge
globalement vers une solution optimale sous des conditions faibles sur
les fonctions <span class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span>.</p></li>
<li><p><strong>Parallelisme</strong> : Les sous-problèmes en <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> peuvent être résolus en parallèle, ce
qui permet d’accélérer la convergence.</p></li>
<li><p><strong>Adaptabilité</strong> : L’ADMM peut être adapté pour
traiter des problèmes avec des contraintes non linéaires en utilisant
des techniques de relaxation.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée rigoureusement en
utilisant les outils de l’optimisation convexe et de la théorie des
points fixes.</p>
</body>
</html>
{% include "footer.html" %}


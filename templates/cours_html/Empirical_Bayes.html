{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Empirical Bayes Methods: A Comprehensive Overview</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Empirical Bayes Methods: A Comprehensive Overview</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The Empirical Bayes (EB) method is a powerful statistical technique
that has gained significant attention in recent years due to its ability
to improve estimation and prediction in various fields. The origins of
Empirical Bayes can be traced back to the works of Herbert Robbins in
the 1950s, who laid the foundation for this approach. The method
combines the principles of Bayesian statistics with empirical data
analysis, providing a robust framework for handling complex
datasets.</p>
<p>Empirical Bayes methods are particularly useful when dealing with
multiple parameters or repeated experiments. They offer a way to
incorporate prior information into the analysis, which can lead to more
accurate and efficient estimates compared to traditional frequentist
methods. The method is indispensable in fields such as genomics,
finance, and machine learning, where large datasets with complex
structures are common.</p>
<h1 id="definitions">Definitions</h1>
<p>Before formally defining Empirical Bayes methods, let’s consider the
scenario we aim to address. Suppose we have a set of independent and
identically distributed (i.i.d.) observations <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, each associated
with an unknown parameter <span class="math inline">\(\theta_i\)</span>.
Our goal is to estimate these parameters accurately. The challenge
arises when the number of parameters is large, and we lack sufficient
data for each parameter.</p>
<p>To tackle this, we introduce a prior distribution <span
class="math inline">\(\pi(\theta)\)</span> that represents our belief
about the parameters before observing the data. The Empirical Bayes
approach estimates this prior distribution from the data itself, hence
the term "empirical."</p>
<div class="definition">
<p><strong>Définition 1</strong>. <em>Let <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be i.i.d. random
variables with densities <span
class="math inline">\(f(x|\theta_i)\)</span>, where <span
class="math inline">\(\theta_i\)</span> are unknown parameters. Assume
that the <span class="math inline">\(\theta_i\)</span>’s are drawn from
a prior distribution <span class="math inline">\(\pi(\theta)\)</span>.
The Empirical Bayes estimator of <span
class="math inline">\(\theta_i\)</span> is given by:</em></p>
<p><em><span class="math display">\[\hat{\theta}_i^{EB} =
\arg\max_{\theta_i} \int \pi(\theta) f(x_i|\theta)
d\theta\]</span></em></p>
<p><em>where <span class="math inline">\(\pi(\theta)\)</span> is
estimated from the data.</em></p>
</div>
<p>Alternatively, we can express the Empirical Bayes estimator using the
posterior distribution:</p>
<p><span class="math display">\[\hat{\theta}_i^{EB} =
\mathbb{E}[\theta_i | X_i, \hat{\pi}]\]</span></p>
<p>where <span class="math inline">\(\hat{\pi}\)</span> is the estimated
prior distribution.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the fundamental theorems in Empirical Bayes theory is due to
Robbins, which provides a justification for the method’s
effectiveness.</p>
<div class="theorem">
<p><strong>Théorème 1</strong> (Robbins’ Theorem). <em>Consider a
sequence of independent observations <span class="math inline">\(X_1,
X_2, \ldots\)</span> with corresponding parameters <span
class="math inline">\(\theta_1, \theta_2, \ldots\)</span>. Assume that
the <span class="math inline">\(\theta_i\)</span>’s are drawn from a
prior distribution <span class="math inline">\(\pi(\theta)\)</span>. Let
<span class="math inline">\(\delta_n\)</span> be an estimator of <span
class="math inline">\(\theta_n\)</span> based on the first <span
class="math inline">\(n\)</span> observations. If:</em></p>
<p><em><span class="math display">\[\mathbb{E}[\delta_n - \theta_n |
X_1, \ldots, X_{n-1}] \leq 0\]</span></em></p>
<p><em>and</em></p>
<p><em><span class="math display">\[\sum_{n=1}^{\infty}
\mathbb{E}[\delta_n - \theta_n | X_1, \ldots, X_{n-1}] &lt;
\infty\]</span></em></p>
<p><em>then <span class="math inline">\(\delta_n\)</span> is
asymptotically efficient.</em></p>
</div>
<p>This theorem shows that under certain conditions, Empirical Bayes
estimators can achieve optimal performance as the number of observations
grows.</p>
<h1 id="proofs">Proofs</h1>
<p>Let’s provide a detailed proof of Robbins’ Theorem. The proof relies
on the concept of asymptotic efficiency and the properties of
conditional expectations.</p>
<div class="proof">
<p><em>Proof.</em> We start by noting that the condition <span
class="math inline">\(\mathbb{E}[\delta_n - \theta_n | X_1, \ldots,
X_{n-1}] \leq 0\)</span> implies that the estimator <span
class="math inline">\(\delta_n\)</span> is unbiased or has a
non-positive bias. The second condition ensures that the cumulative bias
is finite.</p>
<p>By the law of large numbers, as <span class="math inline">\(n \to
\infty\)</span>, the average of the biases converges to zero:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n
\mathbb{E}[\delta_i - \theta_i | X_1, \ldots, X_{i-1}] \to
0\]</span></p>
<p>This convergence implies that the bias of <span
class="math inline">\(\delta_n\)</span> becomes negligible as <span
class="math inline">\(n\)</span> increases. Therefore, the estimator
<span class="math inline">\(\delta_n\)</span> is asymptotically
efficient. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Empirical Bayes methods possess several important properties that
make them attractive for practical applications. Let’s list and discuss
some of these properties.</p>
<ol>
<li><p><strong>Consistency</strong>: Empirical Bayes estimators are
consistent, meaning that as the sample size increases, the estimator
converges to the true parameter value.</p>
<div class="proof">
<p><em>Proof.</em> The consistency of Empirical Bayes estimators follows
from the fact that the estimated prior distribution <span
class="math inline">\(\hat{\pi}\)</span> converges to the true prior
distribution <span class="math inline">\(\pi\)</span> as <span
class="math inline">\(n \to \infty\)</span>. This convergence ensures
that the Empirical Bayes estimator becomes increasingly accurate. ◻</p>
</div></li>
<li><p><strong>Robustness</strong>: Empirical Bayes methods are robust
to model misspecification. Even if the assumed prior distribution is not
perfectly accurate, the method can still provide reasonable
estimates.</p>
<div class="proof">
<p><em>Proof.</em> The robustness of Empirical Bayes methods arises from
the empirical estimation of the prior distribution. By using the data to
estimate <span class="math inline">\(\pi(\theta)\)</span>, the method
adapts to the underlying structure of the parameters, reducing the
impact of model misspecification. ◻</p>
</div></li>
<li><p><strong>Efficiency</strong>: Empirical Bayes estimators can
achieve higher efficiency compared to traditional frequentist
estimators, especially in settings with limited data.</p>
<div class="proof">
<p><em>Proof.</em> The efficiency of Empirical Bayes estimators is a
direct consequence of Robbins’ Theorem. By incorporating prior
information and leveraging the empirical estimation of the prior, the
method can reduce variability and improve accuracy. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Empirical Bayes methods offer a powerful and flexible framework for
parameter estimation and prediction. Their ability to incorporate prior
information and adapt to complex datasets makes them indispensable in
various fields. The theoretical foundations, as exemplified by Robbins’
Theorem, provide a rigorous justification for their effectiveness. As
research continues to advance, Empirical Bayes methods are expected to
play an increasingly important role in statistical analysis and data
science.</p>
</body>
</html>
{% include "footer.html" %}


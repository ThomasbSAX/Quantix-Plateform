{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage binning : une technique fondamentale en traitement du signal</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage binning : une technique fondamentale en
traitement du signal</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage binning, ou mise en boîtes, est une technique de
traitement du signal qui trouve ses racines dans les besoins croissants
d’efficacité et de robustesse face au bruit. Historiquement, cette
méthode a émergé avec le développement des systèmes de communication
numériques et la nécessité de transmettre des informations sur des
canaux bruités. L’idée sous-jacente est de regrouper les valeurs d’un
signal en intervalles, ou "boîtes", afin de réduire la sensibilité au
bruit et d’améliorer la fiabilité de la transmission.</p>
<p>L’encodage binning est indispensable dans les systèmes où le rapport
signal sur bruit (SNR) est faible, ou lorsque la bande passante
disponible est limitée. En regroupant les valeurs du signal en
intervalles bien définis, on peut non seulement réduire l’impact du
bruit, mais aussi simplifier le processus de décodage. Cette technique
est largement utilisée dans les systèmes de communication sans fil, les
capteurs industriels, et même dans le traitement d’images.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement l’encodage binning, il est essentiel de
comprendre le problème que nous cherchons à résoudre. Supposons que nous
ayons un signal analogique <span class="math inline">\(x(t)\)</span> qui
est échantillonné et quantifié en niveaux discrets. Le bruit ajouté
pendant la transmission ou le stockage peut fausser ces niveaux, rendant
difficile la récupération du signal original. L’encodage binning vise à
atténuer cet effet en regroupant les niveaux de quantification proches
dans des intervalles, ou "boîtes".</p>
<div class="definition">
<p>Soit <span class="math inline">\(x\)</span> un signal discret prenant
des valeurs dans un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \dots, x_n\}\)</span>.
Un encodage binning est une fonction <span class="math inline">\(B:
\mathcal{X} \rightarrow \{1, 2, \dots, m\}\)</span> où <span
class="math inline">\(m &lt; n\)</span>, qui associe à chaque valeur
<span class="math inline">\(x_i\)</span> un entier représentant la boîte
dans laquelle elle est regroupée.</p>
<p>Formellement, pour chaque <span class="math inline">\(i \in \{1, 2,
\dots, n\}\)</span>, il existe un intervalle <span
class="math inline">\([a_j, b_j]\)</span> tel que : <span
class="math display">\[B(x_i) = j \quad \text{si et seulement si} \quad
x_i \in [a_j, b_j]\]</span></p>
<p>De plus, les intervalles doivent être disjoints et couvrir tout
l’ensemble <span class="math inline">\(\mathcal{X}\)</span> : <span
class="math display">\[[a_j, b_j] \cap [a_k, b_k] = \emptyset \quad
\text{pour} \quad j \neq k\]</span> <span
class="math display">\[\bigcup_{j=1}^m [a_j, b_j] =
\mathcal{X}\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’encodage binning est basé sur plusieurs théorèmes fondamentaux en
théorie de l’information et du traitement du signal. L’un des plus
importants est le théorème de quantification, qui établit une relation
entre le nombre de niveaux de quantification et la distorsion introduite
par le bruit.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(x\)</span> un signal discret prenant
des valeurs dans <span class="math inline">\(\mathcal{X} = \{x_1, x_2,
\dots, x_n\}\)</span> et soit <span class="math inline">\(B\)</span> un
encodage binning avec <span class="math inline">\(m\)</span> boîtes. La
distorsion moyenne introduite par l’encodage binning est donnée par :
<span class="math display">\[D = \frac{1}{n} \sum_{i=1}^n (x_i -
\hat{x}_i)^2\]</span> où <span class="math inline">\(\hat{x}_i\)</span>
est le représentant de la boîte à laquelle <span
class="math inline">\(x_i\)</span> appartient.</p>
<p>Si les intervalles sont choisis de manière optimale, la distorsion
est minimisée et satisfait : <span class="math display">\[D \leq
\frac{\Delta^2}{12}\]</span> où <span
class="math inline">\(\Delta\)</span> est la largeur maximale des
intervalles.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>La preuve du théorème de quantification repose sur plusieurs
propriétés fondamentales des fonctions de quantification et des
intervalles. Nous allons développer cette preuve étape par étape.</p>
<div class="proof">
<p><em>Proof.</em> Considérons d’abord le cas où les intervalles sont de
largeur constante <span class="math inline">\(\Delta\)</span>. Pour
chaque intervalle <span class="math inline">\([a_j, b_j]\)</span>, le
représentant optimal <span class="math inline">\(\hat{x}_j\)</span> est
le milieu de l’intervalle, c’est-à-dire : <span
class="math display">\[\hat{x}_j = \frac{a_j + b_j}{2}\]</span></p>
<p>La distorsion introduite par l’encodage binning pour une valeur <span
class="math inline">\(x_i\)</span> dans l’intervalle <span
class="math inline">\([a_j, b_j]\)</span> est : <span
class="math display">\[(x_i - \hat{x}_j)^2\]</span></p>
<p>La distorsion moyenne pour cet intervalle est : <span
class="math display">\[D_j = \frac{1}{|[a_j, b_j]|} \sum_{x_i \in [a_j,
b_j]} (x_i - \hat{x}_j)^2\]</span></p>
<p>En utilisant l’hypothèse que les valeurs sont uniformément
distribuées dans chaque intervalle, nous pouvons remplacer la somme par
une intégrale : <span class="math display">\[D_j \approx
\frac{1}{\Delta} \int_{a_j}^{b_j} (x - \hat{x}_j)^2 \, dx\]</span></p>
<p>En calculant cette intégrale, nous obtenons : <span
class="math display">\[D_j = \frac{\Delta^2}{12}\]</span></p>
<p>La distorsion moyenne globale est alors la moyenne des distorsions
sur tous les intervalles : <span class="math display">\[D = \frac{1}{m}
\sum_{j=1}^m D_j\]</span></p>
<p>Puisque chaque <span class="math inline">\(D_j\)</span> est inférieur
ou égal à <span class="math inline">\(\frac{\Delta^2}{12}\)</span>, nous
avons : <span class="math display">\[D \leq
\frac{\Delta^2}{12}\]</span></p>
<p>Ceci termine la preuve du théorème de quantification. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’encodage binning possède plusieurs propriétés intéressantes qui en
font une technique puissante pour le traitement du signal. Nous allons
en discuter quelques-unes et fournir des preuves détaillées.</p>
<div class="corollaire">
<p>L’encodage binning est robuste au bruit additif gaussien. Plus
précisément, si le bruit ajouté est de variance <span
class="math inline">\(\sigma^2\)</span>, la distorsion totale introduite
par l’encodage binning et le bruit est bornée par : <span
class="math display">\[D_{\text{total}} \leq D + \sigma^2\]</span> où
<span class="math inline">\(D\)</span> est la distorsion introduite par
l’encodage binning seul.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La distorsion totale est la somme de la distorsion
introduite par l’encodage binning et de la variance du bruit. Soit <span
class="math inline">\(y_i = x_i + n_i\)</span> où <span
class="math inline">\(n_i\)</span> est un bruit gaussien de variance
<span class="math inline">\(\sigma^2\)</span>. La distorsion totale est
: <span class="math display">\[D_{\text{total}} = \frac{1}{n}
\sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p>
<p>En utilisant la linéarité de l’espérance, nous avons : <span
class="math display">\[D_{\text{total}} = \frac{1}{n} \sum_{i=1}^n (x_i
+ n_i - \hat{x}_i - \hat{n}_i)^2\]</span></p>
<p>En développant cette expression et en utilisant l’indépendance entre
<span class="math inline">\(x_i\)</span> et <span
class="math inline">\(n_i\)</span>, nous obtenons : <span
class="math display">\[D_{\text{total}} = D + \sigma^2\]</span></p>
<p>Ceci termine la preuve de la propriété de robustesse. ◻</p>
</div>
<div class="corollaire">
<p>L’encodage binning est optimal au sens où il minimise la distorsion
pour un nombre donné de boîtes. Plus précisément, si les intervalles
sont choisis de manière à minimiser la distorsion moyenne, alors
l’encodage binning est optimal.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété repose sur le principe
de quantification optimale. Soit <span class="math inline">\(B\)</span>
un encodage binning avec <span class="math inline">\(m\)</span> boîtes.
La distorsion moyenne est : <span class="math display">\[D = \frac{1}{n}
\sum_{i=1}^n (x_i - \hat{x}_i)^2\]</span></p>
<p>Pour minimiser <span class="math inline">\(D\)</span>, il suffit de
choisir les représentants <span class="math inline">\(\hat{x}_j\)</span>
comme les milieux des intervalles. En effet, pour chaque intervalle
<span class="math inline">\([a_j, b_j]\)</span>, le représentant optimal
est celui qui minimise la distorsion moyenne pour cet intervalle. En
choisissant <span class="math inline">\(\hat{x}_j\)</span> comme le
milieu de l’intervalle, nous garantissons que la distorsion est
minimisée.</p>
<p>Ceci termine la preuve de la propriété d’optimalité. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage binning est une technique puissante et flexible pour le
traitement du signal. En regroupant les valeurs d’un signal en
intervalles bien définis, nous pouvons réduire l’impact du bruit et
améliorer la fiabilité de la transmission. Les théorèmes et propriétés
présentés dans cet article montrent que l’encodage binning est non
seulement robuste, mais aussi optimal au sens où il minimise la
distorsion pour un nombre donné de boîtes.</p>
<p>Les applications de l’encodage binning sont nombreuses et variées,
allant des systèmes de communication sans fil aux capteurs industriels
en passant par le traitement d’images. À mesure que les systèmes de
communication deviennent plus complexes et que les exigences en matière
de fiabilité augmentent, l’encodage binning continuera de jouer un rôle
crucial dans le traitement du signal.</p>
</body>
</html>
{% include "footer.html" %}


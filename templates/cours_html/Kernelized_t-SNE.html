{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized t-SNE: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized t-SNE: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The realm of dimensionality reduction has witnessed significant
advancements with the introduction of techniques like t-Distributed
Stochastic Neighbor Embedding (t-SNE). Originally proposed by van der
Maaten and Hinton in 2008, t-SNE has revolutionized the way we visualize
high-dimensional data. However, traditional t-SNE suffers from certain
limitations, particularly in handling non-linear relationships and
preserving global structures.</p>
<p>Kernelized t-SNE emerges as a powerful extension, integrating kernel
methods to enhance the robustness and flexibility of the original
algorithm. This approach not only preserves local structures but also
captures complex, non-linear relationships within the data. The
motivation behind kernelized t-SNE stems from the need to improve the
interpretability and scalability of dimensionality reduction techniques,
making them more adaptable to real-world applications.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand kernelized t-SNE, we must first grasp the underlying
concepts of t-SNE and kernel methods.</p>
<h2 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-Distributed
Stochastic Neighbor Embedding (t-SNE)</h2>
<p>Consider a high-dimensional dataset <span class="math inline">\(X =
\{x_1, x_2, \ldots, x_n\}\)</span> where each <span
class="math inline">\(x_i \in \mathbb{R}^d\)</span>. The goal of t-SNE
is to find a low-dimensional representation <span
class="math inline">\(Y = \{y_1, y_2, \ldots, y_n\}\)</span> where each
<span class="math inline">\(y_i \in \mathbb{R}^2\)</span> or <span
class="math inline">\(\mathbb{R}^3\)</span>, such that the pairwise
similarities in the high-dimensional space are preserved in the
low-dimensional space.</p>
<p>The similarity between two points <span
class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> in the high-dimensional space is
given by a conditional probability: <span class="math display">\[p_{j|i}
= \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i}
\exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}\]</span> where <span
class="math inline">\(\sigma_i\)</span> is the variance of the Gaussian
distribution centered at <span class="math inline">\(x_i\)</span>.</p>
<p>The joint probability <span class="math inline">\(p_{ij}\)</span> is
then defined as: <span class="math display">\[p_{ij} = \frac{p_{j|i} +
p_{i|j}}{2n}\]</span></p>
<p>In the low-dimensional space, t-SNE uses a heavy-tailed distribution
(t-distribution) to model the similarities: <span
class="math display">\[q_{ij} = \frac{(1 + \|y_i -
y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}\]</span></p>
<p>The objective of t-SNE is to minimize the Kullback-Leibler divergence
between the high-dimensional and low-dimensional distributions: <span
class="math display">\[\mathcal{L} = \sum_{i \neq j} p_{ij} \log
\frac{p_{ij}}{q_{ij}}\]</span></p>
<h2 id="kernel-methods">Kernel Methods</h2>
<p>Kernel methods involve mapping the input data into a
higher-dimensional feature space using a kernel function <span
class="math inline">\(\kappa(x_i, x_j)\)</span>. The kernel trick allows
us to compute the inner products in this feature space without
explicitly knowing the mapping.</p>
<p>A common choice for the kernel function is the Radial Basis Function
(RBF): <span class="math display">\[\kappa(x_i, x_j) = \exp(-\gamma
\|x_i - x_j\|^2)\]</span> where <span
class="math inline">\(\gamma\)</span> is a hyperparameter controlling
the width of the kernel.</p>
<h1 id="theoretical-foundations">Theoretical Foundations</h1>
<h2 id="kernelized-t-sne">Kernelized t-SNE</h2>
<p>Kernelized t-SNE extends the traditional t-SNE by incorporating
kernel methods to enhance the modeling of pairwise similarities. The key
idea is to replace the Euclidean distance in the high-dimensional space
with a kernel-induced distance.</p>
<p>Given a kernel matrix <span class="math inline">\(K\)</span> where
<span class="math inline">\(K_{ij} = \kappa(x_i, x_j)\)</span>, the
pairwise similarity in the high-dimensional space can be redefined as:
<span class="math display">\[p_{j|i} = \frac{K_{ij}}{\sum_{k \neq i}
K_{ik}}\]</span></p>
<p>The joint probability <span class="math inline">\(p_{ij}\)</span>
remains: <span class="math display">\[p_{ij} = \frac{p_{j|i} +
p_{i|j}}{2n}\]</span></p>
<p>The objective function for kernelized t-SNE is then: <span
class="math display">\[\mathcal{L} = \sum_{i \neq j} p_{ij} \log
\frac{p_{ij}}{q_{ij}}\]</span></p>
<h2 id="theoretical-guarantees">Theoretical Guarantees</h2>
<p>Kernelized t-SNE provides several theoretical advantages over
traditional t-SNE:</p>
<div class="theorem">
<p>Given a dataset <span class="math inline">\(X\)</span> and its kernel
matrix <span class="math inline">\(K\)</span>, the low-dimensional
representation <span class="math inline">\(Y\)</span> obtained via
kernelized t-SNE preserves the global structure of the data, as measured
by the kernel-induced distance.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof relies on the properties of kernel methods
and the optimization landscape of the t-SNE objective function.
Specifically, we can show that the gradient of <span
class="math inline">\(\mathcal{L}\)</span> with respect to <span
class="math inline">\(Y\)</span> captures both local and global
structures, ensuring that the low-dimensional representation is faithful
to the original data. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Kernelized t-SNE exhibits several important properties that enhance
its applicability:</p>
<ol>
<li><p><strong>Non-linear Relationships</strong>: Kernelized t-SNE can
capture complex, non-linear relationships within the data due to the use
of kernel functions.</p></li>
<li><p><strong>Global Structure Preservation</strong>: Unlike
traditional t-SNE, kernelized t-SNE preserves the global structure of
the data, making it more suitable for visualization tasks.</p></li>
<li><p><strong>Robustness to Hyperparameters</strong>: The use of kernel
functions makes the algorithm more robust to the choice of
hyperparameters, such as the perplexity parameter in traditional
t-SNE.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Kernelized t-SNE represents a significant advancement in the field of
dimensionality reduction. By integrating kernel methods, it addresses
several limitations of traditional t-SNE and provides a more robust and
flexible framework for visualizing high-dimensional data. The
theoretical foundations and properties of kernelized t-SNE make it a
valuable tool for researchers and practitioners alike.</p>
</body>
</html>
{% include "footer.html" %}


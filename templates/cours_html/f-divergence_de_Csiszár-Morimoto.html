{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>f-divergence de Csiszár-Morimoto</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title"><span class="smallcaps">f</span>-divergence de
Csiszár-Morimoto</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’étude des divergences entre distributions de probabilité est un
sujet central en théorie de l’information et en statistique. Parmi ces
divergences, la <span class="smallcaps">f</span>-divergence de
Csiszár-Morimoto joue un rôle crucial dans la mesure des écarts entre
deux distributions. Introduite par Imre Csiszár et Shun-ichi Amari,
cette divergence généralise plusieurs mesures classiques telles que la
divergence de Kullback-Leibler et la divergence <span
class="math inline">\(\chi^2\)</span>.</p>
<p>La <span class="smallcaps">f</span>-divergence émerge naturellement
dans le cadre de l’estimation statistique et de la théorie des tests
d’hypothèses. Elle permet de quantifier l’information mutuelle entre
deux distributions et est indispensable dans des domaines tels que le
machine learning, la compression de données et la cryptographie.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement la définition de la <span
class="smallcaps">f</span>-divergence, il est essentiel de comprendre ce
que nous cherchons à capturer. Imaginons deux distributions de
probabilité, <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous souhaitons
mesurer à quel point <span class="math inline">\(Q\)</span> s’écarte de
<span class="math inline">\(P\)</span>.</p>
<p>Pour ce faire, nous introduisons une fonction convexe <span
class="math inline">\(f: [0, \infty) \rightarrow \mathbb{R}\)</span>
telle que <span class="math inline">\(f(1) = 0\)</span>. Cette fonction
nous permettra de pondérer les écarts entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, et soit <span class="math inline">\(f: [0,
\infty) \rightarrow \mathbb{R}\)</span> une fonction convexe telle que
<span class="math inline">\(f(1) = 0\)</span>. La <span
class="math inline">\(\textsc{f}\)</span>-divergence de Csiszár-Morimoto
est définie par : <span class="math display">\[D_f(P \parallel Q) =
\int_{\Omega} f\left(\frac{dP}{dQ}\right) dQ\]</span> où <span
class="math inline">\(\frac{dP}{dQ}\)</span> est la dérivée de
Radon-Nikodym de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span>.</p>
</div>
<p>Cette définition peut également être formulée de manière équivalente
en utilisant la mesure <span class="math inline">\(P\)</span> : <span
class="math display">\[D_f(P \parallel Q) = \int_{\Omega}
f\left(\frac{dP}{dQ}\right) dP + f&#39;(1)(H(P) - H(Q))\]</span> où
<span class="math inline">\(H(P)\)</span> et <span
class="math inline">\(H(Q)\)</span> sont les entropies de Shannon des
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la <span
class="math inline">\(\textsc{f}\)</span>-divergence est le théorème de
convexité de Csiszár, qui établit une inégalité cruciale pour les
fonctions convexes.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: [0, \infty) \rightarrow
\mathbb{R}\)</span> une fonction convexe telle que <span
class="math inline">\(f(1) = 0\)</span>. Alors, pour toute distribution
de probabilité <span class="math inline">\(P\)</span> et toute mesure
positive <span class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_f(P \parallel Q) \geq f&#39;(1)(H(P) -
H(Q))\]</span> où <span class="math inline">\(H(P)\)</span> et <span
class="math inline">\(H(Q)\)</span> sont les entropies de Shannon des
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur l’utilisation de
l’inégalité de Jensen pour les fonctions convexes. Soit <span
class="math inline">\(X\)</span> une variable aléatoire suivant la
distribution <span class="math inline">\(P\)</span>. Alors, par
l’inégalité de Jensen, on a : <span
class="math display">\[f\left(\mathbb{E}\left[\frac{dP}{dQ}(X)\right]\right)
\leq \mathbb{E}\left[f\left(\frac{dP}{dQ}(X)\right)\right]\]</span> En
utilisant le fait que <span
class="math inline">\(\mathbb{E}\left[\frac{dP}{dQ}(X)\right] =
1\)</span>, on obtient : <span class="math display">\[f(1) \leq
\int_{\Omega} f\left(\frac{dP}{dQ}\right) dQ\]</span> Puisque <span
class="math inline">\(f(1) = 0\)</span>, il vient : <span
class="math display">\[0 \leq D_f(P \parallel Q)\]</span> En ajoutant le
terme <span class="math inline">\(f&#39;(1)(H(P) - H(Q))\)</span>, on
obtient l’inégalité désirée. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la <span
class="math inline">\(\textsc{f}\)</span>-divergence, considérons un
exemple simple où <span class="math inline">\(f(x) = x \log x\)</span>.
Dans ce cas, la <span
class="math inline">\(\textsc{f}\)</span>-divergence coïncide avec la
divergence de Kullback-Leibler.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(f(x) = x \log
x\)</span>. Alors, la <span
class="math inline">\(\textsc{f}\)</span>-divergence est donnée par :
<span class="math display">\[D_f(P \parallel Q) = \int_{\Omega}
\frac{dP}{dQ} \log\left(\frac{dP}{dQ}\right) dQ\]</span> En utilisant le
changement de variable <span class="math inline">\(y =
\frac{dP}{dQ}\)</span>, on obtient : <span class="math display">\[D_f(P
\parallel Q) = \int_{\Omega} y \log y \, dy\]</span> qui est précisément
la définition de la divergence de Kullback-Leibler. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La <span class="math inline">\(\textsc{f}\)</span>-divergence possède
plusieurs propriétés intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour toute fonction convexe
<span class="math inline">\(f\)</span> telle que <span
class="math inline">\(f(1) = 0\)</span>, on a <span
class="math inline">\(D_f(P \parallel Q) \geq 0\)</span>.</p></li>
<li><p><strong>Invariance par transformation</strong> : La <span
class="math inline">\(\textsc{f}\)</span>-divergence est invariante sous
les transformations mesurables.</p></li>
<li><p><strong>Continuité</strong> : La <span
class="math inline">\(\textsc{f}\)</span>-divergence est continue par
rapport aux distributions <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>.</p></li>
</ol>
<div class="proof">
<p><em>Preuve de la propriété (i).</em> La non-négativité découle
directement du théorème de convexité de Csiszár, qui montre que <span
class="math inline">\(D_f(P \parallel Q) \geq 0\)</span>. ◻</p>
</div>
<div class="proof">
<p><em>Preuve de la propriété (ii).</em> Soit <span
class="math inline">\(T: \Omega \rightarrow \Omega&#39;\)</span> une
transformation mesurable. Alors, pour toute fonction mesurable <span
class="math inline">\(g: \Omega&#39; \rightarrow \mathbb{R}\)</span>, on
a : <span class="math display">\[\int_{\Omega} g(T(\omega)) dP(\omega) =
\int_{\Omega&#39;} g(\omega&#39;) d(P \circ
T^{-1})(\omega&#39;)\]</span> En appliquant cette propriété à la <span
class="math inline">\(\textsc{f}\)</span>-divergence, on obtient
l’invariance désirée. ◻</p>
</div>
<div class="proof">
<p><em>Preuve de la propriété (iii).</em> La continuité de la <span
class="math inline">\(\textsc{f}\)</span>-divergence peut être démontrée
en utilisant le théorème de convergence dominée de Lebesgue. Soit <span
class="math inline">\((P_n)\)</span> une suite de distributions
convergeant vers <span class="math inline">\(P\)</span> et <span
class="math inline">\((Q_n)\)</span> une suite de distributions
convergeant vers <span class="math inline">\(Q\)</span>. Alors, pour
toute fonction convexe <span class="math inline">\(f\)</span> telle que
<span class="math inline">\(f(1) = 0\)</span>, on a : <span
class="math display">\[\lim_{n \rightarrow \infty} D_f(P_n \parallel
Q_n) = D_f(P \parallel Q)\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La <span class="math inline">\(\textsc{f}\)</span>-divergence de
Csiszár-Morimoto est un outil puissant et général pour mesurer les
écarts entre deux distributions de probabilité. Ses propriétés
fondamentales, telles que la non-négativité, l’invariance par
transformation et la continuité, en font un outil indispensable dans de
nombreux domaines des mathématiques appliquées. Les théorèmes et preuves
présentés dans cet article illustrent la richesse et la profondeur de
cette notion, ouvrant la voie à de nombreuses applications futures.</p>
</body>
</html>
{% include "footer.html" %}


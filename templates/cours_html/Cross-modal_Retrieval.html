{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Cross-modal Retrieval : Un pont entre les mondes visuels et textuels</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cross-modal Retrieval : Un pont entre les mondes
visuels et textuels</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le <em>Cross-modal Retrieval</em> (Récupération Inter-Modalités)
émerge comme un champ de recherche fascinant à l’intersection des
traitements visuels et textuels. À l’ère du <em>Big Data</em>, où les
utilisateurs génèrent quotidiennement des quantités astronomiques de
données multimodales, la nécessité de systèmes capables d’extraire et de
relier des informations provenant de modalités différentes devient
cruciale. Les moteurs de recherche traditionnels, limités à une seule
modalité (généralement textuelle), peinent à répondre aux besoins
actuels. Le <em>Cross-modal Retrieval</em> vise à combler cette lacune
en permettant la recherche d’informations à travers des modalités
distinctes, comme le texte et l’image.</p>
<p>L’origine historique de cette notion remonte aux premiers travaux sur
la reconnaissance d’images et le traitement du langage naturel.
Cependant, l’avènement des réseaux de neurones profonds a marqué un
tournant décisif, permettant des avancées significatives dans la
compréhension et l’intégration de données multimodales. Aujourd’hui, ce
domaine est indispensable dans des applications variées : recherche
d’images par requête textuelle, recommandation de contenu personnalisé,
ou encore analyse automatisée de médias sociaux.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le <em>Cross-modal Retrieval</em>, commençons par
définir les concepts fondamentaux.</p>
<h2 id="modalité">Modalité</h2>
<p>Une modalité est une forme de représentation des données. Par
exemple, le texte et l’image sont deux modalités distinctes.</p>
<div class="definition">
<p>Soit <span class="math inline">\(M\)</span> un ensemble de modalités.
Une modalité <span class="math inline">\(m \in M\)</span> est une
représentation des données sous une forme spécifique, comme le texte ou
l’image.</p>
</div>
<h2 id="espace-de-représentation-commun">Espace de Représentation
Commun</h2>
<p>Pour permettre la comparaison entre des données de modalités
différentes, il est nécessaire d’introduire un espace de représentation
commun.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. Un espace de représentation commun <span
class="math inline">\(Z\)</span> est un ensemble tel qu’il existe des
fonctions de projection <span class="math inline">\(f: X \rightarrow
Z\)</span> et <span class="math inline">\(g: Y \rightarrow Z\)</span>,
permettant de comparer les données des deux modalités dans <span
class="math inline">\(Z\)</span>.</p>
</div>
<h2 id="cross-modal-retrieval">Cross-modal Retrieval</h2>
<p>Le <em>Cross-modal Retrieval</em> consiste à retrouver des données
d’une modalité en utilisant une requête d’une autre modalité.</p>
<div class="definition">
<p>Soit <span class="math inline">\(Q\)</span> une requête dans la
modalité <span class="math inline">\(m_1\)</span>, et <span
class="math inline">\(D\)</span> un ensemble de données dans la modalité
<span class="math inline">\(m_2\)</span>. Le <em>Cross-modal
Retrieval</em> est le processus de recherche des données <span
class="math inline">\(d \in D\)</span> les plus pertinentes par rapport
à la requête <span class="math inline">\(Q\)</span>, en utilisant un
espace de représentation commun.</p>
</div>
<h1 id="théorèmes-et-algorithmes">Théorèmes et Algorithmes</h1>
<p>Plusieurs algorithmes ont été proposés pour résoudre le problème du
<em>Cross-modal Retrieval</em>. Nous en présentons quelques-uns ici.</p>
<h2 id="algorithme-de-correspondance-bilinéaire">Algorithme de
Correspondance Bilinéaire</h2>
<p>L’algorithme de correspondance bilinéaire est l’un des premiers
algorithmes proposés pour le <em>Cross-modal Retrieval</em>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. Il existe une matrice de transformation
bilinéaire <span class="math inline">\(W\)</span> telle que pour toute
donnée <span class="math inline">\(x \in X\)</span> et <span
class="math inline">\(y \in Y\)</span>, la similarité entre <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans l’espace de représentation commun
est donnée par : <span class="math display">\[s(x, y) = x^T W
y\]</span></p>
</div>
<h2
id="réseaux-de-neurones-profonds-pour-le-cross-modal-retrieval">Réseaux
de Neurones Profonds pour le Cross-modal Retrieval</h2>
<p>Les réseaux de neurones profonds ont révolutionné le domaine du
<em>Cross-modal Retrieval</em>. Ils permettent d’apprendre des
représentations communes pour différentes modalités.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. Il existe un réseau de neurones profond avec
une couche de sortie commune <span class="math inline">\(h\)</span>
telle que pour toute donnée <span class="math inline">\(x \in X\)</span>
et <span class="math inline">\(y \in Y\)</span>, la similarité entre
<span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans l’espace de représentation commun
est donnée par : <span class="math display">\[s(x, y) = h(x)^T
h(y)\]</span></p>
</div>
<h1 id="preuves-et-démonstrations">Preuves et Démonstrations</h1>
<h2 id="preuve-de-lalgorithme-de-correspondance-bilinéaire">Preuve de
l’Algorithme de Correspondance Bilinéaire</h2>
<p>Nous allons démontrer que l’algorithme de correspondance bilinéaire
permet de trouver un espace de représentation commun pour deux modalités
différentes.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. Nous cherchons une matrice de transformation
bilinéaire <span class="math inline">\(W\)</span> telle que pour toute
donnée <span class="math inline">\(x \in X\)</span> et <span
class="math inline">\(y \in Y\)</span>, la similarité entre <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans l’espace de représentation commun
est donnée par : <span class="math display">\[s(x, y) = x^T W
y\]</span></p>
<p>Pour ce faire, nous minimisons la fonction de coût suivante : <span
class="math display">\[\mathcal{L}(W) = \sum_{(x, y) \in D} (s(x, y) -
s^*(x, y))^2\]</span> où <span class="math inline">\(D\)</span> est un
ensemble de paires de données <span class="math inline">\((x,
y)\)</span> et <span class="math inline">\(s^*(x, y)\)</span> est la
similarité vraie entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>.</p>
<p>En résolvant cette optimisation, nous obtenons la matrice <span
class="math inline">\(W\)</span> qui minimise la fonction de coût. ◻</p>
</div>
<h2
id="preuve-des-réseaux-de-neurones-profonds-pour-le-cross-modal-retrieval">Preuve
des Réseaux de Neurones Profonds pour le Cross-modal Retrieval</h2>
<p>Nous allons démontrer que les réseaux de neurones profonds permettent
d’apprendre un espace de représentation commun pour deux modalités
différentes.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. Nous cherchons un réseau de neurones profond
avec une couche de sortie commune <span class="math inline">\(h\)</span>
telle que pour toute donnée <span class="math inline">\(x \in X\)</span>
et <span class="math inline">\(y \in Y\)</span>, la similarité entre
<span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> dans l’espace de représentation commun
est donnée par : <span class="math display">\[s(x, y) = h(x)^T
h(y)\]</span></p>
<p>Pour ce faire, nous minimisons la fonction de coût suivante : <span
class="math display">\[\mathcal{L}(\theta) = \sum_{(x, y) \in D} (s(x,
y) - s^*(x, y))^2\]</span> où <span class="math inline">\(D\)</span> est
un ensemble de paires de données <span class="math inline">\((x,
y)\)</span> et <span class="math inline">\(s^*(x, y)\)</span> est la
similarité vraie entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>. Les paramètres du réseau de neurones
sont notés <span class="math inline">\(\theta\)</span>.</p>
<p>En résolvant cette optimisation, nous obtenons les paramètres <span
class="math inline">\(\theta\)</span> qui minimisent la fonction de
coût. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-symétrie">Propriété de Symétrie</h2>
<p>Le <em>Cross-modal Retrieval</em> possède une propriété de symétrie
importante.</p>
<div class="property">
<p>Soit <span class="math inline">\(Q\)</span> une requête dans la
modalité <span class="math inline">\(m_1\)</span>, et <span
class="math inline">\(D\)</span> un ensemble de données dans la modalité
<span class="math inline">\(m_2\)</span>. La similarité entre la requête
<span class="math inline">\(Q\)</span> et une donnée <span
class="math inline">\(d \in D\)</span> est symétrique, c’est-à-dire :
<span class="math display">\[s(Q, d) = s(d, Q)\]</span></p>
</div>
<h2 id="corollaire-de-lespace-de-représentation-commun">Corollaire de
l’Espace de Représentation Commun</h2>
<p>L’espace de représentation commun possède des propriétés
intéressantes.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux ensembles de données représentant
des modalités différentes. L’espace de représentation commun <span
class="math inline">\(Z\)</span> permet de comparer les données des deux
modalités de manière efficace et précise.</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le <em>Cross-modal Retrieval</em> représente une avancée majeure dans
le domaine du traitement des données multimodales. En permettant la
recherche d’informations à travers des modalités distinctes, il ouvre la
voie à de nombreuses applications innovantes. Les algorithmes et les
réseaux de neurones profonds jouent un rôle clé dans ce domaine, offrant
des solutions efficaces et précises pour la comparaison de données
multimodales.</p>
</body>
</html>
{% include "footer.html" %}


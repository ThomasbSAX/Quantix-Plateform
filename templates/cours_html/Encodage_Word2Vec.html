{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage Word2Vec : Une Approche Vectorielle pour la Représentation des Mots</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage Word2Vec : Une Approche Vectorielle pour la
Représentation des Mots</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage Word2Vec est une technique révolutionnaire dans le domaine
du traitement automatique des langues (TAL). Développée par Tomas
Mikolov et ses collègues en 2013, cette méthode permet de représenter
des mots sous forme de vecteurs dans un espace vectoriel continu.
L’objectif principal est de capturer les relations sémantiques et
syntaxiques entre les mots, ce qui facilite grandement les tâches de
classification, de traduction et d’analyse de texte.</p>
<p>L’émergence de Word2Vec est motivée par le besoin croissant de
modèles linguistiques capables de comprendre le contexte des mots. Les
approches traditionnelles, telles que les sacs de mots (bag-of-words),
ne tiennent pas compte des relations entre les mots, limitant ainsi leur
efficacité. Word2Vec, en revanche, utilise des réseaux de neurones pour
apprendre des représentations vectorielles riches et denses, permettant
une meilleure compréhension du langage naturel.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous cherchons à accomplir. Nous voulons représenter
chaque mot d’un vocabulaire par un vecteur dans un espace à plusieurs
dimensions, de sorte que les mots sémantiquement proches soient
représentés par des vecteurs proches.</p>
<p>Formellement, soit <span class="math inline">\(V\)</span> un
vocabulaire de taille <span class="math inline">\(N\)</span>, et soit
<span class="math inline">\(d\)</span> la dimension de l’espace
vectoriel. Nous cherchons une fonction <span class="math inline">\(f: V
\rightarrow \mathbb{R}^d\)</span> qui associe à chaque mot <span
class="math inline">\(w_i \in V\)</span> un vecteur <span
class="math inline">\(v_{w_i} \in \mathbb{R}^d\)</span>.</p>
<p>La définition formelle de l’encodage Word2Vec peut être donnée comme
suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(V\)</span> un vocabulaire de taille
<span class="math inline">\(N\)</span>, et soit <span
class="math inline">\(d\)</span> la dimension de l’espace vectoriel.
L’encodage Word2Vec est une fonction <span class="math inline">\(f: V
\rightarrow \mathbb{R}^d\)</span> telle que pour tout mot <span
class="math inline">\(w_i \in V\)</span>, il existe un vecteur <span
class="math inline">\(v_{w_i} \in \mathbb{R}^d\)</span> représentant ce
mot.</p>
</div>
<h1 id="théorèmes-et-algorithmes">Théorèmes et Algorithmes</h1>
<p>L’algorithme Word2Vec repose sur deux architectures principales :
Skip-gram et Continuous Bag of Words (CBOW). Nous allons explorer ces
deux approches en détail.</p>
<h2 id="architecture-skip-gram">Architecture Skip-gram</h2>
<p>L’architecture Skip-gram est conçue pour prédire les mots contextuels
donnés un mot cible. Formellement, soit <span
class="math inline">\(w_i\)</span> le mot cible et <span
class="math inline">\(w_{i-j}, \ldots, w_{i-1}, w_{i+1}, \ldots,
w_{i+j}\)</span> les mots contextuels. L’objectif est de maximiser la
probabilité conjointe :</p>
<p><span class="math display">\[\prod_{-j \leq k \leq j, k \neq 0}
p(w_{i+k} | w_i)\]</span></p>
<p>où <span class="math inline">\(p(w_{i+k} | w_i)\)</span> est la
probabilité de prédire le mot contextuel <span
class="math inline">\(w_{i+k}\)</span> donné le mot cible <span
class="math inline">\(w_i\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(V\)</span> un vocabulaire de taille
<span class="math inline">\(N\)</span>, et soit <span
class="math inline">\(d\)</span> la dimension de l’espace vectoriel.
L’architecture Skip-gram maximise la probabilité conjointe :</p>
<p><span class="math display">\[\prod_{-j \leq k \leq j, k \neq 0}
p(w_{i+k} | w_i) = \prod_{-j \leq k \leq j, k \neq 0}
\frac{\exp(v_{w_i}^T v_{w_{i+k}})}{\sum_{w=1}^N \exp(v_{w_i}^T
v_{w})}\]</span></p>
<p>où <span class="math inline">\(v_{w_i}\)</span> et <span
class="math inline">\(v_{w_{i+k}}\)</span> sont les vecteurs de
représentation des mots <span class="math inline">\(w_i\)</span> et
<span class="math inline">\(w_{i+k}\)</span>, respectivement.</p>
</div>
<h2 id="architecture-cbow">Architecture CBOW</h2>
<p>L’architecture CBOW, en revanche, prédit le mot cible donné un
ensemble de mots contextuels. Formellement, soit <span
class="math inline">\(w_{i-j}, \ldots, w_{i-1}, w_{i+1}, \ldots,
w_{i+j}\)</span> les mots contextuels et <span
class="math inline">\(w_i\)</span> le mot cible. L’objectif est de
maximiser la probabilité :</p>
<p><span class="math display">\[p(w_i | w_{i-j}, \ldots, w_{i-1},
w_{i+1}, \ldots, w_{i+j})\]</span></p>
<div class="theorem">
<p>Soit <span class="math inline">\(V\)</span> un vocabulaire de taille
<span class="math inline">\(N\)</span>, et soit <span
class="math inline">\(d\)</span> la dimension de l’espace vectoriel.
L’architecture CBOW maximise la probabilité :</p>
<p><span class="math display">\[p(w_i | w_{i-j}, \ldots, w_{i-1},
w_{i+1}, \ldots, w_{i+j}) = \frac{\exp(v_{\text{context}}^T
v_{w_i})}{\sum_{w=1}^N \exp(v_{\text{context}}^T v_{w})}\]</span></p>
<p>où <span class="math inline">\(v_{\text{context}} = \frac{1}{2j}
\sum_{-j \leq k \leq j, k \neq 0} v_{w_{i+k}}\)</span> est la moyenne
des vecteurs de représentation des mots contextuels.</p>
</div>
<h1 id="preuves-et-démonstrations">Preuves et Démonstrations</h1>
<p>Pour démontrer l’efficacité de Word2Vec, nous devons montrer que les
vecteurs de représentation apprennent effectivement des relations
sémantiques. Une preuve courante repose sur l’utilisation de la méthode
de réduction de dimension, telle que l’analyse en composantes
principales (PCA), pour visualiser les relations entre les mots.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un ensemble de mots <span
class="math inline">\(\{w_1, w_2, \ldots, w_N\}\)</span> et leurs
vecteurs de représentation <span class="math inline">\(\{v_{w_1},
v_{w_2}, \ldots, v_{w_N}\}\)</span>. En appliquant PCA à ces vecteurs,
nous pouvons projeter les dimensions de l’espace vectoriel sur un plan
2D ou 3D.</p>
<p>Si les mots sémantiquement proches sont représentés par des vecteurs
proches dans l’espace vectoriel, alors leur projection sur le plan 2D ou
3D devrait également être proche. Cela démontre que Word2Vec capture
effectivement les relations sémantiques entre les mots. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Word2Vec possède plusieurs propriétés intéressantes qui en font une
méthode puissante pour la représentation des mots.</p>
<ol>
<li><p><strong>Linearité des Relations</strong> : Les vecteurs de
représentation apprennent des relations linéaires entre les mots. Par
exemple, <span class="math inline">\(v_{\text{roi}} - v_{\text{homme}} +
v_{\text{femme}} \approx v_{\text{reine}}\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle de la capacité de Word2Vec à
capturer les relations sémantiques dans l’espace vectoriel. En
effectuant des opérations linéaires sur les vecteurs, nous pouvons
prédire des relations entre les mots. ◻</p>
</div></li>
<li><p><strong>Robustesse aux Variations</strong> : Word2Vec est robuste
aux variations de contexte, ce qui permet de représenter les mots de
manière cohérente même dans des contextes différents.</p>
<div class="proof">
<p><em>Proof.</em> Cette robustesse est due à l’utilisation de réseaux
de neurones pour apprendre les représentations vectorielles. Les réseaux
de neurones sont capables de généraliser à partir des données
d’entraînement, ce qui permet de capturer les variations de
contexte. ◻</p>
</div></li>
<li><p><strong>Efficacité Computationnelle</strong> : Word2Vec est
efficace sur le plan computationnel, ce qui permet de traiter de grands
corpus de texte en un temps raisonnable.</p>
<div class="proof">
<p><em>Proof.</em> L’efficacité computationnelle de Word2Vec est due à
l’utilisation d’algorithmes optimisés, tels que la méthode de gradient
stochastique, pour entraîner les modèles. Ces algorithmes permettent de
traiter de grands volumes de données en un temps raisonnable. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage Word2Vec est une méthode puissante et efficace pour la
représentation des mots dans un espace vectoriel. En capturant les
relations sémantiques et syntaxiques entre les mots, Word2Vec facilite
grandement les tâches de traitement automatique des langues. Les
architectures Skip-gram et CBOW, ainsi que leurs propriétés
intéressantes, font de Word2Vec une méthode incontournable dans le
domaine du TAL.</p>
</body>
</html>
{% include "footer.html" %}


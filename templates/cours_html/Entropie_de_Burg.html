{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Burg: Une Mesure de Complexité pour les Processus Stochastiques</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Burg: Une Mesure de Complexité pour les
Processus Stochastiques</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de Burg émerge comme une mesure fondamentale dans
l’analyse des processus stochastiques, particulièrement dans le contexte
des séries temporelles. Introduite par Burg en 1967, cette notion répond
à un besoin crucial : quantifier la complexité ou l’incertitude
inhérente à un processus aléatoire. L’entropie de Burg est indispensable
dans des domaines tels que la modélisation financière, l’analyse des
signaux biologiques et la prédiction des séries temporelles. Son
importance réside dans sa capacité à capturer l’information mutuelle
entre les observations successives, offrant ainsi une mesure robuste de
la dépendance temporelle.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Burg, commençons par définir les
concepts préliminaires. Supposons que nous ayons une série temporelle
<span class="math inline">\(X = (X_1, X_2, \ldots, X_n)\)</span>. Nous
cherchons à mesurer la quantité d’information contenue dans cette série.
L’entropie de Burg est une extension de l’entropie de Shannon, adaptée
aux processus gaussiens.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = (X_1, X_2, \ldots, X_n)\)</span>
un processus gaussien centré de matrice de covariance <span
class="math inline">\(R = (R_{ij})\)</span>. L’entropie de Burg est
définie comme : <span class="math display">\[H_{\text{Burg}}(X) =
\frac{1}{2} \log \det(2\pi e R)\]</span> où <span
class="math inline">\(\det\)</span> désigne le déterminant de la matrice
<span class="math inline">\(R\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[H_{\text{Burg}}(X) = \frac{n}{2} \log(2\pi e) +
\frac{1}{2} \sum_{i=1}^n \log \lambda_i\]</span> où <span
class="math inline">\(\lambda_i\)</span> sont les valeurs propres de la
matrice <span class="math inline">\(R\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de Burg est le suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X = (X_1, X_2, \ldots, X_n)\)</span>
un processus gaussien centré de matrice de covariance <span
class="math inline">\(R\)</span>. Alors l’entropie de Burg satisfait :
<span class="math display">\[H_{\text{Burg}}(X) = \inf_{A} H(Y)\]</span>
où l’infimum est pris sur toutes les transformations linéaires <span
class="math inline">\(Y = AX\)</span> avec <span
class="math inline">\(\mathbb{E}[YY^T] = I_n\)</span>, et <span
class="math inline">\(H(Y)\)</span> est l’entropie de Shannon de <span
class="math inline">\(Y\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Burg, nous utilisons les propriétés des
processus gaussiens et l’entropie de Shannon.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une transformation linéaire <span
class="math inline">\(Y = AX\)</span> avec <span
class="math inline">\(\mathbb{E}[YY^T] = I_n\)</span>. Puisque <span
class="math inline">\(X\)</span> est gaussien, <span
class="math inline">\(Y\)</span> est également gaussien. L’entropie de
Shannon de <span class="math inline">\(Y\)</span> est donnée par : <span
class="math display">\[H(Y) = \frac{1}{2} \log \det(2\pi e I_n) =
\frac{n}{2} \log(2\pi e)\]</span> D’autre part, l’entropie de Burg de
<span class="math inline">\(X\)</span> est : <span
class="math display">\[H_{\text{Burg}}(X) = \frac{1}{2} \log \det(2\pi e
R)\]</span> Puisque <span class="math inline">\(\mathbb{E}[YY^T] =
I_n\)</span>, nous avons <span class="math inline">\(A R A^T =
I_n\)</span>. Par conséquent, <span class="math inline">\(\det(A)
\det(R) \det(A^T) = 1\)</span>, ce qui implique que <span
class="math inline">\(\det(R) \leq 1\)</span>. Ainsi : <span
class="math display">\[H_{\text{Burg}}(X) = \frac{n}{2} \log(2\pi e) +
\frac{1}{2} \log \det(R) \leq \frac{n}{2} \log(2\pi e)\]</span> Ce qui
prouve que <span class="math inline">\(H_{\text{Burg}}(X)\)</span> est
inférieur à l’entropie de Shannon de toute transformation linéaire <span
class="math inline">\(Y\)</span> avec <span
class="math inline">\(\mathbb{E}[YY^T] = I_n\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de Burg possède plusieurs propriétés intéressantes :</p>
<ol>
<li><p><strong>Invariance par translation</strong> : L’entropie de Burg
est invariante par ajout d’une constante à chaque composante du
processus <span class="math inline">\(X\)</span>.</p></li>
<li><p><strong>Additivité</strong> : Pour deux processus gaussiens
indépendants <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, l’entropie de Burg du processus
combiné <span class="math inline">\((X, Y)\)</span> est la somme des
entropies de Burg de <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Minimisation</strong> : L’entropie de Burg atteint son
minimum lorsque le processus <span class="math inline">\(X\)</span> est
blanc, c’est-à-dire lorsque la matrice de covariance <span
class="math inline">\(R\)</span> est diagonale.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Burg constitue une mesure puissante et élégante pour
quantifier la complexité des processus stochastiques. Son utilisation
dans divers domaines de l’analyse des données et de la modélisation
statistique en fait un outil indispensable pour les chercheurs et les
praticiens. Les propriétés et théorèmes associés à l’entropie de Burg
ouvrent des perspectives prometteuses pour de futures recherches dans le
domaine des séries temporelles et des processus aléatoires.</p>
</body>
</html>
{% include "footer.html" %}


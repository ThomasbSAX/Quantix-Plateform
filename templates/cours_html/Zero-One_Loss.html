{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Zero-One Loss: Une Exploration Mathématique et Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Zero-One Loss: Une Exploration Mathématique et
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le concept de <em>Zero-One Loss</em> émerge dans le cadre de la
théorie de l’apprentissage statistique et des fonctions de perte. Il
s’agit d’une fonction de perte fondamentale, particulièrement utilisée
dans les problèmes de classification binaire. L’origine de cette notion
remonte aux travaux pionniers sur la théorie des jeux et l’estimation
statistique, où la nécessité de quantifier les erreurs de prédiction
s’est imposée.</p>
<p>Le Zero-One Loss est indispensable dans le cadre des algorithmes de
classification, où il permet d’évaluer la performance d’un modèle en
termes de taux d’erreur. Il est particulièrement utile pour comprendre
les limites théoriques des algorithmes d’apprentissage et pour
développer de nouvelles méthodes de régularisation.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire le Zero-One Loss, commençons par comprendre ce que
nous cherchons à mesurer. Dans un problème de classification binaire,
nous avons deux classes : par exemple, les éléments positifs et
négatifs. Notre objectif est de prédire la classe d’un nouvel élément en
fonction des données observées.</p>
<p>Nous cherchons une fonction qui mesure l’erreur de notre prédiction.
Cette fonction doit être égale à zéro si la prédiction est correcte et à
un si la prédiction est incorrecte.</p>
<p>Formellement, nous définissons le Zero-One Loss comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(y \in \{-1, 1\}\)</span> la vraie
classe d’un échantillon et <span class="math inline">\(\hat{y} \in \{-1,
1\}\)</span> la classe prédite par un modèle. Le Zero-One Loss est
défini par : <span class="math display">\[L(y, \hat{y}) =
\mathbb{1}_{\{y \neq \hat{y}\}}\]</span> où <span
class="math inline">\(\mathbb{1}_{\{y \neq \hat{y}\}}\)</span> est la
fonction indicatrice qui vaut 1 si <span class="math inline">\(y \neq
\hat{y}\)</span> et 0 sinon.</p>
</div>
<p>Une autre manière de formuler cette définition est : <span
class="math display">\[L(y, \hat{y}) = \begin{cases}
0 &amp; \text{si } y = \hat{y} \\
1 &amp; \text{si } y \neq \hat{y}
\end{cases}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le Zero-One Loss est lié à plusieurs théorèmes fondamentaux en
théorie de l’apprentissage statistique. L’un des plus importants est le
théorème de la limite bayésienne, qui établit les limites théoriques de
la performance d’un classificateur.</p>
<p>Commençons par comprendre ce que nous cherchons à établir. Nous
voulons montrer qu’il existe une limite inférieure sur l’erreur de
classification, même pour le meilleur classificateur possible.</p>
<p>Formellement, nous avons le théorème suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P(Y|X)\)</span> la distribution
conditionnelle de la classe <span class="math inline">\(Y\)</span>
donnée les caractéristiques <span class="math inline">\(X\)</span>. Le
risque bayésien minimal est défini par : <span
class="math display">\[R^* = \mathbb{E}_{X} \left[ \min_{y \in \{-1,
1\}} P(Y \neq y | X) \right]\]</span> Alors, pour tout classificateur
<span class="math inline">\(h\)</span>, le risque <span
class="math inline">\(R(h)\)</span> satisfait : <span
class="math display">\[R(h) \geq R^*\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la limite bayésienne, nous devons montrer
que le risque bayésien minimal est une borne inférieure sur le risque de
tout classificateur.</p>
<p>Commençons par définir le risque d’un classificateur <span
class="math inline">\(h\)</span> : <span class="math display">\[R(h) =
\mathbb{E}_{X,Y} [L(Y, h(X))]\]</span></p>
<p>En utilisant la définition du Zero-One Loss, nous avons : <span
class="math display">\[R(h) = \mathbb{E}_{X} [P(Y \neq h(X) |
X)]\]</span></p>
<p>Le risque bayésien minimal est obtenu en choisissant <span
class="math inline">\(h\)</span> qui minimise cette espérance : <span
class="math display">\[R^* = \mathbb{E}_{X} \left[ \min_{y \in \{-1,
1\}} P(Y \neq y | X) \right]\]</span></p>
<p>Pour tout classificateur <span class="math inline">\(h\)</span>, nous
avons : <span class="math display">\[P(Y \neq h(X) | X) \geq \min_{y \in
\{-1, 1\}} P(Y \neq y | X)\]</span></p>
<p>En prenant l’espérance sur <span class="math inline">\(X\)</span>,
nous obtenons : <span class="math display">\[R(h) \geq R^*\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le Zero-One Loss possède plusieurs propriétés importantes qui en font
un outil puissant pour l’analyse des algorithmes de classification.</p>
<ol>
<li><p>Le Zero-One Loss est non convexe. Cela signifie que les
algorithmes d’optimisation classiques ne peuvent pas être directement
appliqués pour minimiser cette fonction de perte.</p></li>
<li><p>Le Zero-One Loss est invariant sous les transformations
monotones. Cela signifie que la fonction de perte reste la même si nous
appliquons une transformation monotone aux scores de
prédiction.</p></li>
<li><p>Le Zero-One Loss est robuste aux erreurs de classification. Il ne
pénalise pas les erreurs en fonction de leur ampleur, mais seulement par
leur présence.</p></li>
</ol>
<p>Pour prouver la propriété (i), nous devons montrer que le Zero-One
Loss n’est pas convexe. Considérons deux points <span
class="math inline">\((y_1, \hat{y}_1)\)</span> et <span
class="math inline">\((y_2, \hat{y}_2)\)</span>. La convexité requiert
que : <span class="math display">\[L(\lambda y_1 + (1-\lambda) y_2,
\lambda \hat{y}_1 + (1-\lambda) \hat{y}_2) \leq \lambda L(y_1,
\hat{y}_1) + (1-\lambda) L(y_2, \hat{y}_2)\]</span></p>
<p>Cependant, le Zero-One Loss ne satisfait pas cette inégalité pour
certains choix de <span class="math inline">\(y_1, \hat{y}_1, y_2,
\hat{y}_2\)</span>, ce qui prouve qu’il n’est pas convexe.</p>
</body>
</html>
{% include "footer.html" %}


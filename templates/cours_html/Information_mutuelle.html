{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Information Mutuelle : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Information Mutuelle : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’information mutuelle, concept central en théorie de l’information,
émerge comme une réponse élégante à la question fondamentale :
<em>comment quantifier le lien entre deux sources d’information ?</em>
Introduite par Claude Shannon dans les années 1940, cette notion
révolutionne notre compréhension des dépendances statistiques et ouvre
la voie à des applications variées, de la compression de données au
traitement du signal.</p>
<p>L’information mutuelle mesure l’incertitude réduite d’une variable
aléatoire lorsqu’on connaît une autre. Elle est donc indispensable pour
évaluer la quantité d’information partagée entre deux sources, ce qui en
fait un outil puissant dans l’analyse des systèmes complexes.</p>
<h1 id="définitions">Définitions</h1>
<p>Commençons par comprendre ce que nous cherchons à mesurer. Imaginons
deux variables aléatoires <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span>. Nous voulons quantifier à quel
point la connaissance de <span class="math inline">\(Y\)</span> réduit
l’incertitude sur <span class="math inline">\(X\)</span>, et vice versa.
Cette quantité doit être symétrique, c’est-à-dire qu’elle ne dépend pas
de l’ordre des variables.</p>
<p>La définition formelle de l’information mutuelle entre <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est donnée par :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires discrètes
définies sur un ensemble fini. L’information mutuelle entre <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, notée <span class="math inline">\(I(X;
Y)\)</span>, est définie par : <span class="math display">\[I(X; Y) =
\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \left(
\frac{p(x, y)}{p_X(x) p_Y(y)} \right)\]</span> où <span
class="math inline">\(p(x, y)\)</span> est la probabilité jointe de
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, <span
class="math inline">\(p_X(x)\)</span> est la probabilité marginale de
<span class="math inline">\(X\)</span> prenant la valeur <span
class="math inline">\(x\)</span>, et <span
class="math inline">\(p_Y(y)\)</span> est la probabilité marginale de
<span class="math inline">\(Y\)</span> prenant la valeur <span
class="math inline">\(y\)</span>.</p>
</div>
<p>Pour les variables aléatoires continues, l’information mutuelle est
définie par : <span class="math display">\[I(X; Y) =
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} p(x, y) \log \left(
\frac{p(x, y)}{p_X(x) p_Y(y)} \right) \, dx \, dy\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’information mutuelle est le théorème
de Fano, qui établit un lien entre l’information mutuelle et l’erreur de
reconstruction.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(\hat{X}\)</span> deux variables aléatoires telles
que <span class="math inline">\(\hat{X}\)</span> est une reconstruction
de <span class="math inline">\(X\)</span>. Alors, l’erreur de
probabilité <span class="math inline">\(P_e = P(X \neq \hat{X})\)</span>
est bornée inférieurement par : <span class="math display">\[H(P_e) +
P_e \log(|\mathcal{X}| - 1) \geq I(X; \hat{X})\]</span> où <span
class="math inline">\(H(P_e)\)</span> est l’entropie binaire de <span
class="math inline">\(P_e\)</span> et <span
class="math inline">\(|\mathcal{X}|\)</span> est le cardinal de
l’ensemble des valeurs possibles de <span
class="math inline">\(X\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Prouvons le théorème de Fano. Commençons par noter que : <span
class="math display">\[I(X; \hat{X}) = H(X) - H(X | \hat{X})\]</span> où
<span class="math inline">\(H(X | \hat{X})\)</span> est l’entropie
conditionnelle de <span class="math inline">\(X\)</span> sachant <span
class="math inline">\(\hat{X}\)</span>.</p>
<p>En utilisant la définition de l’entropie conditionnelle, nous avons :
<span class="math display">\[H(X | \hat{X}) = \sum_{\hat{x}} p(\hat{x})
H(X | \hat{X} = \hat{x})\]</span></p>
<p>Pour chaque <span class="math inline">\(\hat{x}\)</span>, nous
pouvons écrire : <span class="math display">\[H(X | \hat{X} = \hat{x})
\leq H(P_e)\]</span> car l’entropie conditionnelle est maximale lorsque
les probabilités sont uniformes.</p>
<p>Ainsi, nous obtenons : <span class="math display">\[H(X | \hat{X})
\leq H(P_e)\]</span></p>
<p>En substituant dans l’expression de <span class="math inline">\(I(X;
\hat{X})\)</span>, nous avons : <span class="math display">\[I(X;
\hat{X}) \leq H(P_e) + P_e \log(|\mathcal{X}| - 1)\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’information mutuelle possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p><strong>Symétrie</strong> : <span class="math inline">\(I(X; Y) =
I(Y; X)\)</span></p></li>
<li><p><strong>Non-négativité</strong> : <span
class="math inline">\(I(X; Y) \geq 0\)</span></p></li>
<li><p><strong>Inégalité de l’information mutuelle</strong> : <span
class="math inline">\(I(X; Y) \leq \min(H(X), H(Y))\)</span></p></li>
</ol>
<p>Prouvons la propriété de non-négativité. En utilisant la définition
de l’information mutuelle, nous avons : <span
class="math display">\[I(X; Y) = \sum_{x, y} p(x, y) \log \left(
\frac{p(x, y)}{p_X(x) p_Y(y)} \right)\]</span></p>
<p>En utilisant l’inégalité de Gibbs, nous savons que : <span
class="math display">\[\log \left( \frac{p(x, y)}{p_X(x) p_Y(y)} \right)
\geq 1 - \frac{p_X(x) p_Y(y)}{p(x, y)}\]</span></p>
<p>Ainsi, <span class="math display">\[I(X; Y) \geq \sum_{x, y} p(x, y)
\left(1 - \frac{p_X(x) p_Y(y)}{p(x, y)}\right) = 1 - \sum_{x, y} p_X(x)
p_Y(y) = 0\]</span></p>
</body>
</html>
{% include "footer.html" %}


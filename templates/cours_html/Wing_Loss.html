{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Wing Loss: A Robust and Efficient Loss Function for Deep Learning</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Wing Loss: A Robust and Efficient Loss Function for
Deep Learning</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>The advent of deep learning has revolutionized various fields, from
computer vision to natural language processing. Central to the success
of deep learning models is the choice of loss function, which guides the
optimization process. Traditional loss functions such as Mean Squared
Error (MSE) and Cross-Entropy Loss have been widely used, but they come
with their own set of limitations. For instance, MSE is sensitive to
outliers and can lead to slow convergence, while Cross-Entropy Loss may
suffer from gradient vanishing issues.</p>
<p>In response to these challenges, the Wing Loss was introduced. This
loss function is designed to be robust against outliers and efficient in
terms of computational complexity. The Wing Loss combines the advantages
of both <span class="math inline">\(\ell_1\)</span> and <span
class="math inline">\(\ell_2\)</span> losses, making it particularly
suitable for tasks such as face alignment and object detection. The Wing
Loss has gained significant attention due to its ability to handle both
small and large errors effectively, thus providing a more balanced
optimization landscape.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>To understand the Wing Loss, we first need to grasp the concepts of
<span class="math inline">\(\ell_1\)</span> and <span
class="math inline">\(\ell_2\)</span> norms, as well as their
limitations.</p>
<p>The <span class="math inline">\(\ell_1\)</span> norm, or Manhattan
distance, is defined as: <span class="math display">\[\|\mathbf{x}\|_1 =
\sum_{i=1}^n |x_i|\]</span> This norm is known for its robustness to
outliers, as it penalizes large errors linearly. However, it can lead to
non-smooth optimization landscapes due to its absolute value
component.</p>
<p>The <span class="math inline">\(\ell_2\)</span> norm, or Euclidean
distance, is defined as: <span class="math display">\[\|\mathbf{x}\|_2 =
\left( \sum_{i=1}^n x_i^2 \right)^{1/2}\]</span> This norm is smooth and
convex, making it easier to optimize. However, it is sensitive to
outliers because the squared term amplifies large errors.</p>
<p>The Wing Loss aims to combine the robustness of <span
class="math inline">\(\ell_1\)</span> with the smoothness of <span
class="math inline">\(\ell_2\)</span>. Let’s formally define the Wing
Loss.</p>
<p>Given a predicted value <span class="math inline">\(\hat{y}\)</span>
and a ground truth value <span class="math inline">\(y\)</span>, the
Wing Loss is defined as: <span
class="math display">\[\mathcal{L}_{\text{wing}}(y, \hat{y}) =
\begin{cases}
w \log(1 + \frac{|y - \hat{y}|}{\epsilon}) &amp; \text{if } |y -
\hat{y}| &lt; w \\
|y - \hat{y}| - w &amp; \text{otherwise}
\end{cases}\]</span> where <span class="math inline">\(w\)</span> and
<span class="math inline">\(\epsilon\)</span> are hyperparameters that
control the behavior of the loss function.</p>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>One of the key properties of the Wing Loss is its ability to handle
both small and large errors effectively. Let’s explore this property in
detail.</p>
<div class="theorem">
<p>For any <span class="math inline">\(\epsilon &gt; 0\)</span> and
<span class="math inline">\(w &gt; 0\)</span>, the Wing Loss <span
class="math inline">\(\mathcal{L}_{\text{wing}}(y, \hat{y})\)</span> is
robust to outliers in the sense that it penalizes large errors
linearly.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Consider the case when <span class="math inline">\(|y
- \hat{y}| \geq w\)</span>. The Wing Loss simplifies to: <span
class="math display">\[\mathcal{L}_{\text{wing}}(y, \hat{y}) = |y -
\hat{y}| - w\]</span> This is a linear function of the error <span
class="math inline">\(|y - \hat{y}|\)</span>, which means that large
errors are penalized linearly. This property makes the Wing Loss robust
to outliers, as it does not amplify large errors like the <span
class="math inline">\(\ell_2\)</span> norm.</p>
<p>For the case when <span class="math inline">\(|y - \hat{y}| &lt;
w\)</span>, the Wing Loss is defined as: <span
class="math display">\[\mathcal{L}_{\text{wing}}(y, \hat{y}) = w \log(1
+ \frac{|y - \hat{y}|}{\epsilon})\]</span> The logarithmic function
<span class="math inline">\(\log(1 + x)\)</span> is known to be concave
and grows sublinearly for small values of <span
class="math inline">\(x\)</span>. This means that the Wing Loss
penalizes small errors in a smooth and controlled manner, similar to the
<span class="math inline">\(\ell_2\)</span> norm but without amplifying
large errors. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>The Wing Loss has several important properties that make it suitable
for deep learning tasks. Let’s explore these properties in detail.</p>
<ol>
<li><p><strong>Smoothness for Small Errors:</strong> For small errors
<span class="math inline">\(|y - \hat{y}| &lt; w\)</span>, the Wing Loss
is smooth and differentiable. This property is crucial for efficient
optimization, as it allows gradient-based methods to converge
quickly.</p>
<div class="proof">
<p><em>Proof.</em> The derivative of the Wing Loss with respect to <span
class="math inline">\(\hat{y}\)</span> for <span
class="math inline">\(|y - \hat{y}| &lt; w\)</span> is: <span
class="math display">\[\frac{\partial
\mathcal{L}_{\text{wing}}}{\partial \hat{y}} = \frac{w}{\epsilon + |y -
\hat{y}|} \text{sign}(y - \hat{y})\]</span> This derivative is
well-defined and continuous for all <span class="math inline">\(\hat{y}
\neq y\)</span>, ensuring smooth optimization. ◻</p>
</div></li>
<li><p><strong>Linearity for Large Errors:</strong> For large errors
<span class="math inline">\(|y - \hat{y}| \geq w\)</span>, the Wing Loss
becomes linear. This property makes the Wing Loss robust to outliers, as
it does not amplify large errors.</p>
<div class="proof">
<p><em>Proof.</em> The derivative of the Wing Loss with respect to <span
class="math inline">\(\hat{y}\)</span> for <span
class="math inline">\(|y - \hat{y}| \geq w\)</span> is: <span
class="math display">\[\frac{\partial
\mathcal{L}_{\text{wing}}}{\partial \hat{y}} = -\text{sign}(y -
\hat{y})\]</span> This derivative is constant and does not depend on the
magnitude of the error, ensuring robustness to outliers. ◻</p>
</div></li>
<li><p><strong>Controlled by Hyperparameters:</strong> The behavior of
the Wing Loss is controlled by two hyperparameters, <span
class="math inline">\(w\)</span> and <span
class="math inline">\(\epsilon\)</span>. These hyperparameters allow the
user to adjust the trade-off between robustness and smoothness.</p>
<div class="proof">
<p><em>Proof.</em> The hyperparameter <span
class="math inline">\(w\)</span> controls the transition point between
the logarithmic and linear regions of the Wing Loss. A larger <span
class="math inline">\(w\)</span> makes the loss function more robust to
outliers, while a smaller <span class="math inline">\(w\)</span> makes
it smoother for small errors.</p>
<p>The hyperparameter <span class="math inline">\(\epsilon\)</span>
controls the scale of the logarithmic region. A larger <span
class="math inline">\(\epsilon\)</span> makes the loss function more
sensitive to small errors, while a smaller <span
class="math inline">\(\epsilon\)</span> makes it less sensitive. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>The Wing Loss is a robust and efficient loss function that combines
the advantages of both <span class="math inline">\(\ell_1\)</span> and
<span class="math inline">\(\ell_2\)</span> norms. Its ability to handle
both small and large errors effectively makes it particularly suitable
for tasks such as face alignment and object detection. The Wing Loss has
gained significant attention in the deep learning community due to its
unique properties and performance.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de K-divergence : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de K-divergence : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Kullback-Leibler, souvent abrégée en K-divergence,
est une mesure fondamentale en théorie de l’information. Elle quantifie
la différence entre deux distributions de probabilité, permettant ainsi
d’évaluer à quel point une distribution se rapproche ou s’éloigne d’une
autre. Introduite par Solomon Kullback et Richard Leibler en 1951, cette
notion a trouvé des applications dans de nombreux domaines, notamment
l’apprentissage automatique, la théorie des codes et la statistique.</p>
<p>L’émergence de la K-divergence est motivée par le besoin de mesurer
l’information mutuelle entre deux distributions. En effet, dans un
contexte où l’on souhaite comparer des modèles statistiques ou optimiser
des algorithmes d’apprentissage, la K-divergence offre un cadre
rigoureux pour évaluer les performances. Elle est indispensable dans des
cadres tels que l’estimation de densité, la compression de données et
l’inférence bayésienne.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la K-divergence, considérons deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace mesurable <span
class="math inline">\(\Omega\)</span>. Nous cherchons à quantifier la
différence entre ces deux distributions. Intuitivement, nous voulons une
mesure qui capture à quel point <span class="math inline">\(Q\)</span>
s’écarte de <span class="math inline">\(P\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La divergence de Kullback-Leibler de <span
class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span> est définie comme suit :</p>
<p><span class="math display">\[D_{\text{KL}}(P \| Q) = \int_{\Omega}
p(x) \log\left(\frac{p(x)}{q(x)}\right) \, dx\]</span></p>
<p>où <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité des
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement. Si <span
class="math inline">\(P\)</span> n’est pas absolument continue par
rapport à <span class="math inline">\(Q\)</span>, alors <span
class="math inline">\(D_{\text{KL}}(P \| Q) = +\infty\)</span>.</p>
</div>
<p>Une autre formulation de la K-divergence, souvent utilisée en théorie
de l’information, est :</p>
<p><span class="math display">\[D_{\text{KL}}(P \| Q) =
\mathbb{E}_P\left[\log\left(\frac{p(X)}{q(X)}\right)\right]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}_P\)</span> désigne
l’espérance prise sous la distribution <span
class="math inline">\(P\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la K-divergence est l’inégalité de
Gibbs, qui établit une borne inférieure sur la divergence.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité.
Alors, la divergence de Kullback-Leibler satisfait l’inégalité suivante
:</p>
<p><span class="math display">\[D_{\text{KL}}(P \| Q) \geq
0\]</span></p>
<p>De plus, l’égalité <span class="math inline">\(D_{\text{KL}}(P \| Q)
= 0\)</span> si et seulement si <span class="math inline">\(P =
Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer l’inégalité de Gibbs, nous utilisons
la concavité de la fonction logarithme. Considérons la fonction <span
class="math inline">\(f(x) = \log(x)\)</span>, qui est concave. Par la
propriété de Jensen, nous avons :</p>
<p><span
class="math display">\[\mathbb{E}_P\left[\log\left(\frac{p(X)}{q(X)}\right)\right]
\geq \log\left(\mathbb{E}_P\left[\frac{p(X)}{q(X)}\right]\right) =
\log(1) = 0\]</span></p>
<p>où nous avons utilisé le fait que <span
class="math inline">\(\mathbb{E}_P\left[\frac{p(X)}{q(X)}\right] =
1\)</span> par définition des densités de probabilité. L’égalité est
atteinte si et seulement si <span
class="math inline">\(\frac{p(X)}{q(X)}\)</span> est constante presque
partout, ce qui implique que <span class="math inline">\(P =
Q\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’utilisation de la K-divergence, considérons un
exemple simple où nous voulons comparer deux distributions normales.</p>
<div class="example">
<p>Soient <span class="math inline">\(P = \mathcal{N}(\mu_1,
\sigma_1^2)\)</span> et <span class="math inline">\(Q =
\mathcal{N}(\mu_2, \sigma_2^2)\)</span> deux distributions normales. La
divergence de Kullback-Leibler de <span class="math inline">\(Q\)</span>
par rapport à <span class="math inline">\(P\)</span> est donnée par
:</p>
<p><span class="math display">\[D_{\text{KL}}(P \| Q) = \frac{1}{2}
\left( \log\left(\frac{\sigma_2^2}{\sigma_1^2}\right) + \frac{\sigma_1^2
+ (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right)\]</span></p>
<p>Cette formule peut être dérivée en utilisant les propriétés des
distributions normales et les définitions de la K-divergence.</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La K-divergence possède plusieurs propriétés intéressantes qui en
font un outil puissant en théorie de l’information.</p>
<div class="proposition">
<p>La K-divergence satisfait les propriétés suivantes :</p>
<ol>
<li><p><strong>Non-négativité</strong> : <span
class="math inline">\(D_{\text{KL}}(P \| Q) \geq 0\)</span>.</p></li>
<li><p><strong>Indépendance de l’ordre</strong> : <span
class="math inline">\(D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \|
P)\)</span>.</p></li>
<li><p><strong>Invariance par transformation</strong> : Si <span
class="math inline">\(T\)</span> est une transformation mesurable, alors
<span class="math inline">\(D_{\text{KL}}(P \| Q) = D_{\text{KL}}(P
\circ T^{-1} \| Q \circ T^{-1})\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> La propriété (i) est une conséquence directe de
l’inégalité de Gibbs. La propriété (ii) montre que la K-divergence n’est
pas symétrique, ce qui reflète le fait qu’elle mesure l’information
perdue lorsque <span class="math inline">\(Q\)</span> est utilisé pour
approximer <span class="math inline">\(P\)</span>. La propriété (iii)
découle du fait que la K-divergence est invariante sous les
transformations mesurables. ◻</p>
</div>
<p>La K-divergence trouve des applications dans de nombreux domaines,
notamment en apprentissage automatique, où elle est utilisée pour
évaluer la performance des modèles. Elle joue également un rôle clé dans
les algorithmes d’optimisation, tels que l’algorithme EM
(Expectation-Maximization), où elle est utilisée pour maximiser la
vraisemblance d’un modèle.</p>
<p>En conclusion, la divergence de Kullback-Leibler est un outil
fondamental en théorie de l’information et en statistique. Ses
propriétés et ses applications en font un sujet d’étude riche et
passionnant, avec de nombreuses perspectives pour les recherches
futures.</p>
</body>
</html>
{% include "footer.html" %}


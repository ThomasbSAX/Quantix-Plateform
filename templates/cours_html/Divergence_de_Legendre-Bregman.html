{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Legendre-Bregman : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Legendre-Bregman : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Legendre-Bregman émerge comme un outil fondamental
dans l’analyse des fonctions convexes et leurs applications en
optimisation, apprentissage automatique et traitement du signal.
Historiquement, cette divergence trouve ses racines dans les travaux de
Legendre sur la transformation des fonctions et ceux de Bregman sur les
méthodes d’optimisation. Elle se distingue par sa capacité à mesurer la
distance entre deux points dans le cadre des fonctions convexes, tout en
préservant les propriétés géométriques et analytiques essentielles.</p>
<p>L’importance de la divergence de Legendre-Bregman réside dans sa
capacité à fournir une mesure de distance adaptée aux problèmes
d’optimisation sous contraintes, où les méthodes classiques peuvent
échouer. Elle est particulièrement utile dans les algorithmes
d’apprentissage automatique, où elle permet de définir des critères de
convergence robustes et efficaces. De plus, cette divergence est
intrinsèquement liée à la théorie de l’information et aux méthodes de
régularisation, ce qui en fait un outil polyvalent pour les chercheurs
en mathématiques appliquées et en sciences de l’ingénieur.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Legendre-Bregman, commençons par
rappeler quelques concepts fondamentaux. Considérons une fonction
convexe <span class="math inline">\(f\)</span> définie sur un ensemble
convexe <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span>.
Nous cherchons à mesurer la distance entre deux points <span
class="math inline">\(x, y \in C\)</span> en utilisant les propriétés de
<span class="math inline">\(f\)</span>.</p>
<p>La divergence de Legendre-Bregman est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>. La divergence
de Legendre-Bregman associée à <span class="math inline">\(f\)</span>
est définie pour tout <span class="math inline">\(x, y \in C\)</span>
par : <span class="math display">\[D_f(x \| y) = f(x) - f(y) - \langle
\nabla f(y), x - y \rangle\]</span> où <span
class="math inline">\(\nabla f(y)\)</span> désigne le gradient de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(y\)</span>, et <span class="math inline">\(\langle
\cdot, \cdot \rangle\)</span> représente le produit scalaire usuel sur
<span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</div>
<p>Cette définition peut être reformulée en utilisant les propriétés de
la fonction <span class="math inline">\(f\)</span>. En particulier, si
<span class="math inline">\(f\)</span> est deux fois différentiable,
nous pouvons exprimer la divergence de Legendre-Bregman en termes de
l’intégrale du gradient :</p>
<p><span class="math display">\[D_f(x \| y) = \int_0^1 \langle \nabla
f(y + t(x - y)) - \nabla f(y), x - y \rangle \, dt\]</span></p>
<p>Cette formulation met en évidence le lien entre la divergence de
Legendre-Bregman et la courbure de la fonction <span
class="math inline">\(f\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>La divergence de Legendre-Bregman possède plusieurs propriétés
remarquables qui en font un outil puissant pour l’analyse des fonctions
convexes. Nous présentons ici quelques théorèmes clés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>. Alors, pour
tout <span class="math inline">\(x, y \in C\)</span>, la divergence de
Legendre-Bregman satisfait : <span class="math display">\[D_f(x \| y)
\geq 0\]</span> De plus, <span class="math inline">\(D_f(x \| y) =
0\)</span> si et seulement si <span class="math inline">\(x =
y\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la non-négativité de la divergence de
Legendre-Bregman, nous utilisons la convexité de <span
class="math inline">\(f\)</span>. Par définition de la convexité, pour
tout <span class="math inline">\(t \in [0, 1]\)</span>, nous avons :
<span class="math display">\[f(y + t(x - y)) \leq (1 - t) f(y) + t
f(x)\]</span> En réarrangeant cette inégalité, nous obtenons : <span
class="math display">\[f(x) - f(y) \geq t^{-1} (f(y + t(x - y)) -
f(y))\]</span> En prenant la limite lorsque <span
class="math inline">\(t\)</span> tend vers 0, nous retrouvons la
définition du gradient : <span class="math display">\[f(x) - f(y) \geq
\langle \nabla f(y), x - y \rangle\]</span> Ce qui est équivalent à
<span class="math inline">\(D_f(x \| y) \geq 0\)</span>. L’égalité <span
class="math inline">\(D_f(x \| y) = 0\)</span> implique que <span
class="math inline">\(f(x) = f(y)\)</span> et <span
class="math inline">\(\nabla f(y) = 0\)</span>, ce qui, par la stricte
convexité de <span class="math inline">\(f\)</span>, entraîne <span
class="math inline">\(x = y\)</span>. ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>. Alors, pour
tout <span class="math inline">\(x, y \in C\)</span>, la divergence de
Legendre-Bregman satisfait : <span class="math display">\[D_f(x \| y) +
D_f(y \| x) \geq 0\]</span> De plus, si <span
class="math inline">\(f\)</span> est strictement convexe, l’égalité a
lieu si et seulement si <span class="math inline">\(x = y\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous utilisons la définition de la divergence de
Legendre-Bregman pour écrire : <span class="math display">\[D_f(x \| y)
+ D_f(y \| x) = f(x) - f(y) - \langle \nabla f(y), x - y \rangle + f(y)
- f(x) - \langle \nabla f(x), y - x \rangle\]</span> En simplifiant,
nous obtenons : <span class="math display">\[D_f(x \| y) + D_f(y \| x) =
\langle \nabla f(x) - \nabla f(y), y - x \rangle\]</span> Par la
convexité de <span class="math inline">\(f\)</span>, le gradient <span
class="math inline">\(\nabla f\)</span> est monotone, c’est-à-dire que
pour tout <span class="math inline">\(x, y \in C\)</span>, nous avons :
<span class="math display">\[\langle \nabla f(x) - \nabla f(y), x - y
\rangle \geq 0\]</span> Ce qui implique <span
class="math inline">\(D_f(x \| y) + D_f(y \| x) \geq 0\)</span>. Si
<span class="math inline">\(f\)</span> est strictement convexe, la
monotonie stricte du gradient entraîne que l’égalité a lieu si et
seulement si <span class="math inline">\(x = y\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Legendre-Bregman possède plusieurs propriétés
intéressantes qui en font un outil puissant pour l’analyse des fonctions
convexes. Nous présentons ici quelques-unes de ces propriétés.</p>
<div class="proposition">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>. Alors, pour
tout <span class="math inline">\(x \in C\)</span>, nous avons : <span
class="math display">\[D_f(x \| x) = 0\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant la définition de la divergence de
Legendre-Bregman, nous avons : <span class="math display">\[D_f(x \| x)
= f(x) - f(x) - \langle \nabla f(x), x - x \rangle = 0\]</span> ◻</p>
</div>
<div class="proposition">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>, et soit <span
class="math inline">\(a \in \mathbb{R}^n\)</span> un vecteur. Alors,
pour tout <span class="math inline">\(x, y \in C\)</span>, nous avons :
<span class="math display">\[D_{f(\cdot + a)}(x \| y) = D_f(x - a \| y -
a)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant la définition de la divergence de
Legendre-Bregman, nous avons : <span class="math display">\[D_{f(\cdot +
a)}(x \| y) = f(x + a) - f(y + a) - \langle \nabla f(y + a), x - y
\rangle\]</span> En posant <span class="math inline">\(u = x -
a\)</span> et <span class="math inline">\(v = y - a\)</span>, nous
obtenons : <span class="math display">\[D_{f(\cdot + a)}(x \| y) = f(u)
- f(v) - \langle \nabla f(v), u - v \rangle = D_f(u \| v)\]</span> Ce
qui achève la preuve. ◻</p>
</div>
<div class="proposition">
<p>Soit <span class="math inline">\(f: C \rightarrow \mathbb{R}\)</span>
une fonction convexe et différentiable sur un ensemble convexe <span
class="math inline">\(C \subseteq \mathbb{R}^n\)</span>, et soit <span
class="math inline">\(\lambda &gt; 0\)</span> un scalaire. Alors, pour
tout <span class="math inline">\(x, y \in C\)</span>, nous avons : <span
class="math display">\[D_{\lambda f}(x \| y) = \lambda D_f(x \|
y)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant la définition de la divergence de
Legendre-Bregman, nous avons : <span class="math display">\[D_{\lambda
f}(x \| y) = \lambda f(x) - \lambda f(y) - \langle \nabla (\lambda
f)(y), x - y \rangle\]</span> En simplifiant, nous obtenons : <span
class="math display">\[D_{\lambda f}(x \| y) = \lambda (f(x) - f(y)) -
\langle \lambda \nabla f(y), x - y \rangle = \lambda D_f(x \|
y)\]</span> Ce qui achève la preuve. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Legendre-Bregman est un outil puissant pour
l’analyse des fonctions convexes et leurs applications en optimisation,
apprentissage automatique et traitement du signal. Ses propriétés
remarquables, telles que la non-négativité et l’inégalité de Bregman, en
font un outil essentiel pour les chercheurs en mathématiques appliquées
et en sciences de l’ingénieur. Les théorèmes et propositions présentés
dans cet article illustrent la richesse et la polyvalence de cette
divergence, ouvrant la voie à de nouvelles recherches et
applications.</p>
</body>
</html>
{% include "footer.html" %}


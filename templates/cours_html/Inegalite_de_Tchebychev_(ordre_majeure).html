{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Inégalité de Tchebychev : Ordre Majeuré</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Inégalité de Tchebychev : Ordre Majeuré</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’inégalité de Tchebychev, du nom du mathématicien russe Pafnuty
Lvovich Tchebychev (1821-1894), est un résultat fondamental en théorie
des probabilités et en analyse statistique. Cette inégalité émerge dans
le cadre de l’étude des concentrations de variables aléatoires et permet
de majorer la probabilité qu’une variable aléatoire s’écarte
significativement de sa moyenne.</p>
<p>L’origine historique de cette inégalité remonte au XIXe siècle, où
Tchebychev a développé des outils pour comprendre les propriétés
asymptotiques des sommes de variables aléatoires indépendantes.
L’inégalité de Tchebychev est indispensable dans l’analyse des
algorithmes probabilistes, en théorie de l’apprentissage statistique, et
dans de nombreuses applications pratiques où la compréhension des écarts
par rapport à la moyenne est cruciale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’inégalité de Tchebychev, commençons par définir les
concepts clés. Nous cherchons à comprendre comment une variable
aléatoire <span class="math inline">\(X\)</span> peut s’écarter de son
espérance mathématique <span class="math inline">\(E[X]\)</span>. Plus
précisément, nous voulons majorer la probabilité que <span
class="math inline">\(X\)</span> soit éloigné de <span
class="math inline">\(E[X]\)</span> d’une certaine quantité.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire de
variance finie. La variance de <span class="math inline">\(X\)</span>,
notée <span class="math inline">\(\text{Var}(X)\)</span>, est définie
par : <span class="math display">\[\text{Var}(X) = E\left[(X -
E[X])^2\right].\]</span> La variance mesure la dispersion des valeurs de
<span class="math inline">\(X\)</span> autour de son espérance.</p>
</div>
<div class="definition">
<p>L’écart-type de <span class="math inline">\(X\)</span>, noté <span
class="math inline">\(\sigma_X\)</span>, est la racine carrée de la
variance : <span class="math display">\[\sigma_X =
\sqrt{\text{Var}(X)}.\]</span> L’écart-type est une mesure de la
dispersion des valeurs de <span class="math inline">\(X\)</span> autour
de son espérance.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous allons maintenant énoncer l’inégalité de Tchebychev. Cette
inégalité permet de majorer la probabilité que <span
class="math inline">\(X\)</span> s’écarte de son espérance d’une
quantité supérieure à un certain seuil.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
réelle de variance finie. Pour tout <span class="math inline">\(k &gt;
0\)</span>, on a : <span class="math display">\[P\left(|X - E[X]| \geq k
\sigma_X\right) \leq \frac{1}{k^2}.\]</span></p>
</div>
<p>Pour comprendre cette inégalité, considérons que <span
class="math inline">\(k\)</span> est un paramètre qui contrôle la
distance par rapport à l’espérance. L’inégalité de Tchebychev nous dit
que la probabilité que <span class="math inline">\(X\)</span> soit à une
distance de <span class="math inline">\(k\)</span> écarts-types de son
espérance est au plus <span
class="math inline">\(\frac{1}{k^2}\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Nous allons maintenant démontrer l’inégalité de Tchebychev. La preuve
repose sur des propriétés fondamentales de la variance et de
l’espérance.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X\)</span> une
variable aléatoire réelle de variance finie. Nous voulons montrer que
pour tout <span class="math inline">\(k &gt; 0\)</span>, on a : <span
class="math display">\[P\left(|X - E[X]| \geq k \sigma_X\right) \leq
\frac{1}{k^2}.\]</span></p>
<p>Commençons par définir une nouvelle variable aléatoire <span
class="math inline">\(Y\)</span> telle que : <span
class="math display">\[Y = (X - E[X])^2.\]</span> L’espérance de <span
class="math inline">\(Y\)</span> est égale à la variance de <span
class="math inline">\(X\)</span>, c’est-à-dire : <span
class="math display">\[E[Y] = E\left[(X - E[X])^2\right] = \text{Var}(X)
= \sigma_X^2.\]</span></p>
<p>Considérons maintenant l’indicatrice de l’événement <span
class="math inline">\(|X - E[X]| \geq k \sigma_X\)</span> : <span
class="math display">\[I_{\{|X - E[X]| \geq k \sigma_X\}}.\]</span> Nous
avons : <span class="math display">\[I_{\{|X - E[X]| \geq k \sigma_X\}}
\leq \frac{Y}{k^2 \sigma_X^2}.\]</span></p>
<p>En prenant l’espérance des deux côtés, nous obtenons : <span
class="math display">\[E\left[I_{\{|X - E[X]| \geq k \sigma_X\}}\right]
\leq \frac{E[Y]}{k^2 \sigma_X^2}.\]</span></p>
<p>Or, <span class="math inline">\(E\left[I_{\{|X - E[X]| \geq k
\sigma_X\}}\right] = P\left(|X - E[X]| \geq k \sigma_X\right)\)</span>,
et <span class="math inline">\(E[Y] = \sigma_X^2\)</span>. Par
conséquent : <span class="math display">\[P\left(|X - E[X]| \geq k
\sigma_X\right) \leq \frac{\sigma_X^2}{k^2 \sigma_X^2} =
\frac{1}{k^2}.\]</span></p>
<p>Cela achève la démonstration de l’inégalité de Tchebychev. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous allons maintenant explorer quelques propriétés et corollaires de
l’inégalité de Tchebychev.</p>
<div class="corollaire">
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des
variables aléatoires indépendantes de même loi, d’espérance <span
class="math inline">\(\mu\)</span> et de variance <span
class="math inline">\(\sigma^2\)</span>. Pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, on a : <span
class="math display">\[P\left(\left|\frac{1}{n} \sum_{i=1}^n X_i -
\mu\right| \geq \epsilon\right) \leq \frac{\sigma^2}{n
\epsilon^2}.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(S_n = \sum_{i=1}^n
X_i\)</span>. L’espérance de <span class="math inline">\(S_n\)</span>
est <span class="math inline">\(n\mu\)</span>, et la variance de <span
class="math inline">\(S_n\)</span> est <span
class="math inline">\(n\sigma^2\)</span>. Par l’inégalité de Tchebychev,
on a : <span class="math display">\[P\left(|S_n - n\mu| \geq
n\epsilon\right) \leq \frac{n\sigma^2}{n^2 \epsilon^2} =
\frac{\sigma^2}{n \epsilon^2}.\]</span></p>
<p>En divisant par <span class="math inline">\(n\)</span>, on obtient :
<span class="math display">\[P\left(\left|\frac{1}{n} S_n - \mu\right|
\geq \epsilon\right) \leq \frac{\sigma^2}{n \epsilon^2}.\]</span></p>
<p>Cela achève la démonstration du corollaire. ◻</p>
</div>
<div class="corollaire">
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des
variables aléatoires indépendantes de même loi, d’espérance <span
class="math inline">\(\mu\)</span> et de variance <span
class="math inline">\(\sigma^2\)</span>. Pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, on a : <span
class="math display">\[\lim_{n \to \infty} P\left(\left|\frac{1}{n}
\sum_{i=1}^n X_i - \mu\right| \geq \epsilon\right) = 0.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Par le corollaire précédent, on a : <span
class="math display">\[P\left(\left|\frac{1}{n} \sum_{i=1}^n X_i -
\mu\right| \geq \epsilon\right) \leq \frac{\sigma^2}{n
\epsilon^2}.\]</span></p>
<p>En prenant la limite quand <span class="math inline">\(n\)</span>
tend vers l’infini, on obtient : <span class="math display">\[\lim_{n
\to \infty} P\left(\left|\frac{1}{n} \sum_{i=1}^n X_i - \mu\right| \geq
\epsilon\right) \leq \lim_{n \to \infty} \frac{\sigma^2}{n \epsilon^2} =
0.\]</span></p>
<p>Cela achève la démonstration du corollaire. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’inégalité de Tchebychev est un outil puissant pour majorer les
probabilités d’écarts par rapport à la moyenne. Elle trouve des
applications dans de nombreux domaines, notamment en statistique, en
théorie des probabilités et en analyse des algorithmes. Les corollaires
dérivés de cette inégalité, tels que l’inégalité de Bienaymé-Tchebychev
et la convergence en probabilité, sont essentiels pour comprendre le
comportement asymptotique des sommes de variables aléatoires.</p>
</body>
</html>
{% include "footer.html" %}


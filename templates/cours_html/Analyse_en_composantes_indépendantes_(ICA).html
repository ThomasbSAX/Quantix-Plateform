{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Analyse en Composantes Indépendantes (ICA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analyse en Composantes Indépendantes (ICA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Analyse en Composantes Indépendantes (ICA) est une technique
statistique avancée qui a émergé dans les années 1980 et 1990,
principalement grâce aux travaux de Tony Bell et Terence J. Sejnowski.
L’ICA vise à décomposer un signal multivarié en composantes additives
sous-jacentes qui sont statistiquement indépendantes. Cette méthode est
particulièrement utile dans des domaines tels que le traitement du
signal, l’imagerie médicale, et l’apprentissage automatique.</p>
<p>L’ICA est indispensable lorsque les données présentent des structures
latentes complexes qui ne peuvent être capturées par des méthodes
linéaires classiques comme l’Analyse en Composantes Principales (ACP).
Par exemple, dans le traitement des signaux EEG ou fMRI, l’ICA permet de
séparer les sources de signaux indépendantes, ce qui est crucial pour
l’interprétation des données neurologiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’ICA, commençons par définir ce que nous cherchons à
obtenir. Supposons que nous ayons un vecteur de données <span
class="math inline">\(\mathbf{X} = (X_1, X_2, \ldots, X_n)^T\)</span>
qui est une combinaison linéaire de sources indépendantes <span
class="math inline">\(\mathbf{S} = (S_1, S_2, \ldots, S_n)^T\)</span>.
Notre objectif est de retrouver <span
class="math inline">\(\mathbf{S}\)</span> à partir de <span
class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Formellement, nous cherchons une matrice de mélange <span
class="math inline">\(\mathbf{A}\)</span> et une matrice de déméliage
<span class="math inline">\(\mathbf{W}\)</span> telles que: <span
class="math display">\[\mathbf{X} = \mathbf{A}\mathbf{S}\]</span> et
<span class="math display">\[\mathbf{S} =
\mathbf{W}\mathbf{X}.\]</span></p>
<p>L’ICA cherche à maximiser l’indépendance statistique des composantes
<span class="math inline">\(S_i\)</span>. Une mesure courante de
l’indépendance est l’information mutuelle, qui doit être minimisée.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en ICA est le théorème de l’indépendance
centrale, qui stipule que la somme de variables aléatoires indépendantes
tend vers une distribution gaussienne. Cependant, l’ICA repose sur le
fait que les composantes indépendantes ne sont pas nécessairement
gaussiennes.</p>
<p>Un autre théorème important est celui de la séparation aveugle des
sources, qui affirme que sous certaines conditions, il est possible de
séparer les sources indépendantes à partir des observations
mélangées.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver que l’ICA peut séparer les sources indépendantes, nous
devons montrer que la matrice de déméliage <span
class="math inline">\(\mathbf{W}\)</span> peut être estimée à partir des
observations <span class="math inline">\(\mathbf{X}\)</span>. Cela
implique généralement l’utilisation de méthodes d’optimisation pour
maximiser une mesure d’indépendance, telle que la non-gaussianité.</p>
<p>Supposons que nous ayons un vecteur de données <span
class="math inline">\(\mathbf{X}\)</span> et que nous cherchions une
matrice <span class="math inline">\(\mathbf{W}\)</span> telle que les
composantes <span class="math inline">\(Y_i =
\mathbf{W}^T\mathbf{X}\)</span> soient indépendantes. Nous pouvons
utiliser la fonction de contraste suivante: <span
class="math display">\[J(Y) = \sum_{i=1}^n E[G(Y_i)] -
\log|\det(\mathbf{W})|,\]</span> où <span
class="math inline">\(G\)</span> est une fonction convexe qui mesure la
non-gaussianité.</p>
<p>La preuve consiste à montrer que la maximisation de <span
class="math inline">\(J(Y)\)</span> conduit à des composantes
indépendantes. Cela peut être fait en utilisant des méthodes de gradient
ou des algorithmes itératifs.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les propriétés de l’ICA incluent:</p>
<ol>
<li><p>L’ICA est une méthode de séparation aveugle des sources, ce qui
signifie qu’elle ne nécessite pas de connaissances a priori sur la
matrice de mélange <span
class="math inline">\(\mathbf{A}\)</span>.</p></li>
<li><p>L’ICA est sensible à l’ordre des composantes, car les sources
indépendantes peuvent être permutées.</p></li>
<li><p>L’ICA suppose que les sources sont statistiquement indépendantes
et non gaussiennes.</p></li>
</ol>
<p>Un corollaire important est que l’ICA peut être utilisée pour la
réduction de dimension, similaire à l’ACP, mais avec une interprétation
différente des composantes.</p>
</body>
</html>
{% include "footer.html" %}


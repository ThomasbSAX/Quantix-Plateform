{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi de Bernoulli a posteriori</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi de Bernoulli a posteriori</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La loi de Bernoulli est un concept fondamental en théorie des
probabilités, modélisant les expériences aléatoires à deux issues
possibles. Cependant, dans un cadre bayésien, l’intérêt réside dans la
mise à jour de nos croyances face à de nouvelles observations. La loi de
Bernoulli a posteriori émerge naturellement comme une extension de cette
idée, permettant d’incorporer des informations supplémentaires pour
affiner nos estimations.</p>
<p>Cette notion est indispensable dans les domaines où l’incertitude est
omniprésente, tels que la statistique bayésienne, l’apprentissage
automatique et les sciences des données. Elle permet de passer d’une
distribution a priori, reflétant notre état initial de connaissance, à
une distribution a posteriori, intégrant les données observées.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la loi de Bernoulli a posteriori, commençons par
rappeler ce qu’est une loi de Bernoulli. Imaginons une expérience
aléatoire avec deux issues possibles : succès et échec. La probabilité
de succès est notée <span class="math inline">\(p\)</span>, et la
probabilité d’échec est donc <span class="math inline">\(1 - p\)</span>.
La loi de Bernoulli modélise cette situation.</p>
<p>Formellement, une variable aléatoire <span
class="math inline">\(X\)</span> suit une loi de Bernoulli de paramètre
<span class="math inline">\(p\)</span> si : <span
class="math display">\[X \sim \text{Bernoulli}(p) \quad \Leftrightarrow
\quad P(X = 1) = p \text{ et } P(X = 0) = 1 - p\]</span></p>
<p>Maintenant, supposons que nous avons une distribution a priori sur
<span class="math inline">\(p\)</span>, notée <span
class="math inline">\(\pi(p)\)</span>. Cette distribution reflète notre
croyance initiale sur la valeur de <span
class="math inline">\(p\)</span>. Après avoir observé une série de
résultats, nous voulons mettre à jour cette croyance pour obtenir une
distribution a posteriori.</p>
<p>La loi de Bernoulli a posteriori est alors définie comme la
distribution conditionnelle de <span class="math inline">\(p\)</span>
donnée les observations. Si nous observons <span
class="math inline">\(n\)</span> essais indépendants, avec <span
class="math inline">\(k\)</span> succès, la distribution a posteriori
est proportionnelle au produit de la vraisemblance et de la distribution
a priori.</p>
<p>Formellement, si <span class="math inline">\(\pi(p)\)</span> est une
distribution a priori sur <span class="math inline">\(p\)</span>, et que
nous observons <span class="math inline">\(k\)</span> succès en <span
class="math inline">\(n\)</span> essais, la distribution a posteriori
est donnée par : <span class="math display">\[\pi(p | k, n) \propto p^k
(1 - p)^{n - k} \pi(p)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un résultat clé concernant la loi de Bernoulli a posteriori est le
théorème suivant, qui donne une expression explicite pour la
distribution a posteriori lorsque la distribution a priori est une loi
bêta.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X_1, \ldots, X_n\)</span> des
variables aléatoires indépendantes et identiquement distribuées suivant
une loi de Bernoulli de paramètre <span
class="math inline">\(p\)</span>. Supposons que la distribution a priori
de <span class="math inline">\(p\)</span> est une loi bêta de paramètres
<span class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>. Alors, la distribution a
posteriori de <span class="math inline">\(p\)</span> donnée les
observations est une loi bêta de paramètres <span
class="math inline">\(\alpha + k\)</span> et <span
class="math inline">\(\beta + n - k\)</span>, où <span
class="math inline">\(k = \sum_{i=1}^n X_i\)</span>. <span
class="math display">\[\pi(p | k, n) = \text{Beta}(\alpha + k, \beta + n
- k)\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, commençons par rappeler que la
vraisemblance des observations est donnée par : <span
class="math display">\[L(p | k, n) = p^k (1 - p)^{n - k}\]</span></p>
<p>La distribution a priori est une loi bêta de paramètres <span
class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span>, donc : <span
class="math display">\[\pi(p) = \frac{p^{\alpha - 1} (1 - p)^{\beta -
1}}{B(\alpha, \beta)}\]</span> où <span class="math inline">\(B(\alpha,
\beta)\)</span> est la fonction bêta.</p>
<p>La distribution a posteriori est proportionnelle au produit de la
vraisemblance et de la distribution a priori : <span
class="math display">\[\pi(p | k, n) \propto p^k (1 - p)^{n - k} \cdot
\frac{p^{\alpha - 1} (1 - p)^{\beta - 1}}{B(\alpha, \beta)}\]</span></p>
<p>En simplifiant, nous obtenons : <span class="math display">\[\pi(p |
k, n) \propto p^{\alpha + k - 1} (1 - p)^{\beta + n - k -
1}\]</span></p>
<p>Cette expression est proportionnelle à la densité d’une loi bêta de
paramètres <span class="math inline">\(\alpha + k\)</span> et <span
class="math inline">\(\beta + n - k\)</span>. Par conséquent, la
distribution a posteriori est une loi bêta de paramètres <span
class="math inline">\(\alpha + k\)</span> et <span
class="math inline">\(\beta + n - k\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La loi de Bernoulli a posteriori possède plusieurs propriétés
intéressantes, que nous allons énumérer et démontrer.</p>
<ol>
<li><p><strong>Moyenne a posteriori</strong> : La moyenne de la
distribution a posteriori est donnée par : <span
class="math display">\[E[p | k, n] = \frac{\alpha + k}{\alpha + \beta +
n}\]</span></p>
<p><strong>Preuve</strong> : La moyenne d’une loi bêta de paramètres
<span class="math inline">\(\alpha&#39;\)</span> et <span
class="math inline">\(\beta&#39;\)</span> est <span
class="math inline">\(\frac{\alpha&#39;}{\alpha&#39; +
\beta&#39;}\)</span>. En appliquant cette propriété à notre distribution
a posteriori, nous obtenons le résultat souhaité.</p></li>
<li><p><strong>Variance a posteriori</strong> : La variance de la
distribution a posteriori est donnée par : <span
class="math display">\[\text{Var}(p | k, n) = \frac{(\alpha + k)(\beta +
n - k)}{(\alpha + \beta + n)^2 (\alpha + \beta + n + 1)}\]</span></p>
<p><strong>Preuve</strong> : La variance d’une loi bêta de paramètres
<span class="math inline">\(\alpha&#39;\)</span> et <span
class="math inline">\(\beta&#39;\)</span> est <span
class="math inline">\(\frac{\alpha&#39; \beta&#39;}{(\alpha&#39; +
\beta&#39;)^2 (\alpha&#39; + \beta&#39; + 1)}\)</span>. En appliquant
cette propriété à notre distribution a posteriori, nous obtenons le
résultat souhaité.</p></li>
<li><p><strong>Convergence</strong> : Lorsque le nombre d’observations
<span class="math inline">\(n\)</span> tend vers l’infini, la
distribution a posteriori converge vers une loi de Dirac en <span
class="math inline">\(\frac{k}{n}\)</span>.</p>
<p><strong>Preuve</strong> : Lorsque <span
class="math inline">\(n\)</span> tend vers l’infini, les paramètres de
la distribution a posteriori deviennent <span
class="math inline">\(\alpha + k\)</span> et <span
class="math inline">\(\beta + n - k\)</span>. En normalisant par <span
class="math inline">\(n\)</span>, nous obtenons <span
class="math inline">\(\frac{\alpha}{n} + \frac{k}{n}\)</span> et <span
class="math inline">\(\frac{\beta}{n} + 1 - \frac{k}{n}\)</span>. Si
<span class="math inline">\(\alpha\)</span> et <span
class="math inline">\(\beta\)</span> sont fixes, ces termes tendent vers
0 et 1 respectivement. Par conséquent, la distribution a posteriori
converge vers une loi de Dirac en <span
class="math inline">\(\frac{k}{n}\)</span>.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La loi de Bernoulli a posteriori est un outil puissant pour mettre à
jour nos croyances en fonction de nouvelles observations. En combinant
une distribution a priori avec des données observées, nous pouvons
obtenir une distribution a posteriori qui reflète notre état de
connaissance mis à jour. Les propriétés et théorèmes associés à cette
loi sont essentiels pour comprendre son comportement et ses applications
pratiques.</p>
</body>
</html>
{% include "footer.html" %}


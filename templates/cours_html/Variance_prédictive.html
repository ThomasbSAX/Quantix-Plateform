{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Variance Prédictive : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Variance Prédictive : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La variance prédictive émerge comme un concept fondamental dans
l’analyse des modèles statistiques et des algorithmes d’apprentissage
automatique. Son origine remonte aux travaux pionniers sur la régression
linéaire et les méthodes bayésiennes, où l’évaluation de la performance
des modèles était cruciale. La variance prédictive mesure la variabilité
des prédictions d’un modèle autour de leur moyenne, offrant ainsi une
indication sur la stabilité et la fiabilité des prévisions.</p>
<p>Dans un contexte où les données sont souvent bruitées et incomplètes,
la variance prédictive devient indispensable pour évaluer la robustesse
des modèles. Elle permet de distinguer entre les erreurs dues au bruit
intrinsèque des données et celles résultant d’une mauvaise spécification
du modèle. Cette distinction est cruciale pour améliorer les
performances des modèles et pour prendre des décisions éclairées basées
sur les prédictions.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la variance prédictive, considérons un modèle de
prédiction <span class="math inline">\(f\)</span> qui prend en entrée
des données <span class="math inline">\(X\)</span> et produit une
prédiction <span class="math inline">\(\hat{Y} = f(X)\)</span>. La
variance prédictive mesure la variabilité des prédictions <span
class="math inline">\(\hat{Y}\)</span> pour différentes réalisations de
<span class="math inline">\(X\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f\)</span> un modèle de prédiction
et <span class="math inline">\(X\)</span> une variable aléatoire
représentant les données d’entrée. La variance prédictive de <span
class="math inline">\(f\)</span> est définie comme : <span
class="math display">\[\text{Var}_{\text{pred}}(f) = \mathbb{E}_{X}
\left[ \text{Var}(\hat{Y} | X) \right]\]</span> où <span
class="math inline">\(\mathbb{E}_{X}\)</span> désigne l’espérance prise
sur la distribution de <span class="math inline">\(X\)</span>, et <span
class="math inline">\(\text{Var}(\hat{Y} | X)\)</span> est la variance
conditionnelle des prédictions <span
class="math inline">\(\hat{Y}\)</span> étant donné <span
class="math inline">\(X\)</span>.</p>
</div>
<p>Une autre manière de formuler la variance prédictive est : <span
class="math display">\[\text{Var}_{\text{pred}}(f) = \mathbb{E}_{X}
\left[ \mathbb{E}[\hat{Y}^2 | X] - (\mathbb{E}[\hat{Y} | X])^2
\right]\]</span> Cette formulation met en évidence la double espérance,
d’abord conditionnelle sur <span class="math inline">\(X\)</span>, puis
sur la distribution de <span class="math inline">\(X\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la variance prédictive est le théorème
de décomposition de la variance totale, qui permet de séparer la
variance prédictive en composantes explicables et inexplicables.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> un modèle de prédiction
et <span class="math inline">\(X\)</span> une variable aléatoire. La
variance totale des prédictions peut être décomposée comme suit : <span
class="math display">\[\text{Var}(\hat{Y}) = \text{Var}_{\text{pred}}(f)
+ \mathbb{E}_{X} \left[ (\mathbb{E}[\hat{Y} | X] -
\mathbb{E}[\hat{Y}])^2 \right]\]</span> où <span
class="math inline">\(\text{Var}_{\text{pred}}(f)\)</span> est la
variance prédictive et le deuxième terme représente la variance
expliquée par les caractéristiques des données <span
class="math inline">\(X\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de décomposition de la variance totale, nous
commençons par rappeler que : <span
class="math display">\[\text{Var}(\hat{Y}) = \mathbb{E}[\hat{Y}^2] -
(\mathbb{E}[\hat{Y}])^2\]</span> En utilisant la loi de l’espérance
totale, nous pouvons écrire : <span
class="math display">\[\mathbb{E}[\hat{Y}^2] = \mathbb{E}_{X} \left[
\mathbb{E}[\hat{Y}^2 | X] \right]\]</span> et <span
class="math display">\[(\mathbb{E}[\hat{Y}])^2 = \left( \mathbb{E}_{X}
\left[ \mathbb{E}[\hat{Y} | X] \right] \right)^2\]</span> En substituant
ces expressions dans la formule de la variance, nous obtenons : <span
class="math display">\[\text{Var}(\hat{Y}) = \mathbb{E}_{X} \left[
\mathbb{E}[\hat{Y}^2 | X] \right] - \left( \mathbb{E}_{X} \left[
\mathbb{E}[\hat{Y} | X] \right] \right)^2\]</span> En utilisant
l’identité <span class="math inline">\(a^2 - b^2 = (a - b)(a +
b)\)</span>, nous pouvons réécrire : <span
class="math display">\[\text{Var}(\hat{Y}) = \mathbb{E}_{X} \left[
(\mathbb{E}[\hat{Y}^2 | X] - (\mathbb{E}[\hat{Y} | X])^2) +
((\mathbb{E}[\hat{Y} | X])^2 - (\mathbb{E}_{X} \left[ \mathbb{E}[\hat{Y}
| X] \right])^2) \right]\]</span> Ce qui donne finalement : <span
class="math display">\[\text{Var}(\hat{Y}) = \text{Var}_{\text{pred}}(f)
+ \mathbb{E}_{X} \left[ (\mathbb{E}[\hat{Y} | X] -
\mathbb{E}[\hat{Y}])^2 \right]\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<ol>
<li><p>La variance prédictive est toujours non négative, c’est-à-dire
<span class="math inline">\(\text{Var}_{\text{pred}}(f) \geq
0\)</span>.</p></li>
<li><p>Si le modèle <span class="math inline">\(f\)</span> est
déterministe, c’est-à-dire que <span class="math inline">\(\hat{Y} =
f(X)\)</span> ne dépend pas de la réalisation de <span
class="math inline">\(X\)</span>, alors la variance prédictive est
nulle.</p></li>
<li><p>La variance prédictive peut être utilisée pour comparer
différents modèles de prédiction. Un modèle avec une faible variance
prédictive est généralement préféré car il produit des prédictions plus
stables.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La variance prédictive est un outil puissant pour évaluer la
performance des modèles de prédiction. Elle permet de quantifier la
variabilité des prédictions et de distinguer entre les erreurs dues au
bruit intrinsèque des données et celles résultant d’une mauvaise
spécification du modèle. En comprenant et en utilisant la variance
prédictive, les chercheurs et les praticiens peuvent améliorer la
robustesse et la fiabilité de leurs modèles, conduisant à des décisions
plus éclairées et plus précises.</p>
</body>
</html>
{% include "footer.html" %}


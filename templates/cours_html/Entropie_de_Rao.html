{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Rao : Une Mesure d’Incertitude Géométrique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Rao : Une Mesure d’Incertitude
Géométrique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de Rao émerge comme une mesure sophistiquée d’incertitude
dans les espaces métriques, offrant une alternative géométrique aux
mesures classiques d’entropie. Son origine réside dans la quête de
quantifier l’incertitude non pas seulement par des probabilités, mais
aussi par les distances entre états. Cette notion devient indispensable
dans l’analyse des systèmes où la structure géométrique sous-jacente
joue un rôle crucial, comme en apprentissage automatique ou en théorie
de l’information géométrique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Rao, considérons un espace métrique
<span class="math inline">\((X, d)\)</span> où <span
class="math inline">\(d\)</span> représente la distance entre deux
points de l’ensemble <span class="math inline">\(X\)</span>. Nous
cherchons une mesure qui quantifie l’incertitude en tenant compte des
distances entre les points. L’idée est que plus les points sont
éloignés, plus l’incertitude est grande.</p>
<p>Formellement, soit <span class="math inline">\(P\)</span> une
distribution de probabilité sur <span class="math inline">\(X\)</span>.
L’entropie de Rao est définie comme suit :</p>
<div class="definition">
<p>L’entropie de Rao <span class="math inline">\(H_R(P)\)</span> d’une
distribution de probabilité <span class="math inline">\(P\)</span> sur
un espace métrique <span class="math inline">\((X, d)\)</span> est
donnée par :</p>
<p><span class="math display">\[H_R(P) = -\sum_{x \in X} P(x) \log
\left( \sum_{y \in X} P(y) d(x, y)^2 \right)\]</span></p>
<p>où <span class="math inline">\(d(x, y)\)</span> est la distance entre
les points <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span>.</p>
</div>
<p>Une autre formulation, plus générale, utilise des intégrales pour les
espaces continus :</p>
<div class="definition">
<p>Pour un espace métrique continu <span class="math inline">\((X,
d)\)</span> avec une distribution de probabilité <span
class="math inline">\(P\)</span> ayant une densité <span
class="math inline">\(p\)</span>, l’entropie de Rao s’écrit :</p>
<p><span class="math display">\[H_R(P) = -\int_X p(x) \log \left( \int_X
p(y) d(x, y)^2 \, dy \right) dx\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lie l’entropie de Rao à l’entropie de
Shannon, montrant ainsi son importance dans la théorie de
l’information.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
et <span class="math inline">\(P\)</span> une distribution de
probabilité sur <span class="math inline">\(X\)</span>. Alors,
l’entropie de Rao <span class="math inline">\(H_R(P)\)</span> et
l’entropie de Shannon <span class="math inline">\(H(S)\)</span> sont
liées par :</p>
<p><span class="math display">\[H_R(P) \leq H(S)\]</span></p>
<p>où <span class="math inline">\(H(S) = -\sum_{x \in X} P(x) \log
P(x)\)</span> pour le cas discret, et <span class="math inline">\(H(S) =
-\int_X p(x) \log p(x) \, dx\)</span> pour le cas continu.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous utilisons l’inégalité de
Jensen et les propriétés des logarithmes.</p>
<div class="preuve">
<p>Considérons d’abord le cas discret. Nous avons :</p>
<p><span class="math display">\[H_R(P) = -\sum_{x \in X} P(x) \log
\left( \sum_{y \in X} P(y) d(x, y)^2 \right)\]</span></p>
<p>En appliquant l’inégalité de Jensen à la fonction concave <span
class="math inline">\(-\log\)</span>, nous obtenons :</p>
<p><span class="math display">\[H_R(P) \leq -\sum_{x \in X} P(x) \log
\left( \sum_{y \in X} P(y) d(x, y)^2 \right)\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(\sum_{y \in X}
P(y) d(x, y)^2 \geq 1\)</span> (par l’inégalité de Cauchy-Schwarz), nous
avons :</p>
<p><span class="math display">\[H_R(P) \leq -\sum_{x \in X} P(x) \log 1
= 0\]</span></p>
<p>Cependant, cette approche est trop simpliste. Une preuve plus
rigoureuse nécessite des outils avancés de la théorie de l’information
géométrique.</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de l’entropie
de Rao.</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour toute distribution de
probabilité <span class="math inline">\(P\)</span> sur un espace
métrique <span class="math inline">\((X, d)\)</span>, nous avons <span
class="math inline">\(H_R(P) \geq 0\)</span>.</p>
<div class="preuve">
<p>La non-négativité découle directement de la définition et des
propriétés du logarithme.</p>
</div></li>
<li><p><strong>Invariance par isométrie</strong> : Si <span
class="math inline">\(T\)</span> est une isométrie de <span
class="math inline">\((X, d)\)</span>, alors pour toute distribution de
probabilité <span class="math inline">\(P\)</span>, nous avons <span
class="math inline">\(H_R(P) = H_R(T_\# P)\)</span>, où <span
class="math inline">\(T_\# P\)</span> est la mesure poussée en avant par
<span class="math inline">\(T\)</span>.</p>
<div class="preuve">
<p>Cela résulte du fait que les isométries préservent les distances,
donc <span class="math inline">\(d(x, y) = d(Tx, Ty)\)</span> pour tout
<span class="math inline">\(x, y \in X\)</span>.</p>
</div></li>
<li><p><strong>Lien avec la variance</strong> : Dans un espace euclidien
<span class="math inline">\(\mathbb{R}^n\)</span> avec la distance
euclidienne, l’entropie de Rao est liée à la variance de la
distribution.</p>
<div class="preuve">
<p>Pour une distribution gaussienne <span class="math inline">\(N(\mu,
\Sigma)\)</span>, l’entropie de Rao peut être exprimée en termes de la
matrice de covariance <span class="math inline">\(\Sigma\)</span>.</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de Rao offre une perspective géométrique riche pour la
quantification de l’incertitude. Ses propriétés et ses liens avec les
mesures classiques d’entropie en font un outil précieux dans de nombreux
domaines, notamment en apprentissage automatique et en théorie de
l’information. Les défis futurs incluent le développement d’algorithmes
efficaces pour son calcul et l’exploration de ses applications dans des
espaces métriques complexes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Bregman symétrisée</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Bregman symétrisée</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Bregman, introduite par L. M. Bregman en 1967, est
une mesure de dissimilarité non symétrique utilisée dans divers domaines
des mathématiques appliquées et de l’apprentissage automatique. Elle
trouve ses racines dans la théorie des fonctions convexes et a été
initialement développée pour résoudre des problèmes d’optimisation. La
symétrisation de cette divergence permet d’introduire une notion de
distance qui est plus adaptée à certaines applications, notamment dans
les algorithmes d’apprentissage et de traitement du signal.</p>
<p>La divergence de Bregman symétrisée émerge comme une réponse à la
nécessité d’une mesure de dissimilarité qui soit à la fois informative
et symétrique. Elle est indispensable dans les cadres où la symétrie est
cruciale, comme dans les problèmes de clustering ou de classification,
où une mesure non symétrique pourrait introduire des biais
indésirables.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la divergence de Bregman symétrisée, commençons par
rappeler la définition de la divergence de Bregman.</p>
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble
convexe et <span class="math inline">\(f : \mathcal{X} \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable
sur <span class="math inline">\(\mathcal{X}\)</span>. La divergence de
Bregman associée à <span class="math inline">\(f\)</span> est définie
pour tout <span class="math inline">\(x, y \in \mathcal{X}\)</span> par
:</p>
<p><span class="math display">\[D_f(x \| y) = f(x) - f(y) - \langle
\nabla f(y), x - y \rangle\]</span></p>
<p>où <span class="math inline">\(\nabla f(y)\)</span> est le gradient
de <span class="math inline">\(f\)</span> en <span
class="math inline">\(y\)</span>.</p>
<p>La divergence de Bregman symétrisée est alors définie comme la
moyenne des divergences de Bregman dans les deux sens :</p>
<p><span class="math display">\[\tilde{D}_f(x \| y) = \frac{1}{2} (D_f(x
\| y) + D_f(y \| x))\]</span></p>
<p>En d’autres termes, pour tout <span class="math inline">\(x, y \in
\mathcal{X}\)</span>, nous avons :</p>
<p><span class="math display">\[\tilde{D}_f(x \| y) = \frac{1}{2} \left(
f(x) - f(y) - \langle \nabla f(y), x - y \rangle + f(y) - f(x) - \langle
\nabla f(x), y - x \rangle \right)\]</span></p>
<p>Simplifiant, nous obtenons :</p>
<p><span class="math display">\[\tilde{D}_f(x \| y) = \frac{1}{2} \left(
2 \langle \nabla f(y) - \nabla f(x), x - y \rangle \right) = \langle
\nabla f(y) - \nabla f(x), x - y \rangle\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant la
divergence de Bregman symétrisée.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f : \mathcal{X} \rightarrow
\mathbb{R}\)</span> une fonction strictement convexe et différentiable
sur <span class="math inline">\(\mathcal{X}\)</span>. Alors, la
divergence de Bregman symétrisée <span
class="math inline">\(\tilde{D}_f\)</span> est convexe en son premier
argument.</p>
<p>Plus précisément, pour tout <span class="math inline">\(x, y, z \in
\mathcal{X}\)</span> et <span class="math inline">\(\lambda \in [0,
1]\)</span>, nous avons :</p>
<p><span class="math display">\[\tilde{D}_f(\lambda x + (1 - \lambda) z
\| y) \leq \lambda \tilde{D}_f(x \| y) + (1 - \lambda) \tilde{D}_f(z \|
y)\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous utilisons la convexité de
<span class="math inline">\(f\)</span> et les propriétés du
gradient.</p>
<div class="proof">
<p><em>Proof.</em> Par définition de la divergence de Bregman
symétrisée, nous avons :</p>
<p><span class="math display">\[\tilde{D}_f(\lambda x + (1 - \lambda) z
\| y) = \langle \nabla f(y) - \nabla f(\lambda x + (1 - \lambda) z),
\lambda x + (1 - \lambda) z - y \rangle\]</span></p>
<p>En utilisant la convexité de <span class="math inline">\(f\)</span>,
nous savons que pour tout <span class="math inline">\(x, z \in
\mathcal{X}\)</span> et <span class="math inline">\(\lambda \in [0,
1]\)</span>, le gradient vérifie :</p>
<p><span class="math display">\[\nabla f(\lambda x + (1 - \lambda) z) =
\lambda \nabla f(x) + (1 - \lambda) \nabla f(z)\]</span></p>
<p>En substituant cette égalité dans l’expression précédente, nous
obtenons :</p>
<p><span class="math display">\[\tilde{D}_f(\lambda x + (1 - \lambda) z
\| y) = \langle \nabla f(y) - (\lambda \nabla f(x) + (1 - \lambda)
\nabla f(z)), \lambda x + (1 - \lambda) z - y \rangle\]</span></p>
<p>En développant le produit scalaire, nous avons :</p>
<p><span class="math display">\[\tilde{D}_f(\lambda x + (1 - \lambda) z
\| y) = \langle \nabla f(y), \lambda x + (1 - \lambda) z - y \rangle -
\lambda \langle \nabla f(x), \lambda x + (1 - \lambda) z - y \rangle -
(1 - \lambda) \langle \nabla f(z), \lambda x + (1 - \lambda) z - y
\rangle\]</span></p>
<p>En utilisant à nouveau la convexité de <span
class="math inline">\(f\)</span>, nous pouvons réécrire chaque terme
:</p>
<p><span class="math display">\[\langle \nabla f(y), \lambda x + (1 -
\lambda) z - y \rangle = \lambda \langle \nabla f(y), x - y \rangle + (1
- \lambda) \langle \nabla f(y), z - y \rangle\]</span></p>
<p><span class="math display">\[-\lambda \langle \nabla f(x), \lambda x
+ (1 - \lambda) z - y \rangle = -\lambda^2 \langle \nabla f(x), x - y
\rangle - \lambda (1 - \lambda) \langle \nabla f(x), z - y
\rangle\]</span></p>
<p><span class="math display">\[-(1 - \lambda) \langle \nabla f(z),
\lambda x + (1 - \lambda) z - y \rangle = -\lambda (1 - \lambda) \langle
\nabla f(z), x - y \rangle - (1 - \lambda)^2 \langle \nabla f(z), z - y
\rangle\]</span></p>
<p>En combinant ces termes, nous obtenons :</p>
<p><span class="math display">\[\tilde{D}_f(\lambda x + (1 - \lambda) z
\| y) = \lambda^2 \langle \nabla f(y), x - y \rangle + (1 - \lambda)^2
\langle \nabla f(y), z - y \rangle + \lambda (1 - \lambda) \langle
\nabla f(y), x - z \rangle\]</span></p>
<p>En utilisant les propriétés de la divergence de Bregman, nous pouvons
montrer que :</p>
<p><span class="math display">\[\lambda^2 \langle \nabla f(y), x - y
\rangle + (1 - \lambda)^2 \langle \nabla f(y), z - y \rangle + \lambda
(1 - \lambda) \langle \nabla f(y), x - z \rangle \leq \lambda
\tilde{D}_f(x \| y) + (1 - \lambda) \tilde{D}_f(z \| y)\]</span></p>
<p>Ce qui achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
divergence de Bregman symétrisée.</p>
<ul>
<li><p><strong>Non négativité</strong> : Pour tout <span
class="math inline">\(x, y \in \mathcal{X}\)</span>, nous avons <span
class="math inline">\(\tilde{D}_f(x \| y) \geq 0\)</span>. Cela découle
du fait que <span class="math inline">\(f\)</span> est strictement
convexe.</p></li>
<li><p><strong>Symétrie</strong> : La divergence de Bregman symétrisée
est symétrique, c’est-à-dire que <span
class="math inline">\(\tilde{D}_f(x \| y) = \tilde{D}_f(y \|
x)\)</span>.</p></li>
<li><p><strong>Identité</strong> : Pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span>, nous avons <span
class="math inline">\(\tilde{D}_f(x \| x) = 0\)</span>.</p></li>
<li><p><strong>Convexité</strong> : Comme démontré dans le théorème
précédent, la divergence de Bregman symétrisée est convexe en son
premier argument.</p></li>
</ul>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Bregman symétrisée est une mesure de dissimilarité
puissante et flexible, qui trouve des applications dans divers domaines
des mathématiques appliquées et de l’apprentissage automatique. Sa
symétrie en fait un outil précieux pour les problèmes où la non-symétrie
de la divergence de Bregman classique pourrait introduire des biais
indésirables. Les propriétés et théorèmes présentés dans cet article
soulignent l’importance et la richesse de cette notion.</p>
</body>
</html>
{% include "footer.html" %}


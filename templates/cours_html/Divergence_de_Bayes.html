{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Bayes : Fondements Théoriques et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Bayes : Fondements Théoriques et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<p>Voici un article scientifique complet en LaTeX sur la divergence de
Bayes :</p>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La théorie de l’information, née au milieu du XXe siècle des travaux
pionniers de Claude Shannon et Norbert Wiener, a profondément transformé
notre compréhension des systèmes complexes. Parmi les concepts clés de
cette théorie, la divergence de Kullback-Leibler occupe une place
centrale. Introduite par Solomon Kullback et Richard Leibler en 1951,
cette mesure quantifie la distance entre deux distributions de
probabilité.</p>
<p>La divergence de Bayes, généralisation naturelle de ce concept,
émerge comme un outil fondamental dans l’analyse des modèles
statistiques bayésiens. Elle trouve ses racines dans le besoin de
comparer des distributions a posteriori, permettant ainsi d’évaluer
l’impact des choix de modèles ou de données sur les inférences
statistiques. Son importance réside dans sa capacité à mesurer la
divergence entre deux distributions conditionnelles, offrant ainsi un
cadre rigoureux pour l’analyse de sensibilité et la validation des
modèles bayésiens.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Considérons deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous cherchons à
quantifier la distance entre ces deux distributions. Intuitivement,
cette distance devrait capturer l’information perdue lorsqu’on utilise
<span class="math inline">\(Q\)</span> pour approximer <span
class="math inline">\(P\)</span>.</p>
<div class="definition">
<p>La divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, notée <span
class="math inline">\(D_{KL}(P||Q)\)</span>, est définie comme : <span
class="math display">\[D_{KL}(P||Q) = \int_{\Omega} p(\omega)
\log\left(\frac{p(\omega)}{q(\omega)}\right) d\mu(\omega)\]</span> où
<span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement, par rapport à une
mesure <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>Dans le cadre bayésien, nous considérons des distributions
conditionnelles. Soit <span class="math inline">\(\theta\)</span> un
paramètre inconnu et <span class="math inline">\(X\)</span> une variable
aléatoire observée. Nous notons : - <span
class="math inline">\(\pi(\theta)\)</span> la distribution a priori de
<span class="math inline">\(\theta\)</span>, - <span
class="math inline">\(f(x|\theta)\)</span> la vraisemblance de <span
class="math inline">\(X\)</span> conditionnellement à <span
class="math inline">\(\theta\)</span>, - <span
class="math inline">\(\pi(\theta|x)\)</span> la distribution a
posteriori de <span class="math inline">\(\theta\)</span>
conditionnellement à <span class="math inline">\(X=x\)</span>.</p>
<div class="definition">
<p>La divergence de Bayes entre deux distributions a posteriori <span
class="math inline">\(\pi(\theta|x_1)\)</span> et <span
class="math inline">\(\pi(\theta|x_2)\)</span>, notée <span
class="math inline">\(D_{Bayes}(x_1||x_2)\)</span>, est définie comme :
<span class="math display">\[D_{Bayes}(x_1||x_2) = \int_{\Theta}
\pi(\theta|x_1) \log\left(\frac{\pi(\theta|x_1)}{\pi(\theta|x_2)}\right)
d\theta\]</span> où <span class="math inline">\(\Theta\)</span> est
l’espace des paramètres.</p>
</div>
<p>Cette définition peut être réécrite en utilisant la règle de Bayes :
<span class="math display">\[D_{Bayes}(x_1||x_2) = \int_{\Theta}
\pi(\theta|x_1)
\log\left(\frac{f(x_1|\theta)\pi(\theta)}{f(x_2|\theta)\pi(\theta)}\right)
d\theta\]</span> <span class="math display">\[= \int_{\Theta}
\pi(\theta|x_1) \log\left(\frac{f(x_1|\theta)}{f(x_2|\theta)}\right)
d\theta\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<div class="theorem">
<p>Pour toute paire de distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, la divergence de Kullback-Leibler
satisfait : <span class="math display">\[D_{KL}(P||Q) \geq 0\]</span>
avec égalité si et seulement si <span class="math inline">\(P =
Q\)</span> presque partout.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par la convexité de la fonction <span
class="math inline">\(-\log(x)\)</span>, nous pouvons utiliser
l’inégalité de Jensen : <span
class="math display">\[-\log\left(\int_{\Omega} p(\omega)
\frac{q(\omega)}{p(\omega)} d\mu(\omega)\right) \leq -\int_{\Omega}
p(\omega) \log\left(\frac{q(\omega)}{p(\omega)}\right)
d\mu(\omega)\]</span> <span
class="math display">\[\log\left(\int_{\Omega} q(\omega)
d\mu(\omega)\right) \leq -\int_{\Omega} p(\omega)
\log\left(\frac{q(\omega)}{p(\omega)}\right) d\mu(\omega)\]</span>
Puisque <span class="math inline">\(\int_{\Omega} q(\omega) d\mu(\omega)
= 1\)</span>, nous obtenons : <span class="math display">\[D_{KL}(P||Q)
\geq 0\]</span> L’égalité est atteinte si et seulement si <span
class="math inline">\(p(\omega)/q(\omega)\)</span> est constante presque
partout, c’est-à-dire lorsque <span class="math inline">\(P = Q\)</span>
presque partout. ◻</p>
</div>
<div class="theorem">
<p>La divergence de Bayes peut être exprimée en termes de la
vraisemblance : <span class="math display">\[D_{Bayes}(x_1||x_2) =
\log\left(\frac{p(x_1)}{p(x_2)}\right) -
\mathbb{E}_{\pi(\theta|x_1)}[\log(f(x_2|\theta))] +
\mathbb{E}_{\pi(\theta|x_1)}[\log(f(x_1|\theta))]\]</span> où <span
class="math inline">\(p(x) = \int_{\Theta}
f(x|\theta)\pi(\theta)d\theta\)</span> est la distribution prédictive de
<span class="math inline">\(X\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous partons de la définition : <span
class="math display">\[D_{Bayes}(x_1||x_2) = \int_{\Theta}
\pi(\theta|x_1)
\log\left(\frac{f(x_1|\theta)\pi(\theta)}{f(x_2|\theta)\pi(\theta)}\right)
d\theta\]</span> <span class="math display">\[= \int_{\Theta}
\pi(\theta|x_1) \log\left(\frac{f(x_1|\theta)}{f(x_2|\theta)}\right)
d\theta\]</span> En utilisant la règle de Bayes, nous pouvons exprimer
<span class="math inline">\(\pi(\theta|x_1)\)</span> en termes de <span
class="math inline">\(f(x_1|\theta)\pi(\theta)/p(x_1)\)</span>. Ainsi :
<span class="math display">\[D_{Bayes}(x_1||x_2) = \int_{\Theta}
\frac{f(x_1|\theta)\pi(\theta)}{p(x_1)}
\log\left(\frac{f(x_1|\theta)}{f(x_2|\theta)}\right) d\theta\]</span>
<span class="math display">\[= \frac{1}{p(x_1)} \int_{\Theta}
f(x_1|\theta)\pi(\theta) \log(f(x_1|\theta)) d\theta - \frac{1}{p(x_1)}
\int_{\Theta} f(x_1|\theta)\pi(\theta) \log(f(x_2|\theta))
d\theta\]</span> <span class="math display">\[= \frac{1}{p(x_1)}
\int_{\Theta} f(x_1|\theta)\pi(\theta) \log(f(x_1|\theta)) d\theta -
\frac{1}{p(x_2)} p(x_2)
\mathbb{E}_{\pi(\theta|x_1)}[\log(f(x_2|\theta))]\]</span> <span
class="math display">\[= \frac{1}{p(x_1)} \int_{\Theta}
f(x_1|\theta)\pi(\theta) \log(f(x_1|\theta)) d\theta -
\mathbb{E}_{\pi(\theta|x_1)}[\log(f(x_2|\theta))]\]</span> Nous
reconnaissons que <span class="math inline">\(\int_{\Theta}
f(x_1|\theta)\pi(\theta) \log(f(x_1|\theta)) d\theta = p(x_1)
\mathbb{E}_{\pi(\theta|x_1)}[\log(f(x_1|\theta))]\)</span>, ce qui donne
le résultat souhaité. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<ol>
<li><p>La divergence de Bayes est invariante sous les transformations
des paramètres. C’est-à-dire, pour toute fonction bijective <span
class="math inline">\(\phi: \Theta \rightarrow \Theta&#39;\)</span> :
<span class="math display">\[D_{Bayes}(x_1||x_2) =
D_{Bayes}(\phi(x_1)||\phi(x_2))\]</span></p></li>
<li><p>La divergence de Bayes satisfait l’inégalité triangulaire : <span
class="math display">\[D_{Bayes}(x_1||x_3) \leq D_{Bayes}(x_1||x_2) +
D_{Bayes}(x_2||x_3)\]</span> Cette propriété est une conséquence de la
convexité de la divergence de Kullback-Leibler.</p></li>
<li><p>Pour des modèles exponentiels, la divergence de Bayes peut être
exprimée en termes des paramètres du modèle. Supposons que <span
class="math inline">\(f(x|\theta) = h(x) \exp(\eta(\theta) \cdot T(x) -
A(\theta))\)</span>, alors : <span
class="math display">\[D_{Bayes}(x_1||x_2) =
\mathbb{E}_{\pi(\theta|x_1)}[A(\theta)] - A(\hat{\theta}(x_1)) +
\log\left(\frac{p(x_1)}{p(x_2)}\right) - (\eta(\hat{\theta}(x_1)) \cdot
T(x_2) - A(\hat{\theta}(x_1)))\]</span> où <span
class="math inline">\(\hat{\theta}(x)\)</span> est le maximum de
vraisemblance.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Bayes s’impose comme un outil indispensable dans
l’arsenal des méthodes bayésiennes. Son utilisation permet de quantifier
l’impact des choix de modèles et de données sur les inférences
statistiques, offrant ainsi un cadre rigoureux pour l’analyse de
sensibilité et la validation des modèles. Les propriétés mathématiques
profondes de cette divergence, telles que sa non-négativité et son
invariance sous les transformations des paramètres, en font un outil
puissant pour l’analyse des systèmes complexes.</p>
<p>Les développements futurs pourraient explorer les applications de la
divergence de Bayes dans des contextes plus larges, tels que
l’apprentissage automatique et les réseaux neuronaux. En particulier,
son utilisation pour la comparaison de modèles hiérarchiques ou
bayésiens complexes pourrait ouvrir de nouvelles perspectives dans
l’analyse des données à grande échelle.</p>
</body>
</html>
{% include "footer.html" %}


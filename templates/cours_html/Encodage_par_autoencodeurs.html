{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage par autoencodeurs : une approche moderne de la réduction de dimensionnalité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage par autoencodeurs : une approche moderne de
la réduction de dimensionnalité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par autoencodeurs, ou <em>autoencoding</em>, est une
technique de réduction de dimensionnalité qui a émergé dans le domaine
de l’apprentissage automatique, notamment avec l’essor des réseaux de
neurones profonds. Cette méthode permet de transformer un ensemble de
données de haute dimension en une représentation de plus faible
dimension tout en préservant les caractéristiques essentielles des
données. L’idée fondamentale derrière cette approche est de construire
un modèle capable d’apprendre une représentation compacte des données en
minimisant la différence entre les entrées et les sorties
reconstruites.</p>
<p>L’autoencodeur est composé de deux parties principales : un encodeur
qui transforme les données d’entrée en une représentation codée de
dimension réduite, et un décodeur qui reconstruit les données originales
à partir de cette représentation codée. Cette structure permet non
seulement de réduire la dimensionnalité des données, mais aussi
d’extraire des caractéristiques pertinentes pour des tâches ultérieures
telles que la classification ou la régularisation.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de définir formellement un autoencodeur, il est essentiel de
comprendre ce que nous cherchons à accomplir. Supposons que nous
disposions d’un ensemble de données <span
class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots, x_n\}\)</span> où
chaque <span class="math inline">\(x_i\)</span> est un vecteur de
dimension <span class="math inline">\(d\)</span>. Notre objectif est de
trouver une représentation <span class="math inline">\(\mathcal{Z} =
\{z_1, z_2, \dots, z_n\}\)</span> de dimension <span
class="math inline">\(k &lt; d\)</span> telle que les données originales
puissent être reconstruites avec une erreur minimale à partir de cette
représentation.</p>
<div class="definition">
<p>Un autoencodeur est une fonction <span class="math inline">\(f :
\mathbb{R}^d \to \mathbb{R}^k\)</span> suivie d’une fonction <span
class="math inline">\(g : \mathbb{R}^k \to \mathbb{R}^d\)</span>, où
<span class="math inline">\(f\)</span> est l’encodeur et <span
class="math inline">\(g\)</span> est le décodeur. L’autoencodeur est
défini par la composition de ces deux fonctions : <span
class="math display">\[f \circ g : \mathbb{R}^d \to \mathbb{R}^k \to
\mathbb{R}^d\]</span> L’objectif est de minimiser la fonction de coût
<span class="math inline">\(L(x, g(f(x)))\)</span> qui mesure l’erreur
entre les données originales <span class="math inline">\(x\)</span> et
les données reconstruites <span
class="math inline">\(g(f(x))\)</span>.</p>
</div>
<p>Pour formaliser cette définition, nous introduisons les notations
suivantes : - <span class="math inline">\(f(x) = s(Wx + b)\)</span> où
<span class="math inline">\(W\)</span> est une matrice de poids, <span
class="math inline">\(b\)</span> un vecteur de biais et <span
class="math inline">\(s\)</span> une fonction d’activation. - <span
class="math inline">\(g(z) = s&#39;(W&#39;z + b&#39;)\)</span> où <span
class="math inline">\(W&#39;\)</span> est une matrice de poids, <span
class="math inline">\(b&#39;\)</span> un vecteur de biais et <span
class="math inline">\(s&#39;\)</span> une fonction d’activation.</p>
<p>Ainsi, l’autoencodeur peut être écrit comme : <span
class="math display">\[g(f(x)) = s&#39;(W&#39;(s(Wx + b)) +
b&#39;)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre les propriétés des autoencodeurs, nous introduisons
un théorème fondamental qui garantit que l’autoencodeur peut apprendre
une représentation compacte des données.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble de données et <span class="math inline">\(f :
\mathbb{R}^d \to \mathbb{R}^k\)</span> un encodeur. Si la fonction de
coût <span class="math inline">\(L(x, g(f(x)))\)</span> est convexe et
si les fonctions d’activation <span class="math inline">\(s\)</span> et
<span class="math inline">\(s&#39;\)</span> sont différentiables, alors
il existe un encodeur optimal <span class="math inline">\(f^*\)</span>
qui minimise la fonction de coût.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur l’application du théorème de la
valeur intermédiaire et des propriétés des fonctions convexes. Nous
savons que pour toute fonction convexe <span
class="math inline">\(L\)</span>, il existe un point critique qui
minimise la fonction. En utilisant les conditions d’optimalité de
premier ordre, nous pouvons montrer que l’encodeur optimal <span
class="math inline">\(f^*\)</span> satisfait les équations suivantes :
<span class="math display">\[\frac{\partial L}{\partial W} = 0, \quad
\frac{\partial L}{\partial b} = 0\]</span> où <span
class="math inline">\(\frac{\partial L}{\partial W}\)</span> et <span
class="math inline">\(\frac{\partial L}{\partial b}\)</span> sont les
dérivées partielles de la fonction de coût par rapport aux poids et aux
biais. En résolvant ce système d’équations, nous obtenons l’encodeur
optimal <span class="math inline">\(f^*\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer les preuves détaillées, nous considérons un exemple
simple où la fonction de coût est l’erreur quadratique moyenne
(MSE).</p>
<div class="theorem">
<p>Soit <span class="math inline">\(L(x, g(f(x))) = \frac{1}{n}
\sum_{i=1}^n \|x_i - g(f(x_i))\|^2\)</span> la fonction de coût MSE. Si
les fonctions d’activation <span class="math inline">\(s\)</span> et
<span class="math inline">\(s&#39;\)</span> sont l’identité, alors
l’autoencodeur peut être écrit comme : <span
class="math display">\[g(f(x)) = W&#39;(Wx + b) + b&#39;\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous commençons par calculer les dérivées partielles
de la fonction de coût par rapport aux poids et aux biais. La dérivée
partielle de <span class="math inline">\(L\)</span> par rapport à <span
class="math inline">\(W\)</span> est : <span
class="math display">\[\frac{\partial L}{\partial W} = \frac{2}{n}
\sum_{i=1}^n (x_i - W&#39;(Wx_i + b)) x_i^T\]</span> En résolvant <span
class="math inline">\(\frac{\partial L}{\partial W} = 0\)</span>, nous
obtenons : <span class="math display">\[W&#39;Wx_i + W&#39;b = x_i \quad
\forall i\]</span> Ce qui implique que <span
class="math inline">\(W&#39;W = I\)</span> et <span
class="math inline">\(W&#39;b = 0\)</span>. De même, la dérivée
partielle de <span class="math inline">\(L\)</span> par rapport à <span
class="math inline">\(b\)</span> est : <span
class="math display">\[\frac{\partial L}{\partial b} = \frac{2}{n}
\sum_{i=1}^n (x_i - W&#39;(Wx_i + b))\]</span> En résolvant <span
class="math inline">\(\frac{\partial L}{\partial b} = 0\)</span>, nous
obtenons : <span class="math display">\[W&#39;b = 0 \quad \text{et}
\quad W&#39;Wx_i = x_i\]</span> Ainsi, l’autoencodeur optimal est donné
par <span class="math inline">\(W&#39;W = I\)</span> et <span
class="math inline">\(W&#39;b = 0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes des
autoencodeurs.</p>
<ol>
<li><p><strong>Propriété de réduction de dimensionnalité</strong> :
L’autoencodeur permet de réduire la dimensionnalité des données en
apprenant une représentation compacte. Cette propriété est
particulièrement utile pour les tâches de visualisation et de
compression des données.</p></li>
<li><p><strong>Propriété de reconstruction</strong> : L’autoencodeur
peut reconstruire les données originales à partir de leur représentation
codée avec une erreur minimale. Cette propriété est cruciale pour les
applications où la qualité de la reconstruction est importante.</p></li>
<li><p><strong>Propriété d’extraction de caractéristiques</strong> :
L’autoencodeur peut extraire des caractéristiques pertinentes des
données, ce qui est utile pour les tâches de classification et de
régularisation.</p></li>
</ol>
<p>Pour illustrer ces propriétés, nous considérons un exemple simple où
les données sont des images en niveaux de gris.</p>
<div class="example">
<p>Soit <span class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble d’images en niveaux de gris de dimension
<span class="math inline">\(d = 28 \times 28\)</span>. Un autoencodeur
peut être utilisé pour réduire la dimensionnalité des images à <span
class="math inline">\(k = 10\)</span> tout en préservant les
caractéristiques essentielles. La représentation codée peut ensuite être
utilisée pour des tâches de classification ou de visualisation.</p>
</div>
<p>En conclusion, l’encodage par autoencodeurs est une technique
puissante pour la réduction de dimensionnalité et l’extraction de
caractéristiques. Les propriétés et les théorèmes présentés dans cet
article montrent que cette approche peut être utilisée efficacement pour
diverses applications en apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


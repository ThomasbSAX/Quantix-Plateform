{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Corrélation de distance : Une exploration mathématique et historique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Corrélation de distance : Une exploration mathématique
et historique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La corrélation de distance est un concept fondamental en statistique
et en analyse des données, émergé pour répondre à la nécessité de
quantifier les relations entre variables dans un espace métrique.
Historiquement, cette notion prend racine dans les travaux pionniers de
Karl Pearson au XIXe siècle, qui introduisit la notion de coefficient de
corrélation pour mesurer le degré de linéarité entre deux variables.
Cependant, la généralisation à des espaces de distance arbitraires a
ouvert des perspectives nouvelles dans l’analyse multivariée et la
classification.</p>
<p>La corrélation de distance est indispensable dans des domaines
variés, allant de la bioinformatique à l’analyse de réseaux sociaux.
Elle permet de capturer des structures complexes dans les données,
au-delà des simples relations linéaires. En particulier, elle est
cruciale pour comprendre les motifs sous-jacents dans des ensembles de
données de haute dimension, où les méthodes traditionnelles peuvent
échouer.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la corrélation de distance, considérons un ensemble
de points dans un espace métrique. Nous cherchons à mesurer à quel point
la distance entre ces points est corrélée avec une autre propriété ou
variable. Par exemple, dans un graphe, nous pourrions vouloir savoir si
la distance entre les nœuds est corrélée avec leur degré.</p>
<p>Formellement, soit <span class="math inline">\((X, d)\)</span> un
espace métrique et <span class="math inline">\(f: X \rightarrow
\mathbb{R}\)</span> une fonction. La corrélation de distance entre <span
class="math inline">\(d\)</span> et <span
class="math inline">\(f\)</span> peut être définie comme suit :</p>
<div class="definition">
<p>La corrélation de distance entre une fonction <span
class="math inline">\(f: X \rightarrow \mathbb{R}\)</span> et une
distance <span class="math inline">\(d\)</span> sur un espace métrique
<span class="math inline">\((X, d)\)</span> est donnée par : <span
class="math display">\[C(d, f) = \frac{\text{Cov}(d(x_i, x_j), f(x_i) -
f(x_j))}{\sqrt{\text{Var}(d(x_i, x_j)) \cdot \text{Var}(f(x_i) -
f(x_j))}}\]</span> où <span class="math inline">\(\text{Cov}\)</span>
désigne la covariance et <span class="math inline">\(\text{Var}\)</span>
la variance, calculées sur tous les couples <span
class="math inline">\((x_i, x_j) \in X^2\)</span>.</p>
</div>
<p>Une autre formulation, plus générale, utilise des intégrales au lieu
de sommes discrètes :</p>
<div class="definition">
<p>Pour une mesure de probabilité <span
class="math inline">\(\mu\)</span> sur <span
class="math inline">\(X\)</span>, la corrélation de distance peut être
définie par : <span class="math display">\[C(d, f) = \frac{\int_{X^2}
(d(x, y) - \mathbb{E}[d])(f(x) - f(y)) \, d\mu(x) \,
d\mu(y)}{\sqrt{\text{Var}_\mu(d) \cdot \text{Var}_\mu(f)}}\]</span> où
<span class="math inline">\(\mathbb{E}[d]\)</span> désigne l’espérance
de la distance <span class="math inline">\(d\)</span> sous <span
class="math inline">\(\mu\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans ce contexte est celui de la
décomposition de la variance, qui relie la corrélation de distance à la
structure intrinsèque des données.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
et <span class="math inline">\(f: X \rightarrow \mathbb{R}\)</span> une
fonction. Alors, la variance de <span class="math inline">\(f\)</span>
peut être décomposée en : <span class="math display">\[\text{Var}(f) =
\text{Var}_\text{intra}(f) + \text{Var}_\text{inter}(f)\]</span> où
<span class="math inline">\(\text{Var}_\text{intra}(f)\)</span> est la
variance intra-clusters et <span
class="math inline">\(\text{Var}_\text{inter}(f)\)</span> est la
variance inter-clusters, définies en fonction de la corrélation de
distance <span class="math inline">\(C(d, f)\)</span>.</p>
</div>
<p>La preuve de ce théorème repose sur des techniques d’analyse
fonctionnelle et de théorie de la mesure. Elle montre comment la
corrélation de distance peut être utilisée pour décomposer la variance
totale d’une fonction en composantes liées à la structure métrique de
l’espace.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la décomposition de la variance, nous
commençons par définir les composantes intra et inter-clusters. Soit
<span class="math inline">\(C\)</span> une partition de <span
class="math inline">\(X\)</span> en clusters. Nous définissons :</p>
<p><span class="math display">\[\text{Var}_\text{intra}(f) = \sum_{c \in
C} \mu(c) \text{Var}_c(f)\]</span> <span
class="math display">\[\text{Var}_\text{inter}(f) =
\text{Var}(\mathbb{E}[f | C])\]</span></p>
<p>où <span class="math inline">\(\mu(c)\)</span> est la mesure du
cluster <span class="math inline">\(c\)</span> et <span
class="math inline">\(\text{Var}_c(f)\)</span> est la variance de <span
class="math inline">\(f\)</span> restreinte à <span
class="math inline">\(c\)</span>.</p>
<p>Ensuite, nous utilisons le fait que la variance totale peut être
exprimée comme :</p>
<p><span class="math display">\[\text{Var}(f) = \mathbb{E}[\text{Var}(f
| C)] + \text{Var}(\mathbb{E}[f | C])\]</span></p>
<p>En identifiant <span
class="math inline">\(\text{Var}_\text{intra}(f)\)</span> avec <span
class="math inline">\(\mathbb{E}[\text{Var}(f | C)]\)</span> et <span
class="math inline">\(\text{Var}_\text{inter}(f)\)</span> avec <span
class="math inline">\(\text{Var}(\mathbb{E}[f | C])\)</span>, nous
obtenons la décomposition souhaitée.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés importantes découlent de la définition de la
corrélation de distance :</p>
<ol>
<li><p>La corrélation de distance est invariante par translation et
scaling de la fonction <span class="math inline">\(f\)</span>. Cela
signifie que si nous ajoutons une constante à <span
class="math inline">\(f\)</span> ou multiplions <span
class="math inline">\(f\)</span> par un facteur non nul, la corrélation
de distance reste inchangée.</p></li>
<li><p>La corrélation de distance est symétrique en <span
class="math inline">\(d\)</span> et <span
class="math inline">\(f\)</span>. Cela signifie que <span
class="math inline">\(C(d, f) = C(f, d)\)</span>.</p></li>
<li><p>La corrélation de distance est bornée entre <span
class="math inline">\(-1\)</span> et <span
class="math inline">\(1\)</span>. Cela signifie que : <span
class="math display">\[-1 \leq C(d, f) \leq 1\]</span></p></li>
</ol>
<p>La preuve de ces propriétés repose sur des manipulations algébriques
simples et des propriétés fondamentales de la covariance et de la
variance.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La corrélation de distance est un outil puissant pour analyser les
relations entre des variables dans un espace métrique. Ses applications
sont vastes et variées, allant de la classification des données à
l’analyse de réseaux. Les théorèmes et propriétés présentés dans cet
article montrent comment cette notion peut être utilisée pour décomposer
la variance et capturer des structures complexes dans les données.</p>
</body>
</html>
{% include "footer.html" %}


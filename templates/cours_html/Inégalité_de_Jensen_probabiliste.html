{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Inégalité de Jensen probabiliste : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inégalité de Jensen probabiliste : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Jensen, du nom du mathématicien danois Johan Jensen,
est un résultat fondamental en théorie des probabilités et en analyse
convexe. Son énoncé simple cache une puissance remarquable, permettant
de relier des valeurs attendues à des fonctions convexes.
Historiquement, cette inégalité émerge dans le cadre de l’étude des
fonctions convexes et des mesures de probabilité. Son importance réside
dans sa capacité à fournir des bornes sur les valeurs attendues de
fonctions convexes, ce qui est indispensable dans de nombreux domaines
comme la finance, l’apprentissage automatique, et les sciences des
données.</p>
<p>L’inégalité de Jensen est indispensable lorsqu’il s’agit d’estimer ou
de borner des quantités complexes. Par exemple, dans le contexte de la
finance, elle permet d’évaluer les risques en fournissant des bornes sur
les pertes attendues. De même, en apprentissage automatique, elle est
utilisée pour analyser la performance des algorithmes et garantir leur
convergence.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégalité de Jensen, il est essentiel de rappeler
quelques définitions clés.</p>
<h2 id="convexité">Convexité</h2>
<p>Considérons une fonction <span class="math inline">\(f: \mathbb{R}^n
\rightarrow \mathbb{R}\)</span>. Intuitivement, une fonction convexe est
une fonction "courbée vers le haut". Pour formaliser cette idée, nous
disons que <span class="math inline">\(f\)</span> est convexe si pour
tout <span class="math inline">\(x, y \in \mathbb{R}^n\)</span> et pour
tout <span class="math inline">\(\lambda \in [0, 1]\)</span>, on a :</p>
<p><span class="math display">\[f(\lambda x + (1 - \lambda) y) \leq
\lambda f(x) + (1 - \lambda) f(y).\]</span></p>
<p>En d’autres termes, la valeur de la fonction en un point
intermédiaire est inférieure ou égale à la combinaison convexe des
valeurs aux points extrêmes.</p>
<h2 id="fonction-convexe">Fonction Convexe</h2>
<p>Formellement, une fonction <span class="math inline">\(f\)</span> est
convexe si et seulement si :</p>
<p><span class="math display">\[\forall x, y \in \mathbb{R}^n, \forall
\lambda \in [0, 1], \quad f(\lambda x + (1 - \lambda) y) \leq \lambda
f(x) + (1 - \lambda) f(y).\]</span></p>
<h2 id="inégalité-de-jensen">Inégalité de Jensen</h2>
<p>Maintenant, nous sommes prêts à énoncer l’inégalité de Jensen.
Supposons que <span class="math inline">\(X\)</span> est une variable
aléatoire prenant ses valeurs dans <span
class="math inline">\(\mathbb{R}^n\)</span>, et que <span
class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
est une fonction convexe. L’inégalité de Jensen affirme que :</p>
<p><span class="math display">\[f(\mathbb{E}[X]) \leq
\mathbb{E}[f(X)],\]</span></p>
<p>à condition que l’espérance <span
class="math inline">\(\mathbb{E}[f(X)]\)</span> soit définie.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="inégalité-de-jensen-1">Inégalité de Jensen</h2>
<p>Pour énoncer l’inégalité de Jensen, nous avons besoin des définitions
précédentes. Supposons que <span class="math inline">\(X\)</span> est
une variable aléatoire et que <span class="math inline">\(f\)</span> est
une fonction convexe. L’inégalité de Jensen s’énonce comme suit :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire à
valeurs dans un espace mesurable <span class="math inline">\((E,
\mathcal{E})\)</span>, et soit <span class="math inline">\(f: E
\rightarrow \mathbb{R}\)</span> une fonction convexe. Si <span
class="math inline">\(\mathbb{E}[f(X)]\)</span> est définie, alors :</p>
<p><span class="math display">\[f(\mathbb{E}[X]) \leq
\mathbb{E}[f(X)].\]</span></p>
</div>
<h2 id="démonstration-de-linégalité-de-jensen">Démonstration de
l’Inégalité de Jensen</h2>
<p>Pour démontrer l’inégalité de Jensen, nous allons utiliser la
définition de la convexité et les propriétés de l’espérance.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(X\)</span> une
variable aléatoire et <span class="math inline">\(f\)</span> une
fonction convexe. Par définition de la convexité, pour tout <span
class="math inline">\(x, y \in E\)</span> et pour tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, on a :</p>
<p><span class="math display">\[f(\lambda x + (1 - \lambda) y) \leq
\lambda f(x) + (1 - \lambda) f(y).\]</span></p>
<p>En prenant <span class="math inline">\(x = \mathbb{E}[X]\)</span> et
<span class="math inline">\(y = X\)</span>, et en choisissant <span
class="math inline">\(\lambda = 1/n\)</span> pour un entier <span
class="math inline">\(n \geq 1\)</span>, nous obtenons :</p>
<p><span class="math display">\[f\left( \frac{1}{n} \mathbb{E}[X] +
\left(1 - \frac{1}{n}\right) X \right) \leq \frac{1}{n} f(\mathbb{E}[X])
+ \left(1 - \frac{1}{n}\right) f(X).\]</span></p>
<p>En prenant l’espérance des deux côtés, nous avons :</p>
<p><span class="math display">\[\mathbb{E}\left[ f\left( \frac{1}{n}
\mathbb{E}[X] + \left(1 - \frac{1}{n}\right) X \right) \right] \leq
\frac{1}{n} f(\mathbb{E}[X]) + \left(1 - \frac{1}{n}\right)
\mathbb{E}[f(X)].\]</span></p>
<p>En faisant tendre <span class="math inline">\(n\)</span> vers
l’infini, nous obtenons :</p>
<p><span class="math display">\[f(\mathbb{E}[X]) \leq
\mathbb{E}[f(X)].\]</span></p>
<p>Ceci conclut la démonstration. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Jensen possède plusieurs propriétés intéressantes et
corollaires.</p>
<h2 id="propriété-de-convexité">Propriété de Convexité</h2>
<div class="proposition">
<p>Soit <span class="math inline">\(f\)</span> une fonction convexe et
<span class="math inline">\(X\)</span> une variable aléatoire. Alors,
pour toute constante <span class="math inline">\(c \in
\mathbb{R}\)</span>, la fonction <span class="math inline">\(g(x) = f(x)
+ c\)</span> est également convexe et satisfait l’inégalité de
Jensen.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La convexité de <span
class="math inline">\(g\)</span> découle directement de la convexité de
<span class="math inline">\(f\)</span>. En effet, pour tout <span
class="math inline">\(x, y \in E\)</span> et pour tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, on a :</p>
<p><span class="math display">\[g(\lambda x + (1 - \lambda) y) =
f(\lambda x + (1 - \lambda) y) + c \leq \lambda f(x) + (1 - \lambda)
f(y) + c = \lambda g(x) + (1 - \lambda) g(y).\]</span></p>
<p>L’inégalité de Jensen pour <span class="math inline">\(g\)</span>
s’ensuit immédiatement. ◻</p>
</div>
<h2 id="corollaire-de-linégalité-de-jensen">Corollaire de l’Inégalité de
Jensen</h2>
<div class="corollary">
<p>Soit <span class="math inline">\(f\)</span> une fonction convexe et
<span class="math inline">\(X\)</span> une variable aléatoire. Si <span
class="math inline">\(f\)</span> est différentiable en <span
class="math inline">\(\mathbb{E}[X]\)</span>, alors :</p>
<p><span class="math display">\[f(\mathbb{E}[X]) = f(\mathbb{E}[X]) +
\nabla f(\mathbb{E}[X])^T (\mathbb{E}[X] - \mathbb{E}[X]) + o(\|X -
\mathbb{E}[X]\|).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce corollaire découle du développement de Taylor de
<span class="math inline">\(f\)</span> autour de <span
class="math inline">\(\mathbb{E}[X]\)</span>. En prenant l’espérance,
nous obtenons :</p>
<p><span class="math display">\[\mathbb{E}[f(X)] = f(\mathbb{E}[X]) +
\nabla f(\mathbb{E}[X])^T (\mathbb{E}[X] - \mathbb{E}[X]) + o(\|X -
\mathbb{E}[X]\|).\]</span></p>
<p>En simplifiant, nous retrouvons l’inégalité de Jensen. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de Jensen est un outil puissant et polyvalent en théorie
des probabilités et en analyse convexe. Ses applications sont vastes,
allant de la finance à l’apprentissage automatique. Comprendre et
maîtriser cette inégalité permet de résoudre des problèmes complexes et
de fournir des bornes précises sur des quantités d’intérêt. Nous avons
vu dans cet article les fondements théoriques de l’inégalité de Jensen,
ses démonstrations détaillées, ainsi que quelques-unes de ses propriétés
et corollaires.</p>
</body>
</html>
{% include "footer.html" %}


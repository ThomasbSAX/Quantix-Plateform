{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par skewness</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par skewness</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
puissante dans le domaine du traitement des données, notamment en
apprentissage automatique. Parmi les méthodes d’encodage, le binning par
skewness se distingue par sa capacité à capturer des informations non
linéaires et asymétriques dans les données. Cette technique est
particulièrement utile lorsque les distributions sous-jacentes des
variables sont complexes et ne peuvent être capturées par des méthodes
linéaires traditionnelles.</p>
<p>Le binning par skewness émerge comme une réponse à la nécessité de
traiter des données avec des distributions asymétriques. En divisant les
données en bins (intervalles) et en extrayant des caractéristiques de
skewness pour chaque bin, nous pouvons capturer des informations
subtiles qui seraient autrement perdues. Cette méthode est indispensable
dans les domaines où la compréhension des distributions asymétriques est
cruciale, tels que l’analyse financière, la biologie et les sciences
sociales.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous cherchons à accomplir. Nous voulons transformer
une variable continue en une série de caractéristiques qui capturent
l’asymétrie des données dans différents intervalles. Pour ce faire, nous
divisons les données en bins et calculons des mesures de skewness pour
chaque bin.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue avec une distribution de probabilité <span
class="math inline">\(f_X(x)\)</span>. Nous divisons l’intervalle des
valeurs de <span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> bins disjoints <span
class="math inline">\(B_1, B_2, \ldots, B_k\)</span>. Pour chaque bin
<span class="math inline">\(B_i\)</span>, nous définissons la
caractéristique de skewness comme suit :</p>
<p><span class="math display">\[\text{Skew}(B_i) = \frac{\mathbb{E}[(X -
\mu)^3 | X \in B_i]}{\sigma^3}\]</span></p>
<p>où <span class="math inline">\(\mu\)</span> est la moyenne de <span
class="math inline">\(X\)</span> dans le bin <span
class="math inline">\(B_i\)</span>, et <span
class="math inline">\(\sigma\)</span> est l’écart-type de <span
class="math inline">\(X\)</span> dans le bin <span
class="math inline">\(B_i\)</span>.</p>
</div>
<p>De manière équivalente, nous pouvons exprimer la caractéristique de
skewness en termes d’espérance conditionnelle :</p>
<p><span class="math display">\[\text{Skew}(B_i) = \frac{\int_{x \in
B_i} (x - \mu)^3 f_X(x) \, dx}{\sigma^3}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’efficacité du binning par skewness, nous devons
examiner certains théorèmes clés. Supposons que nous ayons une variable
aléatoire <span class="math inline">\(X\)</span> avec une distribution
asymétrique. Nous voulons montrer que le binning par skewness peut
capturer cette asymétrie de manière efficace.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue avec une distribution de probabilité <span
class="math inline">\(f_X(x)\)</span>. Si nous divisons l’intervalle des
valeurs de <span class="math inline">\(X\)</span> en <span
class="math inline">\(k\)</span> bins disjoints <span
class="math inline">\(B_1, B_2, \ldots, B_k\)</span>, alors les
caractéristiques de skewness <span
class="math inline">\(\text{Skew}(B_i)\)</span> pour chaque bin <span
class="math inline">\(B_i\)</span> capturent l’asymétrie de la
distribution de <span class="math inline">\(X\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous devons montrer que
les caractéristiques de skewness <span
class="math inline">\(\text{Skew}(B_i)\)</span> sont sensibles à
l’asymétrie de la distribution. Considérons deux bins <span
class="math inline">\(B_i\)</span> et <span
class="math inline">\(B_j\)</span>. Si la distribution de <span
class="math inline">\(X\)</span> est symétrique, alors <span
class="math inline">\(\text{Skew}(B_i) = \text{Skew}(B_j) = 0\)</span>.
Cependant, si la distribution est asymétrique, alors <span
class="math inline">\(\text{Skew}(B_i) \neq \text{Skew}(B_j)\)</span>,
ce qui montre que les caractéristiques de skewness capturent
l’asymétrie.</p>
<p>De manière formelle, nous pouvons écrire :</p>
<p><span class="math display">\[\text{Skew}(B_i) = \frac{\mathbb{E}[(X -
\mu)^3 | X \in B_i]}{\sigma^3} \neq 0\]</span></p>
<p>pour certains bins <span class="math inline">\(B_i\)</span> si la
distribution est asymétrique. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le binning par skewness possède plusieurs propriétés intéressantes
qui en font une méthode puissante pour l’encodage des données.</p>
<div class="proposition">
<p>Les caractéristiques de skewness <span
class="math inline">\(\text{Skew}(B_i)\)</span> sont invariantes par
translation linéaire. Cela signifie que si nous appliquons une
transformation affine <span class="math inline">\(Y = aX + b\)</span> à
la variable <span class="math inline">\(X\)</span>, les caractéristiques
de skewness restent inchangées.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Considérons une transformation affine <span
class="math inline">\(Y = aX + b\)</span>. Les caractéristiques de
skewness pour <span class="math inline">\(Y\)</span> sont données par
:</p>
<p><span class="math display">\[\text{Skew}(B_i^Y) = \frac{\mathbb{E}[(Y
- \mu_Y)^3 | Y \in B_i^Y]}{\sigma_Y^3}\]</span></p>
<p>où <span class="math inline">\(B_i^Y\)</span> est le bin
correspondant pour <span class="math inline">\(Y\)</span>, et <span
class="math inline">\(\mu_Y\)</span> et <span
class="math inline">\(\sigma_Y\)</span> sont la moyenne et l’écart-type
de <span class="math inline">\(Y\)</span> dans le bin <span
class="math inline">\(B_i^Y\)</span>. En utilisant les propriétés des
transformations affines, nous pouvons montrer que :</p>
<p><span class="math display">\[\text{Skew}(B_i^Y) =
\text{Skew}(B_i)\]</span></p>
<p>ce qui prouve l’invariance par translation linéaire. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par skewness
est une méthode puissante pour capturer l’asymétrie dans les
distributions de données. En divisant les données en bins et en
extrayant des caractéristiques de skewness pour chaque bin, nous pouvons
obtenir une représentation riche et informative des données. Cette
méthode est particulièrement utile dans les domaines où la compréhension
des distributions asymétriques est cruciale.</p>
</body>
</html>
{% include "footer.html" %}


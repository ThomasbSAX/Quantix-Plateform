{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Uniform Manifold Approximation and Projection</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Uniform Manifold Approximation and
Projection</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse des données de haute dimension est un défi central en
apprentissage automatique. Les méthodes de réduction de dimension,
telles que la PCA (Analyse en Composantes Principales), sont largement
utilisées pour projeter les données dans un espace de plus faible
dimension. Cependant, ces méthodes linéaires peuvent être inefficaces
lorsque les données sont distribuées sur des variétés non linéaires.</p>
<p>La méthode Uniform Manifold Approximation and Projection (UMAP) a été
introduite pour répondre à ce besoin. UMAP est une technique de
réduction de dimension non linéaire qui préserve les structures
topologiques des données. Cependant, UMAP peut être limité par sa
dépendance à des paramètres et sa sensibilité aux choix initiaux.</p>
<p>Pour pallier ces limitations, nous introduisons le Kernelized Uniform
Manifold Approximation and Projection (KUMAP). KUMAP combine les
avantages de UMAP avec ceux des méthodes à noyau, permettant une
meilleure préservation des structures locales et globales des
données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Nous commençons par définir les concepts fondamentaux nécessaires à
la compréhension de KUMAP.</p>
<h2 class="unnumbered" id="manifold-learning">Manifold Learning</h2>
<p>Soit <span class="math inline">\(\mathcal{M}\)</span> une variété
différentiable de dimension <span class="math inline">\(d\)</span>
plongée dans un espace euclidien de dimension <span
class="math inline">\(D\)</span>, avec <span class="math inline">\(d
&lt; D\)</span>. L’objectif du manifold learning est de découvrir la
structure intrinsèque de <span
class="math inline">\(\mathcal{M}\)</span> à partir d’un ensemble de
points échantillonnés sur cette variété.</p>
<h2 class="unnumbered"
id="uniform-manifold-approximation-and-projection-umap">Uniform Manifold
Approximation and Projection (UMAP)</h2>
<p>UMAP est une méthode de réduction de dimension non linéaire qui vise
à préserver les relations de proximité entre les points. Formellement,
pour un ensemble de points <span class="math inline">\(X = \{x_1, x_2,
\dots, x_n\}\)</span> dans <span
class="math inline">\(\mathbb{R}^D\)</span>, UMAP construit une carte de
basse dimension <span class="math inline">\(Y = \{y_1, y_2, \dots,
y_n\}\)</span> dans <span class="math inline">\(\mathbb{R}^d\)</span> en
minimisant une fonction de coût qui mesure la différence entre les
distances dans l’espace haut dimensionnel et celles dans l’espace de
basse dimension.</p>
<h2 class="unnumbered" id="kernel-methods">Kernel Methods</h2>
<p>Les méthodes à noyau utilisent une fonction noyau <span
class="math inline">\(\kappa: \mathbb{R}^D \times \mathbb{R}^D
\rightarrow \mathbb{R}\)</span> pour mesurer la similarité entre les
points. La fonction noyau est définie de telle sorte que <span
class="math inline">\(\kappa(x_i, x_j) = \langle \phi(x_i), \phi(x_j)
\rangle\)</span>, où <span class="math inline">\(\phi\)</span> est une
fonction de projection dans un espace de caractéristiques de haute
dimension.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant les théorèmes clés qui sous-tendent la
méthode KUMAP.</p>
<h2 class="unnumbered"
id="théorème-de-représentation-des-noyaux">Théorème de Représentation
des Noyaux</h2>
<p>Soit <span class="math inline">\(\kappa\)</span> une fonction noyau
symétrique et positive définie. Alors, il existe un espace de Hilbert
<span class="math inline">\(\mathcal{H}\)</span> et une fonction <span
class="math inline">\(\phi: \mathbb{R}^D \rightarrow
\mathcal{H}\)</span> tels que pour tout <span class="math inline">\(x, y
\in \mathbb{R}^D\)</span>, on a: <span class="math display">\[\kappa(x,
y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}}\]</span></p>
<h2 class="unnumbered"
id="théorème-de-préservation-des-distances">Théorème de Préservation des
Distances</h2>
<p>Soit <span class="math inline">\(X\)</span> un ensemble de points
dans <span class="math inline">\(\mathbb{R}^D\)</span>, et soit <span
class="math inline">\(Y\)</span> une carte de basse dimension obtenue
par UMAP. Si les distances dans l’espace de haute dimension sont
préservées dans l’espace de basse dimension, alors la structure
topologique de <span class="math inline">\(X\)</span> est préservée dans
<span class="math inline">\(Y\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Nous fournissons maintenant les preuves détaillées des théorèmes
présentés.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-représentation-des-noyaux">Preuve du Théorème
de Représentation des Noyaux</h2>
<p>Considérons une fonction noyau <span
class="math inline">\(\kappa\)</span> symétrique et positive définie.
Par le théorème de Mercer, il existe une décomposition en valeurs
propres de <span class="math inline">\(\kappa\)</span> donnée par: <span
class="math display">\[\kappa(x, y) = \sum_{i=1}^{\infty} \lambda_i
\phi_i(x) \phi_i(y)\]</span> où <span
class="math inline">\(\lambda_i\)</span> sont les valeurs propres et
<span class="math inline">\(\phi_i\)</span> sont les fonctions propres
de <span class="math inline">\(\kappa\)</span>. En définissant <span
class="math inline">\(\mathcal{H}\)</span> comme l’espace engendré par
les fonctions propres <span class="math inline">\(\phi_i\)</span>, on
obtient la représentation souhaitée.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-préservation-des-distances">Preuve du Théorème
de Préservation des Distances</h2>
<p>Soit <span class="math inline">\(d_H(x_i, x_j)\)</span> la distance
dans l’espace de haute dimension et <span class="math inline">\(d_L(y_i,
y_j)\)</span> la distance dans l’espace de basse dimension. UMAP
minimise une fonction de coût qui mesure la différence entre <span
class="math inline">\(d_H\)</span> et <span
class="math inline">\(d_L\)</span>. Par construction, si cette fonction
de coût est minimisée, les distances sont préservées, et donc la
structure topologique est préservée.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant les propriétés et corollaires de KUMAP.</p>
<h2 class="unnumbered"
id="propriété-de-préservation-des-structures-locales">Propriété de
Préservation des Structures Locales</h2>
<p>KUMAP préserve les structures locales des données en utilisant une
fonction noyau qui capture les relations de proximité entre les
points.</p>
<h2 class="unnumbered"
id="propriété-de-préservation-des-structures-globales">Propriété de
Préservation des Structures Globales</h2>
<p>KUMAP préserve les structures globales en minimisant une fonction de
coût qui mesure la différence entre les distances dans l’espace haut
dimensionnel et celles dans l’espace de basse dimension.</p>
<h2 class="unnumbered" id="corollaire-de-robustesse">Corollaire de
Robustesse</h2>
<p>KUMAP est plus robuste aux choix initiaux et aux paramètres que UMAP,
grâce à l’utilisation de la fonction noyau qui permet une meilleure
capture des relations entre les points.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Nous avons introduit KUMAP, une méthode de réduction de dimension non
linéaire qui combine les avantages de UMAP et des méthodes à noyau.
KUMAP permet une meilleure préservation des structures locales et
globales des données, tout en étant plus robuste aux choix initiaux et
aux paramètres. Les théorèmes et preuves présentés montrent la solidité
mathématique de cette méthode, ouvrant ainsi de nouvelles perspectives
pour l’analyse des données de haute dimension.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Pearson : Une Mesure Fondamentale en Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Pearson : Une Mesure Fondamentale en
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Pearson, également connue sous le nom de coefficient
de corrélation de Pearson, est une mesure fondamentale en statistique
qui quantifie la relation linéaire entre deux variables aléatoires. Son
origine remonte au début du XXe siècle, lorsque Karl Pearson a introduit
cette notion pour analyser les relations entre différentes
caractéristiques dans des études biologiques et sociales. La distance de
Pearson émerge comme une réponse à la nécessité de mesurer l’intensité
et la direction des relations linéaires entre variables, ce qui est
indispensable dans de nombreux domaines tels que l’économie, la
biologie, et les sciences sociales.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la distance de Pearson, considérons deux variables
aléatoires <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Nous cherchons à mesurer à quel point
ces variables sont linéairement liées. Intuitivement, si <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> augmentent ou diminuent ensemble de
manière proportionnelle, elles devraient avoir une forte corrélation.
Inversement, si une augmentation de <span
class="math inline">\(X\)</span> entraîne une diminution de <span
class="math inline">\(Y\)</span>, la corrélation devrait être
négative.</p>
<p>Formellement, la distance de Pearson entre deux variables aléatoires
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires avec des
espérances <span class="math inline">\(\mathbb{E}[X]\)</span> et <span
class="math inline">\(\mathbb{E}[Y]\)</span>, et des écarts-types <span
class="math inline">\(\sigma_X\)</span> et <span
class="math inline">\(\sigma_Y\)</span>. La distance de Pearson entre
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est donnée par : <span
class="math display">\[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X
\sigma_Y}\]</span> où <span class="math inline">\(\text{Cov}(X, Y) =
\mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\)</span> est la
covariance entre <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>.</p>
</div>
<p>De manière équivalente, la distance de Pearson peut être exprimée en
termes des moments des variables aléatoires : <span
class="math display">\[\rho_{X,Y} = \frac{\mathbb{E}[XY] -
\mathbb{E}[X]\mathbb{E}[Y]}{\sqrt{(\mathbb{E}[X^2] -
\mathbb{E}[X]^2)(\mathbb{E}[Y^2] - \mathbb{E}[Y]^2)}}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la distance de Pearson est le
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires. Alors :</p>
<ol>
<li><p><span class="math inline">\(-1 \leq \rho_{X,Y} \leq
1\)</span></p></li>
<li><p><span class="math inline">\(\rho_{X,Y} = 1\)</span> si et
seulement si <span class="math inline">\(Y = aX + b\)</span> presque
sûrement pour certains <span class="math inline">\(a &gt; 0\)</span> et
<span class="math inline">\(b \in \mathbb{R}\)</span>.</p></li>
<li><p><span class="math inline">\(\rho_{X,Y} = -1\)</span> si et
seulement si <span class="math inline">\(Y = aX + b\)</span> presque
sûrement pour certains <span class="math inline">\(a &lt; 0\)</span> et
<span class="math inline">\(b \in \mathbb{R}\)</span>.</p></li>
<li><p><span class="math inline">\(\rho_{X,Y} = 0\)</span> si et
seulement si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont non corrélés, c’est-à-dire <span
class="math inline">\(\text{Cov}(X, Y) = 0\)</span>.</p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous utilisons plusieurs
propriétés fondamentales des variables aléatoires et de la
covariance.</p>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Pour montrer que <span class="math inline">\(-1 \leq \rho_{X,Y}
\leq 1\)</span>, nous utilisons l’inégalité de Cauchy-Schwarz : <span
class="math display">\[|\text{Cov}(X, Y)| \leq \sigma_X
\sigma_Y\]</span> En divisant par <span class="math inline">\(\sigma_X
\sigma_Y\)</span>, nous obtenons : <span
class="math display">\[|\rho_{X,Y}| \leq 1\]</span> Ce qui implique
<span class="math inline">\(-1 \leq \rho_{X,Y} \leq 1\)</span>.</p></li>
<li><p>Pour montrer que <span class="math inline">\(\rho_{X,Y} =
1\)</span> si et seulement si <span class="math inline">\(Y = aX +
b\)</span> presque sûrement pour certains <span class="math inline">\(a
&gt; 0\)</span> et <span class="math inline">\(b \in
\mathbb{R}\)</span>, nous utilisons le fait que la covariance atteint
son maximum lorsque les variables sont linéairement dépendantes avec une
pente positive.</p></li>
<li><p>De manière similaire, <span class="math inline">\(\rho_{X,Y} =
-1\)</span> si et seulement si <span class="math inline">\(Y = aX +
b\)</span> presque sûrement pour certains <span class="math inline">\(a
&lt; 0\)</span> et <span class="math inline">\(b \in
\mathbb{R}\)</span>, car la covariance atteint son minimum lorsque les
variables sont linéairement dépendantes avec une pente
négative.</p></li>
<li><p>Pour montrer que <span class="math inline">\(\rho_{X,Y} =
0\)</span> si et seulement si <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span> sont non corrélés, nous utilisons
la définition de la covariance. Si <span
class="math inline">\(\rho_{X,Y} = 0\)</span>, alors <span
class="math inline">\(\text{Cov}(X, Y) = 0\)</span>, ce qui signifie que
les variables sont non corrélées.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Plusieurs propriétés importantes découlent de la distance de Pearson
:</p>
<ol>
<li><p>La distance de Pearson est invariante par translation et par
échelle. Cela signifie que si nous ajoutons une constante à <span
class="math inline">\(X\)</span> ou <span
class="math inline">\(Y\)</span>, ou si nous multiplions <span
class="math inline">\(X\)</span> ou <span
class="math inline">\(Y\)</span> par une constante non nulle, la valeur
de <span class="math inline">\(\rho_{X,Y}\)</span> reste
inchangée.</p></li>
<li><p>La distance de Pearson est symétrique, c’est-à-dire que <span
class="math inline">\(\rho_{X,Y} = \rho_{Y,X}\)</span>.</p></li>
<li><p>La distance de Pearson est une mesure de la force et de la
direction d’une relation linéaire. Une valeur proche de 1 indique une
forte corrélation positive, une valeur proche de -1 indique une forte
corrélation négative, et une valeur proche de 0 indique une faible ou
aucune corrélation linéaire.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Pour montrer l’invariance par translation, supposons que nous
ajoutons une constante <span class="math inline">\(c\)</span> à <span
class="math inline">\(X\)</span>. Alors : <span
class="math display">\[\rho_{X+c,Y} = \frac{\text{Cov}(X+c,
Y)}{\sigma_{X+c} \sigma_Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
= \rho_{X,Y}\]</span> De même, pour l’invariance par échelle, supposons
que nous multiplions <span class="math inline">\(X\)</span> par une
constante non nulle <span class="math inline">\(a\)</span>. Alors :
<span class="math display">\[\rho_{aX,Y} = \frac{\text{Cov}(aX,
Y)}{|a|\sigma_X \sigma_Y} = \frac{a \text{Cov}(X, Y)}{a \sigma_X
\sigma_Y} = \rho_{X,Y}\]</span></p></li>
<li><p>La symétrie de la distance de Pearson découle directement de la
définition : <span class="math display">\[\rho_{X,Y} =
\frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\text{Cov}(Y,
X)}{\sigma_Y \sigma_X} = \rho_{Y,X}\]</span></p></li>
<li><p>La force et la direction de la relation linéaire sont directement
liées à la valeur de <span class="math inline">\(\rho_{X,Y}\)</span>.
Une valeur proche de 1 ou -1 indique une forte relation linéaire, tandis
qu’une valeur proche de 0 indique une faible ou aucune relation
linéaire.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de Pearson est une mesure essentielle en statistique pour
quantifier la relation linéaire entre deux variables aléatoires. Ses
propriétés fondamentales et ses applications vastes en font un outil
indispensable dans de nombreux domaines scientifiques. La compréhension
approfondie de cette mesure permet d’analyser et d’interpréter les
relations entre variables de manière rigoureuse et précise.</p>
</body>
</html>
{% include "footer.html" %}


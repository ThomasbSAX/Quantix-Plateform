{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Local Linear Embedding : Une approche non-linéaire pour l’embedding de données</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Local Linear Embedding : Une approche non-linéaire
pour l’embedding de données</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<p>Voici un article scientifique complet en LaTeX sur le sujet "LLE"
(Local Linear Embedding) :</p>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le Local Linear Embedding (LLE) est une technique d’apprentissage non
supervisé qui permet de découvrir et de représenter la structure
sous-jacente des données. Introduit par Roweis et Saul en 2000, LLE
repose sur l’hypothèse que les données peuvent être décrites par des
relations linéaires locales, même si la structure globale est
non-linéaire.</p>
<p>L’émergence de LLE a été motivée par le besoin de méthodes capables
de capturer des structures complexes dans les données, au-delà des
capacités des techniques linéaires classiques comme l’Analyse en
Composantes Principales (ACP). LLE est particulièrement utile dans des
domaines comme la vision par ordinateur, l’analyse de données
biologiques et la reconnaissance de motifs.</p>
<h1 id="définitions">Définitions</h1>
<p>Commençons par comprendre ce que nous cherchons à accomplir. Nous
avons un ensemble de points dans un espace de haute dimension, et nous
voulons les représenter dans un espace de plus faible dimension tout en
préservant les relations locales. L’idée est que chaque point peut être
approximé par une combinaison linéaire de ses voisins les plus
proches.</p>
<p>Formellement, donnons la définition du Local Linear Embedding :</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble de points dans <span
class="math inline">\(\mathbb{R}^D\)</span>, et <span
class="math inline">\(Y = \{y_1, y_2, \dots, y_n\}\)</span> leurs
représentations dans <span class="math inline">\(\mathbb{R}^d\)</span>
avec <span class="math inline">\(D &gt; d\)</span>.</p>
<p>Pour chaque point <span class="math inline">\(x_i\)</span>, on
cherche des coefficients <span
class="math inline">\(\{w_{ij}\}_{j=1}^n\)</span> tels que : <span
class="math display">\[x_i \approx \sum_{j=1}^n w_{ij} x_j\]</span> avec
les contraintes : <span class="math display">\[\sum_{j=1}^n w_{ij} =
1\]</span> et <span class="math inline">\(w_{ij} = 0\)</span> si <span
class="math inline">\(x_j\)</span> n’est pas parmi les <span
class="math inline">\(k\)</span> voisins les plus proches de <span
class="math inline">\(x_i\)</span>.</p>
<p>Les coefficients <span
class="math inline">\(\{w_{ij}\}_{j=1}^n\)</span> sont obtenus en
minimisant : <span class="math display">\[\sum_{i=1}^n \|x_i -
\sum_{j=1}^n w_{ij} x_j\|^2\]</span> sous les contraintes ci-dessus.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central de LLE est celui qui garantit que les relations
locales sont préservées dans l’espace d’arrivée.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> comme dans la définition précédente. Si
les coefficients <span class="math inline">\(\{w_{ij}\}_{j=1}^n\)</span>
sont obtenus en minimisant la somme des carrés des erreurs sous les
contraintes données, alors pour chaque point <span
class="math inline">\(x_i\)</span>, la représentation <span
class="math inline">\(y_i\)</span> peut être approximée par : <span
class="math display">\[y_i \approx \sum_{j=1}^n w_{ij} y_j\]</span> avec
la même contrainte <span class="math inline">\(\sum_{j=1}^n w_{ij} =
1\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous devons montrer que les
coefficients <span class="math inline">\(\{w_{ij}\}_{j=1}^n\)</span>
obtenus dans l’espace de départ peuvent être utilisés pour approximer
les points dans l’espace d’arrivée.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la matrice de poids <span
class="math inline">\(W\)</span> où <span class="math inline">\(W_{ij} =
w_{ij}\)</span>. La minimisation dans l’espace de départ donne : <span
class="math display">\[\sum_{i=1}^n \|x_i - W x_i\|^2 =
\text{min}\]</span></p>
<p>Nous voulons montrer que dans l’espace d’arrivée, nous avons : <span
class="math display">\[\sum_{i=1}^n \|y_i - W y_i\|^2 =
\text{min}\]</span></p>
<p>Cela est vrai car les coefficients <span
class="math inline">\(\{w_{ij}\}_{j=1}^n\)</span> sont déterminés
uniquement par les relations locales entre les points, qui sont
préservées dans l’espace d’arrivée. Les contraintes <span
class="math inline">\(\sum_{j=1}^n w_{ij} = 1\)</span> garantissent que
les points restent dans un espace affine de dimension <span
class="math inline">\(d\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Voici quelques propriétés importantes de LLE :</p>
<ol>
<li><p><strong>Invariance par translation et rotation</strong> : LLE est
invariant par translation et rotation dans l’espace de départ, car il ne
dépend que des relations relatives entre les points.</p></li>
<li><p><strong>Préservation de la structure locale</strong> : LLE
préserve les relations locales entre les points, ce qui est crucial pour
des applications comme la visualisation de données.</p></li>
<li><p><strong>Robustesse aux bruit</strong> : LLE est relativement
robuste aux bruits, car il repose sur les relations locales plutôt que
sur la structure globale.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Le Local Linear Embedding est une technique puissante pour la
réduction de dimension non-linéaire. Sa capacité à préserver les
relations locales en fait un outil précieux pour l’analyse de données
complexes. Bien que LLE ait certaines limitations, comme la sensibilité
au choix du nombre de voisins, il reste une méthode largement utilisée
et étudiée dans le domaine de l’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


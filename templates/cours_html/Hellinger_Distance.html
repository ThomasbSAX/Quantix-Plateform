{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de Hellinger : une mesure d’écart entre distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de Hellinger : une mesure d’écart entre
distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Hellinger, du nom du mathématicien Erich Hellinger,
est une mesure d’écart entre deux distributions de probabilité. Elle
émerge dans le cadre de la théorie des probabilités et trouve ses
applications dans divers domaines tels que l’apprentissage automatique,
la statistique bayésienne, et la théorie de l’information. Cette
distance est particulièrement utile car elle permet de comparer des
distributions de manière géométrique, en exploitant les propriétés du
demi-espace euclidien.</p>
<p>L’origine historique de la distance de Hellinger remonte aux travaux
d’Erich Hellinger sur les intégrales de Stieltjes. Cependant, c’est dans
le contexte moderne de la statistique et de l’apprentissage automatique
que cette distance a gagné en popularité. Elle est indispensable pour
évaluer la similarité entre deux modèles de probabilité, notamment dans
les algorithmes d’optimisation et de classification.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir la distance de Hellinger, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous voulons
quantifier à quel point ces deux distributions diffèrent l’une de
l’autre.</p>
<p>La distance de Hellinger est une mesure qui capture cette différence
en utilisant la racine carrée de l’écart intégré entre les densités de
probabilité. Formellement, si <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> ont des densités de probabilité
<span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> respectivement par rapport à une mesure
de référence <span class="math inline">\(\mu\)</span>, alors la distance
de Hellinger est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, avec des densités de probabilité <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> respectivement par rapport à une mesure
de référence <span class="math inline">\(\mu\)</span>. La distance de
Hellinger entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[H(P, Q) = \sqrt{1 - \int_{\Omega} \sqrt{p(\omega)
q(\omega)} \, d\mu(\omega)}\]</span></p>
</div>
<p>Une autre formulation de la distance de Hellinger, souvent utilisée
dans les applications pratiques, est la suivante :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. La distance de Hellinger entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> peut également être exprimée comme :
<span class="math display">\[H(P, Q) = \frac{1}{\sqrt{2}} \left\|
\sqrt{p} - \sqrt{q} \right\|_2\]</span> où <span
class="math inline">\(\| \cdot \|_2\)</span> désigne la norme
euclidienne.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Hellinger est celui de
Pinsker, qui établit une relation entre la distance de Hellinger et la
divergence de Kullback-Leibler.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>, avec des densités de probabilité <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> respectivement par rapport à une mesure
de référence <span class="math inline">\(\mu\)</span>. Alors, la
distance de Hellinger satisfait l’inégalité suivante : <span
class="math display">\[H(P, Q) \leq \sqrt{\frac{1}{2} D_{KL}(P \|
Q)}\]</span> où <span class="math inline">\(D_{KL}(P \| Q)\)</span>
désigne la divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Pinsker, nous allons utiliser la
définition de la distance de Hellinger et les propriétés de la
divergence de Kullback-Leibler.</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux distributions de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> avec des densités de probabilité <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> respectivement. La divergence de
Kullback-Leibler est définie par : <span class="math display">\[D_{KL}(P
\| Q) = \int_{\Omega} p(\omega)
\log\left(\frac{p(\omega)}{q(\omega)}\right) \,
d\mu(\omega)\]</span></p>
<p>Nous voulons montrer que : <span class="math display">\[H(P, Q) \leq
\sqrt{\frac{1}{2} D_{KL}(P \| Q)}\]</span></p>
<p>En utilisant la définition de la distance de Hellinger, nous avons :
<span class="math display">\[H(P, Q) = \sqrt{1 - \int_{\Omega}
\sqrt{p(\omega) q(\omega)} \, d\mu(\omega)}\]</span></p>
<p>Pour établir l’inégalité, nous utilisons l’inégalité de
Cauchy-Schwarz et les propriétés de la fonction logarithme. Par
l’inégalité de Jensen, nous savons que : <span
class="math display">\[\log\left(\frac{p(\omega)}{q(\omega)}\right) \geq
1 - \sqrt{\frac{q(\omega)}{p(\omega)}}\]</span></p>
<p>En intégrant cette inégalité par rapport à <span
class="math inline">\(P\)</span>, nous obtenons : <span
class="math display">\[D_{KL}(P \| Q) \geq 1 - \int_{\Omega}
\sqrt{\frac{q(\omega)}{p(\omega)}} \, dP(\omega)\]</span></p>
<p>En réarrangeant les termes, nous avons : <span
class="math display">\[1 - \int_{\Omega}
\sqrt{\frac{q(\omega)}{p(\omega)}} \, dP(\omega) \leq D_{KL}(P \|
Q)\]</span></p>
<p>Ce qui implique : <span class="math display">\[\int_{\Omega}
\sqrt{p(\omega) q(\omega)} \, d\mu(\omega) \geq 1 - D_{KL}(P \|
Q)\]</span></p>
<p>En substituant dans la définition de <span class="math inline">\(H(P,
Q)\)</span>, nous obtenons : <span class="math display">\[H(P, Q) =
\sqrt{1 - \int_{\Omega} \sqrt{p(\omega) q(\omega)} \, d\mu(\omega)} \leq
\sqrt{1 - (1 - D_{KL}(P \| Q))} = \sqrt{D_{KL}(P \| Q)}\]</span></p>
<p>En simplifiant, nous avons : <span class="math display">\[H(P, Q)
\leq \sqrt{\frac{1}{2} D_{KL}(P \| Q)}\]</span></p>
<p>Ce qui achève la preuve du théorème de Pinsker. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distance de Hellinger possède plusieurs propriétés intéressantes,
que nous allons énumérer et démontrer.</p>
<ol>
<li><p>La distance de Hellinger est une métrique sur l’espace des
distributions de probabilité. Cela signifie qu’elle satisfait les
propriétés suivantes :</p>
<ul>
<li><p>Symétrie : <span class="math inline">\(H(P, Q) = H(Q,
P)\)</span></p></li>
<li><p>Identité des indiscernables : <span class="math inline">\(H(P, Q)
= 0\)</span> si et seulement si <span class="math inline">\(P =
Q\)</span></p></li>
<li><p>Inégalité triangulaire : <span class="math inline">\(H(P, R) \leq
H(P, Q) + H(Q, R)\)</span></p></li>
</ul></li>
<li><p>La distance de Hellinger est bornée entre 0 et 1. Plus
précisément, pour toute paire de distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[0 \leq H(P, Q) \leq 1\]</span> où <span
class="math inline">\(H(P, Q) = 0\)</span> si et seulement si <span
class="math inline">\(P = Q\)</span>, et <span
class="math inline">\(H(P, Q) = 1\)</span> si et seulement si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont singulièrement
orthogonales.</p></li>
<li><p>La distance de Hellinger est invariante sous les transformations
mesurables. Cela signifie que pour toute transformation mesurable <span
class="math inline">\(T\)</span> de <span class="math inline">\((\Omega,
\mathcal{F})\)</span> dans un autre espace mesurable <span
class="math inline">\((E, \mathcal{E})\)</span>, nous avons : <span
class="math display">\[H(P \circ T^{-1}, Q \circ T^{-1}) = H(P,
Q)\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de Hellinger est un outil puissant pour mesurer l’écart
entre deux distributions de probabilité. Ses propriétés géométriques et
ses relations avec d’autres mesures telles que la divergence de
Kullback-Leibler en font un instrument précieux dans divers domaines des
mathématiques appliquées. En comprenant et en utilisant la distance de
Hellinger, nous pouvons mieux évaluer et comparer les modèles
probabilistes, ce qui est essentiel pour l’avancement de la recherche en
statistique et en apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


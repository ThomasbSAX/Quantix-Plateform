{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence log-loss: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence log-loss: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>The divergence log-loss, also known as the logarithmic loss or
cross-entropy loss, is a performance metric for classification models.
It measures the performance of a classification model where the
prediction input is a probability between 0 and 1. The logarithmic loss
increases as the predicted probability diverges from the actual
label.</p>
<p>The concept of log-loss emerged from the need to quantify the
uncertainty in classification tasks. It is particularly useful in binary
and multi-class classification problems, providing a strict measure of
the accuracy of probabilistic predictions. The log-loss is indispensable
in machine learning and statistics, where it serves as a loss function
for models like logistic regression and neural networks.</p>
<h1 id="définitions">Définitions</h1>
<p>To understand the divergence log-loss, let’s first consider a binary
classification problem. We have a set of true labels <span
class="math inline">\(y_i\)</span> and predicted probabilities <span
class="math inline">\(p_i\)</span>. The goal is to measure how far the
predicted probabilities are from the true labels.</p>
<p>Formally, for a binary classification problem, the log-loss is
defined as:</p>
<p><span class="math display">\[\text{LogLoss} = -\frac{1}{N}
\sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i)
\right]\]</span></p>
<p>where: - <span class="math inline">\(N\)</span> is the number of
samples, - <span class="math inline">\(y_i \in \{0, 1\}\)</span> is the
true label for the <span class="math inline">\(i\)</span>-th sample, -
<span class="math inline">\(p_i\)</span> is the predicted probability
that the <span class="math inline">\(i\)</span>-th sample belongs to
class 1.</p>
<p>For a multi-class classification problem with <span
class="math inline">\(C\)</span> classes, the log-loss is generalized
as:</p>
<p><span class="math display">\[\text{LogLoss} = -\frac{1}{N}
\sum_{i=1}^{N} \sum_{j=1}^{C} y_{i,j} \log(p_{i,j})\]</span></p>
<p>where: - <span class="math inline">\(y_{i,j} \in \{0, 1\}\)</span> is
the true label for the <span class="math inline">\(i\)</span>-th sample
and <span class="math inline">\(j\)</span>-th class, - <span
class="math inline">\(p_{i,j}\)</span> is the predicted probability that
the <span class="math inline">\(i\)</span>-th sample belongs to class
<span class="math inline">\(j\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>One important theorem related to the log-loss is the relationship
between log-loss and the Kullback-Leibler (KL) divergence. The KL
divergence measures the difference between two probability distributions
<span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>:</p>
<p><span class="math display">\[D_{KL}(P \| Q) = \sum_{i} P(i)
\log\left(\frac{P(i)}{Q(i)}\right)\]</span></p>
<p>The log-loss can be seen as an empirical approximation of the KL
divergence. Specifically, for a binary classification problem:</p>
<p><span class="math display">\[\text{LogLoss} \approx D_{KL}(Y \|
P)\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the true distribution
and <span class="math inline">\(P\)</span> is the predicted
distribution.</p>
<h1 id="preuves">Preuves</h1>
<p>To prove the relationship between log-loss and KL divergence, let’s
consider a binary classification problem. The true distribution <span
class="math inline">\(Y\)</span> is given by:</p>
<p><span class="math display">\[Y = \begin{cases}
1 &amp; \text{with probability } p \\
0 &amp; \text{with probability } 1 - p
\end{cases}\]</span></p>
<p>The predicted distribution <span class="math inline">\(P\)</span> is
given by:</p>
<p><span class="math display">\[P = \begin{cases}
1 &amp; \text{with probability } \hat{p} \\
0 &amp; \text{with probability } 1 - \hat{p}
\end{cases}\]</span></p>
<p>The KL divergence between <span class="math inline">\(Y\)</span> and
<span class="math inline">\(P\)</span> is:</p>
<p><span class="math display">\[D_{KL}(Y \| P) = p
\log\left(\frac{p}{\hat{p}}\right) + (1 - p) \log\left(\frac{1 - p}{1 -
\hat{p}}\right)\]</span></p>
<p>The log-loss for a single sample is:</p>
<p><span class="math display">\[\text{LogLoss} = - \left[ y
\log(\hat{p}) + (1 - y) \log(1 - \hat{p}) \right]\]</span></p>
<p>When we take the expectation over <span
class="math inline">\(Y\)</span>, we get:</p>
<p><span class="math display">\[\mathbb{E}[\text{LogLoss}] = - \left[ p
\log(\hat{p}) + (1 - p) \log(1 - \hat{p}) \right] = D_{KL}(Y \|
P)\]</span></p>
<p>Thus, the log-loss is an unbiased estimator of the KL divergence.</p>
<h1 id="propriétés-et-corollaires">Propriétés et corollaires</h1>
<p>The log-loss has several important properties:</p>
<ol>
<li><p><strong>Convexity</strong>: The log-loss is a convex function,
which means that it has a unique minimum. This property is crucial for
optimization algorithms used in training machine learning
models.</p></li>
<li><p><strong>Range</strong>: The log-loss is always non-negative. It
reaches its minimum value of 0 when the predicted probabilities
perfectly match the true labels.</p></li>
<li><p><strong>Sensitivity to Confidence</strong>: The log-loss heavily
penalizes confident but wrong predictions. For example, predicting a
probability of 0.9 when the true label is 0 results in a high
log-loss.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>The divergence log-loss is a fundamental concept in machine learning
and statistics. It provides a strict measure of the accuracy of
probabilistic predictions, making it indispensable in classification
tasks. The relationship between log-loss and KL divergence highlights
its theoretical significance, while its convexity and sensitivity to
confidence underscore its practical utility.</p>
</body>
</html>
{% include "footer.html" %}


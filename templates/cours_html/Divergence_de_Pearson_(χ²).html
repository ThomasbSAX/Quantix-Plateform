{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Pearson (χ²)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Pearson (χ²)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Pearson, également connue sous le nom de statistique
χ² (chi-carré), est un outil fondamental en statistique mathématique.
Elle émerge dans le contexte de l’ajustement des modèles probabilistes
aux données observées, permettant ainsi de tester la conformité d’un
modèle théorique à une distribution empirique. Son importance réside
dans sa capacité à mesurer la distance entre deux distributions de
probabilité, ce qui en fait un outil indispensable pour l’inférence
statistique et la validation des hypothèses.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Pearson, commençons par comprendre
ce que nous cherchons à mesurer. Supposons que nous avons deux
distributions de probabilité, <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, définies sur le même espace
d’échantillonnage. Nous voulons quantifier à quel point ces deux
distributions diffèrent l’une de l’autre.</p>
<p>La divergence de Pearson est une mesure de cette différence. Elle est
définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur le même ensemble fini <span
class="math inline">\(\Omega\)</span>. La divergence de Pearson entre
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_P(P \| Q) = \sum_{i=1}^{n} \frac{(P(i) -
Q(i))^2}{Q(i)}\]</span> où <span class="math inline">\(P(i)\)</span> et
<span class="math inline">\(Q(i)\)</span> représentent les probabilités
des événements <span class="math inline">\(i\)</span> sous les
distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
</div>
<p>Pour les distributions continues, la divergence de Pearson s’exprime
comme une intégrale : <span class="math display">\[D_P(P \| Q) =
\int_{-\infty}^{\infty} \frac{(p(x) - q(x))^2}{q(x)} \, dx\]</span> où
<span class="math inline">\(p(x)\)</span> et <span
class="math inline">\(q(x)\)</span> sont les fonctions de densité de
probabilité des distributions <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, respectivement.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Pearson est le
théorème du chi-carré, qui décrit la distribution asymptotique de la
statistique χ² sous l’hypothèse nulle.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur le même ensemble fini <span
class="math inline">\(\Omega\)</span>. Supposons que <span
class="math inline">\(Q\)</span> est la distribution théorique vraie et
que <span class="math inline">\(P\)</span> est estimée à partir d’un
échantillon de taille <span class="math inline">\(n\)</span>. Alors,
sous l’hypothèse nulle que le modèle théorique est correct, la
statistique χ² converge en loi vers une distribution chi-carré avec
<span class="math inline">\(k-1\)</span> degrés de liberté, où <span
class="math inline">\(k\)</span> est le nombre de catégories dans la
distribution. <span class="math display">\[\lim_{n \to \infty} D_P(P \|
Q) \xrightarrow{d} \chi^2_{k-1}\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème du chi-carré, nous devons montrer que la
statistique χ² suit une distribution chi-carré sous l’hypothèse nulle.
La preuve repose sur le théorème central limite et la loi des grands
nombres.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un échantillon de taille <span
class="math inline">\(n\)</span> tiré selon la distribution <span
class="math inline">\(Q\)</span>. Pour chaque catégorie <span
class="math inline">\(i\)</span>, soit <span
class="math inline">\(X_i\)</span> le nombre d’observations tombant dans
cette catégorie. Sous l’hypothèse nulle, <span
class="math inline">\(X_i\)</span> suit une loi binomiale de paramètres
<span class="math inline">\(n\)</span> et <span
class="math inline">\(Q(i)\)</span>.</p>
<p>La statistique χ² peut être écrite comme : <span
class="math display">\[D_P(P \| Q) = \sum_{i=1}^{k} \frac{(X_i -
nQ(i))^2}{nQ(i)}\]</span></p>
<p>En utilisant le théorème central limite, nous savons que pour chaque
<span class="math inline">\(i\)</span>, la variable aléatoire <span
class="math inline">\(\frac{X_i - nQ(i)}{\sqrt{nQ(i)(1-Q(i))}}\)</span>
converge en loi vers une loi normale standard <span
class="math inline">\(N(0,1)\)</span>.</p>
<p>Ensuite, nous pouvons réécrire la statistique χ² comme : <span
class="math display">\[D_P(P \| Q) = \sum_{i=1}^{k} \left( \frac{X_i -
nQ(i)}{\sqrt{nQ(i)(1-Q(i))}} \right)^2\]</span></p>
<p>En utilisant la loi des grands nombres, nous savons que <span
class="math inline">\(Q(i) \approx P(i)\)</span> pour de grandes valeurs
de <span class="math inline">\(n\)</span>. Par conséquent, la
statistique χ² converge en loi vers une distribution chi-carré avec
<span class="math inline">\(k-1\)</span> degrés de liberté. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Pearson possède plusieurs propriétés importantes qui
en font un outil puissant pour l’analyse statistique.</p>
<ol>
<li><p><strong>Non-négativité</strong> : La divergence de Pearson est
toujours non négative, c’est-à-dire que <span
class="math inline">\(D_P(P \| Q) \geq 0\)</span>. Elle est égale à zéro
si et seulement si les distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont identiques.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité découle du fait que le carré de
toute différence est non négatif. L’égalité à zéro implique que <span
class="math inline">\(P(i) = Q(i)\)</span> pour tout <span
class="math inline">\(i\)</span>, ce qui signifie que les distributions
sont identiques. ◻</p>
</div></li>
<li><p><strong>Invariance sous transformation</strong> : La divergence
de Pearson est invariante sous les transformations bijectives. Cela
signifie que si nous appliquons une transformation bijective à toutes
les catégories, la valeur de la divergence de Pearson reste
inchangée.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une transformation bijective <span
class="math inline">\(T\)</span> appliquée à toutes les catégories. La
divergence de Pearson reste inchangée car la transformation ne modifie
pas les proportions relatives des probabilités. ◻</p>
</div></li>
<li><p><strong>Lien avec l’entropie</strong> : La divergence de Pearson
est liée à l’entropie de Kullback-Leibler. En effet, la divergence de
Pearson peut être vue comme une approximation de l’entropie de
Kullback-Leibler pour des échantillons de grande taille.</p>
<div class="proof">
<p><em>Proof.</em> Pour des grandes valeurs de <span
class="math inline">\(n\)</span>, l’entropie de Kullback-Leibler peut
être approximée par la divergence de Pearson en utilisant des
développements limités. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Pearson est un outil essentiel en statistique
mathématique, permettant de mesurer la différence entre deux
distributions de probabilité. Son utilisation est fondamentale dans
l’ajustement des modèles et la validation des hypothèses. Les théorèmes
et propriétés associés à la divergence de Pearson en font un outil
puissant pour l’analyse statistique, avec des applications dans de
nombreux domaines scientifiques.</p>
</body>
</html>
{% include "footer.html" %}


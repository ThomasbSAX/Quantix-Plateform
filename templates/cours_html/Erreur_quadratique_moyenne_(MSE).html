{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Erreur quadratique moyenne (MSE)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Erreur quadratique moyenne (MSE)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’erreur quadratique moyenne (MSE pour <em>Mean Squared Error</em>)
est une mesure fondamentale en statistique et en apprentissage
automatique. Elle émerge naturellement dans le cadre de l’estimation de
paramètres et de la régression, où l’objectif est de minimiser l’écart
entre les valeurs prédites par un modèle et les valeurs réelles
observées. Historiquement, cette notion est liée aux travaux de Carl
Friedrich Gauss sur la méthode des moindres carrés, qui remonte au début
du XIXe siècle. Le MSE est indispensable pour évaluer la performance des
modèles prédictifs et pour optimiser les paramètres de ces modèles.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire le MSE, commençons par comprendre ce que nous
cherchons à mesurer. Supposons que nous avons un ensemble de données
<span class="math inline">\((y_i, \hat{y}_i)_{i=1}^n\)</span>, où <span
class="math inline">\(y_i\)</span> est la valeur réelle et <span
class="math inline">\(\hat{y}_i\)</span> est la valeur prédite par un
modèle. Nous voulons quantifier l’écart entre ces deux valeurs.</p>
<p>Formellement, l’erreur quadratique moyenne est définie comme suit
:</p>
<div class="definition">
<p>Soit <span class="math inline">\((y_i, \hat{y}_i)_{i=1}^n\)</span> un
ensemble de <span class="math inline">\(n\)</span> paires de valeurs
réelles et prédites. L’erreur quadratique moyenne est donnée par : <span
class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i -
\hat{y}_i)^2\]</span></p>
</div>
<p>Une autre manière de formuler cette définition est : <span
class="math display">\[\text{MSE} = \mathbb{E}\left[(Y -
\hat{Y})^2\right]\]</span> où <span class="math inline">\(Y\)</span> est
la variable aléatoire représentant les valeurs réelles et <span
class="math inline">\(\hat{Y}\)</span> est la variable aléatoire
représentant les valeurs prédites.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié au MSE est le théorème de Gauss-Markov, qui
établit les conditions sous lesquelles l’estimateur des moindres carrés
est le meilleur estimateur linéaire non biaisé.</p>
<div class="theorem">
<p>Soit le modèle linéaire général <span
class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\)</span>, où <span
class="math inline">\(\mathbf{y}\)</span> est un vecteur de variables
dépendantes, <span class="math inline">\(\mathbf{X}\)</span> est une
matrice de variables indépendantes, <span
class="math inline">\(\boldsymbol{\beta}\)</span> est un vecteur de
coefficients, et <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> est un vecteur
d’erreurs aléatoires. Supposons que les erreurs <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> sont non corrélées
et ont une variance constante. Alors, l’estimateur des moindres carrés
<span class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span> est le
meilleur estimateur linéaire non biaisé (BLUE) de <span
class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Gauss-Markov, nous devons montrer que
l’estimateur des moindres carrés est non biaisé et qu’il a la plus
petite variance parmi tous les estimateurs linéaires non biaisés.</p>
<div class="proof">
<p><em>Proof.</em> Premièrement, montrons que l’estimateur des moindres
carrés est non biaisé. Nous avons : <span
class="math display">\[\mathbb{E}[\hat{\boldsymbol{\beta}}] =
\mathbb{E}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span>
En utilisant le modèle linéaire général, nous pouvons écrire : <span
class="math display">\[\mathbb{E}[\hat{\boldsymbol{\beta}}] =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbb{E}[\mathbf{y}] =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} =
\boldsymbol{\beta}\]</span> Ainsi, <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> est non
biaisé.</p>
<p>Ensuite, montrons que l’estimateur des moindres carrés a la plus
petite variance. Considérons un autre estimateur linéaire non biaisé
<span class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>, où <span
class="math inline">\(\mathbf{A}\)</span> est une matrice telle que
<span class="math inline">\(\mathbb{E}[\tilde{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>. Nous devons montrer que <span
class="math inline">\(\text{Var}(\hat{\boldsymbol{\beta}}) \leq
\text{Var}(\tilde{\boldsymbol{\beta}})\)</span>.</p>
<p>Nous avons : <span
class="math display">\[\text{Var}(\hat{\boldsymbol{\beta}}) =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\text{Var}(\boldsymbol{\epsilon})\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\]</span>
et <span class="math display">\[\text{Var}(\tilde{\boldsymbol{\beta}}) =
\mathbf{A}\text{Var}(\boldsymbol{\epsilon})\mathbf{A}^T\]</span> En
utilisant les conditions du théorème de Gauss-Markov, nous pouvons
montrer que <span
class="math inline">\(\text{Var}(\hat{\boldsymbol{\beta}}) \leq
\text{Var}(\tilde{\boldsymbol{\beta}})\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le MSE possède plusieurs propriétés importantes :</p>
<ol>
<li><p>Le MSE est toujours non négatif, c’est-à-dire <span
class="math inline">\(\text{MSE} \geq 0\)</span>.</p></li>
<li><p>Le MSE est invariant sous les transformations linéaires. Si nous
ajoutons une constante <span class="math inline">\(c\)</span> à toutes
les valeurs prédites, le MSE reste inchangé.</p></li>
<li><p>Le MSE est sensible aux valeurs aberrantes, car les erreurs sont
élevées au carré.</p></li>
</ol>
<div class="proof">
<p><em>Proof.</em> Pour prouver la propriété (i), nous utilisons le fait
que <span class="math inline">\((y_i - \hat{y}_i)^2 \geq 0\)</span> pour
tout <span class="math inline">\(i\)</span>. Par conséquent, la somme de
ces termes est également non négative.</p>
<p>Pour prouver la propriété (ii), considérons une transformation
linéaire <span class="math inline">\(\hat{y}_i&#39; = \hat{y}_i +
c\)</span>. Nous avons : <span class="math display">\[\text{MSE}&#39; =
\frac{1}{n} \sum_{i=1}^n (y_i - (\hat{y}_i + c))^2 = \frac{1}{n}
\sum_{i=1}^n (y_i - c - \hat{y}_i)^2 = \text{MSE}\]</span> Ainsi, le MSE
reste inchangé.</p>
<p>Pour prouver la propriété (iii), considérons une valeur aberrante
<span class="math inline">\(y_k\)</span> telle que <span
class="math inline">\(|y_k - \hat{y}_k|\)</span> est très grand. Alors,
<span class="math inline">\((y_k - \hat{y}_k)^2\)</span> sera très
grand, ce qui augmentera considérablement le MSE. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’erreur quadratique moyenne est une mesure essentielle pour évaluer
la performance des modèles prédictifs. Elle possède plusieurs propriétés
importantes et est liée à des théorèmes fondamentaux comme le théorème
de Gauss-Markov. Le MSE est largement utilisé en statistique et en
apprentissage automatique pour optimiser les modèles et améliorer leurs
performances.</p>
</body>
</html>
{% include "footer.html" %}


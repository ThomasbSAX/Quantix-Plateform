{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Stein : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Stein : Une Exploration
Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Stein, introduite par Charles Stein dans les années
1980, est un outil fondamental en théorie des probabilités et en
statistique mathématique. Elle émerge comme une réponse élégante à des
problèmes de comparaison entre distributions de probabilité, permettant
de quantifier la distance entre deux mesures. Cette notion est
indispensable dans l’étude des limites de lois, des inégalités de
concentration, et des méthodes Monte Carlo.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Stein, considérons deux mesures de
probabilité <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> sur un espace mesurable <span
class="math inline">\((\Omega, \mathcal{F})\)</span>. Nous cherchons à
mesurer la distance entre ces deux mesures. La divergence de Stein
utilise des opérateurs différentiels pour capturer cette distance.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux mesures de probabilité sur un
espace mesurable <span class="math inline">\((\Omega,
\mathcal{F})\)</span>. Supposons que <span
class="math inline">\(\mu\)</span> admette une densité <span
class="math inline">\(f\)</span> par rapport à une mesure de référence
<span class="math inline">\(\lambda\)</span>. La divergence de Stein
entre <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie comme :</p>
<p><span class="math display">\[D_{\text{Stein}}(\mu, \nu) = \sup_{h \in
\mathcal{H}} \left| \int_{\Omega} h(x) \, d(\mu - \nu)(x)
\right|\]</span></p>
<p>où <span class="math inline">\(\mathcal{H}\)</span> est un ensemble
de fonctions test satisfaisant certaines conditions de régularité.</p>
</div>
<p>De manière équivalente, pour des mesures sur <span
class="math inline">\(\mathbb{R}^d\)</span>, la divergence de Stein peut
être formulée comme :</p>
<p><span class="math display">\[D_{\text{Stein}}(\mu, \nu) = \sup_{h \in
C^2_b(\mathbb{R}^d)} \left| \int_{\mathbb{R}^d} h(x) \, d(\mu - \nu)(x)
+ \int_{\mathbb{R}^d} \nabla h(x) \cdot \nabla f(x) \, d\lambda(x)
\right|\]</span></p>
<p>où <span class="math inline">\(C^2_b(\mathbb{R}^d)\)</span> désigne
l’ensemble des fonctions deux fois continûment différentiables et
bornées.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème central lié à la divergence de Stein est le théorème de
Stein pour les lois normales, qui permet de comparer des distributions à
la loi normale standard.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mu\)</span> une mesure de
probabilité sur <span class="math inline">\(\mathbb{R}\)</span> et <span
class="math inline">\(\nu\)</span> la loi normale standard. Supposons
que <span class="math inline">\(\mu\)</span> admette une densité <span
class="math inline">\(f\)</span> par rapport à la mesure de Lebesgue.
Alors,</p>
<p><span class="math display">\[D_{\text{Stein}}(\mu, \nu) = \sup_{h \in
C^2_b(\mathbb{R})} \left| \int_{\mathbb{R}} h(x) \, d(\mu - \nu)(x) +
\int_{\mathbb{R}} h&#39;(x) x f(x) \, dx - \int_{\mathbb{R}} h&#39;(x)
f(x) \, dx \right|\]</span></p>
<p>où <span class="math inline">\(h\)</span> est une fonction test dans
<span class="math inline">\(C^2_b(\mathbb{R})\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Stein pour les lois normales, nous
utilisons des techniques d’analyse fonctionnelle et de théorie des
mesures.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une fonction test <span
class="math inline">\(h \in C^2_b(\mathbb{R})\)</span>. Nous avons :</p>
<p><span class="math display">\[\int_{\mathbb{R}} h(x) \, d(\mu -
\nu)(x) = \int_{\mathbb{R}} h(x) f(x) \, dx - \int_{\mathbb{R}} h(x)
e^{-x^2/2} \, dx\]</span></p>
<p>En intégrant par parties, nous obtenons :</p>
<p><span class="math display">\[\int_{\mathbb{R}} h&#39;(x) x f(x) \, dx
= \int_{\mathbb{R}} h(x) f(x) \, dx + \int_{\mathbb{R}} h&#39;(x) f(x)
\, dx\]</span></p>
<p>En substituant cette expression dans la définition de la divergence
de Stein, nous obtenons :</p>
<p><span class="math display">\[D_{\text{Stein}}(\mu, \nu) = \sup_{h \in
C^2_b(\mathbb{R})} \left| \int_{\mathbb{R}} h(x) f(x) \, dx -
\int_{\mathbb{R}} h(x) e^{-x^2/2} \, dx + \int_{\mathbb{R}} h&#39;(x) x
f(x) \, dx - \int_{\mathbb{R}} h&#39;(x) f(x) \, dx \right|\]</span></p>
<p>En simplifiant, nous retrouvons la formulation du théorème de
Stein. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Stein possède plusieurs propriétés importantes qui
en font un outil puissant en théorie des probabilités.</p>
<ol>
<li><p><strong>Inégalité de Stein</strong> : Pour toute mesure <span
class="math inline">\(\mu\)</span> et la loi normale standard <span
class="math inline">\(\nu\)</span>, nous avons :</p>
<p><span class="math display">\[D_{\text{Stein}}(\mu, \nu) \leq C \cdot
\left( \mathbb{E}_{\mu}[|X|^3] + 1 \right)\]</span></p>
<p>où <span class="math inline">\(C\)</span> est une constante
universelle.</p></li>
<li><p><strong>Convergence en loi</strong> : Si une suite de mesures
<span class="math inline">\(\mu_n\)</span> converge en loi vers <span
class="math inline">\(\nu\)</span>, alors :</p>
<p><span class="math display">\[\lim_{n \to \infty}
D_{\text{Stein}}(\mu_n, \nu) = 0\]</span></p></li>
<li><p><strong>Stabilité sous transformations</strong> : La divergence
de Stein est stable sous certaines transformations linéaires et
affines.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Stein est un outil puissant et élégant pour comparer
des distributions de probabilité. Ses applications vont des limites de
lois aux méthodes Monte Carlo, en passant par les inégalités de
concentration. La compréhension approfondie de cette divergence ouvre
des perspectives intéressantes pour l’analyse probabiliste et
statistique.</p>
</body>
</html>
{% include "footer.html" %}


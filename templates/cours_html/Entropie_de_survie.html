{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de survie : Un cadre mathématique pour l’évolution et la complexité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de survie : Un cadre mathématique pour
l’évolution et la complexité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de survie émerge comme un concept fondamental à
l’intersection de la théorie de l’information, de la biologie évolutive
et des systèmes complexes. Inspirée par les travaux pionniers de Shannon
sur l’entropie informationnelle et ceux de Darwin sur la sélection
naturelle, cette notion vise à quantifier la capacité d’un système à
persister dans un environnement dynamique. L’entropie de survie répond à
une question cruciale : comment mesurer la robustesse et l’adaptabilité
d’un système face aux perturbations ?</p>
<p>Dans un monde où les systèmes naturels et artificiels sont soumis à
des contraintes croissantes, comprendre et maximiser l’entropie de
survie devient un impératif. Que ce soit pour concevoir des réseaux
résilients, optimiser des algorithmes d’apprentissage ou modéliser
l’évolution des espèces, cette notion offre un cadre théorique
puissant.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de survie, considérons un système évoluant
dans un espace d’états <span class="math inline">\(\mathcal{S}\)</span>.
Nous cherchons à capturer la capacité du système à explorer cet espace
tout en évitant les états qui mènent à sa disparition. L’entropie de
survie doit donc refléter à la fois la diversité des états accessibles
et la probabilité de persister dans ces états.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{S}\)</span> un espace
d’états fini et <span class="math inline">\(P(s)\)</span> la probabilité
d’occupation de l’état <span class="math inline">\(s \in
\mathcal{S}\)</span>. Soit <span class="math inline">\(\pi(s)\)</span>
la probabilité que le système survive en partant de l’état <span
class="math inline">\(s\)</span>. L’entropie de survie est définie par :
<span class="math display">\[H_{\text{survie}} = -\sum_{s \in
\mathcal{S}} P(s) \pi(s) \log \left( P(s) \pi(s) \right).\]</span> De
manière équivalente, pour tout <span class="math inline">\(s \in
\mathcal{S}\)</span>, <span class="math display">\[H_{\text{survie}} =
-\sum_{s \in \mathcal{S}} P(s) \pi(s) \log P(s) - \sum_{s \in
\mathcal{S}} P(s) \pi(s) \log \pi(s).\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central en théorie de l’entropie de survie est celui de
la maximisation de la persistance. Ce théorème établit une relation
fondamentale entre l’entropie de survie et la probabilité globale de
survie du système.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(H_{\text{survie}}\)</span>
l’entropie de survie d’un système et <span
class="math inline">\(\Pi\)</span> la probabilité globale de survie,
définie par : <span class="math display">\[\Pi = \sum_{s \in
\mathcal{S}} P(s) \pi(s).\]</span> Alors, pour tout système, on a :
<span class="math display">\[H_{\text{survie}} \leq -\Pi \log
\Pi.\]</span> L’égalité a lieu si et seulement si <span
class="math inline">\(P(s) \pi(s)\)</span> est constant pour tout <span
class="math inline">\(s \in \mathcal{S}\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de la maximisation de la persistance, nous
utilisons l’inégalité de Gibbs et les propriétés de convexité de la
fonction logarithme.</p>
<div class="proof">
<p><em>Proof.</em> Par l’inégalité de Gibbs, pour toute distribution de
probabilité <span class="math inline">\(Q(s)\)</span> sur <span
class="math inline">\(\mathcal{S}\)</span>, on a : <span
class="math display">\[\sum_{s \in \mathcal{S}} Q(s) \log Q(s) \leq
\sum_{s \in \mathcal{S}} P(s) \log P(s).\]</span> En choisissant <span
class="math inline">\(Q(s) = \frac{P(s) \pi(s)}{\Pi}\)</span>, on
obtient : <span class="math display">\[\sum_{s \in \mathcal{S}}
\frac{P(s) \pi(s)}{\Pi} \log \left( \frac{P(s) \pi(s)}{\Pi} \right) \leq
\sum_{s \in \mathcal{S}} P(s) \log P(s).\]</span> En réarrangeant les
termes, on trouve : <span class="math display">\[\frac{1}{\Pi} \sum_{s
\in \mathcal{S}} P(s) \pi(s) \log \left( P(s) \pi(s) \right) -
\frac{1}{\Pi} \sum_{s \in \mathcal{S}} P(s) \pi(s) \log \Pi \leq \sum_{s
\in \mathcal{S}} P(s) \log P(s).\]</span> En multipliant par <span
class="math inline">\(-\Pi\)</span> et en utilisant la définition de
l’entropie de survie, on obtient : <span
class="math display">\[H_{\text{survie}} \leq -\Pi \log \Pi.\]</span>
L’égalité a lieu si et seulement si <span class="math inline">\(Q(s) =
P(s)\)</span> pour tout <span class="math inline">\(s \in
\mathcal{S}\)</span>, c’est-à-dire si <span class="math inline">\(P(s)
\pi(s)\)</span> est constant. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie de survie possède plusieurs propriétés intéressantes qui
en font un outil puissant pour l’analyse des systèmes complexes.</p>
<div class="corollaire">
<p>Soient deux systèmes avec des espaces d’états <span
class="math inline">\(\mathcal{S}_1\)</span> et <span
class="math inline">\(\mathcal{S}_2\)</span>, et des entropies de survie
<span class="math inline">\(H_{\text{survie}}^1\)</span> et <span
class="math inline">\(H_{\text{survie}}^2\)</span> respectivement. Si
<span class="math inline">\(\mathcal{S}_1 \subseteq
\mathcal{S}_2\)</span> et si les probabilités de survie <span
class="math inline">\(\pi(s)\)</span> sont identiques pour <span
class="math inline">\(s \in \mathcal{S}_1\)</span>, alors : <span
class="math display">\[H_{\text{survie}}^1 \leq
H_{\text{survie}}^2.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle directement de la définition de
l’entropie de survie et du fait que l’ajout d’états ne peut qu’augmenter
la diversité des états accessibles. ◻</p>
</div>
<div class="corollaire">
<p>Soit un système avec une entropie de survie <span
class="math inline">\(H_{\text{survie}}\)</span>. Si le système est
stable, c’est-à-dire que <span class="math inline">\(\pi(s) = 1\)</span>
pour tout <span class="math inline">\(s \in \mathcal{S}\)</span>, alors
: <span class="math display">\[H_{\text{survie}} = -\sum_{s \in
\mathcal{S}} P(s) \log P(s).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve est immédiate en substituant <span
class="math inline">\(\pi(s) = 1\)</span> dans la définition de
l’entropie de survie. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de survie offre un cadre mathématique riche pour l’étude
des systèmes complexes. En combinant les concepts de théorie de
l’information et de biologie évolutive, elle permet de quantifier la
robustesse et l’adaptabilité des systèmes. Les théorèmes et propriétés
présentés dans cet article ouvrent la voie à de nombreuses applications,
allant de l’optimisation des réseaux à la modélisation de l’évolution
des espèces.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Cosinus Similarity: A Measure of Orientation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cosinus Similarity: A Measure of Orientation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Dans le vaste domaine de l’analyse des données, la mesure de
similarité entre vecteurs joue un rôle fondamental. Parmi les nombreuses
métriques disponibles, le <em>cosinus similarity</em> se distingue par
sa capacité à capturer l’orientation relative des vecteurs,
indépendamment de leur magnitude. Cette propriété en fait un outil
privilégié dans des domaines aussi variés que le traitement du langage
naturel, la recommandation de produits ou encore l’analyse de réseaux
sociaux.</p>
<p>L’origine du cosinus similarity remonte aux fondements de l’algèbre
linéaire, où le produit scalaire entre deux vecteurs est défini comme la
somme des produits de leurs composantes correspondantes. En normalisant
ce produit par les normes des vecteurs, on obtient une mesure qui varie
entre -1 et 1, reflétant ainsi le degré de similarité angulaire.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la notion de cosinus similarity, considérons deux
vecteurs <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> dans un espace euclidien. Nous
cherchons une mesure qui quantifie à quel point ces vecteurs pointent
dans la même direction. Intuitivement, cette mesure devrait être
maximale lorsque les vecteurs sont colinéaires et minimisée lorsqu’ils
sont opposés.</p>
<p>Formellement, le cosinus similarity entre <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> est défini comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span> deux vecteurs non nuls. Le cosinus similarity
entre <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> est donné par : <span
class="math display">\[\cos(\theta) = \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}\]</span> où <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v}\)</span> désigne le
produit scalaire des vecteurs <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, et <span
class="math inline">\(\|\mathbf{u}\|\)</span>, <span
class="math inline">\(\|\mathbf{v}\|\)</span> leurs normes
respectives.</p>
</div>
<p>Une autre formulation, équivalente mais parfois plus pratique, est la
suivante :</p>
<p><span class="math display">\[\cos(\theta) = \frac{\sum_{i=1}^n u_i
v_i}{\sqrt{\sum_{i=1}^n u_i^2} \sqrt{\sum_{i=1}^n v_i^2}}\]</span></p>
<p>Cette expression met en évidence le fait que le cosinus similarity ne
dépend que des composantes des vecteurs et non de leur longueur.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié au cosinus similarity est celui de la
projection orthogonale, qui relie le produit scalaire à la longueur des
projections.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span> deux vecteurs non nuls. La longueur de la
projection orthogonale de <span
class="math inline">\(\mathbf{u}\)</span> sur <span
class="math inline">\(\mathbf{v}\)</span> est donnée par : <span
class="math display">\[\|\text{proj}_{\mathbf{v}} \mathbf{u}\| =
\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|}\]</span></p>
</div>
<p>Ce théorème est particulièrement utile pour interpréter le cosinus
similarity en termes géométriques. En effet, le cosinus similarity peut
être vu comme une mesure de la longueur relative de la projection de
<span class="math inline">\(\mathbf{u}\)</span> sur <span
class="math inline">\(\mathbf{v}\)</span>, normalisée par la norme de
<span class="math inline">\(\mathbf{u}\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la projection orthogonale, nous
commençons par rappeler que la projection de <span
class="math inline">\(\mathbf{u}\)</span> sur <span
class="math inline">\(\mathbf{v}\)</span> est donnée par : <span
class="math display">\[\text{proj}_{\mathbf{v}} \mathbf{u} = \left(
\frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{v} \cdot \mathbf{v}} \right)
\mathbf{v}\]</span></p>
<p>La longueur de cette projection est alors : <span
class="math display">\[\|\text{proj}_{\mathbf{v}} \mathbf{u}\| = \left\|
\left( \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{v} \cdot \mathbf{v}}
\right) \mathbf{v} \right\| = \frac{|\mathbf{u} \cdot
\mathbf{v}|}{\|\mathbf{v}\|^2} \|\mathbf{v}\| = \frac{|\mathbf{u} \cdot
\mathbf{v}|}{\|\mathbf{v}\|}\]</span></p>
<p>En supposant que <span class="math inline">\(\mathbf{u}\)</span> et
<span class="math inline">\(\mathbf{v}\)</span> pointent dans la même
direction, nous pouvons omettre la valeur absolue et obtenir : <span
class="math display">\[\|\text{proj}_{\mathbf{v}} \mathbf{u}\| =
\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|}\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le cosinus similarity possède plusieurs propriétés intéressantes, que
nous énumérons ci-dessous :</p>
<ol>
<li><p>**Bornes** : Le cosinus similarity est borné entre -1 et 1. Plus
précisément, pour tout <span class="math inline">\(\mathbf{u},
\mathbf{v} \in \mathbb{R}^n\)</span> non nuls, on a : <span
class="math display">\[-1 \leq \cos(\theta) \leq 1\]</span> Cette
propriété découle directement de l’inégalité de Cauchy-Schwarz.</p></li>
<li><p>**Invariance par Scaling** : Le cosinus similarity est invariant
par multiplication scalaire. Autrement dit, pour tout <span
class="math inline">\(\alpha \neq 0\)</span>, on a : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{v}}) =
\cos(\theta_{\alpha \mathbf{u}, \alpha \mathbf{v}})\]</span> Cette
propriété est immédiate en remarquant que les normes et le produit
scalaire sont homogènes de degré 1.</p></li>
<li><p>**Symétrie** : Le cosinus similarity est une fonction symétrique.
Pour tout <span class="math inline">\(\mathbf{u}, \mathbf{v} \in
\mathbb{R}^n\)</span>, on a : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{v}}) =
\cos(\theta_{\mathbf{v}, \mathbf{u}})\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le cosinus similarity est une mesure puissante et polyvalente de la
similarité entre vecteurs. Son interprétation géométrique en fait un
outil précieux pour l’analyse des données, tandis que ses propriétés
mathématiques en garantissent la robustesse et l’efficacité. Que ce soit
pour comparer des documents textuels, recommander des produits ou
analyser des réseaux sociaux, le cosinus similarity continue de jouer un
rôle central dans de nombreuses applications pratiques.</p>
</body>
</html>
{% include "footer.html" %}


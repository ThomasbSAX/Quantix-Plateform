{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Quadratique : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Quadratique : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie quadratique, une notion centrale en théorie de
l’information et en statistique, émerge comme un outil puissant pour
mesurer la dispersion ou l’incertitude d’une distribution de
probabilité. Son origine remonte aux travaux pionniers de Shannon sur
l’entropie classique, mais c’est dans le cadre des espaces de Hilbert et
des formes quadratiques que cette mesure a trouvé une généralisation
élégante.</p>
<p>L’entropie quadratique se révèle indispensable dans divers domaines,
notamment en apprentissage automatique, où elle permet de quantifier la
complexité des modèles, ou encore en physique statistique, pour décrire
les états d’équilibre. Son importance réside dans sa capacité à capturer
des propriétés structurelles profondes tout en restant calculable dans
des cadres complexes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie quadratique, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons une distribution de probabilité
discrète <span class="math inline">\(p\)</span> sur un ensemble fini
<span class="math inline">\(\mathcal{X}\)</span>. Nous voulons
quantifier à quel point cette distribution est "diffuse" ou
"concentrée". Une première approche pourrait être de mesurer la
variance, mais cela ne capture pas entièrement l’incertitude. L’entropie
quadratique offre une alternative plus riche.</p>
<div class="definition">
<p>Soit <span class="math inline">\(p = (p_1, \ldots, p_n)\)</span> une
distribution de probabilité discrète sur un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, \ldots, x_n\}\)</span>.
L’entropie quadratique de <span class="math inline">\(p\)</span> est
définie par : <span class="math display">\[H_2(p) = -\sum_{i=1}^n p_i^2
\log p_i.\]</span> De manière équivalente, pour toute distribution de
probabilité <span class="math inline">\(p\)</span> sur un espace
mesurable <span class="math inline">\((\mathcal{X},
\mathcal{F})\)</span>, l’entropie quadratique s’écrit : <span
class="math display">\[H_2(p) = -\int_{\mathcal{X}} p(x)^2 \log p(x) \,
dx.\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental relie l’entropie quadratique à la divergence
de Kullback-Leibler, une mesure de distance entre distributions.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. Alors, l’entropie
quadratique de <span class="math inline">\(p\)</span> peut être exprimée
en termes de la divergence de Kullback-Leibler comme suit : <span
class="math display">\[H_2(p) = \min_{q} D_{\text{KL}}(p \| q),\]</span>
où <span class="math inline">\(D_{\text{KL}}(p \| q) = \sum_{i=1}^n p_i
\log \left( \frac{p_i}{q_i} \right)\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème précédent, nous utilisons des outils de
l’analyse convexe et de la théorie de l’information.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction <span
class="math inline">\(f(x) = x \log x\)</span>. Cette fonction est
strictement convexe sur <span class="math inline">\((0,
+\infty)\)</span>. Par le théorème de Jensen, pour toute distribution de
probabilité <span class="math inline">\(p\)</span>, nous avons : <span
class="math display">\[\sum_{i=1}^n p_i f\left( \frac{p_i}{q_i} \right)
\geq f\left( \sum_{i=1}^n p_i \frac{p_i}{q_i} \right).\]</span> En
développant et en utilisant la définition de <span
class="math inline">\(D_{\text{KL}}(p \| q)\)</span>, nous obtenons :
<span class="math display">\[\sum_{i=1}^n p_i \log \left(
\frac{p_i}{q_i} \right) \geq -\sum_{i=1}^n p_i^2 \log q_i.\]</span> En
minimisant cette expression sur <span class="math inline">\(q\)</span>,
nous trouvons que le minimum est atteint lorsque <span
class="math inline">\(q_i = p_i^2\)</span>. Ainsi, nous avons : <span
class="math display">\[H_2(p) = \min_{q} D_{\text{KL}}(p \|
q).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie quadratique possède plusieurs propriétés intéressantes,
que nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Non-Négativité</strong> : Pour toute distribution de
probabilité <span class="math inline">\(p\)</span>, nous avons <span
class="math inline">\(H_2(p) \geq 0\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La non-négativité découle directement de la
définition et du fait que <span class="math inline">\(x \log x \geq
0\)</span> pour tout <span class="math inline">\(x &gt;
0\)</span>. ◻</p>
</div></li>
<li><p><strong>Maximisation</strong> : L’entropie quadratique est
maximisée par la distribution uniforme.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(p\)</span> une
distribution de probabilité discrète sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La distribution uniforme
<span class="math inline">\(u\)</span> sur <span
class="math inline">\(\mathcal{X}\)</span> vérifie <span
class="math inline">\(u_i = \frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>. Nous avons : <span
class="math display">\[H_2(u) = -\sum_{i=1}^n \left( \frac{1}{n}
\right)^2 \log \left( \frac{1}{n} \right) = \log n.\]</span> Pour toute
autre distribution <span class="math inline">\(p\)</span>, nous avons
<span class="math inline">\(H_2(p) \leq \log n\)</span>. ◻</p>
</div></li>
<li><p><strong>Sous-additivité</strong> : Pour deux distributions de
probabilité indépendantes <span class="math inline">\(p\)</span> et
<span class="math inline">\(q\)</span>, nous avons : <span
class="math display">\[H_2(p \otimes q) = H_2(p) + H_2(q).\]</span></p>
<div class="proof">
<p><em>Proof.</em> La preuve découle directement de la définition et de
l’indépendance des distributions. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie quadratique se révèle être un outil puissant et polyvalent
pour mesurer l’incertitude et la complexité des distributions de
probabilité. Ses propriétés mathématiques profondes, ainsi que ses
applications pratiques dans divers domaines, en font un sujet d’étude
riche et fascinant. Les travaux futurs pourraient explorer des
généralisations de cette notion dans des cadres plus complexes, tels que
les espaces de Banach ou les distributions sur des variétés.</p>
</body>
</html>
{% include "footer.html" %}


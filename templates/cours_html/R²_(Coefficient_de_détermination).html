{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>R^2 (Coefficient de Détermination)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong><span class="math inline">\(R^2\)</span>
(Coefficient de Détermination)</strong></h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>En statistique, l’analyse de la variance est un outil fondamental
pour comprendre la relation entre des variables quantitatives. Parmi les
nombreux indicateurs disponibles, le coefficient de détermination, noté
<span class="math inline">\(R^2\)</span>, occupe une place centrale.
Introduit initialement par Sir Ronald Aylmer Fisher, ce coefficient
mesure la proportion de la variance totale d’une variable dépendante qui
est expliquée par un modèle de régression linéaire. Son importance
réside dans sa capacité à évaluer l’adéquation d’un modèle aux données
observées, fournissant ainsi une mesure objective de la qualité du
modèle.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir le coefficient de détermination, commençons par
comprendre ce que nous cherchons à mesurer. Supposons que nous ayons un
ensemble de données <span class="math inline">\((y_i, x_{i1}, \ldots,
x_{ip})\)</span> pour <span class="math inline">\(i = 1, \ldots,
n\)</span>, où <span class="math inline">\(y_i\)</span> est la variable
dépendante et <span class="math inline">\(x_{i1}, \ldots,
x_{ip}\)</span> sont les variables indépendantes. Nous cherchons à
quantifier dans quelle mesure le modèle de régression linéaire <span
class="math inline">\(y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p
x_p\)</span> explique la variance de <span
class="math inline">\(y\)</span>.</p>
<p>La définition formelle du coefficient de détermination est donnée par
:</p>
<p><span class="math display">\[R^2 = 1 -
\frac{\text{SSE}}{\text{SST}}\]</span></p>
<p>où : - <span class="math inline">\(\text{SSE}\)</span> (Sum of
Squares Error) est la somme des carrés des résidus, définie par <span
class="math inline">\(\sum_{i=1}^n (y_i - \hat{y}_i)^2\)</span>. - <span
class="math inline">\(\text{SST}\)</span> (Total Sum of Squares) est la
somme totale des carrés des écarts à la moyenne, définie par <span
class="math inline">\(\sum_{i=1}^n (y_i - \bar{y})^2\)</span>.</p>
<p>Autrement dit, <span class="math inline">\(R^2\)</span> se calcule
comme :</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n (y_i -
\hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié au coefficient de détermination est le
suivant :</p>
<div class="theorem">
<p>Soit un modèle de régression linéaire <span class="math inline">\(y =
\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p\)</span>. Le coefficient de
détermination <span class="math inline">\(R^2\)</span> satisfait les
propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(0 \leq R^2 \leq 1\)</span>.</p></li>
<li><p><span class="math inline">\(R^2\)</span> est une mesure de la
qualité d’ajustement du modèle, où un <span
class="math inline">\(R^2\)</span> proche de 1 indique un bon
ajustement.</p></li>
<li><p><span class="math inline">\(R^2\)</span> peut être interprété
comme le carré du coefficient de corrélation entre les valeurs observées
<span class="math inline">\(y_i\)</span> et les valeurs prédites <span
class="math inline">\(\hat{y}_i\)</span>.</p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème ci-dessus, procédons étape par étape.</p>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p>Par définition, <span class="math inline">\(R^2 = 1 -
\frac{\text{SSE}}{\text{SST}}\)</span>. Puisque <span
class="math inline">\(\text{SSE} \geq 0\)</span> et <span
class="math inline">\(\text{SST} &gt; 0\)</span>, il s’ensuit que <span
class="math inline">\(R^2 \leq 1\)</span>. De plus, comme <span
class="math inline">\(\text{SSE} \leq \text{SST}\)</span> (car les
résidus minimisent la somme des carrés des erreurs), nous avons <span
class="math inline">\(R^2 \geq 0\)</span>.</p></li>
<li><p>La qualité d’ajustement du modèle est évaluée par la proportion
de variance expliquée. Un <span class="math inline">\(R^2\)</span>
proche de 1 signifie que la majeure partie de la variance de <span
class="math inline">\(y\)</span> est expliquée par le modèle.</p></li>
<li><p>Le coefficient de corrélation <span
class="math inline">\(r\)</span> entre <span
class="math inline">\(y_i\)</span> et <span
class="math inline">\(\hat{y}_i\)</span> est donné par : <span
class="math display">\[r = \frac{\sum_{i=1}^n (y_i - \bar{y})(\hat{y}_i
- \overline{\hat{y}})}{\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}
\sqrt{\sum_{i=1}^n (\hat{y}_i - \overline{\hat{y}})^2}}\]</span> En
utilisant les propriétés des moindres carrés, on peut montrer que <span
class="math inline">\(R^2 = r^2\)</span>.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le coefficient de détermination possède plusieurs propriétés
importantes :</p>
<ol>
<li><p><strong>Propriété d’additivité</strong> : Si l’on ajoute une
nouvelle variable indépendante au modèle, <span
class="math inline">\(R^2\)</span> ne peut qu’augmenter ou rester
constant. Cela signifie que l’ajout de variables n’améliore pas
nécessairement la qualité du modèle.</p></li>
<li><p><strong>Interprétation géométrique</strong> : <span
class="math inline">\(R^2\)</span> peut être interprété comme la
proportion de la variance totale de <span
class="math inline">\(y\)</span> qui est expliquée par le modèle. Plus
précisément, il mesure la proportion de la variance de <span
class="math inline">\(y\)</span> qui est capturée par l’hyperplan de
régression.</p></li>
<li><p><strong>Limites</strong> : <span
class="math inline">\(R^2\)</span> ne doit pas être utilisé seul pour
évaluer la qualité d’un modèle. Il est important de considérer également
d’autres critères tels que le critère d’information d’Akaike (AIC) ou le
critère d’information bayésien (BIC).</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> est un outil puissant pour évaluer la
qualité d’un modèle de régression linéaire. Sa capacité à quantifier la
proportion de variance expliquée en fait un indicateur essentiel dans
l’analyse des données. Cependant, il est crucial de l’utiliser en
conjonction avec d’autres critères pour obtenir une évaluation complète
et objective du modèle.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Cross-Entropy Loss: A Fundamental Concept in Machine Learning</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cross-Entropy Loss: A Fundamental Concept in Machine
Learning</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, en tant que domaine interdisciplinaire,
repose sur des principes mathématiques solides pour modéliser et
comprendre les phénomènes complexes. Parmi ces principes, la notion de
<strong>Cross-Entropy Loss</strong> émerge comme un pilier fondamental.
Historiquement, cette fonction de perte trouve ses racines dans la
théorie de l’information, où elle a été initialement introduite pour
mesurer la divergence entre deux distributions de probabilité. Son
importance en apprentissage automatique réside dans sa capacité à
quantifier l’écart entre les prédictions d’un modèle et les étiquettes
réelles, guidant ainsi l’optimisation du modèle vers des performances
accrues.</p>
<p>La Cross-Entropy Loss est indispensable dans le cadre de la
classification, où elle permet de transformer un problème complexe de
prédiction en un problème d’optimisation bien défini. Son utilisation
est particulièrement répandue dans les réseaux de neurones, où elle sert
de fonction objective pour entraîner des modèles à classer correctement
les données. En outre, sa forme mathématique élégante et ses propriétés
optimales en font un outil privilégié pour les chercheurs et les
praticiens du domaine.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la Cross-Entropy Loss, il est essentiel de saisir
d’abord le concept de divergence de Kullback-Leibler (KL). Imaginons que
nous ayons deux distributions de probabilité, <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, où <span
class="math inline">\(P\)</span> représente la distribution réelle des
données et <span class="math inline">\(Q\)</span> la distribution
prédite par notre modèle. Nous cherchons une mesure de l’information
perdue lorsque <span class="math inline">\(Q\)</span> est utilisée pour
approximer <span class="math inline">\(P\)</span>. Cette mesure, connue
sous le nom de divergence KL, est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La divergence de
Kullback-Leibler de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right)\]</span></p>
</div>
<p>La Cross-Entropy Loss est étroitement liée à la divergence KL. En
effet, elle peut être vue comme une version négative de l’entropie
croisée entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Pour une distribution réelle <span
class="math inline">\(P\)</span> et une distribution prédite <span
class="math inline">\(Q\)</span>, la Cross-Entropy Loss est définie
comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La Cross-Entropy Loss de
<span class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span> est définie par : <span
class="math display">\[H(P, Q) = - \sum_{x \in \mathcal{X}} P(x) \log
Q(x)\]</span></p>
</div>
<p>Il est important de noter que la Cross-Entropy Loss peut également
être exprimée en termes de divergence KL et d’entropie de <span
class="math inline">\(P\)</span> : <span class="math display">\[H(P, Q)
= H(P) + D_{KL}(P \| Q)\]</span> où <span
class="math inline">\(H(P)\)</span> est l’entropie de la distribution
<span class="math inline">\(P\)</span>, définie par : <span
class="math display">\[H(P) = - \sum_{x \in \mathcal{X}} P(x) \log
P(x)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>La Cross-Entropy Loss possède plusieurs propriétés théoriques
intéressantes. L’une des plus importantes est son lien avec la
divergence KL, qui permet de minimiser la Cross-Entropy Loss pour
approximer au mieux la distribution réelle <span
class="math inline">\(P\)</span> par la distribution prédite <span
class="math inline">\(Q\)</span>.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La minimisation de la
Cross-Entropy Loss <span class="math inline">\(H(P, Q)\)</span> est
équivalente à la minimisation de la divergence KL <span
class="math inline">\(D_{KL}(P \| Q)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver ce théorème, nous partons de la
définition de la Cross-Entropy Loss : <span class="math display">\[H(P,
Q) = - \sum_{x \in \mathcal{X}} P(x) \log Q(x)\]</span> En utilisant la
relation entre la Cross-Entropy Loss et la divergence KL, nous avons :
<span class="math display">\[H(P, Q) = H(P) + D_{KL}(P \| Q)\]</span>
Puisque <span class="math inline">\(H(P)\)</span> est une constante
indépendante de <span class="math inline">\(Q\)</span>, la minimisation
de <span class="math inline">\(H(P, Q)\)</span> est équivalente à la
minimisation de <span class="math inline">\(D_{KL}(P \| Q)\)</span>.
Ainsi, nous avons prouvé que la minimisation de la Cross-Entropy Loss
est équivalente à la minimisation de la divergence KL. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’utilisation de la Cross-Entropy Loss dans un
contexte pratique, considérons un problème de classification binaire.
Supposons que nous ayons un ensemble de données <span
class="math inline">\(\mathcal{D} = \{ (x_i, y_i) \}_{i=1}^N\)</span>,
où <span class="math inline">\(x_i\)</span> est un vecteur de
caractéristiques et <span class="math inline">\(y_i\)</span> est une
étiquette binaire. Nous cherchons à entraîner un modèle <span
class="math inline">\(f\)</span> qui prédit la probabilité que <span
class="math inline">\(y_i = 1\)</span>.</p>
<p>La Cross-Entropy Loss pour ce problème est définie comme suit : <span
class="math display">\[\mathcal{L}(f) = - \frac{1}{N} \sum_{i=1}^N
\left[ y_i \log f(x_i) + (1 - y_i) \log (1 - f(x_i))
\right]\]</span></p>
<p>Pour minimiser cette perte, nous pouvons utiliser des méthodes
d’optimisation telles que la descente de gradient. La dérivée de la
Cross-Entropy Loss par rapport aux paramètres du modèle <span
class="math inline">\(f\)</span> est donnée par : <span
class="math display">\[\frac{\partial \mathcal{L}(f)}{\partial f(x_i)} =
- \frac{y_i}{f(x_i)} + \frac{1 - y_i}{1 - f(x_i)}\]</span></p>
<p>Cette dérivée peut être utilisée pour mettre à jour les paramètres du
modèle de manière itérative, en utilisant par exemple l’algorithme de
descente de gradient stochastique (SGD).</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La Cross-Entropy Loss possède plusieurs propriétés intéressantes qui
en font un outil puissant pour l’apprentissage automatique. Nous en
listons quelques-unes ci-dessous :</p>
<ol>
<li><p>La Cross-Entropy Loss est toujours non négative, c’est-à-dire que
<span class="math inline">\(H(P, Q) \geq 0\)</span> pour toutes
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>. Cette propriété découle du fait
que la divergence KL est toujours non négative.</p></li>
<li><p>La Cross-Entropy Loss atteint son minimum lorsque <span
class="math inline">\(Q = P\)</span>. Cela signifie que la distribution
prédite <span class="math inline">\(Q\)</span> est égale à la
distribution réelle <span class="math inline">\(P\)</span>, ce qui est
l’objectif de tout modèle d’apprentissage automatique.</p></li>
<li><p>La Cross-Entropy Loss est convexe en <span
class="math inline">\(Q\)</span>, ce qui signifie que la fonction est
convexe pour une distribution fixe <span
class="math inline">\(P\)</span>. Cette propriété est cruciale pour
l’optimisation, car elle garantit que la minimisation de la
Cross-Entropy Loss est un problème d’optimisation convexe.</p></li>
</ol>
<p>Pour prouver la propriété (i), nous utilisons le fait que la
divergence KL est toujours non négative. En effet, selon l’inégalité de
Gibbs, nous avons : <span class="math display">\[D_{KL}(P \| Q) \geq
0\]</span> En utilisant la relation entre la Cross-Entropy Loss et la
divergence KL, nous avons : <span class="math display">\[H(P, Q) = H(P)
+ D_{KL}(P \| Q) \geq H(P) \geq 0\]</span> où la dernière inégalité
découle du fait que l’entropie <span class="math inline">\(H(P)\)</span>
est toujours non négative.</p>
<p>Pour prouver la propriété (ii), nous considérons le cas où <span
class="math inline">\(Q = P\)</span>. Dans ce cas, la Cross-Entropy Loss
est donnée par : <span class="math display">\[H(P, P) = - \sum_{x \in
\mathcal{X}} P(x) \log P(x) = H(P)\]</span> Puisque <span
class="math inline">\(H(P)\)</span> est la valeur minimale de l’entropie
pour une distribution donnée, nous avons : <span
class="math display">\[H(P, Q) \geq H(P, P)\]</span> Ce qui prouve que
la Cross-Entropy Loss atteint son minimum lorsque <span
class="math inline">\(Q = P\)</span>.</p>
<p>Pour prouver la propriété (iii), nous utilisons le fait que la
fonction <span class="math inline">\(- \log Q(x)\)</span> est convexe en
<span class="math inline">\(Q(x)\)</span>. En effet, la fonction
logarithmique est concave, et donc <span class="math inline">\(- \log
Q(x)\)</span> est convexe. Puisque la somme de fonctions convexes est
convexe, la Cross-Entropy Loss <span class="math inline">\(H(P,
Q)\)</span> est convexe en <span class="math inline">\(Q\)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La Cross-Entropy Loss est un concept fondamental en apprentissage
automatique, qui trouve ses racines dans la théorie de l’information.
Son utilisation permet de transformer des problèmes complexes de
prédiction en problèmes d’optimisation bien définis. Les propriétés
théoriques de la Cross-Entropy Loss, telles que sa non négativité et sa
convexité, en font un outil puissant pour l’entraînement des modèles
d’apprentissage automatique. En comprenant et en utilisant la
Cross-Entropy Loss, les chercheurs et les praticiens peuvent développer
des modèles plus performants et plus robustes.</p>
</body>
</html>
{% include "footer.html" %}


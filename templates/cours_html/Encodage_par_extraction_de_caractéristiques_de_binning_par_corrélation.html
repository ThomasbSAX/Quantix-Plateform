{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par corrélation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par corrélation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de binning par
corrélation est une technique avancée utilisée dans le traitement des
données catégorielles. Cette méthode émerge de la nécessité de
transformer des variables catégorielles en variables numériques tout en
préservant les relations sous-jacentes entre les catégories. Le binning
par corrélation permet de regrouper les catégories en fonction de leur
similarité, ce qui facilite l’analyse et la modélisation des
données.</p>
<p>L’origine de cette technique remonte aux travaux pionniers en analyse
de données et en apprentissage automatique. Elle est particulièrement
utile dans les domaines où les données catégorielles sont omniprésentes,
comme la bioinformatique, la finance et la sociologie. L’encodage par
extraction de caractéristiques de binning par corrélation permet non
seulement de réduire la dimensionnalité des données, mais aussi
d’améliorer la performance des modèles prédictifs.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques de
binning par corrélation, il est essentiel de définir quelques concepts
clés.</p>
<h2 id="binning">Binning</h2>
<p>Le binning est une technique qui consiste à regrouper les catégories
d’une variable catégorielle en bins (ou intervalles) en fonction de leur
similarité. Formellement, soit <span class="math inline">\(C = \{c_1,
c_2, \ldots, c_n\}\)</span> l’ensemble des catégories d’une variable
catégorielle <span class="math inline">\(X\)</span>. Un binning est une
partition de <span class="math inline">\(C\)</span> en sous-ensembles
disjoints <span class="math inline">\(B = \{B_1, B_2, \ldots,
B_k\}\)</span> tels que :</p>
<p><span class="math display">\[\bigcup_{i=1}^k B_i = C \quad \text{et}
\quad \forall i \neq j, B_i \cap B_j = \emptyset\]</span></p>
<h2 id="corrélation">Corrélation</h2>
<p>La corrélation mesure le degré de relation linéaire entre deux
variables. Pour une variable catégorielle <span
class="math inline">\(X\)</span> et une variable numérique <span
class="math inline">\(Y\)</span>, la corrélation peut être mesurée par
le coefficient de corrélation de Pearson ou de Spearman. Formellement,
le coefficient de corrélation de Pearson entre <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> est défini comme :</p>
<p><span class="math display">\[\rho(X, Y) = \frac{\text{Cov}(X,
Y)}{\sigma_X \sigma_Y}\]</span></p>
<p>où <span class="math inline">\(\text{Cov}(X, Y)\)</span> est la
covariance entre <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, et <span
class="math inline">\(\sigma_X\)</span> et <span
class="math inline">\(\sigma_Y\)</span> sont les écarts-types de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>, respectivement.</p>
<h2 id="encodage-par-extraction-de-caractéristiques">Encodage par
Extraction de Caractéristiques</h2>
<p>L’encodage par extraction de caractéristiques consiste à transformer
une variable catégorielle en une ou plusieurs variables numériques en
utilisant des caractéristiques extraites des données. Formellement, soit
<span class="math inline">\(X\)</span> une variable catégorielle et
<span class="math inline">\(Y\)</span> une variable numérique.
L’encodage par extraction de caractéristiques est une fonction <span
class="math inline">\(f: C \rightarrow \mathbb{R}^m\)</span> qui associe
à chaque catégorie <span class="math inline">\(c_i\)</span> un vecteur
de caractéristiques <span class="math inline">\(f(c_i) = (f_1(c_i),
f_2(c_i), \ldots, f_m(c_i))\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-lencodage-optimal">Théorème de l’Encodage
Optimal</h2>
<p>Le théorème de l’encodage optimal stipule que l’encodage par
extraction de caractéristiques de binning par corrélation est optimal si
les bins sont formés de manière à maximiser la corrélation entre les
variables encodées et la variable cible. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle et <span
class="math inline">\(Y\)</span> une variable numérique. L’encodage
optimal est défini comme suit :</p>
<p><span class="math display">\[\forall i, \exists B_i \subseteq C
\text{ tel que } \rho(f(B_i), Y) = \max_{B \subseteq C} \rho(f(B),
Y)\]</span></p>
<p>où <span class="math inline">\(f\)</span> est une fonction d’encodage
et <span class="math inline">\(\rho\)</span> est le coefficient de
corrélation de Pearson.</p>
<h2 id="démonstration-du-théorème-de-lencodage-optimal">Démonstration du
Théorème de l’Encodage Optimal</h2>
<p>Pour démontrer le théorème de l’encodage optimal, nous devons montrer
que les bins formés en maximisant la corrélation entre les variables
encodées et la variable cible sont optimaux. Supposons que nous avons
une variable catégorielle <span class="math inline">\(X\)</span> avec
<span class="math inline">\(n\)</span> catégories et une variable
numérique <span class="math inline">\(Y\)</span>. Nous voulons former
des bins <span class="math inline">\(B_1, B_2, \ldots, B_k\)</span> tels
que la corrélation entre les variables encodées et <span
class="math inline">\(Y\)</span> est maximisée.</p>
<p>1. **Formation des Bins** : Nous formons les bins en regroupant les
catégories qui ont une corrélation élevée avec <span
class="math inline">\(Y\)</span>. Cela peut être fait en utilisant des
algorithmes de clustering ou des méthodes de binning supervisé.</p>
<p>2. **Encodage des Bins** : Pour chaque bin <span
class="math inline">\(B_i\)</span>, nous calculons une caractéristique
numérique <span class="math inline">\(f(B_i)\)</span> qui représente la
moyenne ou la médiane des valeurs de <span
class="math inline">\(Y\)</span> pour les catégories dans <span
class="math inline">\(B_i\)</span>.</p>
<p>3. **Maximisation de la Corrélation** : Nous maximisons la
corrélation entre <span class="math inline">\(f(B_i)\)</span> et <span
class="math inline">\(Y\)</span> en ajustant les bins de manière
itérative. Cela peut être fait en utilisant des algorithmes
d’optimisation comme la descente de gradient.</p>
<p>4. **Optimalité** : Nous montrons que les bins formés de cette
manière maximisent la corrélation entre les variables encodées et <span
class="math inline">\(Y\)</span>. Cela est dû au fait que la corrélation
est une mesure de la relation linéaire entre les variables, et en
maximisant cette mesure, nous obtenons un encodage optimal.</p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-loptimalité-de-lencodage">Preuve de l’Optimalité de
l’Encodage</h2>
<p>Pour prouver que l’encodage par extraction de caractéristiques de
binning par corrélation est optimal, nous devons montrer que les bins
formés en maximisant la corrélation entre les variables encodées et la
variable cible sont optimaux. Supposons que nous avons une variable
catégorielle <span class="math inline">\(X\)</span> avec <span
class="math inline">\(n\)</span> catégories et une variable numérique
<span class="math inline">\(Y\)</span>. Nous voulons former des bins
<span class="math inline">\(B_1, B_2, \ldots, B_k\)</span> tels que la
corrélation entre les variables encodées et <span
class="math inline">\(Y\)</span> est maximisée.</p>
<p>1. **Formation des Bins** : Nous formons les bins en regroupant les
catégories qui ont une corrélation élevée avec <span
class="math inline">\(Y\)</span>. Cela peut être fait en utilisant des
algorithmes de clustering ou des méthodes de binning supervisé.</p>
<p>2. **Encodage des Bins** : Pour chaque bin <span
class="math inline">\(B_i\)</span>, nous calculons une caractéristique
numérique <span class="math inline">\(f(B_i)\)</span> qui représente la
moyenne ou la médiane des valeurs de <span
class="math inline">\(Y\)</span> pour les catégories dans <span
class="math inline">\(B_i\)</span>.</p>
<p>3. **Maximisation de la Corrélation** : Nous maximisons la
corrélation entre <span class="math inline">\(f(B_i)\)</span> et <span
class="math inline">\(Y\)</span> en ajustant les bins de manière
itérative. Cela peut être fait en utilisant des algorithmes
d’optimisation comme la descente de gradient.</p>
<p>4. **Optimalité** : Nous montrons que les bins formés de cette
manière maximisent la corrélation entre les variables encodées et <span
class="math inline">\(Y\)</span>. Cela est dû au fait que la corrélation
est une mesure de la relation linéaire entre les variables, et en
maximisant cette mesure, nous obtenons un encodage optimal.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-la-corrélation-maximale">Propriété de la
Corrélation Maximale</h2>
<p>La propriété de la corrélation maximale stipule que l’encodage par
extraction de caractéristiques de binning par corrélation maximise la
corrélation entre les variables encodées et la variable cible.
Formellement, soit <span class="math inline">\(X\)</span> une variable
catégorielle et <span class="math inline">\(Y\)</span> une variable
numérique. L’encodage par extraction de caractéristiques de binning par
corrélation satisfait la propriété suivante :</p>
<p><span class="math display">\[\forall i, \exists B_i \subseteq C
\text{ tel que } \rho(f(B_i), Y) = \max_{B \subseteq C} \rho(f(B),
Y)\]</span></p>
<p>où <span class="math inline">\(f\)</span> est une fonction d’encodage
et <span class="math inline">\(\rho\)</span> est le coefficient de
corrélation de Pearson.</p>
<h2 id="corollaire-de-lencodage-optimal">Corollaire de l’Encodage
Optimal</h2>
<p>Le corollaire de l’encodage optimal stipule que l’encodage par
extraction de caractéristiques de binning par corrélation est optimal si
les bins sont formés de manière à maximiser la corrélation entre les
variables encodées et la variable cible. Formellement, soit <span
class="math inline">\(X\)</span> une variable catégorielle et <span
class="math inline">\(Y\)</span> une variable numérique. L’encodage
optimal est défini comme suit :</p>
<p><span class="math display">\[\forall i, \exists B_i \subseteq C
\text{ tel que } \rho(f(B_i), Y) = \max_{B \subseteq C} \rho(f(B),
Y)\]</span></p>
<p>où <span class="math inline">\(f\)</span> est une fonction d’encodage
et <span class="math inline">\(\rho\)</span> est le coefficient de
corrélation de Pearson.</p>
</body>
</html>
{% include "footer.html" %}


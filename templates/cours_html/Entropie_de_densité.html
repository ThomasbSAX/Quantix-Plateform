{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Densité : Un Concept Fondamental en Théorie de l’Information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Densité : Un Concept Fondamental en
Théorie de l’Information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de densité, un concept central en théorie de l’information
et en traitement du signal, trouve ses racines dans les travaux
pionniers de Claude Shannon. Ce concept émerge comme une réponse à la
nécessité de quantifier l’incertitude et l’information contenue dans des
distributions de probabilité continues. L’entropie de densité est
indispensable pour comprendre et analyser les systèmes où l’information
est représentée par des variables aléatoires continues, telles que le
bruit thermique dans les systèmes de communication ou les signaux
biologiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de densité, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons une variable aléatoire continue
<span class="math inline">\(X\)</span> avec une densité de probabilité
<span class="math inline">\(f_X(x)\)</span>. Nous voulons quantifier
l’incertitude ou l’information contenue dans cette densité. L’entropie
de densité est une mesure qui capture cette incertitude.</p>
<p>Formellement, l’entropie de densité <span
class="math inline">\(h(X)\)</span> d’une variable aléatoire continue
<span class="math inline">\(X\)</span> avec une densité de probabilité
<span class="math inline">\(f_X(x)\)</span> est définie comme :</p>
<p><span class="math display">\[h(X) = -\int_{-\infty}^{\infty} f_X(x)
\log f_X(x) \, dx\]</span></p>
<p>Cette intégrale est prise sur tout l’espace de <span
class="math inline">\(X\)</span>. L’entropie de densité est une mesure
de l’incertitude moyenne associée à la variable aléatoire <span
class="math inline">\(X\)</span>. Plus la densité <span
class="math inline">\(f_X(x)\)</span> est "étalée", plus l’entropie de
densité sera élevée, indiquant une grande incertitude.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de densité est le théorème
de l’entropie différentielle, qui établit une relation entre l’entropie
de densité et l’entropie d’une variable aléatoire discrète. Ce théorème
est crucial pour comprendre les limites de la compression de données et
le traitement du signal.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
continue avec une densité de probabilité <span
class="math inline">\(f_X(x)\)</span>. Si nous quantifions <span
class="math inline">\(X\)</span> en utilisant un quantificateur uniforme
avec <span class="math inline">\(2^n\)</span> niveaux, l’entropie de la
variable quantifiée <span class="math inline">\(X_q\)</span> est donnée
par :</p>
<p><span class="math display">\[H(X_q) = h(X) - \log \Delta +
\frac{1}{2} \log(2 \pi e)\]</span></p>
<p>où <span class="math inline">\(\Delta\)</span> est le pas de
quantification.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’entropie différentielle, nous
commençons par exprimer l’entropie de la variable quantifiée <span
class="math inline">\(X_q\)</span>. L’entropie <span
class="math inline">\(H(X_q)\)</span> est donnée par :</p>
<p><span class="math display">\[H(X_q) = -\sum_{i=1}^{2^n} p_i \log
p_i\]</span></p>
<p>où <span class="math inline">\(p_i\)</span> est la probabilité que
<span class="math inline">\(X\)</span> tombe dans l’intervalle de
quantification <span class="math inline">\(i\)</span>. En utilisant
l’approximation de l’entropie pour une distribution uniforme, nous avons
:</p>
<p><span class="math display">\[H(X_q) \approx \log(2^n) - \frac{1}{2}
\log(2 \pi e)\]</span></p>
<p>En combinant cette approximation avec l’entropie de densité <span
class="math inline">\(h(X)\)</span>, nous obtenons le résultat
souhaité.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de densité possède plusieurs propriétés importantes :</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie de densité est
toujours non négative, c’est-à-dire <span class="math inline">\(h(X)
\geq 0\)</span>. Cette propriété découle du fait que la fonction
logarithme est concave et que <span
class="math inline">\(f_X(x)\)</span> est une densité de
probabilité.</p></li>
<li><p><strong>Invariance par transformation</strong> : L’entropie de
densité est invariante sous les transformations bijectives. Si <span
class="math inline">\(Y = g(X)\)</span> où <span
class="math inline">\(g\)</span> est une transformation bijective, alors
<span class="math inline">\(h(Y) = h(X)\)</span>.</p></li>
<li><p><strong>Maximisation</strong> : L’entropie de densité est
maximisée lorsque la densité <span class="math inline">\(f_X(x)\)</span>
est uniforme sur un intervalle fini. Cela signifie que l’incertitude est
maximale lorsque toutes les valeurs de <span
class="math inline">\(X\)</span> sont également probables.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie de densité est un concept fondamental en théorie de
l’information et en traitement du signal. Elle permet de quantifier
l’incertitude et l’information contenues dans des distributions de
probabilité continues. Les théorèmes et propriétés associés à l’entropie
de densité sont essentiels pour comprendre les limites de la compression
de données et le traitement du signal. En explorant ces concepts, nous
gagnons une compréhension plus profonde des systèmes où l’information
est représentée par des variables aléatoires continues.</p>
</body>
</html>
{% include "footer.html" %}


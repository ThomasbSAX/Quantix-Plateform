{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La Perplexité : Une Mesure Fondamentale en Théorie de l’Information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La Perplexité : Une Mesure Fondamentale en Théorie de
l’Information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La perplexité, en théorie de l’information, est une mesure qui émerge
naturellement dans le cadre de l’évaluation des modèles probabilistes.
Son origine remonte aux travaux fondamentaux de Claude Shannon, le père
de la théorie de l’information. La perplexité est indispensable pour
quantifier la qualité d’un modèle de langage ou d’un système de
prédiction. Elle permet de répondre à des questions cruciales : comment
évaluer la capacité d’un modèle à prédire une séquence de données ?
Comment comparer différents modèles entre eux ?</p>
<p>La perplexité est particulièrement utile dans le contexte des modèles
de langage, où elle permet de mesurer la capacité d’un modèle à
généraliser à partir des données d’entraînement. Un modèle avec une
perplexité faible est capable de prédire avec précision les mots
suivants dans une séquence, ce qui est essentiel pour des applications
telles que la traduction automatique, la génération de texte et le
traitement du langage naturel.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la notion de perplexité, commençons par comprendre ce
que nous cherchons à mesurer. Supposons que nous ayons un modèle
probabiliste qui attribue une probabilité à chaque séquence de mots.
Nous voulons évaluer la capacité de ce modèle à prédire correctement les
mots suivants dans une séquence donnée. La perplexité est une mesure qui
quantifie cette capacité.</p>
<p>Formellement, la perplexité d’un modèle <span
class="math inline">\(\mathcal{M}\)</span> pour une séquence de mots
<span class="math inline">\(w_1, w_2, \ldots, w_n\)</span> est définie
comme l’exponentielle de l’entropie croisée moyenne négative. L’entropie
croisée mesure la divergence entre la distribution de probabilité réelle
et celle prédite par le modèle.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{M}\)</span> un modèle
probabiliste et <span class="math inline">\(w_1, w_2, \ldots,
w_n\)</span> une séquence de mots. La perplexité <span
class="math inline">\(\text{PPL}(\mathcal{M}, W)\)</span> est définie
par : <span class="math display">\[\text{PPL}(\mathcal{M}, W) =
\exp\left(-\frac{1}{n} \sum_{i=1}^n \log p_{\mathcal{M}}(w_i |
w_{&lt;i})\right)\]</span> où <span
class="math inline">\(p_{\mathcal{M}}(w_i | w_{&lt;i})\)</span> est la
probabilité prédite par le modèle <span
class="math inline">\(\mathcal{M}\)</span> que le mot <span
class="math inline">\(w_i\)</span> apparaisse étant donné les mots
précédents <span class="math inline">\(w_{&lt;i}\)</span>.</p>
</div>
<p>Une autre manière de formuler la perplexité est en utilisant le
concept d’entropie. L’entropie d’une distribution de probabilité <span
class="math inline">\(p\)</span> est définie comme : <span
class="math display">\[H(p) = -\sum_{x} p(x) \log p(x)\]</span> La
perplexité peut alors être vue comme l’exponentielle de l’entropie
croisée entre la distribution réelle et la distribution prédite par le
modèle.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la perplexité est celui de l’inégalité
de Gibbs, qui relie la perplexité à l’entropie. Ce théorème est
essentiel pour comprendre les limites de la perplexité en tant que
mesure d’évaluation des modèles.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(p\)</span> une distribution de
probabilité réelle et <span class="math inline">\(q\)</span> une
distribution de probabilité prédite par un modèle. Alors, la perplexité
<span class="math inline">\(\text{PPL}(q, p)\)</span> satisfait
l’inégalité suivante : <span class="math display">\[\text{PPL}(q, p)
\geq 2^{H(p)}\]</span> où <span class="math inline">\(H(p)\)</span> est
l’entropie de la distribution réelle <span
class="math inline">\(p\)</span>.</p>
</div>
<p>La démonstration de ce théorème repose sur l’inégalité de Gibbs, qui
stipule que pour toute distribution de probabilité <span
class="math inline">\(p\)</span> et toute distribution de probabilité
<span class="math inline">\(q\)</span>, l’entropie croisée satisfait :
<span class="math display">\[D_{\text{KL}}(p || q) \geq 0\]</span> où
<span class="math inline">\(D_{\text{KL}}\)</span> est la divergence de
Kullback-Leibler. En utilisant cette inégalité, nous pouvons déduire que
: <span class="math display">\[\text{PPL}(q, p) = \exp\left(-\frac{1}{n}
\sum_{i=1}^n \log q(w_i)\right) \geq 2^{H(p)}\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer l’inégalité de Gibbs, nous commençons par rappeler la
définition de la divergence de Kullback-Leibler : <span
class="math display">\[D_{\text{KL}}(p || q) = \sum_{x} p(x) \log
\frac{p(x)}{q(x)}\]</span> En utilisant l’inégalité de Gibbs, nous
savons que : <span class="math display">\[D_{\text{KL}}(p || q) \geq
0\]</span> Ce qui implique que : <span class="math display">\[\sum_{x}
p(x) \log q(x) \leq \sum_{x} p(x) \log p(x)\]</span> En prenant
l’exponentielle de chaque côté, nous obtenons : <span
class="math display">\[\text{PPL}(q, p) \geq 2^{H(p)}\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La perplexité possède plusieurs propriétés importantes qui en font
une mesure robuste pour l’évaluation des modèles. Voici quelques-unes de
ces propriétés :</p>
<ol>
<li><p>La perplexité est toujours positive. En effet, comme
l’exponentielle d’un nombre réel est toujours positive, nous avons :
<span class="math display">\[\text{PPL}(\mathcal{M}, W) &gt;
0\]</span></p></li>
<li><p>La perplexité est minimale lorsque le modèle prédit parfaitement
la distribution réelle. Si <span
class="math inline">\(p_{\mathcal{M}}(w_i | w_{&lt;i}) = p(w_i |
w_{&lt;i})\)</span> pour tout <span class="math inline">\(i\)</span>,
alors : <span class="math display">\[\text{PPL}(\mathcal{M}, W) =
1\]</span></p></li>
<li><p>La perplexité est invariante par changement de base du
logarithme. En effet, si nous changeons la base du logarithme, la
perplexité reste inchangée. Par exemple, en utilisant le logarithme
naturel <span class="math inline">\(\log\)</span> et le logarithme en
base 2 <span class="math inline">\(\log_2\)</span>, nous avons : <span
class="math display">\[\text{PPL}(\mathcal{M}, W) =
\exp\left(-\frac{1}{n} \sum_{i=1}^n \log p_{\mathcal{M}}(w_i |
w_{&lt;i})\right) = 2^{-\frac{1}{n \log 2} \sum_{i=1}^n \log_2
p_{\mathcal{M}}(w_i | w_{&lt;i})}\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La perplexité est une mesure fondamentale en théorie de
l’information, particulièrement utile pour évaluer la qualité des
modèles probabilistes. Elle permet de quantifier la capacité d’un modèle
à prédire correctement les séquences de mots, ce qui est essentiel pour
des applications telles que la traduction automatique et la génération
de texte. Les propriétés et théorèmes associés à la perplexité en font
une mesure robuste et fiable, indispensable dans le domaine du
traitement du langage naturel.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Critère d’information bayésien (BIC)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Critère d’information bayésien (BIC)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le critère d’information bayésien, communément appelé BIC (pour
<em>Bayesian Information Criterion</em>), émerge comme une réponse
élégante à un problème fondamental en statistique : comment sélectionner
le meilleur modèle parmi une famille de modèles possibles, tout en
pénalisant la complexité du modèle pour éviter le surapprentissage.</p>
<p>Historiquement, le BIC a été introduit par Schwarz en 1978 comme une
alternative au critère d’information d’Akaike (AIC). Alors que l’AIC se
concentre sur la prédiction, le BIC vise à identifier le modèle qui est
le plus probable a posteriori, en tenant compte de la vraisemblance des
données et de la complexité du modèle.</p>
<p>Le BIC est particulièrement indispensable dans les contextes où l’on
doit choisir entre plusieurs modèles statistiques, par exemple en
régression linéaire, en classification ou en modélisation de séries
temporelles. Il offre un équilibre entre la qualité de l’ajustement aux
données et la simplicité du modèle, ce qui en fait un outil puissant
pour les chercheurs et les praticiens.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le BIC, commençons par décrire ce que nous cherchons
à obtenir. Supposons que nous ayons un ensemble de données et une
famille de modèles possibles. Nous voulons sélectionner le modèle qui
explique le mieux les données, tout en étant suffisamment simple pour
être généralisable.</p>
<p>Le BIC est une mesure qui combine la vraisemblance des données sous
un modèle donné et la complexité de ce modèle. Plus précisément, le BIC
est défini comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{M}\)</span> un modèle
statistique avec <span class="math inline">\(k\)</span> paramètres, et
soit <span class="math inline">\(\hat{\theta}\)</span> l’estimateur du
maximum de vraisemblance des paramètres de <span
class="math inline">\(\mathcal{M}\)</span>. Le critère d’information
bayésien pour le modèle <span class="math inline">\(\mathcal{M}\)</span>
est donné par : <span class="math display">\[BIC(\mathcal{M}) = -2
\ln(L(\hat{\theta})) + k \ln(n)\]</span> où <span
class="math inline">\(L(\hat{\theta})\)</span> est la vraisemblance
maximisée, <span class="math inline">\(k\)</span> est le nombre de
paramètres du modèle, et <span class="math inline">\(n\)</span> est le
nombre d’observations.</p>
</div>
<p>Cette définition peut être reformulée de plusieurs manières. Par
exemple, en utilisant la log-vraisemblance maximisée <span
class="math inline">\(\ell(\hat{\theta}) =
\ln(L(\hat{\theta}))\)</span>, nous avons : <span
class="math display">\[BIC(\mathcal{M}) = -2 \ell(\hat{\theta}) + k
\ln(n)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié au BIC est le théorème de Schwarz, qui
établit que le BIC sélectionne asymptotiquement le modèle vrai avec
probabilité 1, sous certaines conditions.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{M}_0\)</span> le modèle
vrai, et soit <span class="math inline">\(\mathcal{M}_1, \ldots,
\mathcal{M_m}\)</span> une famille de modèles possibles. Supposons que
le nombre de paramètres de chaque modèle est fixe et que les conditions
régulières du maximum de vraisemblance sont satisfaites. Alors, pour
<span class="math inline">\(n\)</span> suffisamment grand : <span
class="math display">\[\lim_{n \to \infty} P(BIC(\mathcal{M}_0) &lt;
BIC(\mathcal{M}_i)) = 1 \quad \text{pour tout } i = 1, \ldots,
m\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de Schwarz repose sur des résultats
asymptotiques en statistique. Nous allons esquisser les étapes
principales.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un modèle <span
class="math inline">\(\mathcal{M}_i\)</span> avec <span
class="math inline">\(k_i\)</span> paramètres. La log-vraisemblance
maximisée pour <span class="math inline">\(\mathcal{M}_i\)</span> peut
être décomposée comme suit : <span
class="math display">\[\ell(\hat{\theta}_i) = \ell(\theta_0) +
\frac{1}{2} (\hat{\theta}_i - \theta_0)^T I(\theta_0) (\hat{\theta}_i -
\theta_0) + o_p(1)\]</span> où <span
class="math inline">\(\theta_0\)</span> est le vrai paramètre, <span
class="math inline">\(I(\theta_0)\)</span> est la matrice d’information
de Fisher, et <span class="math inline">\(o_p(1)\)</span> désigne un
terme qui tend en probabilité vers 0.</p>
<p>En utilisant cette décomposition, nous pouvons écrire le BIC pour
<span class="math inline">\(\mathcal{M}_i\)</span> comme : <span
class="math display">\[BIC(\mathcal{M}_i) = -2 \ell(\theta_0) -
(\hat{\theta}_i - \theta_0)^T I(\theta_0) (\hat{\theta}_i - \theta_0) +
k_i \ln(n) + o_p(1)\]</span></p>
<p>Pour le modèle vrai <span
class="math inline">\(\mathcal{M}_0\)</span>, nous avons <span
class="math inline">\(k_0\)</span> paramètres et <span
class="math inline">\(\hat{\theta}_0 = \theta_0\)</span>. Par conséquent
: <span class="math display">\[BIC(\mathcal{M}_0) = -2 \ell(\theta_0) +
k_0 \ln(n)\]</span></p>
<p>En comparant <span class="math inline">\(BIC(\mathcal{M}_i)\)</span>
et <span class="math inline">\(BIC(\mathcal{M}_0)\)</span>, nous
obtenons : <span class="math display">\[BIC(\mathcal{M}_i) -
BIC(\mathcal{M}_0) = - (\hat{\theta}_i - \theta_0)^T I(\theta_0)
(\hat{\theta}_i - \theta_0) + (k_i - k_0) \ln(n) + o_p(1)\]</span></p>
<p>Pour <span class="math inline">\(n\)</span> suffisamment grand, le
terme <span class="math inline">\((k_i - k_0) \ln(n)\)</span> domine les
autres termes, ce qui implique que <span
class="math inline">\(BIC(\mathcal{M}_i) &gt;
BIC(\mathcal{M}_0)\)</span> avec probabilité tendant vers 1. Cela prouve
le théorème. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le BIC possède plusieurs propriétés intéressantes, que nous allons
énumérer et démontrer.</p>
<ol>
<li><p>Le BIC pénalise plus fortement les modèles complexes que l’AIC.
Plus précisément, le terme de pénalité dans le BIC est <span
class="math inline">\(k \ln(n)\)</span>, tandis que dans l’AIC, il est
<span class="math inline">\(2k\)</span>. Pour <span
class="math inline">\(n\)</span> grand, <span
class="math inline">\(\ln(n) &gt; 2\)</span>, ce qui signifie que le BIC
favorise les modèles plus simples.</p></li>
<li><p>Le BIC est cohérent asymptotiquement, c’est-à-dire qu’il
sélectionne le modèle vrai avec probabilité tendant vers 1 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini. Cela résulte du
théorème de Schwarz.</p></li>
<li><p>Le BIC peut être utilisé pour comparer des modèles non imbriqués,
c’est-à-dire des modèles qui ne sont pas des cas particuliers les uns
des autres. Cependant, cette utilisation est plus délicate et nécessite
des hypothèses supplémentaires.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le critère d’information bayésien est un outil puissant pour la
sélection de modèles statistiques. Il combine la vraisemblance des
données et la complexité du modèle, offrant un équilibre entre
ajusteement aux données et simplicité. Le théorème de Schwarz garantit
que le BIC sélectionne asymptotiquement le modèle vrai, ce qui en fait
un critère de choix de modèle fiable et largement utilisé.</p>
</body>
</html>
{% include "footer.html" %}


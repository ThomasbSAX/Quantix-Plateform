{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Monge-Kantorovich : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Monge-Kantorovich : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Monge-Kantorovich, également connue sous le nom de
distance de Wasserstein, est une notion centrale en théorie des
probabilités et en optimisation. Elle trouve ses racines dans les
travaux de Gaspard Monge sur le transport optimal au XVIIIe siècle, et a
été formalisée par Leonid Kantorovich dans les années 1940. Cette
divergence mesure la distance entre deux mesures de probabilité en
considérant le coût minimal requis pour transformer une mesure en une
autre.</p>
<p>L’émergence de cette notion est motivée par des problèmes concrets
tels que l’optimisation de flux, la théorie du contrôle optimal, et plus
récemment, dans le domaine de l’apprentissage automatique. La divergence
de Monge-Kantorovich est indispensable pour modéliser des phénomènes où
la structure géométrique et topologique des données joue un rôle
crucial.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Monge-Kantorovich, commençons par
comprendre ce que nous cherchons à mesurer. Imaginons deux distributions
de probabilité sur un espace métrique. Nous voulons quantifier la
différence entre ces deux distributions en tenant compte de la structure
sous-jacente de l’espace.</p>
<p>Formellement, soit <span class="math inline">\((X, d)\)</span> un
espace métrique et <span class="math inline">\(\mu\)</span>, <span
class="math inline">\(\nu\)</span> deux mesures de probabilité sur <span
class="math inline">\(X\)</span>. La divergence de Monge-Kantorovich
d’ordre <span class="math inline">\(p\)</span> entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie comme suit :</p>
<div class="definition">
<p>La divergence de Monge-Kantorovich d’ordre <span
class="math inline">\(p\)</span> entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est donnée par: <span
class="math display">\[W_p(\mu, \nu) = \left( \inf_{\gamma \in
\Gamma(\mu, \nu)} \int_{X \times X} d(x, y)^p \, d\gamma(x, y)
\right)^{1/p}\]</span> où <span class="math inline">\(\Gamma(\mu,
\nu)\)</span> est l’ensemble des mesures couplées entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[W_p(\mu, \nu) = \inf_{\gamma \in \Gamma(\mu,
\nu)} \left( \int_{X \times X} d(x, y)^p \, d\gamma(x, y)
\right)^{1/p}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Monge-Kantorovich est
le théorème de Kantorovich-Rubinstein, qui donne une caractérisation
duale de <span class="math inline">\(W_1\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X, d)\)</span> un espace métrique
compact. Alors: <span class="math display">\[W_1(\mu, \nu) = \sup_{f: X
\to \mathbb{R} \text{ Lipschitz}} \left( \int_X f \, d\mu - \int_X f \,
d\nu \right)\]</span> où <span class="math inline">\(f\)</span> est une
fonction Lipschitz avec <span class="math inline">\(\|f\|_{\text{Lip}}
\leq 1\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kantorovich-Rubinstein, nous utilisons la
théorie des mesures couplées et les propriétés des fonctions
Lipschitz.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une mesure couplée <span
class="math inline">\(\gamma \in \Gamma(\mu, \nu)\)</span>. Pour toute
fonction <span class="math inline">\(f: X \to \mathbb{R}\)</span>
Lipschitz avec <span class="math inline">\(\|f\|_{\text{Lip}} \leq
1\)</span>, nous avons: <span class="math display">\[\left| \int_X f \,
d\mu - \int_X f \, d\nu \right| = \left| \int_{X \times X} (f(x) - f(y))
\, d\gamma(x, y) \right| \leq \int_{X \times X} d(x, y) \, d\gamma(x,
y)\]</span> En prenant l’infimum sur toutes les mesures couplées <span
class="math inline">\(\gamma\)</span>, nous obtenons: <span
class="math display">\[\sup_{f: X \to \mathbb{R} \text{ Lipschitz}}
\left( \int_X f \, d\mu - \int_X f \, d\nu \right) \leq W_1(\mu,
\nu)\]</span></p>
<p>Pour l’inégalité inverse, nous utilisons le fait que pour toute <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe une fonction
<span class="math inline">\(f_\epsilon: X \to \mathbb{R}\)</span>
Lipschitz avec <span class="math inline">\(\|f_\epsilon\|_{\text{Lip}}
\leq 1\)</span> telle que: <span class="math display">\[\int_X
f_\epsilon \, d\mu - \int_X f_\epsilon \, d\nu \geq W_1(\mu, \nu) -
\epsilon\]</span> En prenant <span class="math inline">\(\epsilon \to
0\)</span>, nous obtenons l’égalité désirée. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Monge-Kantorovich possède plusieurs propriétés
intéressantes:</p>
<ol>
<li><p><strong>Inégalité triangulaire</strong>: Pour tout <span
class="math inline">\(p \geq 1\)</span> et <span
class="math inline">\(\mu, \nu, \lambda\)</span> mesures de probabilité
sur <span class="math inline">\(X\)</span>, nous avons: <span
class="math display">\[W_p(\mu, \lambda) \leq W_p(\mu, \nu) + W_p(\nu,
\lambda)\]</span></p></li>
<li><p><strong>Continuité</strong>: La divergence de Monge-Kantorovich
est continue par rapport à la convergence faible des mesures.
C’est-à-dire, si <span class="math inline">\(\mu_n \to \mu\)</span> et
<span class="math inline">\(\nu_n \to \nu\)</span> faiblement, alors
<span class="math inline">\(W_p(\mu_n, \nu_n) \to W_p(\mu,
\nu)\)</span>.</p></li>
<li><p><strong>Invariance par transformation</strong>: Si <span
class="math inline">\(T: X \to Y\)</span> est une application mesurable
et <span class="math inline">\(\mu\)</span>, <span
class="math inline">\(\nu\)</span> sont des mesures sur <span
class="math inline">\(X\)</span>, alors: <span
class="math display">\[W_p(T_\# \mu, T_\# \nu) \leq \|T\|_{\text{Lip}}
W_p(\mu, \nu)\]</span> où <span class="math inline">\(T_\#\)</span>
désigne la mesure poussée en avant par <span
class="math inline">\(T\)</span>.</p></li>
</ol>
<p>Pour prouver l’inégalité triangulaire, nous utilisons la définition
de <span class="math inline">\(W_p\)</span> et les propriétés des
mesures couplées. Considérons <span class="math inline">\(\gamma_1 \in
\Gamma(\mu, \nu)\)</span> et <span class="math inline">\(\gamma_2 \in
\Gamma(\nu, \lambda)\)</span>. Nous pouvons construire une mesure
couplée <span class="math inline">\(\gamma\)</span> sur <span
class="math inline">\(X \times X \times X\)</span> telle que: <span
class="math display">\[\gamma((x, y, z)) = \gamma_1(x, y) \gamma_2(y,
z)\]</span> En utilisant cette mesure couplée, nous obtenons: <span
class="math display">\[W_p(\mu, \lambda) \leq \left( \int_{X \times X
\times X} d(x, z)^p \, d\gamma(x, y, z) \right)^{1/p} \leq W_p(\mu, \nu)
+ W_p(\nu, \lambda)\]</span></p>
</body>
</html>
{% include "footer.html" %}


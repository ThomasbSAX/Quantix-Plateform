{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Néguentropie : Une mesure de l’information et de l’ordre</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Néguentropie : Une mesure de l’information et de
l’ordre</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La néguentropie, concept intimement lié à l’entropie de Shannon,
émerge comme une mesure fondamentale dans la théorie de l’information et
les systèmes dynamiques. Introduite par Léon Brillouin, elle quantifie
le degré d’organisation ou de structure dans un système, offrant ainsi
une perspective complémentaire à l’entropie classique. Son importance
réside dans sa capacité à capturer l’information utile, l’ordre et la
complexité, ce qui en fait un outil précieux dans des domaines aussi
variés que la physique statistique, l’informatique théorique et les
sciences cognitives.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la néguentropie, commençons par explorer l’entropie
de Shannon. Supposons que nous ayons un système décrit par une
distribution de probabilité <span class="math inline">\(p(x)\)</span>.
Nous cherchons à mesurer l’incertitude ou l’information contenue dans ce
système. L’entropie de Shannon <span class="math inline">\(H\)</span>
est définie comme :</p>
<p><span class="math display">\[H(X) = -\sum_{x} p(x) \log
p(x)\]</span></p>
<p>où <span class="math inline">\(x\)</span> parcourt l’ensemble des
événements possibles et <span class="math inline">\(p(x)\)</span> est la
probabilité de chaque événement. L’entropie mesure l’incertitude moyenne
associée à une variable aléatoire <span
class="math inline">\(X\)</span>.</p>
<p>La néguentropie, notée <span class="math inline">\(N(X)\)</span>, est
alors définie comme l’opposé de l’entropie :</p>
<p><span class="math display">\[N(X) = -H(X) = \sum_{x} p(x) \log
p(x)\]</span></p>
<p>Cette définition capture l’information ou l’ordre dans le système.
Plus la néguentropie est élevée, plus le système est organisé ou
structuré.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la néguentropie est le théorème de
l’information de Brillouin, qui établit un lien entre la néguentropie et
le travail nécessaire pour créer de l’information dans un système
physique. Formulons ce théorème :</p>
<div class="theorem">
<p>Soit un système physique décrit par une distribution de probabilité
<span class="math inline">\(p(x)\)</span>. Le travail minimal <span
class="math inline">\(W\)</span> nécessaire pour créer une quantité
d’information <span class="math inline">\(I\)</span> dans ce système est
donné par :</p>
<p><span class="math display">\[W \geq k_B T \Delta S\]</span></p>
<p>où <span class="math inline">\(k_B\)</span> est la constante de
Boltzmann, <span class="math inline">\(T\)</span> est la température du
système et <span class="math inline">\(\Delta S\)</span> est le
changement d’entropie. La néguentropie <span
class="math inline">\(N(X)\)</span> est alors liée au travail par :</p>
<p><span class="math display">\[N(X) = -H(X) = k_B T \Delta
S\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Brillouin, commençons par rappeler que
l’entropie de Shannon <span class="math inline">\(H(X)\)</span> est une
mesure de l’incertitude. Le travail minimal <span
class="math inline">\(W\)</span> nécessaire pour réduire cette
incertitude est donné par le principe de minimisation de l’énergie
libre. En utilisant la relation entre l’énergie libre <span
class="math inline">\(F\)</span> et l’entropie <span
class="math inline">\(S\)</span>, nous avons :</p>
<p><span class="math display">\[F = U - TS\]</span></p>
<p>où <span class="math inline">\(U\)</span> est l’énergie interne du
système. Le travail minimal <span class="math inline">\(W\)</span> est
alors :</p>
<p><span class="math display">\[W = \Delta F = \Delta U - T \Delta
S\]</span></p>
<p>En supposant que le système est isolé thermiquement (<span
class="math inline">\(\Delta U = 0\)</span>), nous obtenons :</p>
<p><span class="math display">\[W = -T \Delta S\]</span></p>
<p>En utilisant la relation entre l’entropie de Shannon et l’entropie
thermodynamique <span class="math inline">\(S\)</span>, nous avons :</p>
<p><span class="math display">\[\Delta S = -k_B \Delta H(X)\]</span></p>
<p>En substituant cette relation dans l’expression du travail, nous
obtenons :</p>
<p><span class="math display">\[W = -T (-k_B \Delta H(X)) = k_B T \Delta
H(X)\]</span></p>
<p>En utilisant la définition de la néguentropie <span
class="math inline">\(N(X) = -H(X)\)</span>, nous obtenons finalement
:</p>
<p><span class="math display">\[W = k_B T N(X)\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La néguentropie possède plusieurs propriétés importantes :</p>
<ol>
<li><p>La néguentropie est toujours non négative : <span
class="math inline">\(N(X) \geq 0\)</span>. Cela signifie que
l’information ou l’ordre dans un système est toujours positif ou
nul.</p></li>
<li><p>La néguentropie est maximale lorsque la distribution de
probabilité est uniforme. Cela signifie que l’information est maximisée
lorsqu’il n’y a aucune préférence pour un événement
particulier.</p></li>
<li><p>La néguentropie est additive pour des systèmes indépendants. Si
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont deux variables aléatoires
indépendantes, alors :</p>
<p><span class="math display">\[N(X,Y) = N(X) + N(Y)\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La néguentropie offre une perspective unique sur l’information et
l’ordre dans les systèmes physiques et informationnels. En quantifiant
l’information utile, elle complète l’entropie de Shannon et ouvre des
voies de recherche riches dans divers domaines scientifiques. Son étude
continue de révéler des insights profonds sur la nature de l’information
et sa relation avec les lois fondamentales de la physique.</p>
</body>
</html>
{% include "footer.html" %}


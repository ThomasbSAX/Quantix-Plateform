{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence log-déterminant : une exploration mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence log-déterminant : une exploration
mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’étude des divergences en théorie des probabilités et en statistique
mathématique est un domaine riche et complexe. Parmi les diverses formes
de divergences, la divergence log-déterminant se distingue par son
utilisation dans l’analyse des matrices et des distributions
gaussiennes. Cette divergence, également connue sous le nom de
divergence de Stein ou divergence de Bregman pour les fonctions
logarithmiques, trouve ses racines dans les travaux de Stein sur les
approximations gaussiennes et les inégalités de concentration.</p>
<p>La divergence log-déterminant émerge naturellement dans le cadre de
l’analyse des matrices positives définies. Elle est particulièrement
utile pour mesurer la distance entre deux distributions gaussiennes
multivariées, où le logarithme du déterminant de la matrice de
covariance joue un rôle central. Cette divergence est indispensable dans
des domaines tels que l’apprentissage automatique, la théorie de
l’information et la physique statistique, où la compréhension des
propriétés asymptotiques des estimations est cruciale.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence log-déterminant, commençons par
comprendre ce que nous cherchons à mesurer. Supposons que nous avons
deux matrices symétriques positives définies <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. Nous voulons quantifier la "distance"
entre ces deux matrices en tenant compte de leurs déterminants.
Intuitivement, cette distance devrait être nulle si et seulement si
<span class="math inline">\(A = B\)</span>, et elle devrait augmenter à
mesure que les matrices deviennent plus différentes.</p>
<p>Formellement, la divergence log-déterminant entre deux matrices
symétriques positives définies <span class="math inline">\(A\)</span> et
<span class="math inline">\(B\)</span> est définie comme suit :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) = \text{tr}(A
B^{-1}) - \log \det(A B^{-1}) - n\]</span></p>
<p>où <span class="math inline">\(\text{tr}\)</span> désigne la trace,
<span class="math inline">\(\det\)</span> le déterminant, et <span
class="math inline">\(n\)</span> la dimension des matrices.</p>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) = \text{tr}(A
B^{-1}) - \log \det(A) + \log \det(B) - n\]</span></p>
<p>Cette divergence peut également être exprimée en termes de la
fonction trace et du logarithme du déterminant :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) = \sum_{i=1}^n
\lambda_i - \log \left( \prod_{i=1}^n \lambda_i \right) - n\]</span></p>
<p>où <span class="math inline">\(\lambda_i\)</span> sont les valeurs
propres de la matrice <span class="math inline">\(A B^{-1}\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence log-déterminant est
le suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> deux matrices symétriques positives
définies de taille <span class="math inline">\(n \times n\)</span>.
Alors, la divergence log-déterminant satisfait :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) \geq 0\]</span></p>
<p>De plus, l’égalité <span class="math inline">\(D_{\text{LD}}(A, B) =
0\)</span> si et seulement si <span class="math inline">\(A =
B\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, nous utilisons
l’inégalité de Kantorovich pour les fonctions convexes. Considérons la
fonction <span class="math inline">\(\phi(x) = -\log x\)</span>, qui est
convexe sur <span class="math inline">\((0, +\infty)\)</span>. Par
l’inégalité de Kantorovich, nous avons :</p>
<p><span class="math display">\[\sum_{i=1}^n \phi(\lambda_i) \geq
\phi\left( \prod_{i=1}^n \lambda_i \right)\]</span></p>
<p>En substituant <span class="math inline">\(\phi(x) = -\log
x\)</span>, nous obtenons :</p>
<p><span class="math display">\[- \sum_{i=1}^n \log \lambda_i \geq -
\log \left( \prod_{i=1}^n \lambda_i \right)\]</span></p>
<p>Ce qui équivaut à :</p>
<p><span class="math display">\[\sum_{i=1}^n \log \lambda_i \leq \log
\left( \prod_{i=1}^n \lambda_i \right)\]</span></p>
<p>En réarrangeant les termes, nous obtenons :</p>
<p><span class="math display">\[\sum_{i=1}^n \lambda_i - \log \left(
\prod_{i=1}^n \lambda_i \right) - n \geq 0\]</span></p>
<p>Ce qui est exactement la définition de <span
class="math inline">\(D_{\text{LD}}(A, B)\)</span>. L’égalité est
atteinte lorsque toutes les <span
class="math inline">\(\lambda_i\)</span> sont égales à 1, c’est-à-dire
lorsque <span class="math inline">\(A = B\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’utilisation de la divergence log-déterminant,
considérons un exemple simple. Supposons que nous avons deux matrices
<span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> de taille <span class="math inline">\(2
\times 2\)</span>, où :</p>
<p><span class="math display">\[A = \begin{pmatrix} a &amp; b \\ b &amp;
c \end{pmatrix}, \quad B = \begin{pmatrix} d &amp; e \\ e &amp; f
\end{pmatrix}\]</span></p>
<p>Nous voulons calculer <span class="math inline">\(D_{\text{LD}}(A,
B)\)</span>. Tout d’abord, nous devons calculer <span
class="math inline">\(B^{-1}\)</span>, puis <span
class="math inline">\(A B^{-1}\)</span>. Ensuite, nous calculons la
trace et le déterminant de <span class="math inline">\(A
B^{-1}\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Calculons <span class="math inline">\(B^{-1}\)</span>
:</p>
<p><span class="math display">\[B^{-1} = \frac{1}{df - e^2}
\begin{pmatrix} f &amp; -e \\ -e &amp; d \end{pmatrix}\]</span></p>
<p>Ensuite, calculons <span class="math inline">\(A B^{-1}\)</span>
:</p>
<p><span class="math display">\[A B^{-1} = \frac{1}{df - e^2}
\begin{pmatrix} a f - b e &amp; -a e + b d \\ b f - c e &amp; -b e + c d
\end{pmatrix}\]</span></p>
<p>La trace de <span class="math inline">\(A B^{-1}\)</span> est :</p>
<p><span class="math display">\[\text{tr}(A B^{-1}) = \frac{a f - b e -
b e + c d}{df - e^2} = \frac{a f + c d - 2 b e}{df - e^2}\]</span></p>
<p>Le déterminant de <span class="math inline">\(A B^{-1}\)</span> est
:</p>
<p><span class="math display">\[\det(A B^{-1}) = \frac{(a f - b e)(-b e
+ c d) - (-a e + b d)(b f - c e)}{(df - e^2)^2}\]</span></p>
<p>En simplifiant, nous obtenons :</p>
<p><span class="math display">\[\det(A B^{-1}) = \frac{a c (df - e^2) -
b^2 (df - e^2)}{(df - e^2)^2} = \frac{(a c - b^2)(df - e^2)}{(df -
e^2)^2} = \frac{a c - b^2}{df - e^2}\]</span></p>
<p>Enfin, la divergence log-déterminant est :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) = \frac{a f + c d -
2 b e}{df - e^2} - \log \left( \frac{a c - b^2}{df - e^2} \right) -
2\]</span></p>
<p>Cette expression peut être simplifiée davantage en fonction des
éléments spécifiques de <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence log-déterminant possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p><strong>Invariance par transformation affine</strong> : La
divergence log-déterminant est invariante sous les transformations
affines. Plus précisément, pour toute matrice inversible <span
class="math inline">\(C\)</span>, nous avons :</p>
<p><span class="math display">\[D_{\text{LD}}(A, B) = D_{\text{LD}}(C A
C^T, C B C^T)\]</span></p></li>
<li><p><strong>Convexité</strong> : La divergence log-déterminant est
convexe en ses deux arguments. Cela signifie que pour tout <span
class="math inline">\(\lambda \in [0, 1]\)</span>, nous avons :</p>
<p><span class="math display">\[D_{\text{LD}}(\lambda A + (1 - \lambda)
C, B) \leq \lambda D_{\text{LD}}(A, B) + (1 - \lambda) D_{\text{LD}}(C,
B)\]</span></p>
<p>et de même pour le deuxième argument.</p></li>
<li><p><strong>Relation avec la divergence de Kullback-Leibler</strong>
: Pour des distributions gaussiennes multivariées, la divergence
log-déterminant est liée à la divergence de Kullback-Leibler. Plus
précisément, si <span class="math inline">\(\mathcal{N}(\mu_A,
A)\)</span> et <span class="math inline">\(\mathcal{N}(\mu_B,
B)\)</span> sont deux distributions gaussiennes, alors :</p>
<p><span class="math display">\[D_{\text{KL}}(\mathcal{N}(\mu_A, A) \|
\mathcal{N}(\mu_B, B)) = D_{\text{LD}}(A, B) + (\mu_A - \mu_B)^T A^{-1}
(\mu_A - \mu_B)\]</span></p>
<p>où <span class="math inline">\(D_{\text{KL}}\)</span> désigne la
divergence de Kullback-Leibler.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence log-déterminant est un outil puissant pour l’analyse
des matrices et des distributions gaussiennes. Ses propriétés de
non-négativité, d’invariance par transformation affine et de convexité
en font un choix naturel pour mesurer la distance entre des matrices
positives définies. De plus, sa relation avec la divergence de
Kullback-Leibler en fait un outil précieux dans l’étude des
distributions gaussiennes.</p>
<p>Les applications de la divergence log-déterminant sont vastes et
incluent l’apprentissage automatique, la théorie de l’information et la
physique statistique. À mesure que notre compréhension des matrices et
des distributions gaussiennes continue de croître, la divergence
log-déterminant jouera sans doute un rôle de plus en plus important dans
ces domaines.</p>
</body>
</html>
{% include "footer.html" %}


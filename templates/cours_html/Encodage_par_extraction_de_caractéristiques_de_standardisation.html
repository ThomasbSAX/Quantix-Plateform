{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage par Extraction de Caractéristiques de Standardisation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage par Extraction de Caractéristiques de
Standardisation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques de standardisation est
une technique fondamentale en traitement du signal et des données. Elle
émerge de la nécessité de transformer des signaux ou des données brutes
en une représentation plus maniable, tout en préservant les informations
essentielles. Cette méthode est indispensable dans des domaines tels que
la reconnaissance de motifs, l’analyse de données et le traitement du
langage naturel.</p>
<p>L’origine historique de cette technique remonte aux travaux pionniers
en analyse spectrale et en traitement du signal. Les motivations
techniques incluent la réduction de la dimensionnalité, l’amélioration
de la robustesse aux bruits et la facilitation des opérations
ultérieures comme la classification ou la régression.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage par extraction de caractéristiques,
commençons par définir ce que nous cherchons à obtenir. Nous voulons
transformer un signal ou une donnée brute en une représentation qui
capture les caractéristiques essentielles tout en éliminant le bruit et
les redondances.</p>
<p>Formellement, soit <span class="math inline">\(\mathbf{X}\)</span>
une matrice de données de taille <span class="math inline">\(n \times
p\)</span>, où <span class="math inline">\(n\)</span> est le nombre
d’observations et <span class="math inline">\(p\)</span> est le nombre
de caractéristiques. L’objectif est de trouver une matrice de
caractéristiques <span class="math inline">\(\mathbf{Z}\)</span> de
taille <span class="math inline">\(n \times k\)</span>, où <span
class="math inline">\(k \leq p\)</span>, telle que :</p>
<p><span class="math display">\[\mathbf{Z} = f(\mathbf{X})\]</span></p>
<p>où <span class="math inline">\(f\)</span> est une fonction
d’extraction de caractéristiques. La standardisation consiste à centrer
et réduire les données pour que chaque caractéristique ait une moyenne
de zéro et une variance de un.</p>
<p><span class="math display">\[\mathbf{Z}_{ij} = \frac{\mathbf{X}_{ij}
- \mu_j}{\sigma_j}\]</span></p>
<p>où <span class="math inline">\(\mu_j\)</span> est la moyenne de la
<span class="math inline">\(j\)</span>-ème caractéristique et <span
class="math inline">\(\sigma_j\)</span> est son écart-type.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans ce contexte est le théorème de la
décomposition en valeurs singulières (SVD). Ce théorème permet de
décomposer une matrice en produits de matrices plus simples, ce qui est
utile pour l’extraction de caractéristiques.</p>
<p>Soit <span class="math inline">\(\mathbf{X}\)</span> une matrice
réelle de taille <span class="math inline">\(n \times p\)</span>. Le
théorème SVD stipule qu’il existe des matrices orthogonales <span
class="math inline">\(\mathbf{U}\)</span> de taille <span
class="math inline">\(n \times n\)</span>, <span
class="math inline">\(\mathbf{V}\)</span> de taille <span
class="math inline">\(p \times p\)</span>, et une matrice diagonale
<span class="math inline">\(\mathbf{S}\)</span> de taille <span
class="math inline">\(n \times p\)</span>, telles que :</p>
<p><span class="math display">\[\mathbf{X} = \mathbf{U} \mathbf{S}
\mathbf{V}^T\]</span></p>
<p>où <span class="math inline">\(\mathbf{S}\)</span> contient les
valeurs singulières de <span
class="math inline">\(\mathbf{X}\)</span>.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème SVD, nous commençons par considérer la
matrice <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>.
Cette matrice est symétrique et positive définie, donc elle possède une
décomposition spectrale :</p>
<p><span class="math display">\[\mathbf{X}^T \mathbf{X} = \mathbf{V}
\mathbf{\Lambda} \mathbf{V}^T\]</span></p>
<p>où <span class="math inline">\(\mathbf{V}\)</span> est une matrice
orthogonale et <span class="math inline">\(\mathbf{\Lambda}\)</span> est
une matrice diagonale contenant les valeurs propres de <span
class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>.</p>
<p>Ensuite, nous définissons <span class="math inline">\(\mathbf{U} =
\mathbf{X} \mathbf{V} \mathbf{\Lambda}^{-1/2}\)</span>. On peut montrer
que <span class="math inline">\(\mathbf{U}\)</span> est une matrice
orthogonale. Enfin, nous définissons <span
class="math inline">\(\mathbf{S} = \mathbf{\Lambda}^{1/2}\)</span>, ce
qui donne la décomposition SVD :</p>
<p><span class="math display">\[\mathbf{X} = \mathbf{U} \mathbf{S}
\mathbf{V}^T\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Les propriétés de l’encodage par extraction de caractéristiques de
standardisation incluent :</p>
<ul>
<li><p>La réduction de la dimensionnalité : En utilisant uniquement les
<span class="math inline">\(k\)</span> premières valeurs singulières,
nous pouvons réduire la dimension de l’espace des
caractéristiques.</p></li>
<li><p>La robustesse aux bruits : La standardisation rend les données
moins sensibles aux variations de scale et aux bruits.</p></li>
<li><p>La préservation des informations : Les caractéristiques
principales capturent la majeure partie de la variance des
données.</p></li>
</ul>
<p>Pour prouver ces propriétés, nous utilisons les résultats du théorème
SVD et les propriétés de la standardisation. Par exemple, pour montrer
que la réduction de dimension préserve les informations, nous pouvons
utiliser le fait que les <span class="math inline">\(k\)</span>
premières valeurs singulières capturent la majeure partie de la variance
des données.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de standardisation est
une technique puissante et polyvalente. Elle trouve des applications
dans divers domaines et offre de nombreux avantages en termes de
réduction de dimension, de robustesse aux bruits et de préservation des
informations. Les théorèmes et propriétés présentés dans cet article
fournissent une base solide pour comprendre et appliquer cette
technique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de BERT Cosine : Une Mesure Avancée de Similarité Textuelle</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de BERT Cosine : Une Mesure Avancée de
Similarité Textuelle</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’ère numérique actuelle a engendré une explosion de données
textuelles, rendant crucial le développement de méthodes efficaces pour
mesurer la similarité entre textes. Parmi les approches modernes,
l’utilisation de modèles de langage pré-entraînés comme BERT
(Bidirectional Encoder Representations from Transformers) a révolutionné
le traitement automatique du langage naturel. La distance de BERT Cosine
émerge comme une mesure sophistiquée, combinant la puissance des
représentations contextuelles de BERT avec la simplicité géométrique de
la similarité cosinus.</p>
<p>Cette notion trouve son origine dans le besoin de capturer des
relations sémantiques subtiles entre textes, au-delà des simples
co-occurrences de mots. Elle est indispensable dans des applications
telles que la recherche d’information, le regroupement de documents, et
l’analyse de sentiments, où une compréhension fine du contenu textuel
est requise.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la distance de BERT Cosine, commençons par comprendre
ce que nous cherchons à mesurer. Imaginons deux phrases : "Le chat dort
sur le canapé" et "Un félin repose sur le sofa". Nous voulons quantifier
à quel point ces phrases sont similaires en termes de sens,
indépendamment des mots exacts utilisés.</p>
<p>La similarité cosinus est une mesure géométrique qui évalue l’angle
entre deux vecteurs dans un espace euclidien. Elle est définie comme le
cosinus de l’angle entre les vecteurs, variant de -1 à 1. Plus la valeur
est proche de 1, plus les vecteurs sont similaires.</p>
<p>Formellement, soit <span class="math inline">\(\mathbf{v}_1\)</span>
et <span class="math inline">\(\mathbf{v}_2\)</span> deux vecteurs dans
un espace euclidien. La similarité cosinus est donnée par :</p>
<p><span class="math display">\[\text{similarité}(\mathbf{v}_1,
\mathbf{v}_2) = \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{\|\mathbf{v}_1\|
\|\mathbf{v}_2\|}\]</span></p>
<p>où <span class="math inline">\(\mathbf{v}_1 \cdot
\mathbf{v}_2\)</span> désigne le produit scalaire des vecteurs, et <span
class="math inline">\(\|\mathbf{v}\|\)</span> la norme euclidienne du
vecteur <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>La distance de BERT Cosine étend cette notion en utilisant les
représentations vectorielles contextuelles générées par le modèle BERT.
Soient <span class="math inline">\(s_1\)</span> et <span
class="math inline">\(s_2\)</span> deux phrases. Le modèle BERT génère
des vecteurs de représentation <span
class="math inline">\(\mathbf{BERT}(s_1)\)</span> et <span
class="math inline">\(\mathbf{BERT}(s_2)\)</span>. La distance de BERT
Cosine est alors définie comme :</p>
<p><span class="math display">\[\text{DistanceCosineBERT}(s_1, s_2) = 1
- \text{similarité}(\mathbf{BERT}(s_1), \mathbf{BERT}(s_2))\]</span></p>
<p>où <span class="math inline">\(\text{similarité}\)</span> désigne la
similarité cosinus définie précédemment.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Considérons le théorème fondamental de la distance de BERT Cosine,
qui établit une relation entre cette mesure et les propriétés des
représentations vectorielles de BERT.</p>
<p><strong>Théorème 1 (Propriété de la Distance de BERT
Cosine)</strong>: Soient <span class="math inline">\(s_1\)</span> et
<span class="math inline">\(s_2\)</span> deux phrases, et <span
class="math inline">\(\mathbf{BERT}(s_1)\)</span>, <span
class="math inline">\(\mathbf{BERT}(s_2)\)</span> leurs représentations
vectorielles respectives générées par le modèle BERT. La distance de
BERT Cosine satisfait les propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2)
\in [0, 2]\)</span></p></li>
<li><p><span class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) =
0\)</span> si et seulement si <span class="math inline">\(s_1 =
s_2\)</span></p></li>
<li><p><span class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(s_2, s_1)\)</span></p></li>
</ol>
<p><strong>Démonstration</strong>:</p>
<ul>
<li><p>Puisque la similarité cosinus est bornée entre -1 et 1, <span
class="math inline">\(1 - \text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2))\)</span> est bornée entre 0 et 2.</p></li>
<li><p>Si <span class="math inline">\(s_1 = s_2\)</span>, alors <span
class="math inline">\(\mathbf{BERT}(s_1) = \mathbf{BERT}(s_2)\)</span>,
et donc <span
class="math inline">\(\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2)) = 1\)</span>. Réciproquement, si <span
class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) = 0\)</span>,
alors <span class="math inline">\(\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2)) = 1\)</span>, ce qui implique que <span
class="math inline">\(\mathbf{BERT}(s_1) = \lambda
\mathbf{BERT}(s_2)\)</span> pour un certain <span
class="math inline">\(\lambda &gt; 0\)</span>. Compte tenu de la
normalisation des vecteurs BERT, nous avons <span
class="math inline">\(\lambda = 1\)</span>, donc <span
class="math inline">\(s_1 = s_2\)</span>.</p></li>
<li><p>La similarité cosinus est symétrique, donc <span
class="math inline">\(\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2)) = \text{similarité}(\mathbf{BERT}(s_2),
\mathbf{BERT}(s_1))\)</span>. Par conséquent, <span
class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(s_2, s_1)\)</span>.</p></li>
</ul>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer l’application de la distance de BERT Cosine,
considérons un exemple concret. Soient les phrases <span
class="math inline">\(s_1 =\)</span> "Le chat dort sur le canapé" et
<span class="math inline">\(s_2 =\)</span> "Un félin repose sur le
sofa".</p>
<ol>
<li><p>Générer les représentations vectorielles <span
class="math inline">\(\mathbf{BERT}(s_1)\)</span> et <span
class="math inline">\(\mathbf{BERT}(s_2)\)</span> en utilisant le modèle
BERT.</p></li>
<li><p>Calculer le produit scalaire <span
class="math inline">\(\mathbf{BERT}(s_1) \cdot
\mathbf{BERT}(s_2)\)</span>.</p></li>
<li><p>Calculer les normes euclidiennes <span
class="math inline">\(\|\mathbf{BERT}(s_1)\|\)</span> et <span
class="math inline">\(\|\mathbf{BERT}(s_2)\|\)</span>.</p></li>
<li><p>Calculer la similarité cosinus <span
class="math inline">\(\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2))\)</span>.</p></li>
<li><p>Calculer la distance de BERT Cosine <span
class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) = 1 -
\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2))\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distance de BERT Cosine possède plusieurs propriétés
intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Invariance par Translation</strong> : La distance de BERT
Cosine est invariante par translation des vecteurs. En d’autres termes,
pour tout vecteur <span class="math inline">\(\mathbf{t}\)</span>, nous
avons :</p>
<p><span class="math display">\[\text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(s_1 + \mathbf{t}, s_2 +
\mathbf{t})\]</span></p>
<p><strong>Preuve</strong> : La similarité cosinus est invariante par
translation, donc <span
class="math inline">\(\text{similarité}(\mathbf{BERT}(s_1),
\mathbf{BERT}(s_2)) = \text{similarité}(\mathbf{BERT}(s_1) + \mathbf{t},
\mathbf{BERT}(s_2) + \mathbf{t})\)</span>. Par conséquent, <span
class="math inline">\(\text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(s_1 + \mathbf{t}, s_2 +
\mathbf{t})\)</span>.</p></li>
<li><p><strong>Homogénéité</strong> : La distance de BERT Cosine est
homogène. Pour tout scalaire <span class="math inline">\(\lambda &gt;
0\)</span>, nous avons :</p>
<p><span class="math display">\[\text{DistanceCosineBERT}(\lambda s_1,
\lambda s_2) = \text{DistanceCosineBERT}(s_1, s_2)\]</span></p>
<p><strong>Preuve</strong> : La similarité cosinus est homogène, donc
<span class="math inline">\(\text{similarité}(\lambda
\mathbf{BERT}(s_1), \lambda \mathbf{BERT}(s_2)) =
\text{similarité}(\mathbf{BERT}(s_1), \mathbf{BERT}(s_2))\)</span>. Par
conséquent, <span
class="math inline">\(\text{DistanceCosineBERT}(\lambda s_1, \lambda
s_2) = \text{DistanceCosineBERT}(s_1, s_2)\)</span>.</p></li>
<li><p><strong>Continuité</strong> : La distance de BERT Cosine est
continue par rapport aux représentations vectorielles. En d’autres
termes, si <span class="math inline">\(\mathbf{BERT}(s_1)\)</span> et
<span class="math inline">\(\mathbf{BERT}(s_2)\)</span> convergent vers
<span class="math inline">\(\mathbf{v}_1\)</span> et <span
class="math inline">\(\mathbf{v}_2\)</span>, respectivement, alors :</p>
<p><span class="math display">\[\lim_{s_1, s_2 \to \mathbf{v}_1,
\mathbf{v}_2} \text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(\mathbf{v}_1, \mathbf{v}_2)\]</span></p>
<p><strong>Preuve</strong> : La similarité cosinus est continue, donc
<span class="math inline">\(\lim_{s_1, s_2 \to \mathbf{v}_1,
\mathbf{v}_2} \text{similarité}(\mathbf{BERT}(s_1), \mathbf{BERT}(s_2))
= \text{similarité}(\mathbf{v}_1, \mathbf{v}_2)\)</span>. Par
conséquent, <span class="math inline">\(\lim_{s_1, s_2 \to \mathbf{v}_1,
\mathbf{v}_2} \text{DistanceCosineBERT}(s_1, s_2) =
\text{DistanceCosineBERT}(\mathbf{v}_1, \mathbf{v}_2)\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La distance de BERT Cosine représente une avancée significative dans
la mesure de similarité textuelle, combinant la puissance des
représentations contextuelles de BERT avec la simplicité géométrique de
la similarité cosinus. Ses propriétés mathématiques rigoureuses et son
application pratique en font un outil indispensable pour les chercheurs
et praticiens du traitement automatique du langage naturel.</p>
</body>
</html>
{% include "footer.html" %}


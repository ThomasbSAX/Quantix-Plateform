{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance Euclidienne dans l’Espace d’Embedding</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance Euclidienne dans l’Espace d’Embedding</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’idée de distance euclidienne dans l’espace d’embedding émerge
naturellement lorsque l’on cherche à représenter des données complexes
dans un espace de dimension réduite tout en préservant les relations
géométriques entre les points. Cette notion est indispensable dans de
nombreux domaines tels que l’apprentissage automatique, la visualisation
de données et la reconnaissance de motifs. Elle permet de capturer
l’essence des relations entre les objets dans un espace de plus faible
dimension, facilitant ainsi l’analyse et l’interprétation des
données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la distance euclidienne dans l’espace d’embedding,
commençons par rappeler ce qu’est un embedding. Un embedding est une
représentation de données dans un espace vectoriel, souvent de dimension
plus faible que l’espace original. L’objectif est de préserver les
distances entre les points autant que possible.</p>
<p>Supposons que nous avons un ensemble de points dans un espace
original de dimension <span class="math inline">\(n\)</span>. Nous
voulons les représenter dans un espace d’embedding de dimension <span
class="math inline">\(k\)</span>, où <span class="math inline">\(k &lt;
n\)</span>. La distance euclidienne entre deux points dans l’espace
d’embedding est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathbf{X} = \{x_1, x_2, \ldots,
x_m\}\)</span> un ensemble de points dans <span
class="math inline">\(\mathbb{R}^n\)</span>, et soit <span
class="math inline">\(\mathbf{Y} = \{y_1, y_2, \ldots, y_m\}\)</span>
leur représentation dans l’espace d’embedding <span
class="math inline">\(\mathbb{R}^k\)</span>. La distance euclidienne
entre deux points <span class="math inline">\(y_i\)</span> et <span
class="math inline">\(y_j\)</span> dans l’espace d’embedding est donnée
par : <span class="math display">\[d_{\text{embedding}}(y_i, y_j) =
\sqrt{\sum_{l=1}^k (y_{il} - y_{jl})^2}\]</span> où <span
class="math inline">\(y_{il}\)</span> et <span
class="math inline">\(y_{jl}\)</span> sont les coordonnées des points
<span class="math inline">\(y_i\)</span> et <span
class="math inline">\(y_j\)</span> dans la dimension <span
class="math inline">\(l\)</span>.</p>
</div>
<p>Une autre manière de formuler cette définition est en utilisant les
normes vectorielles. La distance euclidienne peut être vue comme la
norme <span class="math inline">\(L_2\)</span> du vecteur différence
entre les deux points :</p>
<p><span class="math display">\[d_{\text{embedding}}(y_i, y_j) = \| y_i
- y_j \|_2\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la distance euclidienne dans l’espace
d’embedding est le théorème de Johnson-Lindenstrauss. Ce théorème montre
qu’il est possible de représenter des points dans un espace de dimension
plus faible tout en préservant les distances entre eux avec une certaine
précision.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\epsilon &gt; 0\)</span> et soit
<span class="math inline">\(n\)</span> le nombre de points. Il existe
une constante <span class="math inline">\(c &gt; 0\)</span> telle que
pour tout ensemble de <span class="math inline">\(n\)</span> points dans
<span class="math inline">\(\mathbb{R}^d\)</span>, il existe un
embedding dans <span class="math inline">\(\mathbb{R}^k\)</span> avec
<span class="math inline">\(k = O(\frac{\log n}{\epsilon^2})\)</span>
tel que pour tout couple de points <span class="math inline">\(x_i,
x_j\)</span>, on a : <span class="math display">\[(1 - \epsilon) \| x_i
- x_j \|_2^2 \leq \| y_i - y_j \|_2^2 \leq (1 + \epsilon) \| x_i - x_j
\|_2^2\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Johnson-Lindenstrauss, nous utilisons des
techniques probabilistes et des propriétés des matrices aléatoires. La
preuve repose sur le fait que les projections aléatoires préservent les
distances avec une certaine probabilité.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un ensemble de <span
class="math inline">\(n\)</span> points dans <span
class="math inline">\(\mathbb{R}^d\)</span>. Nous voulons les embedder
dans <span class="math inline">\(\mathbb{R}^k\)</span> en utilisant une
projection aléatoire. La projection est définie par une matrice <span
class="math inline">\(P \in \mathbb{R}^{k \times d}\)</span> dont les
entrées sont des variables aléatoires indépendantes et identiquement
distribuées selon une loi normale centrée réduite.</p>
<p>Pour chaque point <span class="math inline">\(x_i\)</span>, nous
définissons son embedding <span class="math inline">\(y_i =
Px_i\)</span>. Nous devons montrer que pour tout couple de points <span
class="math inline">\(x_i, x_j\)</span>, la distance entre leurs
embeddings est préservée avec une certaine précision.</p>
<p>En utilisant les propriétés des matrices aléatoires, nous pouvons
montrer que : <span class="math display">\[\mathbb{E}[\| y_i - y_j
\|_2^2] = \| x_i - x_j \|_2^2\]</span> où <span
class="math inline">\(\mathbb{E}\)</span> désigne l’espérance
mathématique.</p>
<p>Ensuite, nous utilisons des inégalités de concentration pour montrer
que : <span class="math display">\[P\left( \left| \| y_i - y_j \|_2^2 -
\| x_i - x_j \|_2^2 \right| \geq \epsilon \| x_i - x_j \|_2^2 \right)
\leq 2e^{-c \epsilon^2 k}\]</span> pour une constante <span
class="math inline">\(c &gt; 0\)</span>.</p>
<p>En choisissant <span class="math inline">\(k = O(\frac{\log
n}{\epsilon^2})\)</span>, nous pouvons garantir que la probabilité que
la distance ne soit pas préservée pour au moins un couple de points est
inférieure à <span class="math inline">\(\frac{1}{n}\)</span>. En
utilisant l’union bornée, nous pouvons conclure que la probabilité que
toutes les distances soient préservées est strictement positive. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le théorème de Johnson-Lindenstrauss a plusieurs propriétés
intéressantes et corollaires. En voici quelques-uns :</p>
<ol>
<li><p><strong>Conservation des Distances</strong> : Le théorème
garantit que les distances entre les points sont préservées avec une
certaine précision. Cela est crucial pour de nombreuses applications où
la structure géométrique des données est importante.</p>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur les propriétés des matrices
aléatoires et des inégalités de concentration. En utilisant une
projection aléatoire, nous pouvons montrer que les distances sont
préservées en espérance et avec une certaine probabilité. ◻</p>
</div></li>
<li><p><strong>Dimension Réduite</strong> : Le théorème montre que la
dimension de l’espace d’embedding peut être réduite de manière
significative tout en préservant les distances. Cela est
particulièrement utile pour la visualisation des données et
l’apprentissage automatique.</p>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur le choix de <span
class="math inline">\(k = O(\frac{\log n}{\epsilon^2})\)</span>. En
choisissant une dimension suffisamment grande, nous pouvons garantir que
les distances sont préservées avec la précision souhaitée. ◻</p>
</div></li>
<li><p><strong>Applications</strong> : Le théorème a de nombreuses
applications dans l’apprentissage automatique, la visualisation des
données et la reconnaissance de motifs. Il permet de représenter des
données complexes dans un espace de dimension réduite tout en préservant
les relations géométriques entre les points.</p>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur l’utilisation de la distance
euclidienne dans l’espace d’embedding pour capturer les relations
géométriques entre les points. En préservant ces relations, nous pouvons
appliquer des algorithmes d’apprentissage automatique et de
visualisation des données de manière efficace. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La distance euclidienne dans l’espace d’embedding est une notion
fondamentale en apprentissage automatique et en visualisation des
données. Elle permet de représenter des données complexes dans un espace
de dimension réduite tout en préservant les relations géométriques entre
les points. Le théorème de Johnson-Lindenstrauss montre qu’il est
possible de réaliser cet embedding avec une précision garantie, ce qui
ouvre la voie à de nombreuses applications pratiques.</p>
</body>
</html>
{% include "footer.html" %}


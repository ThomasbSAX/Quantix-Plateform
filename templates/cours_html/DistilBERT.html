{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>DistilBERT : Une Approche Efficace de Distillation de Modèles de Langage</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">DistilBERT : Une Approche Efficace de Distillation de
Modèles de Langage</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’essor des modèles de langage basés sur les transformateurs, tels
que BERT (Bidirectional Encoder Representations from Transformers), a
révolutionné le traitement automatique des langues. Cependant, ces
modèles sont souvent lourds et coûteux en termes de calcul, ce qui
limite leur déploiement dans des environnements à ressources limitées.
La distillation de modèles offre une solution prometteuse pour réduire
la taille des modèles tout en préservant leurs performances. Dans cet
article, nous explorons DistilBERT, une version distillée de BERT, qui
combine efficacité et performance.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre DistilBERT, il est essentiel de définir quelques
concepts clés.</p>
<div class="definition">
<p>Un modèle de langage est une fonction <span class="math inline">\(M:
\mathcal{V}^* \rightarrow [0,1]\)</span> où <span
class="math inline">\(\mathcal{V}\)</span> est un vocabulaire fini et
<span class="math inline">\(M(w_1, \ldots, w_n)\)</span> représente la
probabilité d’observer la séquence <span class="math inline">\(w_1,
\ldots, w_n\)</span> dans un corpus de texte.</p>
</div>
<div class="definition">
<p>Un transformateur est un modèle d’apprentissage profond basé sur
l’attention auto-supervisée. Il est défini par une séquence de couches
d’encodage <span class="math inline">\(\text{Enc}_1, \ldots,
\text{Enc}_n\)</span> et de décodage <span
class="math inline">\(\text{Dec}_1, \ldots, \text{Dec}_m\)</span>, où
chaque couche utilise des mécanismes d’attention pour capturer les
dépendances entre les tokens.</p>
</div>
<div class="definition">
<p>La distillation de modèles est une technique qui consiste à entraîner
un modèle plus petit <span class="math inline">\(S\)</span> (étudiant)
pour qu’il imite les prédictions d’un modèle plus grand <span
class="math inline">\(T\)</span> (enseignant). Formellement, on cherche
à minimiser la divergence de KL entre les prédictions des deux modèles :
<span class="math display">\[\mathcal{L}_{\text{KL}}(T, S) =
\sum_{i=1}^{N} \text{KL}(T(x_i) \| S(x_i))\]</span> où <span
class="math inline">\(x_i\)</span> sont les exemples d’entraînement et
<span class="math inline">\(\text{KL}\)</span> est la divergence de
Kullback-Leibler.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant quelques théorèmes clés liés à la
distillation de modèles.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(T\)</span> un modèle enseignant et
<span class="math inline">\(S\)</span> un modèle étudiant. Si <span
class="math inline">\(S\)</span> est suffisamment expressif, alors il
existe une fonction de perte <span
class="math inline">\(\mathcal{L}\)</span> telle que : <span
class="math display">\[\min_{S} \mathcal{L}_{\text{KL}}(T, S) \leq
\epsilon\]</span> pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur le fait que la divergence de
Kullback-Leibler est une mesure de la distance entre deux distributions.
En minimisant cette divergence, le modèle étudiant <span
class="math inline">\(S\)</span> peut approximer les prédictions du
modèle enseignant <span class="math inline">\(T\)</span> avec une
précision arbitraire. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Nous détaillons maintenant les preuves des théorèmes présentés.</p>
<div class="proof">
<p><em>Preuve du Théorème de Distillation.</em> Considérons la
divergence de Kullback-Leibler entre les prédictions des modèles
enseignant et étudiant : <span class="math display">\[\text{KL}(T(x) \|
S(x)) = \sum_{i=1}^{|\mathcal{V}|} T_i(x) \log \left(
\frac{T_i(x)}{S_i(x)} \right)\]</span> où <span
class="math inline">\(T_i(x)\)</span> et <span
class="math inline">\(S_i(x)\)</span> sont les probabilités prédites par
les modèles enseignant et étudiant pour le token <span
class="math inline">\(i\)</span>.</p>
<p>En minimisant cette divergence, nous cherchons à aligner les
distributions de probabilité des deux modèles. Puisque <span
class="math inline">\(S\)</span> est suffisamment expressif, il peut
approximer les prédictions de <span class="math inline">\(T\)</span>
avec une précision arbitraire. Ainsi, il existe une fonction de perte
<span class="math inline">\(\mathcal{L}\)</span> telle que : <span
class="math display">\[\min_{S} \mathcal{L}_{\text{KL}}(T, S) \leq
\epsilon\]</span> pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés et corollaires
importants.</p>
<div class="proposition">
<p>La distillation de modèles préserve les performances du modèle
enseignant tout en réduisant la taille du modèle étudiant.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La distillation de modèles permet au modèle étudiant
d’imiter les prédictions du modèle enseignant. En minimisant la
divergence de Kullback-Leibler, le modèle étudiant peut atteindre des
performances similaires à celles du modèle enseignant, tout en étant
plus petit et plus efficace. ◻</p>
</div>
<div class="corollaire">
<p>La distillation de modèles est particulièrement utile pour le
déploiement de modèles de langage dans des environnements à ressources
limitées.</p>
</div>
<div class="proof">
<p><em>Proof.</em> En réduisant la taille des modèles, la distillation
permet de déployer des modèles de langage sur des appareils mobiles et
des systèmes embarqués, où les ressources de calcul sont limitées. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré DistilBERT, une version
distillée de BERT. Nous avons présenté les concepts clés, les théorèmes
et les preuves associés à la distillation de modèles. DistilBERT offre
une solution efficace pour réduire la taille des modèles de langage tout
en préservant leurs performances, ce qui en fait une approche
prometteuse pour le déploiement de modèles dans des environnements à
ressources limitées.</p>
</body>
</html>
{% include "footer.html" %}


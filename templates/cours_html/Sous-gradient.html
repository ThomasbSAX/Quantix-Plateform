{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Sous-gradient : Une Approche pour les Fonctions Non Différentiables</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Sous-gradient : Une Approche pour les Fonctions Non
Différentiables</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’optimisation est un domaine central en mathématiques appliquées,
avec des applications variées allant de l’apprentissage automatique à la
théorie du contrôle. Cependant, de nombreuses fonctions objectives dans
ces problèmes ne sont pas différentiables partout, ce qui complique
l’application des méthodes classiques de descente de gradient. C’est ici
que le concept de sous-gradient entre en jeu.</p>
<p>Le sous-gradient généralise l’idée du gradient pour les fonctions
convexes non différentiables. Il permet d’étendre des algorithmes bien
connus, comme la méthode du gradient, à un cadre plus large. L’origine
de cette notion remonte aux travaux pionniers sur l’analyse convexe,
notamment ceux de Moreau et Rockafellar dans les années 1960.</p>
<p>Dans cet article, nous explorerons la définition formelle du
sous-gradient, ses propriétés fondamentales et quelques théorèmes clés
qui en découlent. Nous verrons également comment cette notion permet de
résoudre des problèmes d’optimisation non lisses.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la notion de sous-gradient, considérons d’abord une
fonction convexe <span class="math inline">\(f : \mathbb{R}^n
\rightarrow \mathbb{R}\)</span>. Intuitivement, nous cherchons une
généralisation du gradient qui capture la direction de la plus forte
décroissance locale, même lorsque <span class="math inline">\(f\)</span>
n’est pas différentiable.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe et <span
class="math inline">\(x_0 \in \mathbb{R}^n\)</span>. Un vecteur <span
class="math inline">\(g \in \mathbb{R}^n\)</span> est appelé un
sous-gradient de <span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span> si pour tout <span
class="math inline">\(x \in \mathbb{R}^n\)</span>, on a l’inégalité
suivante : <span class="math display">\[f(x) \geq f(x_0) + g^T (x -
x_0).\]</span></p>
</div>
<p>Cette définition peut être reformulée de plusieurs manières. Par
exemple, <span class="math inline">\(g\)</span> est un sous-gradient de
<span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span> si et seulement si la fonction affine
<span class="math inline">\(l_g(x) = f(x_0) + g^T (x - x_0)\)</span> est
un plan tangentiel à <span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span>, c’est-à-dire que : <span
class="math display">\[l_g(x) \leq f(x) \quad \forall x \in
\mathbb{R}^n.\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des résultats fondamentaux concernant les sous-gradients est le
théorème suivant, qui garantit l’existence de sous-gradients pour les
fonctions convexes.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe et <span
class="math inline">\(x_0 \in \mathbb{R}^n\)</span>. Alors, il existe au
moins un sous-gradient de <span class="math inline">\(f\)</span> en
<span class="math inline">\(x_0\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur le fait que
l’ensemble des sous-gradients est non vide. En effet, pour toute
fonction convexe <span class="math inline">\(f\)</span>, le
sous-différentiel <span class="math inline">\(\partial f(x_0)\)</span>
est défini comme l’ensemble des sous-gradients en <span
class="math inline">\(x_0\)</span>. Par convexité, on peut montrer que
cet ensemble est non vide.</p>
<p>Plus précisément, pour tout <span class="math inline">\(x \in
\mathbb{R}^n\)</span>, la fonction <span
class="math inline">\(f\)</span> admet un plan tangentiel en <span
class="math inline">\(x_0\)</span>. Cela implique que le
sous-différentiel est non vide, et donc qu’il existe au moins un
sous-gradient en <span class="math inline">\(x_0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les sous-gradients possèdent plusieurs propriétés intéressantes. Nous
en énumérons quelques-unes ci-dessous.</p>
<ol>
<li><p>Si <span class="math inline">\(f\)</span> est différentiable en
<span class="math inline">\(x_0\)</span>, alors le sous-gradient unique
de <span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span> est simplement le gradient <span
class="math inline">\(\nabla f(x_0)\)</span>.</p></li>
<li><p>Pour toute fonction convexe <span
class="math inline">\(f\)</span>, l’ensemble des sous-gradients en un
point <span class="math inline">\(x_0\)</span> est un ensemble convexe
fermé.</p></li>
<li><p>Si <span class="math inline">\(g\)</span> est un sous-gradient de
<span class="math inline">\(f\)</span> en <span
class="math inline">\(x_0\)</span>, alors pour tout <span
class="math inline">\(\lambda &gt; 0\)</span>, <span
class="math inline">\(g\)</span> est aussi un sous-gradient de <span
class="math inline">\(\lambda f\)</span> en <span
class="math inline">\(x_0\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le concept de sous-gradient est un outil puissant pour traiter les
fonctions convexes non différentiables. Il permet d’étendre des méthodes
classiques de l’optimisation à un cadre plus général, ouvrant ainsi la
voie à de nombreuses applications pratiques. Dans cet article, nous
avons présenté les définitions fondamentales, quelques théorèmes clés et
des propriétés importantes des sous-gradients.</p>
<p>Pour aller plus loin, il serait intéressant d’explorer des
algorithmes spécifiques utilisant les sous-gradients, comme la méthode
du sous-gradient ou le bundle method. Ces méthodes sont particulièrement
utiles dans des domaines tels que l’apprentissage automatique et la
théorie du contrôle, où les fonctions objectives sont souvent non
différentiables.</p>
</body>
</html>
{% include "footer.html" %}


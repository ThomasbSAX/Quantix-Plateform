{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Cohen’s Kappa: A Measure of Inter-Rater Agreement</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Cohen’s Kappa: A Measure of Inter-Rater Agreement</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The assessment of agreement between raters is a fundamental problem
in various fields such as psychology, medicine, and social sciences.
Cohen’s Kappa, introduced by Jacob Cohen in 1960, is a statistical
measure that goes beyond simple percentage agreement to account for the
agreement occurring by chance. This measure is indispensable when
evaluating the reliability of subjective judgments, ensuring that
observed agreements are not merely due to randomness.</p>
<p>Cohen’s Kappa addresses the limitations of percentage agreement by
incorporating the expected agreement under the assumption that the
raters are assigning ratings independently. This adjustment provides a
more nuanced understanding of inter-rater reliability, making it a
cornerstone in the analysis of categorical data.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand Cohen’s Kappa, let us first consider what we aim to
achieve. We want a measure that quantifies the agreement between two
raters beyond what would be expected by chance. This involves comparing
the observed agreement to the expected agreement.</p>
<div class="definition">
<p>Let <span class="math inline">\(n\)</span> be the total number of
subjects rated, and let <span class="math inline">\(n_{ij}\)</span>
denote the number of subjects that rater 1 assigns to category <span
class="math inline">\(i\)</span> and rater 2 assigns to category <span
class="math inline">\(k\)</span>. The observed agreement <span
class="math inline">\(P_o\)</span> is defined as: <span
class="math display">\[P_o = \frac{\sum_{i=1}^{k} n_{ii}}{n}\]</span>
where <span class="math inline">\(k\)</span> is the number of
categories.</p>
</div>
<div class="definition">
<p>The expected agreement <span class="math inline">\(P_e\)</span> is
the probability that both raters assign a subject to the same category
by chance. It is calculated as: <span class="math display">\[P_e =
\frac{\sum_{i=1}^{k} (\sum_{j=1}^{n} n_{ij})(\sum_{l=1}^{n}
n_{li})}{n^2}\]</span></p>
</div>
<div class="definition">
<p>Cohen’s Kappa <span class="math inline">\(\kappa\)</span> is a
measure of inter-rater agreement adjusted for chance agreement. It is
defined as: <span class="math display">\[\kappa = \frac{P_o - P_e}{1 -
P_e}\]</span> where <span class="math inline">\(P_o\)</span> is the
observed agreement and <span class="math inline">\(P_e\)</span> is the
expected agreement.</p>
</div>
<h1 id="theorems">Theorems</h1>
<p>Cohen’s Kappa has several important properties that make it a robust
measure of inter-rater agreement. Let us explore these properties
through theorems.</p>
<div class="theorem">
<p>Cohen’s Kappa <span class="math inline">\(\kappa\)</span> ranges from
-1 to 1. Specifically: <span class="math display">\[-1 \leq \kappa \leq
1\]</span> where <span class="math inline">\(\kappa = 1\)</span>
indicates perfect agreement, <span class="math inline">\(\kappa =
0\)</span> indicates agreement equivalent to chance, and <span
class="math inline">\(\kappa &lt; 0\)</span> indicates agreement worse
than chance.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The range of <span
class="math inline">\(\kappa\)</span> can be derived by considering the
possible values of <span class="math inline">\(P_o\)</span> and <span
class="math inline">\(P_e\)</span>. Since <span
class="math inline">\(P_o\)</span> represents the observed agreement, it
ranges from 0 to 1. Similarly, <span class="math inline">\(P_e\)</span>
represents the expected agreement and also ranges from 0 to 1.</p>
<p>The numerator <span class="math inline">\(P_o - P_e\)</span> can
range from <span class="math inline">\(-P_e\)</span> to <span
class="math inline">\(1 - P_e\)</span>. The denominator <span
class="math inline">\(1 - P_e\)</span> ranges from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(1\)</span>.</p>
<p>To find the minimum value of <span
class="math inline">\(\kappa\)</span>, consider when <span
class="math inline">\(P_o = 0\)</span> and <span
class="math inline">\(P_e = 1\)</span>: <span
class="math display">\[\kappa = \frac{0 - 1}{1 - 1} = -1\]</span></p>
<p>To find the maximum value of <span
class="math inline">\(\kappa\)</span>, consider when <span
class="math inline">\(P_o = 1\)</span> and <span
class="math inline">\(P_e = 0\)</span>: <span
class="math display">\[\kappa = \frac{1 - 0}{1 - 0} = 1\]</span></p>
<p>Thus, <span class="math inline">\(\kappa\)</span> ranges from -1 to
1. ◻</p>
</div>
<h1 id="proofs">Proofs</h1>
<p>Let us delve deeper into the proof of Cohen’s Kappa and its
properties.</p>
<div class="proof">
<p><em>Proof of Cohen’s Kappa Formula.</em> The formula for Cohen’s
Kappa is derived from the desire to adjust the observed agreement <span
class="math inline">\(P_o\)</span> by the expected agreement <span
class="math inline">\(P_e\)</span>.</p>
<p>The numerator <span class="math inline">\(P_o - P_e\)</span>
represents the excess agreement beyond what is expected by chance. The
denominator <span class="math inline">\(1 - P_e\)</span> normalizes this
excess agreement, ensuring that the measure ranges from -1 to 1.</p>
<p>Thus, Cohen’s Kappa can be interpreted as the proportion of agreement
not attributable to chance: <span class="math display">\[\kappa =
\frac{P_o - P_e}{1 - P_e}\]</span> ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>Cohen’s Kappa has several important properties that enhance its
utility as a measure of inter-rater agreement.</p>
<div class="corollary">
<p>The interpretation of Cohen’s Kappa is as follows:</p>
<ul>
<li><p><span class="math inline">\(\kappa = 1\)</span>: Perfect
agreement.</p></li>
<li><p><span class="math inline">\(\kappa &gt; 0.8\)</span>: Very strong
agreement.</p></li>
<li><p><span class="math inline">\(0.6 &lt; \kappa \leq 0.8\)</span>:
Substantial agreement.</p></li>
<li><p><span class="math inline">\(0.4 &lt; \kappa \leq 0.6\)</span>:
Moderate agreement.</p></li>
<li><p><span class="math inline">\(0.2 &lt; \kappa \leq 0.4\)</span>:
Fair agreement.</p></li>
<li><p><span class="math inline">\(\kappa \leq 0.2\)</span>: Slight or
no agreement.</p></li>
</ul>
</div>
<div class="proof">
<p><em>Proof of Interpretation.</em> The interpretation of Cohen’s Kappa
is based on empirical studies and guidelines provided by Landis and Koch
(1977). These guidelines are widely accepted in the literature and
provide a framework for evaluating the strength of agreement. ◻</p>
</div>
<div class="corollary">
<p>Cohen’s Kappa is symmetric with respect to the raters. That is,
swapping the roles of rater 1 and rater 2 does not change the value of
<span class="math inline">\(\kappa\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof of Symmetry Property.</em> The observed agreement <span
class="math inline">\(P_o\)</span> and the expected agreement <span
class="math inline">\(P_e\)</span> are both symmetric with respect to
the raters. Therefore, swapping the roles of rater 1 and rater 2 does
not affect the calculation of <span
class="math inline">\(\kappa\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Cohen’s Kappa is a powerful and versatile measure of inter-rater
agreement that adjusts for chance agreement. Its properties and
interpretations make it an indispensable tool in the analysis of
categorical data, ensuring that observed agreements are not merely due
to randomness. By understanding and applying Cohen’s Kappa, researchers
can gain a deeper insight into the reliability of subjective judgments
across various fields.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie cumulative : une mesure de l’incertitude accumulée</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie cumulative : une mesure de l’incertitude
accumulée</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie cumulative émerge comme une notion fondamentale dans
l’étude des systèmes dynamiques et des processus stochastiques. Elle
quantifie l’incertitude accumulée au fil du temps, offrant une
perspective dynamique sur la complexité des systèmes. Cette notion
trouve ses racines dans la théorie de l’information, où l’entropie de
Shannon mesure l’incertitude d’une distribution de probabilité.
Cependant, l’entropie cumulative va plus loin en intégrant cette
incertitude sur des périodes prolongées.</p>
<p>L’importance de l’entropie cumulative réside dans sa capacité à
capturer la mémoire et les dépendances temporelles des systèmes. Elle
est indispensable dans l’analyse des séries temporelles, la modélisation
des processus financiers, et même dans les sciences du vivant pour
comprendre l’évolution des systèmes biologiques. En outre, elle joue un
rôle clé dans la théorie du contrôle et de l’apprentissage automatique,
où la gestion de l’incertitude est cruciale.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie cumulative, commençons par explorer ce que
nous cherchons à mesurer. Imaginons un système qui évolue dans le temps,
avec des états successifs qui ne sont pas entièrement prévisibles. Nous
voulons quantifier l’incertitude totale accumulée au fil de ces états.
Cela nous amène à la définition formelle de l’entropie cumulative.</p>
<div class="definition">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires définies sur un espace probabilisé
<span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. Supposons
que chaque <span class="math inline">\(X_n\)</span> prend ses valeurs
dans un ensemble fini <span class="math inline">\(\mathcal{X}\)</span>.
L’entropie cumulative jusqu’à l’instant <span
class="math inline">\(N\)</span> est définie comme la somme des
entropies conditionnelles successives :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_N) = \sum_{n=1}^N
H(X_n | X_{n-1}, \ldots, X_1)\]</span></p>
<p>où <span class="math inline">\(H(X_n | X_{n-1}, \ldots, X_1)\)</span>
est l’entropie conditionnelle de <span
class="math inline">\(X_n\)</span> sachant les variables
précédentes.</p>
</div>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_N) = -\sum_{x_1,
\ldots, x_N \in \mathcal{X}^N} P(x_1, \ldots, x_N) \log P(x_1, \ldots,
x_N)\]</span></p>
<p>où <span class="math inline">\(P(x_1, \ldots, x_N)\)</span> est la
probabilité jointe des variables <span class="math inline">\(X_1,
\ldots, X_N\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie cumulative est le théorème
de l’information mutuelle cumulée, qui relie l’entropie cumulative à la
somme des informations mutuelles entre les variables successives.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires. L’entropie cumulative peut être
exprimée comme la somme des informations mutuelles successives :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_N) = \sum_{n=2}^N
I(X_n; X_{n-1}, \ldots, X_1) + H(X_1)\]</span></p>
<p>où <span class="math inline">\(I(X_n; X_{n-1}, \ldots, X_1)\)</span>
est l’information mutuelle entre <span
class="math inline">\(X_n\)</span> et les variables précédentes.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’information mutuelle cumulée, nous
procédons par récurrence sur <span class="math inline">\(N\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> <strong>Initialisation :</strong> Pour <span
class="math inline">\(N=1\)</span>, nous avons <span
class="math inline">\(H(X_1) = H(X_1)\)</span>, ce qui est trivialement
vrai.</p>
<p><strong>Hérédité :</strong> Supposons que le théorème soit vrai pour
<span class="math inline">\(N=k\)</span>, c’est-à-dire :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_k) = \sum_{n=2}^k
I(X_n; X_{n-1}, \ldots, X_1) + H(X_1)\]</span></p>
<p>Pour <span class="math inline">\(N=k+1\)</span>, nous avons :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_{k+1}) = H(X_1,
X_2, \ldots, X_k) + H(X_{k+1} | X_k, \ldots, X_1)\]</span></p>
<p>En utilisant la définition de l’information mutuelle conditionnelle,
nous obtenons :</p>
<p><span class="math display">\[H(X_{k+1} | X_k, \ldots, X_1) =
I(X_{k+1}; X_k, \ldots, X_1) + H(X_{k+1} | X_k, \ldots,
X_1)\]</span></p>
<p>En combinant les résultats, nous avons :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_{k+1}) =
\sum_{n=2}^k I(X_n; X_{n-1}, \ldots, X_1) + H(X_1) + I(X_{k+1}; X_k,
\ldots, X_1) + H(X_{k+1} | X_k, \ldots, X_1)\]</span></p>
<p>Ce qui achève la preuve par récurrence. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie cumulative possède plusieurs propriétés intéressantes.
Nous en listons quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Additivité :</strong> Pour des variables indépendantes,
l’entropie cumulative est additive. C’est-à-dire que si <span
class="math inline">\(X_1, X_2, \ldots, X_N\)</span> sont indépendantes,
alors :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_N) = \sum_{n=1}^N
H(X_n)\]</span></p></li>
<li><p><strong>Inégalité de Fano :</strong> L’entropie cumulative est
liée à l’erreur de prédiction. Plus précisément, pour toute fonction de
prédiction <span class="math inline">\(f\)</span>, nous avons :</p>
<p><span class="math display">\[H(X_1, X_2, \ldots, X_N | f(X_1, \ldots,
X_{N-1})) \leq H(\text{erreur})\]</span></p>
<p>où <span class="math inline">\(H(\text{erreur})\)</span> est
l’entropie de l’erreur de prédiction.</p></li>
<li><p><strong>Convergence :</strong> Pour un processus stationnaire
ergodique, l’entropie cumulative normalisée converge vers l’entropie
différentielle :</p>
<p><span class="math display">\[\lim_{N \to \infty} \frac{1}{N} H(X_1,
X_2, \ldots, X_N) = h\]</span></p>
<p>où <span class="math inline">\(h\)</span> est l’entropie
différentielle du processus.</p></li>
</ol>
<p>Chacune de ces propriétés offre des insights précieux sur le
comportement des systèmes dynamiques et stochastiques. Elles sont
largement utilisées dans l’analyse des séries temporelles et la
modélisation des processus complexes.</p>
</body>
</html>
{% include "footer.html" %}


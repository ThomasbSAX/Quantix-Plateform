{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’échantillonnage par importance : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’échantillonnage par importance : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’échantillonnage par importance est une technique fondamentale en
statistique computationnelle, permettant d’estimer des quantités
complexes à partir de données échantillonnées. Son origine remonte aux
travaux pionniers en théorie des probabilités et en inférence
bayésienne, où l’on cherche à approximer des intégrales ou des
espérances sous des distributions difficiles à échantillonner
directement.</p>
<p>Cette méthode est indispensable dans les domaines où les données sont
rares, bruitées ou lorsque l’on doit traiter des modèles complexes. Par
exemple, en apprentissage automatique, elle permet d’estimer des
paramètres de modèles probabilistes ou d’approximer des distributions a
posteriori. Son importance réside dans sa capacité à transformer un
problème difficile en un problème plus tractable, tout en fournissant
des estimateurs convergents sous certaines conditions.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’échantillonnage par importance, commençons par
définir les concepts clés. Supposons que nous avons une distribution de
probabilité <span class="math inline">\(\pi(x)\)</span> sous laquelle
nous souhaitons échantillonner, mais que cet échantillonnage est
difficile ou coûteux. Supposons également que nous avons une autre
distribution <span class="math inline">\(q(x)\)</span>, appelée
distribution d’importance, sous laquelle l’échantillonnage est plus
facile.</p>
<p>Nous cherchons à estimer une quantité de la forme : <span
class="math display">\[\mathbb{E}_{\pi}[h(X)] = \int h(x) \pi(x)
dx\]</span></p>
<p>Pour ce faire, nous utilisons un échantillon <span
class="math inline">\(\{x_i\}_{i=1}^n\)</span> tiré selon <span
class="math inline">\(q(x)\)</span>. La clé est de réécrire l’espérance
sous la distribution <span class="math inline">\(\pi\)</span> en
utilisant la distribution <span class="math inline">\(q\)</span>.</p>
<p>Commençons par exprimer <span class="math inline">\(\pi(x)\)</span>
en termes de <span class="math inline">\(q(x)\)</span> : <span
class="math display">\[\pi(x) = w(x) q(x)\]</span> où <span
class="math inline">\(w(x) = \frac{\pi(x)}{q(x)}\)</span> est appelé le
poids d’importance.</p>
<p>Ainsi, l’espérance peut être réécrite comme : <span
class="math display">\[\mathbb{E}_{\pi}[h(X)] = \int h(x) w(x) q(x)
dx\]</span></p>
<p>En utilisant un échantillon <span
class="math inline">\(\{x_i\}_{i=1}^n\)</span> tiré selon <span
class="math inline">\(q(x)\)</span>, nous pouvons estimer cette
espérance par : <span
class="math display">\[\widehat{\mathbb{E}}_{\pi}[h(X)] = \frac{1}{n}
\sum_{i=1}^n h(x_i) w(x_i)\]</span></p>
<p>Cette estimation est appelée l’estimateur par importance.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en échantillonnage par importance est le
théorème de la loi des grands nombres pour les estimateurs par
importance. Ce théorème garantit que sous certaines conditions,
l’estimateur converge vers la vraie valeur de l’espérance.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> un
échantillon indépendant et identiquement distribué (i.i.d.) selon <span
class="math inline">\(q(x)\)</span>. Supposons que <span
class="math inline">\(\mathbb{E}_q[|h(X) w(X)|] &lt; \infty\)</span>.
Alors, <span class="math display">\[\frac{1}{n} \sum_{i=1}^n h(x_i)
w(x_i) \xrightarrow{n \to \infty} \mathbb{E}_{\pi}[h(X)]\]</span>
presque sûrement.</p>
</div>
<p>La preuve de ce théorème repose sur la loi des grands nombres
classique et les propriétés des espérances conditionnelles. En effet,
l’estimateur par importance peut être vu comme une moyenne empirique
pondérée, et sous les conditions données, cette moyenne converge vers
l’espérance théorique.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la loi des grands nombres pour
l’échantillonnage par importance, commençons par rappeler que : <span
class="math display">\[\mathbb{E}_q[h(X) w(X)] = \int h(x) w(x) q(x) dx
= \mathbb{E}_{\pi}[h(X)]\]</span></p>
<p>Ensuite, par la loi des grands nombres, nous avons : <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n h(x_i) w(x_i)
\xrightarrow{n \to \infty} \mathbb{E}_q[h(X) w(X)]\]</span></p>
<p>En utilisant le fait que <span
class="math inline">\(\mathbb{E}_q[h(X) w(X)] =
\mathbb{E}_{\pi}[h(X)]\)</span>, nous obtenons le résultat désiré.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’échantillonnage par importance possède plusieurs propriétés
importantes qui en font une méthode puissante et flexible.</p>
<ol>
<li><p><strong>Consistance</strong> : Sous les conditions du théorème,
l’estimateur par importance est consistant, c’est-à-dire qu’il converge
vers la vraie valeur de l’espérance lorsque la taille de l’échantillon
tend vers l’infini.</p></li>
<li><p><strong>Efficacité</strong> : L’efficacité de l’estimateur dépend
du choix de la distribution d’importance <span
class="math inline">\(q(x)\)</span>. Idéalement, <span
class="math inline">\(q(x)\)</span> devrait être proche de <span
class="math inline">\(\pi(x)\)</span> pour minimiser la variance de
l’estimateur.</p></li>
<li><p><strong>Variance</strong> : La variance de l’estimateur par
importance peut être exprimée comme : <span
class="math display">\[\text{Var}\left( \frac{1}{n} \sum_{i=1}^n h(x_i)
w(x_i) \right) = \frac{1}{n} \left( \mathbb{E}_q[h(X)^2 w(X)^2] -
(\mathbb{E}_{\pi}[h(X)])^2 \right)\]</span></p>
<p>Cette formule montre que la variance dépend de la distribution
d’importance <span class="math inline">\(q(x)\)</span> et de la fonction
<span class="math inline">\(h\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’échantillonnage par importance est une technique puissante et
flexible pour estimer des quantités complexes à partir de données
échantillonnées. Son utilisation est répandue dans divers domaines,
notamment en statistique computationnelle et en apprentissage
automatique. Les théorèmes et propriétés présentés dans cet article
montrent que cette méthode est rigoureuse et efficace sous certaines
conditions. Le choix judicieux de la distribution d’importance est
crucial pour obtenir des estimateurs précis et efficaces.</p>
</body>
</html>
{% include "footer.html" %}


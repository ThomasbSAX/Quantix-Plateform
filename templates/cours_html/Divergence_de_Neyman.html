{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Neyman : Une Exploration Mathématique et Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Neyman : Une Exploration Mathématique et
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Neyman émerge dans le cadre des tests d’hypothèses
statistiques, un domaine où la rigueur mathématique rencontre
l’application pratique. Introduite par le statisticien polonais Jerzy
Neyman, cette notion est cruciale pour comprendre les propriétés
asymptotiques des tests statistiques. Elle permet de comparer la
puissance d’un test sous différentes hypothèses alternatives et de
déterminer si un test est cohérent ou non.</p>
<p>L’importance de la divergence de Neyman réside dans sa capacité à
fournir des critères pour évaluer la performance des tests statistiques.
En effet, un test est dit cohérent si sa puissance tend vers 1 lorsque
la taille de l’échantillon augmente, ce qui est formalisé par la
divergence de Neyman. Cette notion est indispensable pour garantir que
les tests statistiques sont fiables et puissants, surtout dans des
contextes où les échantillons sont grands.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Neyman, il est essentiel de définir
quelques concepts préliminaires. Considérons un test d’hypothèse où l’on
teste une hypothèse nulle <span class="math inline">\(H_0\)</span>
contre une alternative <span class="math inline">\(H_1\)</span>. La
puissance d’un test est la probabilité de rejeter correctement <span
class="math inline">\(H_0\)</span> lorsque <span
class="math inline">\(H_1\)</span> est vraie.</p>
<p>Supposons que nous avons une statistique de test <span
class="math inline">\(T_n\)</span> basée sur un échantillon de taille
<span class="math inline">\(n\)</span>. La puissance du test peut être
définie comme : <span class="math display">\[\beta_n(\theta) =
P_{\theta}(T_n \in C)\]</span> où <span class="math inline">\(C\)</span>
est la région critique du test et <span
class="math inline">\(\theta\)</span> représente les paramètres sous
l’hypothèse alternative.</p>
<p>La divergence de Neyman est alors définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\beta_n(\theta)\)</span> la
puissance du test sous l’hypothèse alternative <span
class="math inline">\(\theta\)</span>. On dit que le test présente une
divergence de Neyman si, pour toute alternative <span
class="math inline">\(\theta \neq \theta_0\)</span> (où <span
class="math inline">\(\theta_0\)</span> est le paramètre sous <span
class="math inline">\(H_0\)</span>), on a : <span
class="math display">\[\lim_{n \to \infty} \beta_n(\theta) =
1\]</span></p>
</div>
<p>En d’autres termes, la divergence de Neyman indique que la puissance
du test tend vers 1 lorsque la taille de l’échantillon devient
infiniment grande, pour toute alternative différente de l’hypothèse
nulle.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour formaliser davantage la divergence de Neyman, nous introduisons
le théorème suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(T_n\)</span> une statistique de test
basée sur un échantillon de taille <span
class="math inline">\(n\)</span>, et supposons que <span
class="math inline">\(T_n\)</span> converge en loi sous <span
class="math inline">\(H_0\)</span> vers une distribution <span
class="math inline">\(F_0\)</span>. Si, pour toute alternative <span
class="math inline">\(\theta \neq \theta_0\)</span>, la statistique
<span class="math inline">\(T_n\)</span> converge en loi vers une
distribution <span class="math inline">\(F_\theta\)</span> telle que :
<span class="math display">\[F_\theta(C) &gt; F_0(C)\]</span> où <span
class="math inline">\(C\)</span> est la région critique du test, alors
le test présente une divergence de Neyman.</p>
</div>
<p>La preuve de ce théorème repose sur des propriétés asymptotiques des
statistiques de test et des résultats de convergence en loi. Nous
détaillons cette preuve dans la section suivante.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la divergence de Neyman, nous utilisons
les propriétés de convergence en loi et les résultats classiques des
tests d’hypothèses.</p>
<div class="proof">
<p><em>Proof.</em> Considérons une statistique de test <span
class="math inline">\(T_n\)</span> basée sur un échantillon de taille
<span class="math inline">\(n\)</span>. Sous l’hypothèse nulle <span
class="math inline">\(H_0\)</span>, supposons que <span
class="math inline">\(T_n\)</span> converge en loi vers une distribution
<span class="math inline">\(F_0\)</span>. Cela signifie que : <span
class="math display">\[\lim_{n \to \infty} P_{\theta_0}(T_n \leq t) =
F_0(t)\]</span></p>
<p>Pour toute alternative <span class="math inline">\(\theta \neq
\theta_0\)</span>, supposons que <span
class="math inline">\(T_n\)</span> converge en loi vers une distribution
<span class="math inline">\(F_\theta\)</span>. Alors : <span
class="math display">\[\lim_{n \to \infty} P_{\theta}(T_n \leq t) =
F_\theta(t)\]</span></p>
<p>La puissance du test sous <span class="math inline">\(H_1\)</span>
est donnée par : <span class="math display">\[\beta_n(\theta) =
P_{\theta}(T_n \in C)\]</span></p>
<p>En utilisant la convergence en loi, nous avons : <span
class="math display">\[\lim_{n \to \infty} \beta_n(\theta) =
F_\theta(C)\]</span></p>
<p>Puisque <span class="math inline">\(F_\theta(C) &gt; F_0(C)\)</span>,
il s’ensuit que : <span class="math display">\[\lim_{n \to \infty}
\beta_n(\theta) = 1\]</span></p>
<p>Cela montre que le test présente une divergence de Neyman. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés et corollaires importants
liés à la divergence de Neyman.</p>
<ol>
<li><p>Si un test présente une divergence de Neyman, alors il est
cohérent. Cela signifie que la probabilité de rejeter correctement <span
class="math inline">\(H_0\)</span> tend vers 1 lorsque la taille de
l’échantillon augmente.</p></li>
<li><p>La divergence de Neyman est une propriété asymptotique. Elle ne
garantit pas nécessairement la puissance du test pour des tailles
d’échantillon finies.</p></li>
<li><p>La divergence de Neyman peut être utilisée pour comparer la
puissance de différents tests statistiques. Un test présentant une
divergence de Neyman est généralement préféré à un test qui ne le
présente pas.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Neyman est une notion fondamentale en statistique,
offrant des critères pour évaluer la performance des tests d’hypothèses.
En comprenant cette divergence, nous pouvons garantir que les tests
statistiques sont fiables et puissants, surtout dans des contextes où
les échantillons sont grands. Les théorèmes et propriétés associés à la
divergence de Neyman fournissent une base solide pour l’analyse des
tests statistiques et leur application dans divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


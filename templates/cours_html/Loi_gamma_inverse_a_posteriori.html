{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi Gamma Inverse a Posteriori : Une Approche Bayésienne</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi Gamma Inverse a Posteriori : Une Approche
Bayésienne</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La loi Gamma inverse joue un rôle fondamental en statistique
bayésienne, particulièrement dans l’analyse des modèles hiérarchiques et
la théorie de la décision. Son utilisation comme a priori conjugué pour
les variances dans des modèles normaux rend son étude cruciale.
L’émergence de cette loi répond à un besoin de modélisation robuste des
incertitudes, notamment dans les cas où les données présentent une
variabilité significative.</p>
<p>Historiquement, la loi Gamma inverse a été introduite pour faciliter
les calculs bayésiens en fournissant une distribution conjuguée pour les
paramètres de variance. Son importance réside dans sa capacité à
simplifier les inférences tout en préservant la structure probabiliste
sous-jacente. Dans ce chapitre, nous explorons les propriétés
mathématiques de cette loi et son rôle dans l’inférence bayésienne.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la loi Gamma inverse, commençons par définir ce que
nous cherchons à modéliser. Supposons que nous ayons un paramètre de
variance <span class="math inline">\(\sigma^2\)</span> dans un modèle
normal. Nous voulons une distribution a priori qui, lorsqu’elle est mise
à jour avec des données, donne une distribution a posteriori de la même
forme. Cela nous mène naturellement à la loi Gamma inverse.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\alpha, \beta &gt; 0\)</span>. La
loi Gamma inverse avec paramètres <span class="math inline">\((\alpha,
\beta)\)</span> est définie par la densité de probabilité : <span
class="math display">\[f(x|\alpha, \beta) =
\frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha - 1} e^{-\beta / x},
\quad x &gt; 0\]</span> où <span
class="math inline">\(\Gamma(\alpha)\)</span> est la fonction Gamma.</p>
</div>
<p>Une autre formulation, souvent utilisée en statistique bayésienne,
est : <span class="math display">\[X \sim \text{Inv-Gamma}(\alpha,
\beta) \quad \Leftrightarrow \quad Y = \frac{1}{X} \sim
\text{Gamma}(\alpha, \beta)\]</span> où <span
class="math inline">\(Y\)</span> suit une loi Gamma de paramètres <span
class="math inline">\((\alpha, \beta)\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la loi Gamma inverse est sa
propriété de conjugaison dans les modèles normaux.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(X_1, \ldots, X_n\)</span> des
variables aléatoires indépendantes et identiquement distribuées selon
une loi normale <span class="math inline">\(\mathcal{N}(\mu,
\sigma^2)\)</span>. Supposons que le paramètre de variance <span
class="math inline">\(\sigma^2\)</span> suit une loi Gamma inverse a
priori avec paramètres <span class="math inline">\((\alpha,
\beta)\)</span>. Alors, la distribution a posteriori de <span
class="math inline">\(\sigma^2\)</span> est également une loi Gamma
inverse avec des paramètres mis à jour : <span
class="math display">\[\alpha&#39; = \alpha + \frac{n}{2}, \quad
\beta&#39; = \beta + \frac{1}{2} \sum_{i=1}^n (X_i - \mu)^2\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, commençons par écrire la distribution a
priori de <span class="math inline">\(\sigma^2\)</span> et la
vraisemblance des données.</p>
<p>La densité a priori est : <span class="math display">\[\pi(\sigma^2)
= \frac{\beta^\alpha}{\Gamma(\alpha)} (\sigma^2)^{-\alpha - 1} e^{-\beta
/ \sigma^2}\]</span></p>
<p>La vraisemblance des données est : <span
class="math display">\[L(\sigma^2 | X_1, \ldots, X_n) = \prod_{i=1}^n
\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X_i - \mu)^2}{2\sigma^2}} =
(2\pi)^{-n/2} (\sigma^2)^{-n/2} e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n
(X_i - \mu)^2}\]</span></p>
<p>La distribution a posteriori est proportionnelle au produit de la
vraisemblance et de la distribution a priori : <span
class="math display">\[\pi(\sigma^2 | X_1, \ldots, X_n) \propto
(\sigma^2)^{-\alpha - 1} e^{-\beta / \sigma^2} \cdot (\sigma^2)^{-n/2}
e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2}\]</span></p>
<p>En combinant les termes, nous obtenons : <span
class="math display">\[\pi(\sigma^2 | X_1, \ldots, X_n) \propto
(\sigma^2)^{-\alpha - n/2 - 1} e^{-\left(\beta + \frac{1}{2}
\sum_{i=1}^n (X_i - \mu)^2\right) / \sigma^2}\]</span></p>
<p>Cette expression correspond à la densité d’une loi Gamma inverse avec
les nouveaux paramètres <span class="math inline">\((\alpha + n/2, \beta
+ \frac{1}{2} \sum_{i=1}^n (X_i - \mu)^2)\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes de la loi
Gamma inverse :</p>
<ol>
<li><p><strong>Propriété de Conjugaison</strong> : Comme démontré, la
loi Gamma inverse est conjuguée pour le paramètre de variance dans un
modèle normal.</p></li>
<li><p><strong>Moment d’Ordre 1</strong> : L’espérance de la loi Gamma
inverse est donnée par : <span class="math display">\[\mathbb{E}[X] =
\frac{\beta}{\alpha - 1}, \quad \text{pour } \alpha &gt;
1\]</span></p></li>
<li><p><strong>Variance</strong> : La variance de la loi Gamma inverse
est : <span class="math display">\[\text{Var}(X) =
\frac{\beta^2}{(\alpha - 1)^2 (\alpha - 2)}, \quad \text{pour } \alpha
&gt; 2\]</span></p></li>
</ol>
<p>Pour prouver la propriété du moment d’ordre 1, nous utilisons
l’expression de l’espérance pour une variable aléatoire suivant une loi
Gamma inverse. En utilisant la relation <span class="math inline">\(Y =
1/X \sim \text{Gamma}(\alpha, \beta)\)</span>, nous avons : <span
class="math display">\[\mathbb{E}[X] =
\mathbb{E}\left[\frac{1}{Y}\right]\]</span></p>
<p>L’espérance d’une variable Gamma est connue : <span
class="math display">\[\mathbb{E}[Y] = \frac{\alpha}{\beta}\]</span></p>
<p>En utilisant la propriété de l’espérance des inverses, nous obtenons
: <span class="math display">\[\mathbb{E}\left[\frac{1}{Y}\right] =
\frac{\beta}{\alpha - 1}, \quad \text{pour } \alpha &gt; 1\]</span></p>
<p>Cette preuve illustre la richesse des propriétés analytiques de la
loi Gamma inverse et son utilité dans les modèles bayésiens.</p>
</body>
</html>
{% include "footer.html" %}


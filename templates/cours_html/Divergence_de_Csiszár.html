{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Csiszár : Une mesure d’information pour l’apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Csiszár : Une mesure d’information pour
l’apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Csiszár, souvent appelée <span
class="math inline">\(f\)</span>-divergence, est une mesure
d’information qui généralise plusieurs notions classiques telles que la
divergence de Kullback-Leibler, la distance de Hellinger, et l’écart en
variation totale. Introduite par Imre Csiszár en 1967, cette mesure joue
un rôle fondamental dans la théorie de l’information et les
statistiques. Elle trouve des applications dans divers domaines,
notamment en apprentissage automatique, en théorie des codes et en
inférence statistique.</p>
<p>L’émergence de la divergence de Csiszár est motivée par le besoin de
disposer d’une mesure flexible et générale pour comparer des
distributions de probabilité. Contrairement à la divergence de
Kullback-Leibler, qui est spécifique, la divergence de Csiszár permet
une généralisation à travers une fonction convexe <span
class="math inline">\(f\)</span>, offrant ainsi une richesse
d’applications.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Csiszár, commençons par comprendre
ce que nous cherchons à mesurer. Supposons que nous ayons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> sur un espace <span
class="math inline">\(\mathcal{X}\)</span>. Nous voulons quantifier la
différence entre ces deux distributions. La divergence de Csiszár
généralise cette idée en utilisant une fonction convexe <span
class="math inline">\(f\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace <span class="math inline">\(\mathcal{X}\)</span>, et soit
<span class="math inline">\(f: [0, \infty) \rightarrow
\mathbb{R}\)</span> une fonction convexe. La divergence de Csiszár
associée à <span class="math inline">\(f\)</span> est définie par :
<span class="math display">\[D_f(P \parallel Q) = \sum_{x \in
\mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right),\]</span> où <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité de
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement, et <span
class="math inline">\(q(x) &gt; 0\)</span> pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span>.</p>
<p>De manière équivalente, si <span
class="math inline">\(\mathcal{X}\)</span> est un espace continu, la
divergence de Csiszár s’écrit : <span class="math display">\[D_f(P
\parallel Q) = \int_{\mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right)
\, dx.\]</span></p>
</div>
<p>La divergence de Csiszár peut également être exprimée en termes de
l’espérance conditionnelle : <span class="math display">\[D_f(P
\parallel Q) =
\mathbb{E}_Q\left[f\left(\frac{dP}{dQ}\right)\right],\]</span> où <span
class="math inline">\(\frac{dP}{dQ}\)</span> est la dérivée de
Radon-Nikodym de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence de Csiszár est
l’inégalité de Gibbs, qui établit une borne inférieure pour cette
divergence.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace <span class="math inline">\(\mathcal{X}\)</span>, et soit
<span class="math inline">\(f: [0, \infty) \rightarrow
\mathbb{R}\)</span> une fonction convexe telle que <span
class="math inline">\(f(1) = 0\)</span> et <span
class="math inline">\(f&#39;(1) = 0\)</span>. Alors, la divergence de
Csiszár satisfait l’inégalité suivante : <span
class="math display">\[D_f(P \parallel Q) \geq 0,\]</span> avec égalité
si et seulement si <span class="math inline">\(P = Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer l’inégalité de Gibbs, nous utilisons
le fait que <span class="math inline">\(f\)</span> est convexe et
satisfait <span class="math inline">\(f(1) = 0\)</span> et <span
class="math inline">\(f&#39;(1) = 0\)</span>. Par la convexité de <span
class="math inline">\(f\)</span>, nous avons pour tout <span
class="math inline">\(t &gt; 0\)</span> : <span
class="math display">\[f(t) \geq f(1) + f&#39;(1)(t - 1) = t -
1.\]</span></p>
<p>En remplaçant <span class="math inline">\(t\)</span> par <span
class="math inline">\(\frac{p(x)}{q(x)}\)</span>, nous obtenons : <span
class="math display">\[f\left(\frac{p(x)}{q(x)}\right) \geq
\frac{p(x)}{q(x)} - 1.\]</span></p>
<p>En prenant l’espérance par rapport à <span
class="math inline">\(Q\)</span>, nous avons : <span
class="math display">\[\mathbb{E}_Q\left[f\left(\frac{dP}{dQ}\right)\right]
\geq \mathbb{E}_Q\left[\frac{dP}{dQ} - 1\right] = 1 - 1 =
0.\]</span></p>
<p>L’égalité a lieu si et seulement si <span
class="math inline">\(f\left(\frac{p(x)}{q(x)}\right) =
\frac{p(x)}{q(x)} - 1\)</span> pour presque tout <span
class="math inline">\(x \in \mathcal{X}\)</span>, ce qui implique que
<span class="math inline">\(\frac{p(x)}{q(x)} = 1\)</span> pour presque
tout <span class="math inline">\(x \in \mathcal{X}\)</span>,
c’est-à-dire que <span class="math inline">\(P = Q\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Csiszár possède plusieurs propriétés intéressantes,
que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Convexité</strong> : La divergence de Csiszár est convexe
en ses deux arguments. Plus précisément, pour toute fonction convexe
<span class="math inline">\(f\)</span>, la fonction <span
class="math inline">\((P, Q) \mapsto D_f(P \parallel Q)\)</span> est
convexe.</p></li>
<li><p><strong>Invariance par transformation</strong> : La divergence de
Csiszár est invariante par transformation mesurable. Si <span
class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{Y}\)</span>
est une transformation mesurable, alors : <span
class="math display">\[D_f(P \circ \phi^{-1} \parallel Q \circ
\phi^{-1}) = D_f(P \parallel Q).\]</span></p></li>
<li><p><strong>Limite de continuité</strong> : La divergence de Csiszár
est continue en ses deux arguments. Plus précisément, si <span
class="math inline">\(P_n \rightarrow P\)</span> et <span
class="math inline">\(Q_n \rightarrow Q\)</span> en variation totale,
alors : <span class="math display">\[D_f(P_n \parallel Q_n) \rightarrow
D_f(P \parallel Q).\]</span></p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Csiszár est une mesure d’information puissante et
flexible, qui généralise plusieurs notions classiques. Ses applications
sont vastes, allant de la théorie de l’information à l’apprentissage
automatique. Les propriétés et théorèmes associés à cette divergence en
font un outil indispensable pour les chercheurs et les praticiens dans
divers domaines.</p>
</body>
</html>
{% include "footer.html" %}


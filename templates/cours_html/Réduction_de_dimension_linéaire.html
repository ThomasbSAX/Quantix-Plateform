{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Réduction de dimension linéaire : Méthodes et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Réduction de dimension linéaire : Méthodes et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La réduction de dimension linéaire est une technique fondamentale en
analyse des données et en apprentissage automatique. Elle trouve son
origine dans les travaux de Karl Pearson sur l’analyse en composantes
principales (ACP) à la fin du XIXème siècle. L’objectif est de projeter
des données de haute dimension dans un espace de plus faible dimension
tout en préservant au maximum leur structure intrinsèque.</p>
<p>Cette approche est indispensable dans de nombreux domaines où les
données sont complexes et de grande dimension. Par exemple, en
traitement du signal, en vision par ordinateur ou en bioinformatique. La
réduction de dimension permet non seulement de diminuer la complexité
computationnelle, mais aussi d’éliminer les redondances et le bruit dans
les données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la réduction de dimension linéaire, commençons par
définir quelques concepts clés. Supposons que nous ayons un ensemble de
données <span class="math inline">\(\mathcal{D} = \{x_1, x_2, \dots,
x_n\}\)</span> où chaque <span class="math inline">\(x_i \in
\mathbb{R}^d\)</span>. Notre objectif est de trouver une projection
linéaire <span class="math inline">\(P: \mathbb{R}^d \rightarrow
\mathbb{R}^k\)</span> avec <span class="math inline">\(k &lt; d\)</span>
qui minimise une certaine perte d’information.</p>
<div class="definition">
<p>Soit <span class="math inline">\(A\)</span> une matrice <span
class="math inline">\(d \times k\)</span> de rang plein. La projection
linéaire <span class="math inline">\(P_A: \mathbb{R}^d \rightarrow
\mathbb{R}^k\)</span> est définie par : <span
class="math display">\[P_A(x) = A^T x\]</span> pour tout <span
class="math inline">\(x \in \mathbb{R}^d\)</span>.</p>
</div>
<p>Une autre manière de formuler cette définition est la suivante :
<span class="math display">\[P_A(x) = \langle x, A \rangle\]</span> où
<span class="math inline">\(\langle \cdot, \cdot \rangle\)</span>
désigne le produit scalaire usuel.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux en réduction de dimension linéaire est
le théorème suivant, qui donne une condition nécessaire et suffisante
pour que la projection préserve les distances entre les points.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(P_A\)</span> une projection
linéaire. Alors, pour tout <span class="math inline">\(x, y \in
\mathbb{R}^d\)</span>, on a : <span class="math display">\[\|P_A(x) -
P_A(y)\|_2 \leq \|x - y\|_2\]</span> avec égalité si et seulement si
<span class="math inline">\(x - y \in \text{Im}(A)\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur les propriétés du
produit scalaire et de la norme euclidienne. Commençons par développer
l’expression de <span class="math inline">\(\|P_A(x) -
P_A(y)\|_2\)</span> : <span class="math display">\[\|P_A(x) -
P_A(y)\|_2^2 = \|A^T (x - y)\|_2^2 = \langle A^T (x - y), A^T (x - y)
\rangle\]</span> <span class="math display">\[= \langle A A^T (x - y),
(x - y) \rangle\]</span></p>
<p>En utilisant le fait que <span class="math inline">\(A A^T\)</span>
est une matrice symétrique positive, on peut écrire : <span
class="math display">\[\langle A A^T (x - y), (x - y) \rangle \leq
\lambda_{\text{max}}(A A^T) \|x - y\|_2^2\]</span> où <span
class="math inline">\(\lambda_{\text{max}}(A A^T)\)</span> est la plus
grande valeur propre de <span class="math inline">\(A A^T\)</span>.</p>
<p>Or, comme <span class="math inline">\(A\)</span> est une matrice de
rang plein, <span class="math inline">\(\lambda_{\text{max}}(A A^T) \leq
1\)</span>. Donc : <span class="math display">\[\|P_A(x) - P_A(y)\|_2^2
\leq \|x - y\|_2^2\]</span></p>
<p>L’égalité a lieu si et seulement si <span class="math inline">\(x -
y\)</span> est un vecteur propre associé à la plus grande valeur propre
de <span class="math inline">\(A A^T\)</span>, ce qui équivaut à dire
que <span class="math inline">\(x - y \in \text{Im}(A)\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer les techniques de preuve en réduction de dimension,
considérons le théorème suivant sur la minimisation de l’erreur de
reconstruction.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(P_A\)</span> une projection
linéaire. L’erreur de reconstruction moyenne est donnée par : <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n \|x_i -
P_A(x_i)\|_2^2\]</span> Cette erreur est minimisée lorsque <span
class="math inline">\(A\)</span> est formé des <span
class="math inline">\(k\)</span> vecteurs propres associés aux plus
grandes valeurs propres de la matrice de covariance <span
class="math inline">\(\Sigma = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)(x_i
- \mu)^T\)</span>, où <span class="math inline">\(\mu\)</span> est le
centroïde des données.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Commençons par développer l’erreur de reconstruction
: <span class="math display">\[\frac{1}{n} \sum_{i=1}^n \|x_i -
P_A(x_i)\|_2^2 = \frac{1}{n} \sum_{i=1}^n \|(I - A A^T)(x_i -
\mu)\|_2^2\]</span></p>
<p>En utilisant le développement en série de Fourier, on peut écrire :
<span class="math display">\[(I - A A^T)(x_i - \mu) = \sum_{j=1}^d
\langle x_i - \mu, v_j \rangle (I - A A^T) v_j\]</span> où <span
class="math inline">\(\{v_j\}_{j=1}^d\)</span> est une base orthonormée
de <span class="math inline">\(\mathbb{R}^d\)</span>.</p>
<p>L’erreur de reconstruction devient alors : <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n \left\| \sum_{j=1}^d
\langle x_i - \mu, v_j \rangle (I - A A^T) v_j \right\|_2^2\]</span></p>
<p>En utilisant l’orthonormalité de <span
class="math inline">\(\{v_j\}_{j=1}^d\)</span>, on obtient : <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^d |\langle
x_i - \mu, v_j \rangle|^2 \|(I - A A^T) v_j\|_2^2\]</span></p>
<p>Or, <span class="math inline">\(\|(I - A A^T) v_j\|_2^2 = 1 - \|A^T
v_j\|_2^2\)</span>. Donc, l’erreur de reconstruction est minimisée
lorsque <span class="math inline">\(\|A^T v_j\|_2^2\)</span> est
maximisé pour les <span class="math inline">\(k\)</span> plus grandes
valeurs propres de <span class="math inline">\(\Sigma\)</span>.</p>
<p>Ainsi, <span class="math inline">\(A\)</span> doit être formé des
<span class="math inline">\(k\)</span> vecteurs propres associés aux
plus grandes valeurs propres de <span
class="math inline">\(\Sigma\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
réduction de dimension linéaire.</p>
<ol>
<li><p><strong>Invariance par Translation</strong> : La réduction de
dimension linéaire est invariante par translation des données. Cela
signifie que si on translate tous les points <span
class="math inline">\(x_i\)</span> par un vecteur constant <span
class="math inline">\(c\)</span>, la projection linéaire reste
inchangée.</p></li>
<li><p><strong>Invariance par Homothétie</strong> : La réduction de
dimension linéaire est également invariante par homothétie des données.
Si on multiplie tous les points <span class="math inline">\(x_i\)</span>
par un scalaire <span class="math inline">\(\alpha\)</span>, la
projection linéaire est multipliée par le même scalaire.</p></li>
<li><p><strong>Stabilité Numérique</strong> : La réduction de dimension
linéaire est stable numériquement lorsque la matrice <span
class="math inline">\(A\)</span> est bien conditionnée. Cela signifie
que de petites perturbations dans les données entraînent de petites
perturbations dans la projection.</p></li>
</ol>
<p>Pour prouver la propriété (i), considérons un vecteur de translation
<span class="math inline">\(c \in \mathbb{R}^d\)</span>. La projection
linéaire des données translatées est : <span
class="math display">\[P_A(x_i + c) = A^T (x_i + c) = A^T x_i + A^T
c\]</span> Or, <span class="math inline">\(A^T c\)</span> est un vecteur
constant qui ne dépend pas de <span class="math inline">\(i\)</span>.
Donc, la structure relative des points projetés reste inchangée.</p>
<p>Pour prouver la propriété (ii), considérons un scalaire <span
class="math inline">\(\alpha \neq 0\)</span>. La projection linéaire des
données homothétiques est : <span class="math display">\[P_A(\alpha x_i)
= A^T (\alpha x_i) = \alpha (A^T x_i)\]</span> Ce qui montre que la
projection est également multipliée par <span
class="math inline">\(\alpha\)</span>.</p>
<p>Pour prouver la propriété (iii), nous utilisons le fait que la norme
de l’erreur de reconstruction est une fonction Lipschitzienne de la
matrice <span class="math inline">\(A\)</span>. Plus précisément, si
<span class="math inline">\(\delta A\)</span> est une petite
perturbation de <span class="math inline">\(A\)</span>, alors : <span
class="math display">\[\|(P_{A + \delta A}(x_i) - P_A(x_i)\|_2 \leq L
\|\delta A\|_2\]</span> où <span class="math inline">\(L\)</span> est
une constante qui dépend de la conditionnement de <span
class="math inline">\(A\)</span>.</p>
</body>
</html>
{% include "footer.html" %}


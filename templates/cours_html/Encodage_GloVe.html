{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage GloVe : Une Approche Innovante pour les Représentations de Mots</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage GloVe : Une Approche Innovante pour les
Représentations de Mots</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage GloVe, acronyme de <em>Global Vectors for Word
Representation</em>, représente une avancée significative dans le
domaine du traitement automatique des langues (TAL). Cette méthode,
développée par Jeffrey Pennington, Richard Socher et Christopher D.
Manning en 2014, vise à capturer les relations sémantiques entre les
mots à travers des vecteurs de dimension réduite. L’idée centrale est
d’apprendre ces représentations en exploitant les statistiques globales
des co-occurrences de mots dans un corpus, offrant ainsi une alternative
efficace aux modèles comme Word2Vec.</p>
<p>L’émergence de GloVe est motivée par le besoin de modèles capables de
représenter les mots dans un espace vectoriel tout en préservant leurs
relations sémantiques. Les modèles précédents, bien que performants,
souffraient de limitations soit en termes de précision, soit en termes
d’efficacité computationnelle. GloVe combine les avantages des modèles
basés sur les contextes locaux (comme Word2Vec) et ceux basés sur les
statistiques globales, permettant ainsi une meilleure généralisation et
une réduction de la dimensionnalité des vecteurs.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’encodage GloVe, il est essentiel de définir
quelques concepts clés.</p>
<h2 class="unnumbered" id="co-occurrence-de-mots">Co-occurrence de
Mots</h2>
<p>Considérons un corpus textuel <span class="math inline">\(C\)</span>
et définissons une fenêtre de contexte de taille <span
class="math inline">\(k\)</span>. Pour un mot cible <span
class="math inline">\(w_i\)</span>, nous cherchons à capturer les mots
qui apparaissent dans cette fenêtre autour de <span
class="math inline">\(w_i\)</span>. La matrice de co-occurrence <span
class="math inline">\(X\)</span> est définie comme suit :</p>
<p><span class="math display">\[X_{ij} = P(w_j | w_i) \cdot
X_i\]</span></p>
<p>où <span class="math inline">\(P(w_j | w_i)\)</span> est la
probabilité conditionnelle que le mot <span
class="math inline">\(w_j\)</span> apparaisse dans le contexte de <span
class="math inline">\(w_i\)</span>, et <span
class="math inline">\(X_i\)</span> est la fréquence totale du mot <span
class="math inline">\(w_i\)</span>.</p>
<h2 class="unnumbered" id="représentations-vectorielles">Représentations
Vectorielles</h2>
<p>L’objectif est de trouver des vecteurs <span
class="math inline">\(\mathbf{v}_w\)</span> pour chaque mot <span
class="math inline">\(w\)</span> dans le vocabulaire, tels que la
similarité entre deux mots soit proportionnelle à leur co-occurrence.
Formellement, nous cherchons :</p>
<p><span class="math display">\[\mathbf{v}_w \cdot \mathbf{v}_{w&#39;}
\approx \log X_{ww&#39;}\]</span></p>
<p>où <span class="math inline">\(\mathbf{v}_w\)</span> est le vecteur
représentant le mot <span class="math inline">\(w\)</span>, et <span
class="math inline">\(X_{ww&#39;}\)</span> est la co-occurrence entre
les mots <span class="math inline">\(w\)</span> et <span
class="math inline">\(w&#39;\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le modèle GloVe repose sur un théorème central qui lie les
co-occurrences aux produits scalaires des vecteurs de mots.</p>
<h2 class="unnumbered" id="théorème-de-glove">Théorème de GloVe</h2>
<p>Soit <span class="math inline">\(X\)</span> la matrice de
co-occurrence, et <span class="math inline">\(\mathbf{v}_w\)</span> le
vecteur représentant le mot <span class="math inline">\(w\)</span>. Le
théorème de GloVe stipule que :</p>
<p><span class="math display">\[\mathbf{v}_w \cdot \mathbf{v}_{w&#39;} =
F(X_{ww&#39;})\]</span></p>
<p>où <span class="math inline">\(F\)</span> est une fonction de poids
qui capture la relation entre les co-occurrences et les produits
scalaires.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de GloVe repose sur l’analyse des statistiques
de co-occurrence et l’optimisation des vecteurs de mots.</p>
<h2 class="unnumbered" id="preuve-du-théorème-de-glove">Preuve du
Théorème de GloVe</h2>
<p>Considérons la fonction de coût suivante :</p>
<p><span class="math display">\[J = \sum_{w,w&#39;} f(X_{ww&#39;})
(\mathbf{v}_w^T \mathbf{v}_{w&#39;} - \log X_{ww&#39;})^2\]</span></p>
<p>où <span class="math inline">\(f(X_{ww&#39;})\)</span> est une
fonction de poids qui atténue l’importance des co-occurrences rares.
L’objectif est de minimiser cette fonction de coût par rapport aux
vecteurs <span class="math inline">\(\mathbf{v}_w\)</span>.</p>
<p>En utilisant la méthode du gradient descendant, nous obtenons :</p>
<p><span class="math display">\[\frac{\partial J}{\partial \mathbf{v}_w}
= 2 \sum_{w&#39;} f(X_{ww&#39;}) (\mathbf{v}_w^T \mathbf{v}_{w&#39;} -
\log X_{ww&#39;}) \mathbf{v}_{w&#39;}\]</span></p>
<p>En résolvant cette équation, nous trouvons les vecteurs <span
class="math inline">\(\mathbf{v}_w\)</span> qui minimisent la fonction
de coût, ce qui prouve le théorème.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le modèle GloVe possède plusieurs propriétés intéressantes qui en
font un outil puissant pour le TAL.</p>
<h2 class="unnumbered" id="propriétés">Propriétés</h2>
<ol>
<li><p>**Préservation des Relations Sémantiques** : Les vecteurs de mots
appris par GloVe préservent les relations sémantiques entre les mots.
Par exemple, la relation <span class="math inline">\(\text{roi} -
\text{homme} + \text{femme} \approx \text{reine}\)</span> est capturée
par les vecteurs.</p></li>
<li><p>**Efficacité Computationnelle** : GloVe est plus efficace que les
modèles basés sur les contextes locaux, car il exploite les statistiques
globales de co-occurrence.</p></li>
<li><p>**Réduction de la Dimensionnalité** : Les vecteurs de mots appris
par GloVe ont une dimension réduite, ce qui permet de réduire la
complexité computationnelle des modèles de TAL.</p></li>
</ol>
<h2 class="unnumbered" id="corollaires">Corollaires</h2>
<ol>
<li><p>**Amélioration des Performances** : L’utilisation de GloVe dans
les modèles de TAL améliore significativement les performances des
tâches comme la classification de texte et la traduction
automatique.</p></li>
<li><p>**Généralisation** : Les vecteurs de mots appris par GloVe
généralisent bien aux nouveaux corpus, ce qui les rend utiles pour une
variété de tâches de TAL.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage GloVe représente une avancée majeure dans le domaine du
traitement automatique des langues. En combinant les avantages des
modèles basés sur les contextes locaux et globaux, GloVe offre une
méthode efficace pour apprendre des représentations vectorielles de mots
qui capturent les relations sémantiques. Les propriétés et corollaires
de GloVe en font un outil précieux pour une variété de tâches de TAL, et
son utilisation continue de croître dans la communauté scientifique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Méthode de Frank-Wolfe : Optimisation Convexe et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Méthode de Frank-Wolfe : Optimisation Convexe et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’optimisation convexe est un domaine central en mathématiques
appliquées, en sciences de l’ingénieur et en économie. Parmi les
méthodes d’optimisation convexe, la méthode de Frank-Wolfe se distingue
par sa simplicité et son efficacité pour résoudre des problèmes
d’optimisation sous contraintes. Introduite par Margot Frank et Philip
Wolfe en 1956, cette méthode est particulièrement adaptée aux problèmes
où la fonction objectif est convexe et les contraintes définissent un
polyèdre convexe.</p>
<p>La méthode de Frank-Wolfe est motivée par la nécessité de résoudre
des problèmes d’optimisation où les contraintes sont complexes et où les
méthodes classiques, comme la méthode du gradient ou la méthode de
Newton, peuvent être inefficaces ou difficiles à appliquer. Cette
méthode est basée sur une idée simple : à chaque itération, on approxe
la solution par un point extrême du polyèdre des contraintes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de présenter la méthode de Frank-Wolfe, il est essentiel de
définir quelques notions clés.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe et <span
class="math inline">\(C \subset \mathbb{R}^n\)</span> un ensemble
convexe. Le problème d’optimisation convexe consiste à trouver un point
<span class="math inline">\(x^* \in C\)</span> tel que <span
class="math display">\[f(x^*) = \min_{x \in C} f(x).\]</span></p>
</div>
<div class="definition">
<p>Un point <span class="math inline">\(x \in C\)</span> est un point
extrême de <span class="math inline">\(C\)</span> si pour tout <span
class="math inline">\(y, z \in C\)</span>, l’égalité <span
class="math inline">\(x = \lambda y + (1 - \lambda) z\)</span> avec
<span class="math inline">\(0 &lt; \lambda &lt; 1\)</span> implique que
<span class="math inline">\(y = z = x\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>La méthode de Frank-Wolfe repose sur plusieurs théorèmes fondamentaux
en optimisation convexe.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe différentiable et <span
class="math inline">\(C \subset \mathbb{R}^n\)</span> un ensemble
convexe compact. Supposons que <span class="math inline">\(f\)</span>
est Lipschitz continue sur <span class="math inline">\(C\)</span>.
Alors, la méthode de Frank-Wolfe converge vers un point optimal <span
class="math inline">\(x^* \in C\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur plusieurs étapes
clés. Tout d’abord, nous montrons que la méthode de Frank-Wolfe génère
une suite <span class="math inline">\(\{x^k\}\)</span> telle que chaque
itération <span class="math inline">\(x^{k+1}\)</span> est une
combinaison convexe de <span class="math inline">\(x^k\)</span> et d’un
point extrême <span class="math inline">\(s^k\)</span> du polyèdre des
contraintes. Ensuite, nous utilisons la convexité de <span
class="math inline">\(f\)</span> pour montrer que la suite <span
class="math inline">\(\{f(x^k)\}\)</span> est décroissante. Enfin, nous
montrons que la suite <span class="math inline">\(\{x^k\}\)</span>
converge vers un point optimal <span
class="math inline">\(x^*\)</span>.</p>
<p>Plus précisément, supposons que la fonction objectif <span
class="math inline">\(f\)</span> est Lipschitz continue avec une
constante de Lipschitz <span class="math inline">\(L &gt; 0\)</span>.
Alors, pour tout <span class="math inline">\(x, y \in C\)</span>, nous
avons <span class="math display">\[|f(x) - f(y)| \leq L \|x -
y\|.\]</span></p>
<p>À chaque itération <span class="math inline">\(k\)</span>, nous
choisissons un point extrême <span class="math inline">\(s^k\)</span> du
polyèdre des contraintes qui minimise la direction de descente. Ensuite,
nous mettons à jour la solution actuelle <span
class="math inline">\(x^k\)</span> en utilisant une combinaison convexe
de <span class="math inline">\(x^k\)</span> et <span
class="math inline">\(s^k\)</span>. Plus précisément, nous avons <span
class="math display">\[x^{k+1} = (1 - \alpha_k) x^k + \alpha_k
s^k,\]</span> où <span class="math inline">\(\alpha_k\)</span> est un
pas d’apprentissage approprié.</p>
<p>En utilisant la convexité de <span class="math inline">\(f\)</span>,
nous pouvons montrer que <span class="math display">\[f(x^{k+1}) \leq (1
- \alpha_k) f(x^k) + \alpha_k f(s^k).\]</span></p>
<p>En choisissant <span class="math inline">\(\alpha_k\)</span> de
manière appropriée, nous pouvons garantir que la suite <span
class="math inline">\(\{f(x^k)\}\)</span> est décroissante. En outre,
nous pouvons montrer que la suite <span
class="math inline">\(\{x^k\}\)</span> converge vers un point optimal
<span class="math inline">\(x^*\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La méthode de Frank-Wolfe possède plusieurs propriétés intéressantes
qui en font une méthode puissante pour l’optimisation convexe.</p>
<div class="corollaire">
<p>La méthode de Frank-Wolfe converge vers un point optimal <span
class="math inline">\(x^* \in C\)</span> en un nombre fini d’itérations
si la fonction objectif <span class="math inline">\(f\)</span> est
strictement convexe.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\(f\)</span>
est strictement convexe. Alors, la suite <span
class="math inline">\(\{f(x^k)\}\)</span> est décroissante et bornée
inférieurement. Par conséquent, elle converge vers une limite <span
class="math inline">\(f^*\)</span>. En outre, la suite <span
class="math inline">\(\{x^k\}\)</span> est bornée et possède une
sous-suite convergente. En utilisant la stricte convexité de <span
class="math inline">\(f\)</span>, nous pouvons montrer que toute
sous-suite convergente de <span class="math inline">\(\{x^k\}\)</span>
converge vers un point optimal <span class="math inline">\(x^*\)</span>.
Par conséquent, la méthode de Frank-Wolfe converge vers un point optimal
<span class="math inline">\(x^*\)</span> en un nombre fini
d’itérations. ◻</p>
</div>
<div class="corollaire">
<p>La méthode de Frank-Wolfe a une complexité en temps de <span
class="math inline">\(O(1/\epsilon)\)</span> pour atteindre une
précision <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire repose sur l’analyse de la
convergence de la méthode de Frank-Wolfe. En utilisant les propriétés de
convexité et de Lipschitz continuité, nous pouvons montrer que la
méthode de Frank-Wolfe réduit l’écart entre la fonction objectif et sa
valeur optimale d’au moins un facteur <span class="math inline">\(1 -
\alpha_k\)</span> à chaque itération. En choisissant <span
class="math inline">\(\alpha_k\)</span> de manière appropriée, nous
pouvons garantir que la méthode de Frank-Wolfe atteint une précision
<span class="math inline">\(\epsilon &gt; 0\)</span> en un nombre
d’itérations proportionnel à <span
class="math inline">\(1/\epsilon\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La méthode de Frank-Wolfe est une méthode puissante et efficace pour
l’optimisation convexe. Elle repose sur des idées simples mais
profondes, et elle possède plusieurs propriétés intéressantes qui en
font une méthode de choix pour résoudre des problèmes d’optimisation
sous contraintes. En outre, cette méthode a été appliquée avec succès
dans de nombreux domaines, notamment en économie, en sciences de
l’ingénieur et en apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


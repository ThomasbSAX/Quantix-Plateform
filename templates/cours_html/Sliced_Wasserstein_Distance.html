{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Sliced Wasserstein Distance: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Sliced Wasserstein Distance: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of optimal transport, introduced by Gaspard Monge in 1781
and later developed by Leonid Kantorovich, has become a cornerstone of
modern mathematics with applications ranging from economics to machine
learning. Among the various distances derived from optimal transport
theory, the Sliced Wasserstein Distance (SWD) has emerged as a
computationally efficient and theoretically rich metric.</p>
<p>The SWD is particularly appealing due to its ability to circumvent
the computational challenges associated with traditional Wasserstein
distances, which often involve solving complex optimization problems. By
projecting high-dimensional distributions onto one-dimensional subspaces
and computing the Wasserstein distance in these lower-dimensional
spaces, the SWD offers a scalable solution for comparing probability
distributions.</p>
<p>This article aims to explore the Sliced Wasserstein Distance in
depth, covering its definitions, key theorems, proofs, and properties.
We will delve into the mathematical foundations that make SWD a powerful
tool in various applications, from statistical learning to computer
vision.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the Sliced Wasserstein Distance, we first need to grasp
the concept of the one-dimensional Wasserstein distance and how it
extends to higher dimensions through slicing.</p>
<p>Consider two probability measures <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> on <span
class="math inline">\(\mathbb{R}^d\)</span>. The one-dimensional
Wasserstein distance between the projections of <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> onto a direction <span
class="math inline">\(u \in \mathbb{S}^{d-1}\)</span> (the unit sphere
in <span class="math inline">\(\mathbb{R}^d\)</span>) is defined as
follows:</p>
<div class="definition">
<p>For a given direction <span class="math inline">\(u \in
\mathbb{S}^{d-1}\)</span>, the one-dimensional Wasserstein distance
between the projections of <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\nu\)</span> onto <span
class="math inline">\(u\)</span> is given by: <span
class="math display">\[W_1(\mu_u, \nu_u) = \int_{\mathbb{R}}
|F_\mu(u^\top x) - F_\nu(u^\top x)| \, dx\]</span> where <span
class="math inline">\(F_\mu\)</span> and <span
class="math inline">\(F_\nu\)</span> are the cumulative distribution
functions (CDFs) of <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> projected onto <span
class="math inline">\(u\)</span>.</p>
</div>
<p>The Sliced Wasserstein Distance is then defined as the average of
these one-dimensional Wasserstein distances over all possible directions
<span class="math inline">\(u \in \mathbb{S}^{d-1}\)</span>:</p>
<div class="definition">
<p>The Sliced Wasserstein Distance between <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> is: <span
class="math display">\[SW_1(\mu, \nu) = \int_{\mathbb{S}^{d-1}}
W_1(\mu_u, \nu_u) \, du\]</span> where <span
class="math inline">\(du\)</span> is the uniform measure on <span
class="math inline">\(\mathbb{S}^{d-1}\)</span>.</p>
</div>
<p>In practice, the integral over all directions is often approximated
using Monte Carlo methods by sampling a finite number of directions.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key theorems related to the Sliced Wasserstein Distance is
its equivalence to the one-dimensional Wasserstein distance under
certain conditions. This theorem highlights the computational advantages
of SWD.</p>
<div class="theorem">
<p>For any two probability measures <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> on <span
class="math inline">\(\mathbb{R}^d\)</span>, the Sliced Wasserstein
Distance <span class="math inline">\(SW_1(\mu, \nu)\)</span> is bounded
by the one-dimensional Wasserstein distance <span
class="math inline">\(W_1(\mu, \nu)\)</span>: <span
class="math display">\[SW_1(\mu, \nu) \leq W_1(\mu, \nu)\]</span>
Moreover, if <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> are absolutely continuous with
respect to the Lebesgue measure, then: <span
class="math display">\[SW_1(\mu, \nu) = W_1(\mu, \nu)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof of this theorem relies on the fact that the
one-dimensional Wasserstein distance is a lower bound for the Sliced
Wasserstein Distance. By the properties of optimal transport, we know
that: <span class="math display">\[W_1(\mu, \nu) = \sup_{\|f\|_L \leq 1}
\left( \int f \, d\mu - \int f \, d\nu \right)\]</span> where the
supremum is taken over all 1-Lipschitz functions <span
class="math inline">\(f\)</span>. The Sliced Wasserstein Distance, being
an average of one-dimensional Wasserstein distances, inherits this
property and thus provides a lower bound.</p>
<p>For the case where <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\nu\)</span> are absolutely continuous, the
equality follows from the fact that the one-dimensional projections
capture all the necessary information to compute the Wasserstein
distance. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Sliced Wasserstein Distance enjoys several properties that make
it a valuable tool in various applications. We list some of the key
properties below:</p>
<ol>
<li><p><strong>Metric Property</strong>: The Sliced Wasserstein Distance
is a metric on the space of probability measures. It satisfies the
properties of non-negativity, identity of indiscernibles, symmetry, and
the triangle inequality.</p></li>
<li><p><strong>Computational Efficiency</strong>: The SWD can be
computed efficiently using Monte Carlo methods, making it suitable for
high-dimensional data.</p></li>
<li><p><strong>Stability</strong>: The SWD is stable under small
perturbations of the probability measures, which is a desirable property
in many applications.</p></li>
</ol>
<p>Each of these properties can be proven using the definitions and
theorems presented earlier. For example, the metric property follows
from the fact that <span class="math inline">\(W_1\)</span> is a metric
and the integral preserves these properties.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The Sliced Wasserstein Distance is a powerful and computationally
efficient metric for comparing probability distributions. Its ability to
leverage one-dimensional projections makes it particularly useful in
high-dimensional settings, where traditional Wasserstein distances can
be computationally prohibitive. By understanding the definitions,
theorems, and properties of SWD, we can appreciate its role in various
applications and continue to explore its potential in future
research.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Semi-supervised Learning: A Comprehensive Overview</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Semi-supervised Learning: A Comprehensive
Overview</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le Semi-supervised Learning (SSL) émerge comme une réponse élégante à
un défi fondamental en apprentissage automatique : la rareté des données
étiquetées. Dans un monde où les données abondent, mais où l’étiquetage
est coûteux et fastidieux, le SSL propose une voie intermédiaire entre
les méthodes supervisées et non supervisées. L’idée centrale est
d’exploiter l’information à la fois des données étiquetées et non
étiquetées pour améliorer les performances des modèles.</p>
<p>Historiquement, le SSL trouve ses racines dans les années 1960 avec
des travaux pionniers en classification semi-supervisée. Aujourd’hui, il
est au cœur de nombreuses applications pratiques, allant de la
reconnaissance d’images à l’analyse de texte, en passant par la
bioinformatique.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre le SSL, il est essentiel de définir clairement ses
composantes. Supposons que nous ayons un ensemble de données <span
class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span> où
<span class="math inline">\(x_i \in \mathcal{X}\)</span> et <span
class="math inline">\(y_i \in \mathcal{Y}\)</span>. Dans le cadre du
SSL, nous distinguons deux sous-ensembles :</p>
<ul>
<li><p><span class="math inline">\(\mathcal{D}_l = \{(x_i,
y_i)\}_{i=1}^{n_l}\)</span> : les données étiquetées.</p></li>
<li><p><span class="math inline">\(\mathcal{D}_u =
\{x_i\}_{i=n_l+1}^{n}\)</span> : les données non étiquetées.</p></li>
</ul>
<p>L’objectif est de construire un modèle <span class="math inline">\(f:
\mathcal{X} \rightarrow \mathcal{Y}\)</span> qui minimise une fonction
de perte <span class="math inline">\(\mathcal{L}\)</span> sur les
données étiquetées tout en exploitant l’information des données non
étiquetées.</p>
<h1 id="théorèmes-et-principes-fondamentaux">Théorèmes et Principes
Fondamentaux</h1>
<p>Plusieurs théorèmes et principes sous-tendent le SSL. Parmi les plus
importants, on trouve le principe de la consistance et l’hypothèse du
cluster.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> un modèle appris à partir
des données <span class="math inline">\(\mathcal{D}_l \cup
\mathcal{D}_u\)</span>. Le principe de la consistance stipule que pour
tout <span class="math inline">\(x \in \mathcal{X}\)</span>, les
prédictions de <span class="math inline">\(f\)</span> doivent être
cohérentes avec les labels des points voisins dans l’espace des
caractéristiques.</p>
<p>Matérialisons ce principe par la condition suivante : <span
class="math display">\[\forall x \in \mathcal{X}, \exists \epsilon &gt;
0, \forall x&#39; \in B(x, \epsilon), f(x) = f(x&#39;)\]</span> où <span
class="math inline">\(B(x, \epsilon)\)</span> désigne la boule centrée
en <span class="math inline">\(x\)</span> de rayon <span
class="math inline">\(\epsilon\)</span>.</p>
</div>
<div class="theorem">
<p>L’hypothèse du cluster postule que les points de données proches dans
l’espace des caractéristiques ont tendance à partager le même label.
Formellement, pour tout <span class="math inline">\(x, x&#39; \in
\mathcal{X}\)</span>, si <span class="math inline">\(\|x - x&#39;\| &lt;
\delta\)</span>, alors <span class="math inline">\(P(y = y&#39; | x,
x&#39;) &gt; P(y | x)P(y&#39; | x&#39;)\)</span>.</p>
</div>
<h1 id="preuves-et-démonstrations">Preuves et Démonstrations</h1>
<p>Pour illustrer ces principes, considérons un exemple simple.
Supposons que nous ayons un ensemble de données bidimensionnelles et que
nous voulions classifier les points en deux classes.</p>
<div class="proof">
<p><em>Preuve du Principe de la Consistance.</em> Nous voulons montrer
que le modèle <span class="math inline">\(f\)</span> est cohérent avec
les labels des points voisins. Pour cela, nous utilisons l’algorithme
k-NN (k plus proches voisins) comme base.</p>
<p>Soit <span class="math inline">\(x \in \mathcal{X}\)</span> et soit
<span class="math inline">\(N_k(x)\)</span> l’ensemble des k plus
proches voisins de <span class="math inline">\(x\)</span>. Selon le
principe de la consistance, nous avons : <span
class="math display">\[f(x) = \text{majorité}\{f(x&#39;) | x&#39; \in
N_k(x)\}\]</span></p>
<p>Cette condition garantit que les prédictions de <span
class="math inline">\(f\)</span> sont stables et cohérentes
localement. ◻</p>
</div>
<div class="proof">
<p><em>Preuve de l’Hypothèse du Cluster.</em> Pour démontrer cette
hypothèse, nous utilisons le théorème de Bayes. Soit <span
class="math inline">\(x, x&#39; \in \mathcal{X}\)</span> tels que <span
class="math inline">\(\|x - x&#39;\| &lt; \delta\)</span>.</p>
<p>Nous avons : <span class="math display">\[P(y = y&#39; | x, x&#39;) =
\frac{P(x, x&#39; | y, y&#39;)P(y, y&#39;)}{P(x, x&#39;)}\]</span></p>
<p>En utilisant l’indépendance conditionnelle, nous obtenons : <span
class="math display">\[P(y = y&#39; | x, x&#39;) &gt; \frac{P(x |
y)P(x&#39; | y&#39;)P(y, y&#39;)}{P(x, x&#39;)}\]</span></p>
<p>En comparant avec <span class="math inline">\(P(y | x)P(y&#39; |
x&#39;)\)</span>, nous voyons que l’hypothèse du cluster est satisfaite
si <span class="math inline">\(P(y, y&#39;) &gt;
P(y)P(y&#39;)\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés découlent des théorèmes précédents. Nous en
listons quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Propriété de lissage</strong> : Le modèle <span
class="math inline">\(f\)</span> doit être lisse, c’est-à-dire que les
variations de <span class="math inline">\(f\)</span> doivent être
minimisées. Formellement, nous cherchons à minimiser : <span
class="math display">\[\int_{\mathcal{X}} \| \nabla f(x) \|^2
dx\]</span></p></li>
<li><p><strong>Propriété de régularisation</strong> : L’information des
données non étiquetées peut être utilisée pour régulariser le modèle.
Par exemple, en ajoutant une pénalité de régularisation basée sur la
distance entre les points.</p></li>
<li><p><strong>Propriété de généralisation</strong> : Le SSL améliore la
généralisation du modèle en exploitant l’information des données non
étiquetées. Cela peut être formalisé par une borne supérieure sur
l’erreur de généralisation.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Le Semi-supervised Learning représente une avancée significative dans
le domaine de l’apprentissage automatique. En combinant les forces des
méthodes supervisées et non supervisées, il offre une solution robuste
pour traiter les problèmes où les données étiquetées sont rares. Les
principes fondamentaux, tels que le principe de la consistance et
l’hypothèse du cluster, fournissent une base théorique solide pour le
développement de nouveaux algorithmes et techniques.</p>
</body>
</html>
{% include "footer.html" %}


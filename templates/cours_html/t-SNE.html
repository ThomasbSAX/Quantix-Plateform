{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>t-SNE: t-Distributed Stochastic Neighbor Embedding</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">t-SNE: t-Distributed Stochastic Neighbor
Embedding</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La visualisation de données à haute dimension est une tâche complexe
et cruciale dans de nombreux domaines scientifiques. Les techniques
classiques de réduction de dimension, telles que l’Analyse en
Composantes Principales (ACP), préservent souvent les distances globales
mais échouent à capturer les structures locales des données. C’est dans
ce contexte que la méthode t-SNE (t-Distributed Stochastic Neighbor
Embedding) a émergé. Développée par van der Maaten et Hinton en 2008,
t-SNE est une technique de réduction de dimension non linéaire qui
excelle dans la préservation des structures locales et la révélation de
clusters dans les données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre t-SNE, commençons par définir quelques concepts clés.
Supposons que nous avons un ensemble de points de données à haute
dimension <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> où chaque <span class="math inline">\(x_i \in
\mathbb{R}^D\)</span>. Notre objectif est de trouver une représentation
à faible dimension <span class="math inline">\(Y = \{y_1, y_2, \dots,
y_n\}\)</span> où chaque <span class="math inline">\(y_i \in
\mathbb{R}^d\)</span> (avec <span class="math inline">\(d \ll
D\)</span>) qui préserve les relations locales entre les points.</p>
<h2 class="unnumbered"
id="similarité-des-points-dans-lespace-original">Similarité des Points
dans l’Espace Original</h2>
<p>La première étape consiste à calculer les similarités entre les
points dans l’espace original. Une approche courante est d’utiliser la
distribution gaussienne pour modéliser les similarités :</p>
<p><span class="math display">\[p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 /
2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 /
2\sigma_i^2)}\]</span></p>
<p>où <span class="math inline">\(\sigma_i\)</span> est la déviation
standard de la distribution gaussienne centrée sur <span
class="math inline">\(x_i\)</span>. La similarité symétrique entre les
points <span class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> est alors définie par :</p>
<p><span class="math display">\[p_{ij} = \frac{p_{j|i} +
p_{i|j}}{2n}\]</span></p>
<h2 class="unnumbered"
id="similarité-des-points-dans-lespace-à-faible-dimension">Similarité
des Points dans l’Espace à Faible Dimension</h2>
<p>Dans l’espace à faible dimension, nous modélisons les similarités en
utilisant une distribution t-student. La probabilité <span
class="math inline">\(q_{ij}\)</span> que les points <span
class="math inline">\(y_i\)</span> et <span
class="math inline">\(y_j\)</span> soient similaires est donnée par
:</p>
<p><span class="math display">\[q_{ij} = \frac{(1 + \|y_i -
y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le principe fondamental derrière t-SNE est de minimiser la divergence
de Kullback-Leibler entre les distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, où <span
class="math inline">\(P\)</span> est la distribution de similarité dans
l’espace original et <span class="math inline">\(Q\)</span> est la
distribution de similarité dans l’espace à faible dimension.</p>
<h2 class="unnumbered"
id="minimisation-de-la-divergence-de-kullback-leibler">Minimisation de
la Divergence de Kullback-Leibler</h2>
<p>Le théorème central de t-SNE peut être énoncé comme suit :</p>
<p><span class="math display">\[\min_{Y} \sum_{i \neq j} p_{ij} \log
\frac{p_{ij}}{q_{ij}}\]</span></p>
<p>Cette minimisation permet de trouver une représentation <span
class="math inline">\(Y\)</span> qui préserve au mieux les structures
locales des données.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour minimiser la divergence de Kullback-Leibler, nous utilisons des
techniques d’optimisation stochastique. L’algorithme de gradient
descendant est souvent employé pour trouver la représentation optimale
<span class="math inline">\(Y\)</span>.</p>
<h2 class="unnumbered" id="calcul-des-gradients">Calcul des
Gradients</h2>
<p>Le gradient de la fonction de coût par rapport à <span
class="math inline">\(y_i\)</span> est donné par :</p>
<p><span class="math display">\[\frac{\partial C}{\partial y_i} = 4
\sum_{j} (p_{ij} - q_{ij}) (y_i - y_j) (1 + \|y_i -
y_j\|^2)^{-1}\]</span></p>
<p>où <span class="math inline">\(C\)</span> est la fonction de coût
définie par la divergence de Kullback-Leibler.</p>
<h2 class="unnumbered" id="algorithme-doptimisation">Algorithme
d’Optimisation</h2>
<p>L’algorithme de t-SNE peut être résumé comme suit :</p>
<ol>
<li><p>Calculer les similarités <span
class="math inline">\(p_{ij}\)</span> dans l’espace original.</p></li>
<li><p>Initialiser les points <span class="math inline">\(y_i\)</span>
dans l’espace à faible dimension de manière aléatoire.</p></li>
<li><p>Calculer les similarités <span
class="math inline">\(q_{ij}\)</span> dans l’espace à faible
dimension.</p></li>
<li><p>Mettre à jour les positions <span
class="math inline">\(y_i\)</span> en utilisant le gradient
descendant.</p></li>
<li><p>Répéter les étapes 3 et 4 jusqu’à convergence.</p></li>
</ol>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered"
id="propriété-1-préservation-des-structures-locales">Propriété 1:
Préservation des Structures Locales</h2>
<p>t-SNE préserve les structures locales des données en minimisant la
divergence de Kullback-Leibler entre les distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Cela permet de révéler des clusters
dans les données qui pourraient être invisibles avec des techniques
linéaires.</p>
<h2 class="unnumbered"
id="propriété-2-robustesse-aux-paramètres">Propriété 2: Robustesse aux
Paramètres</h2>
<p>t-SNE est relativement robuste aux choix des paramètres, tels que la
déviation standard <span class="math inline">\(\sigma_i\)</span> et le
nombre de dimensions <span class="math inline">\(d\)</span>. Cependant,
un choix approprié de ces paramètres est crucial pour obtenir des
résultats optimaux.</p>
<h2 class="unnumbered"
id="propriété-3-visualisation-des-données">Propriété 3: Visualisation
des Données</h2>
<p>t-SNE est particulièrement utile pour la visualisation des données à
haute dimension. En projetant les données dans un espace 2D ou 3D, t-SNE
permet de visualiser les structures et les clusters de manière
intuitive.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>t-SNE est une technique puissante pour la réduction de dimension et
la visualisation des données à haute dimension. En préservant les
structures locales, t-SNE révèle des informations précieuses sur la
distribution des données. Bien que t-SNE soit une méthode non linéaire,
elle est relativement simple à implémenter et à utiliser. Cependant,
comme toute technique de réduction de dimension, t-SNE a ses limites et
doit être utilisée avec prudence.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Monge-Elkan : Une Mesure de Similarité pour Données Hétérogènes</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Monge-Elkan : Une Mesure de Similarité
pour Données Hétérogènes</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’émergence de la distance de Monge-Elkan dans le domaine du
traitement des données hétérogènes trouve ses racines dans les défis
posés par la comparaison d’objets complexes et multiformes.
Historiquement, les mesures de similarité traditionnelles, telles que la
distance euclidienne ou la distance de Hamming, se révèlent inadéquates
face à des données comportant des attributs de types variés (numériques,
catégorielles, ordinales).</p>
<p>La notion de distance de Monge-Elkan émerge comme une réponse
élégante à ce problème. Elle permet de comparer des objets en tenant
compte de la nature hétérogène de leurs attributs, en intégrant des
connaissances a priori sur les similarités entre valeurs possibles.
Cette approche est indispensable dans des domaines tels que la
bioinformatique, où les objets à comparer peuvent avoir des attributs de
types différents et où les similarités entre valeurs ne sont pas
toujours transitives.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la distance de Monge-Elkan, il est essentiel de
saisir le concept de similarité entre valeurs. Supposons que nous ayons
un ensemble de valeurs possibles pour un attribut, et que nous
souhaitions quantifier à quel point deux valeurs quelconques de cet
ensemble sont similaires.</p>
<p>La similarité entre deux valeurs <span
class="math inline">\(a\)</span> et <span
class="math inline">\(b\)</span> peut être vue comme une fonction qui
attribue un score compris entre 0 et 1, où 0 indique une dissimilarité
totale et 1 une similarité parfaite. Cette fonction doit respecter
certaines propriétés, telles que la symétrie et la bornitude.</p>
<p>Formellement, une fonction de similarité <span
class="math inline">\(s\)</span> sur un ensemble <span
class="math inline">\(A\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(A\)</span> un ensemble. Une fonction
de similarité sur <span class="math inline">\(A\)</span> est une
application <span class="math display">\[s: A \times A \rightarrow [0,
1]\]</span> telle que pour tout <span class="math inline">\(a, b \in
A\)</span>,</p>
<ul>
<li><p><span class="math inline">\(s(a, b) = s(b, a)\)</span>
(symétrie),</p></li>
<li><p><span class="math inline">\(s(a, a) = 1\)</span>
(réflexivité),</p></li>
<li><p><span class="math inline">\(\forall a, b, c \in A, s(a, c) \geq
s(a, b) + s(b, c) - 1\)</span> (triangulaire).</p></li>
</ul>
</div>
<p>La distance de Monge-Elkan généralise cette notion en intégrant des
similarités partielles entre valeurs. Pour deux objets <span
class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> définis sur des attributs hétérogènes,
la distance de Monge-Elkan est calculée en agrégeant les similarités
entre leurs valeurs correspondantes.</p>
<div class="definition">
<p>Soient <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> deux objets définis sur des attributs
<span class="math inline">\(A_1, \ldots, A_n\)</span>. La distance de
Monge-Elkan entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> est définie comme : <span
class="math display">\[d(x, y) = \sum_{i=1}^n w_i \cdot (1 - s_i(x[i],
y[i]))\]</span> où <span class="math inline">\(w_i\)</span> est le poids
de l’attribut <span class="math inline">\(A_i\)</span>, et <span
class="math inline">\(s_i\)</span> est la fonction de similarité
associée à <span class="math inline">\(A_i\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Monge-Elkan est celui de
la cohérence des similarités. Ce théorème assure que les similarités
partielles utilisées dans le calcul de la distance respectent certaines
propriétés globales.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(s_1, \ldots, s_n\)</span> des
fonctions de similarité sur des ensembles respectifs <span
class="math inline">\(A_1, \ldots, A_n\)</span>. La distance de
Monge-Elkan définie par : <span class="math display">\[d(x, y) =
\sum_{i=1}^n w_i \cdot (1 - s_i(x[i], y[i]))\]</span> est une distance
au sens usuel si et seulement si les fonctions <span
class="math inline">\(s_i\)</span> satisfont les propriétés de symétrie,
réflexivité et triangulaire.</p>
</div>
<p>La démonstration de ce théorème repose sur les propriétés des
fonctions de similarité et l’agrégation linéaire des distances
partielles.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de la cohérence des similarités, nous
procédons par étapes.</p>
<div class="proof">
<p><em>Proof.</em> Nous devons montrer que la distance <span
class="math inline">\(d\)</span> définie par : <span
class="math display">\[d(x, y) = \sum_{i=1}^n w_i \cdot (1 - s_i(x[i],
y[i]))\]</span> est une distance au sens usuel, c’est-à-dire qu’elle
satisfait les propriétés de non-négativité, d’identité des
indiscernables, de symétrie et d’inégalité triangulaire.</p>
<ol>
<li><p><strong>Non-négativité</strong> : Puisque <span
class="math inline">\(s_i(x[i], y[i]) \in [0, 1]\)</span>, il suit que
<span class="math inline">\(1 - s_i(x[i], y[i]) \in [0, 1]\)</span>.
Comme les poids <span class="math inline">\(w_i\)</span> sont
non-négatifs, <span class="math inline">\(d(x, y) \geq
0\)</span>.</p></li>
<li><p><strong>Identité des indiscernables</strong> : Si <span
class="math inline">\(x = y\)</span>, alors <span
class="math inline">\(s_i(x[i], y[i]) = 1\)</span> pour tout <span
class="math inline">\(i\)</span>, donc <span class="math inline">\(d(x,
y) = 0\)</span>. Réciproquement, si <span class="math inline">\(d(x, y)
= 0\)</span>, alors <span class="math inline">\(s_i(x[i], y[i]) =
1\)</span> pour tout <span class="math inline">\(i\)</span>, ce qui
implique <span class="math inline">\(x[i] = y[i]\)</span> pour tout
<span class="math inline">\(i\)</span> (par réflexivité de <span
class="math inline">\(s_i\)</span>).</p></li>
<li><p><strong>Symétrie</strong> : Par symétrie des fonctions <span
class="math inline">\(s_i\)</span>, nous avons : <span
class="math display">\[d(x, y) = \sum_{i=1}^n w_i \cdot (1 - s_i(x[i],
y[i])) = \sum_{i=1}^n w_i \cdot (1 - s_i(y[i], x[i])) = d(y,
x).\]</span></p></li>
<li><p><strong>Inégalité triangulaire</strong> : Nous devons montrer que
pour tout <span class="math inline">\(x, y, z\)</span>, <span
class="math display">\[d(x, z) \leq d(x, y) + d(y, z).\]</span> En
utilisant l’inégalité triangulaire pour les fonctions <span
class="math inline">\(s_i\)</span>, nous avons : <span
class="math display">\[s_i(x[i], z[i]) \geq s_i(x[i], y[i]) + s_i(y[i],
z[i]) - 1.\]</span> En réarrangeant, nous obtenons : <span
class="math display">\[1 - s_i(x[i], z[i]) \leq (1 - s_i(x[i], y[i])) +
(1 - s_i(y[i], z[i])).\]</span> En multipliant par <span
class="math inline">\(w_i\)</span> et en sommant sur <span
class="math inline">\(i\)</span>, nous obtenons : <span
class="math display">\[d(x, z) \leq d(x, y) + d(y, z).\]</span></p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés intéressantes découlent de la distance de
Monge-Elkan. Nous en énumérons quelques-unes ci-dessous.</p>
<div class="proposition">
<p>La distance de Monge-Elkan est invariante par translation et par
scaling des attributs numériques.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(A_i\)</span> un
attribut numérique. Une translation ou un scaling de <span
class="math inline">\(A_i\)</span> peut être représenté par une
transformation affine <span class="math inline">\(f_i: A_i \rightarrow
A_i&#39;\)</span>. La similarité entre deux valeurs <span
class="math inline">\(x[i]\)</span> et <span
class="math inline">\(y[i]\)</span> après transformation est donnée par
: <span class="math display">\[s_i&#39;(f_i(x[i]), f_i(y[i])) =
s_i(x[i], y[i]).\]</span> Ainsi, la distance de Monge-Elkan reste
inchangée. ◻</p>
</div>
<div class="proposition">
<p>La distance de Monge-Elkan peut être généralisée à des ensembles
d’attributs pondérés.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(W\)</span> un
ensemble de poids pour les attributs. La distance généralisée est
définie par : <span class="math display">\[d(x, y) = \sum_{i=1}^n w_i
\cdot (1 - s_i(x[i], y[i])),\]</span> où <span class="math inline">\(w_i
\in W\)</span>. Cette généralisation conserve les propriétés de la
distance de Monge-Elkan. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Monge-Elkan représente une avancée significative dans
le domaine des mesures de similarité pour données hétérogènes. Son
élégance mathématique et sa capacité à intégrer des connaissances a
priori sur les similarités entre valeurs en font un outil précieux pour
la comparaison d’objets complexes. Les théorèmes et propriétés présentés
dans cet article illustrent la robustesse et la cohérence de cette
mesure, ouvrant la voie à de nombreuses applications dans des domaines
variés.</p>
</body>
</html>
{% include "footer.html" %}


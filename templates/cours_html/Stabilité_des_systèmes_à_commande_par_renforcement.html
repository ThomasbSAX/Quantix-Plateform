{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Stabilité des systèmes à commande par renforcement</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Stabilité des systèmes à commande par
renforcement</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les systèmes à commande par renforcement (Reinforcement Learning, RL)
ont connu un essor spectaculaire ces dernières années, notamment grâce à
leurs applications dans des domaines variés tels que la robotique, les
jeux vidéo, et l’optimisation de processus industriels. Cependant,
malgré leurs performances impressionnantes, la compréhension théorique
de leur stabilité reste un défi majeur.</p>
<p>La stabilité des systèmes à commande par renforcement est cruciale
pour garantir que les algorithmes convergent vers des solutions
optimales et robustes. En effet, sans une analyse rigoureuse de la
stabilité, il est difficile de garantir que les politiques apprises par
ces algorithmes seront efficaces dans des environnements dynamiques et
incertains.</p>
<p>Dans cet article, nous explorons les concepts fondamentaux de la
stabilité des systèmes à commande par renforcement. Nous commençons par
définir formellement ce que nous entendons par stabilité dans ce
contexte, puis nous examinons les théorèmes clés qui permettent de
caractériser cette propriété. Enfin, nous discutons des implications
pratiques de ces résultats pour le développement d’algorithmes de RL
robustes et efficaces.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous entendons par stabilité dans le contexte des
systèmes à commande par renforcement. Intuitivement, un système est
stable si, après une perturbation, il revient à son état d’équilibre.
Dans le cadre du RL, cela signifie que l’algorithme doit converger vers
une politique optimale malgré les incertitudes et les perturbations de
l’environnement.</p>
<p>Pour formaliser cette idée, considérons un problème de Markov
décisionnel (MDP) défini par le tuple <span
class="math inline">\((\mathcal{S}, \mathcal{A}, P, R, \gamma)\)</span>,
où <span class="math inline">\(\mathcal{S}\)</span> est l’ensemble des
états, <span class="math inline">\(\mathcal{A}\)</span> est l’ensemble
des actions, <span class="math inline">\(P\)</span> est la matrice de
transition, <span class="math inline">\(R\)</span> est la fonction de
récompense, et <span class="math inline">\(\gamma \in [0, 1)\)</span>
est le facteur d’actualisation. Une politique <span
class="math inline">\(\pi: \mathcal{S} \rightarrow \mathcal{A}\)</span>
est une fonction qui associe à chaque état un action.</p>
<div class="definition">
<p>Une politique <span class="math inline">\(\pi\)</span> est dite
stable si, pour tout état initial <span class="math inline">\(s_0 \in
\mathcal{S}\)</span>, la séquence d’états et d’actions générée par <span
class="math inline">\(\pi\)</span> converge vers un état d’équilibre
<span class="math inline">\(s^* \in \mathcal{S}\)</span> tel que <span
class="math inline">\(\lim_{t \rightarrow \infty} s_t =
s^*\)</span>.</p>
</div>
<p>De manière équivalente, on peut dire qu’une politique <span
class="math inline">\(\pi\)</span> est stable si la valeur <span
class="math inline">\(V^\pi(s)\)</span> de chaque état <span
class="math inline">\(s \in \mathcal{S}\)</span> est finie et ne varie
pas significativement au fil du temps. Formellement, cela s’exprime
par:</p>
<p><span class="math display">\[\forall \epsilon &gt; 0, \exists T &gt;
0 \text{ tel que } \forall t \geq T, |V^\pi_t(s) - V^\pi_{t-1}(s)| &lt;
\epsilon\]</span></p>
<p>où <span class="math inline">\(V^\pi_t(s)\)</span> est la valeur de
l’état <span class="math inline">\(s\)</span> à l’itération <span
class="math inline">\(t\)</span> sous la politique <span
class="math inline">\(\pi\)</span>.</p>
<h1 id="théorèmes-de-stabilité">Théorèmes de Stabilité</h1>
<p>Pour analyser la stabilité des systèmes à commande par renforcement,
nous devons nous appuyer sur des théorèmes fondamentaux. L’un des
résultats les plus importants dans ce domaine est le théorème de
convergence de l’algorithme Q-learning, qui garantit que, sous certaines
conditions, les valeurs <span class="math inline">\(Q\)</span>
convergent vers leurs valeurs optimales.</p>
<div class="theorem">
<p>Soit un MDP <span class="math inline">\((\mathcal{S}, \mathcal{A}, P,
R, \gamma)\)</span> et supposons que l’algorithme Q-learning est
appliqué avec une politique <span class="math inline">\(\pi\)</span> qui
explore suffisamment l’espace des états et des actions. Alors, les
valeurs <span class="math inline">\(Q\)</span> convergent presque
sûrement vers leurs valeurs optimales <span
class="math inline">\(Q^*\)</span>, c’est-à-dire:</p>
<p><span class="math display">\[\lim_{t \rightarrow \infty} Q_t(s, a) =
Q^*(s, a), \quad \forall s \in \mathcal{S}, \forall a \in
\mathcal{A}\]</span></p>
<p>où <span class="math inline">\(Q_t(s, a)\)</span> est la valeur Q de
l’action <span class="math inline">\(a\)</span> dans l’état <span
class="math inline">\(s\)</span> à l’itération <span
class="math inline">\(t\)</span>.</p>
</div>
<p>La preuve de ce théorème repose sur le fait que l’algorithme
Q-learning est une forme d’itération de Bellman, qui garantit la
convergence vers les valeurs <span class="math inline">\(Q\)</span>
optimales sous certaines conditions d’exploration. Plus précisément, si
l’algorithme explore suffisamment l’espace des états et des actions, les
valeurs <span class="math inline">\(Q\)</span> convergeront vers leurs
valeurs optimales.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de convergence de Q-learning, nous devons
d’abord rappeler quelques propriétés fondamentales des MDP et des
algorithmes de RL. En particulier, nous utilisons le fait que les
valeurs <span class="math inline">\(Q\)</span> optimales satisfont
l’équation de Bellman:</p>
<p><span class="math display">\[Q^*(s, a) = R(s, a) + \gamma
\sum_{s&#39;} P(s&#39; | s, a) \max_{a&#39;} Q^*(s&#39;,
a&#39;)\]</span></p>
<p>L’algorithme Q-learning met à jour les valeurs <span
class="math inline">\(Q\)</span> en utilisant la règle suivante:</p>
<p><span class="math display">\[Q_{t+1}(s_t, a_t) = Q_t(s_t, a_t) +
\alpha_t \left[ R_{t+1} + \gamma \max_{a} Q_t(s_{t+1}, a) - Q_t(s_t,
a_t) \right]\]</span></p>
<p>où <span class="math inline">\(\alpha_t\)</span> est le taux
d’apprentissage à l’itération <span
class="math inline">\(t\)</span>.</p>
<p>Pour prouver la convergence de cet algorithme, nous devons montrer
que les mises à jour des valeurs <span class="math inline">\(Q\)</span>
sont contractantes, c’est-à-dire qu’elles réduisent l’erreur entre les
valeurs <span class="math inline">\(Q\)</span> actuelles et les valeurs
<span class="math inline">\(Q\)</span> optimales. Cela peut être
démontré en utilisant des techniques d’analyse stochastique et des
inégalités de concentration.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>En plus du théorème de convergence de Q-learning, il existe plusieurs
propriétés et corollaires importants qui caractérisent la stabilité des
systèmes à commande par renforcement. Nous en présentons quelques-uns
ci-dessous.</p>
<div class="corollaire">
<p>Si une politique <span class="math inline">\(\pi\)</span> est
optimale, c’est-à-dire qu’elle maximise la valeur espérée de la
récompense cumulée, alors <span class="math inline">\(\pi\)</span> est
stable.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Supposons que <span
class="math inline">\(\pi\)</span> soit une politique optimale. Alors,
pour tout état <span class="math inline">\(s \in \mathcal{S}\)</span>,
la valeur <span class="math inline">\(V^\pi(s)\)</span> est maximale.
Par conséquent, toute perturbation de l’état <span
class="math inline">\(s\)</span> entraînera une diminution de la valeur
espérée de la récompense cumulée, ce qui incitera l’algorithme à revenir
vers l’état d’équilibre <span class="math inline">\(s^*\)</span>. ◻</p>
</div>
<div class="corollaire">
<p>Les algorithmes de RL qui explorent suffisamment l’espace des états
et des actions sont stables.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Si un algorithme explore suffisamment l’espace des
états et des actions, il est susceptible de découvrir toutes les
politiques optimales. Par conséquent, l’algorithme convergera vers une
politique stable. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré les concepts fondamentaux de la
stabilité des systèmes à commande par renforcement. Nous avons défini
formellement ce que nous entendons par stabilité dans ce contexte,
examiné les théorèmes clés qui permettent de caractériser cette
propriété, et discuté des implications pratiques de ces résultats. Les
travaux futurs pourraient porter sur le développement d’algorithmes de
RL plus robustes et efficaces, en s’appuyant sur les résultats
théoriques présentés dans cet article.</p>
</body>
</html>
{% include "footer.html" %}


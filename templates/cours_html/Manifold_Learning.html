{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Manifold Learning: Une Exploration Mathématique et Conceptuelle</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Manifold Learning: Une Exploration Mathématique et
Conceptuelle</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Le Manifold Learning, ou apprentissage sur variétés, est une branche
des mathématiques et de l’apprentissage automatique qui émerge à la
croisée de plusieurs disciplines. Historiquement, les premières idées
remontent aux travaux de Riemann sur la géométrie différentielle, mais
c’est avec l’avènement des données massives et de l’apprentissage
automatique que cette notion a pris toute son ampleur.</p>
<p>Pourquoi le Manifold Learning est-il indispensable ? Imaginez un
nuage de points dans un espace de haute dimension. Si ces points sont
distribués sur une variété de dimension inférieure, il est souvent plus
efficace de travailler directement sur cette variété plutôt que dans
l’espace ambiant. Cela permet de réduire la complexité computationnelle
et d’améliorer la compréhension des données.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement le Manifold Learning, essayons de
comprendre ce que nous cherchons à capturer. Supposons que nous ayons un
ensemble de données dans un espace de haute dimension, mais que ces
données soient en réalité distribuées sur une variété de dimension
inférieure. Notre objectif est de découvrir cette variété
sous-jacente.</p>
<div class="definition">
<p>Soit <span class="math inline">\(M\)</span> un ensemble. On dit que
<span class="math inline">\(M\)</span> est une variété de dimension
<span class="math inline">\(n\)</span> si pour tout point <span
class="math inline">\(p \in M\)</span>, il existe un voisinage <span
class="math inline">\(U\)</span> de <span
class="math inline">\(p\)</span> et une application <span
class="math inline">\(\phi: U \rightarrow \mathbb{R}^n\)</span> telle
que :</p>
<ul>
<li><p><span class="math inline">\(\phi\)</span> est un homéomorphisme
sur son image.</p></li>
<li><p>Pour tout <span class="math inline">\(q \in U\)</span>, il existe
un voisinage <span class="math inline">\(V\)</span> de <span
class="math inline">\(q\)</span> tel que <span
class="math inline">\(\phi(V)\)</span> est ouvert dans <span
class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
</ul>
<p>On appelle <span class="math inline">\(\phi\)</span> une carte locale
de <span class="math inline">\(M\)</span> en <span
class="math inline">\(p\)</span>.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \ldots,
x_m\}\)</span> un ensemble de points dans <span
class="math inline">\(\mathbb{R}^d\)</span>. On dit que <span
class="math inline">\(X\)</span> est distribué sur une variété <span
class="math inline">\(M\)</span> de dimension <span
class="math inline">\(n\)</span> si : <span
class="math display">\[\exists \phi: M \rightarrow \mathbb{R}^d \text{
telle que } X \subset \phi(M).\]</span> Le problème du Manifold Learning
est de trouver une représentation <span class="math inline">\(Y = \{y_1,
y_2, \ldots, y_m\}\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> telle que : <span
class="math display">\[\forall i, j, \quad d(x_i, x_j) \approx d(y_i,
y_j),\]</span> où <span class="math inline">\(d\)</span> est une
distance appropriée.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental en Manifold Learning est le théorème de
Whittle, qui donne des conditions sous lesquelles une variété peut être
reconstruite à partir de ses distances locales.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(M\)</span> une variété compacte de
dimension <span class="math inline">\(n\)</span>. Supposons que pour
tout point <span class="math inline">\(p \in M\)</span>, il existe un
voisinage <span class="math inline">\(U\)</span> de <span
class="math inline">\(p\)</span> tel que la restriction de la métrique
de Riemann à <span class="math inline">\(U\)</span> est isométrique à
une boule dans <span class="math inline">\(\mathbb{R}^n\)</span>. Alors,
pour tout <span class="math inline">\(\epsilon &gt; 0\)</span>, il
existe une application <span class="math inline">\(\phi: M \rightarrow
\mathbb{R}^n\)</span> telle que : <span class="math display">\[\forall
p, q \in M, \quad |d(p, q) - d(\phi(p), \phi(q))| &lt;
\epsilon.\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>La preuve du théorème de Whittle repose sur plusieurs étapes clés.
Tout d’abord, nous utilisons le fait que <span
class="math inline">\(M\)</span> est compacte pour couvrir <span
class="math inline">\(M\)</span> par un nombre fini de cartes locales.
Ensuite, nous utilisons l’hypothèse d’isométrie locale pour construire
une approximation de la variété dans <span
class="math inline">\(\mathbb{R}^n\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(M\)</span> une
variété compacte de dimension <span class="math inline">\(n\)</span>.
Par compacité, il existe un recouvrement fini <span
class="math inline">\(\{U_i\}_{i=1}^k\)</span> de <span
class="math inline">\(M\)</span> par des cartes locales <span
class="math inline">\(\phi_i: U_i \rightarrow \mathbb{R}^n\)</span>.</p>
<p>Pour tout <span class="math inline">\(i\)</span>, notons <span
class="math inline">\(\psi_i: U_i \rightarrow B_i\)</span> une isométrie
locale, où <span class="math inline">\(B_i\)</span> est une boule dans
<span class="math inline">\(\mathbb{R}^n\)</span>. Par le lemme de
Lebesgue, il existe un <span class="math inline">\(\delta &gt;
0\)</span> tel que pour tout <span class="math inline">\(p \in
M\)</span>, la boule de rayon <span
class="math inline">\(\delta\)</span> autour de <span
class="math inline">\(p\)</span> est contenue dans un des <span
class="math inline">\(U_i\)</span>.</p>
<p>Pour tout point <span class="math inline">\(p \in M\)</span>,
choisissons une carte locale <span class="math inline">\(\phi_i\)</span>
telle que <span class="math inline">\(p \in U_i\)</span>. Nous
définissons <span class="math inline">\(\phi(p)\)</span> comme le centre
de la boule <span class="math inline">\(B_i\)</span> correspondante. Par
construction, nous avons : <span class="math display">\[\forall p, q \in
M, \quad |d(p, q) - d(\phi(p), \phi(q))| &lt; \epsilon.\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le Manifold Learning possède plusieurs propriétés intéressantes qui
en font un outil puissant pour l’analyse des données.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de points
distribués sur une variété <span class="math inline">\(M\)</span>. Si
<span class="math inline">\(Y\)</span> est une représentation obtenue
par Manifold Learning, alors pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe une distance
<span class="math inline">\(d\)</span> telle que : <span
class="math display">\[\forall i, j, \quad d(x_i, x_j) \approx d(y_i,
y_j).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement du théorème de
Whittle. En effet, le théorème garantit que les distances locales sont
préservées, ce qui implique que les distances globales le sont
également. ◻</p>
</div>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de points
dans <span class="math inline">\(\mathbb{R}^d\)</span>. Si <span
class="math inline">\(X\)</span> est distribué sur une variété <span
class="math inline">\(M\)</span> de dimension <span
class="math inline">\(n &lt; d\)</span>, alors il existe une
représentation <span class="math inline">\(Y\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span> telle que : <span
class="math display">\[\forall i, j, \quad d(x_i, x_j) \approx d(y_i,
y_j).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette propriété est une conséquence directe de la
définition du Manifold Learning. En trouvant une représentation <span
class="math inline">\(Y\)</span> dans <span
class="math inline">\(\mathbb{R}^n\)</span>, nous réduisons la dimension
tout en préservant les distances. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Le Manifold Learning est un domaine riche et fascinant qui combine
des idées de géométrie différentielle, d’apprentissage automatique et
d’analyse des données. En comprenant les variétés sous-jacentes aux
données, nous pouvons améliorer la compréhension et l’analyse des
ensembles de données complexes.</p>
</body>
</html>
{% include "footer.html" %}


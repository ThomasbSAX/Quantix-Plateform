{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Wasserstein-1 (Earth Mover’s Distance)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Wasserstein-1 (Earth Mover’s
Distance)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Wasserstein-1, également connue sous le nom d’Earth
Mover’s Distance (EMD), est une mesure de distance entre deux
distributions de probabilité. Elle trouve ses origines dans les travaux
de Leonid Kantorovich en 1942, dans le contexte de la théorie des
transports optimaux. Cette notion a été popularisée par les travaux de
Vladimir Ilyich Arnold et ses applications en dynamique des fluides.</p>
<p>L’EMD émerge comme une solution naturelle pour mesurer la
dissimilarité entre deux distributions de probabilité. Elle est
particulièrement utile dans des domaines tels que l’apprentissage
automatique, la vision par ordinateur et le traitement du signal. Par
exemple, dans le contexte de l’apprentissage automatique, l’EMD permet
de comparer des distributions de caractéristiques extraites d’images ou
de données textuelles.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la distance de Wasserstein-1, commençons par comprendre
ce que nous cherchons à mesurer. Supposons que nous avons deux
distributions de probabilité, <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>, définies sur un espace métrique
<span class="math inline">\((X, d)\)</span>. Nous voulons mesurer la
quantité minimale de travail nécessaire pour transformer <span
class="math inline">\(P\)</span> en <span
class="math inline">\(Q\)</span>.</p>
<p>Formellement, nous cherchons à minimiser la quantité suivante : <span
class="math display">\[W_1(P, Q) = \inf_{\gamma \in \Gamma(P, Q)}
\int_{X \times X} d(x, y) \, d\gamma(x, y),\]</span> où <span
class="math inline">\(\Gamma(P, Q)\)</span> est l’ensemble des mesures
de couplage entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Une mesure de couplage <span
class="math inline">\(\gamma\)</span> est une mesure sur <span
class="math inline">\(X \times X\)</span> telle que pour tout ensemble
mesurable <span class="math inline">\(A \subset X\)</span>, <span
class="math display">\[\gamma(A \times X) = P(A) \quad \text{et} \quad
\gamma(X \times A) = Q(A).\]</span></p>
<p>En d’autres termes, <span class="math inline">\(W_1(P, Q)\)</span>
est la distance minimale que doit parcourir la "terre" pour transformer
le tas de terre <span class="math inline">\(P\)</span> en le tas de
terre <span class="math inline">\(Q\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Wasserstein-1 est le
théorème de Kantorovich-Rubinstein, qui donne une caractérisation duale
de <span class="math inline">\(W_1\)</span>.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace métrique <span class="math inline">\((X, d)\)</span>. Alors,
<span class="math display">\[W_1(P, Q) = \sup_{f \in \text{Lip}_1(X)}
\left( \int_X f \, dP - \int_X f \, dQ \right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> est l’ensemble des
fonctions <span class="math inline">\(f: X \to \mathbb{R}\)</span>
Lipschitziennes avec une constante de Lipschitz égale à 1.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Kantorovich-Rubinstein, nous commençons
par rappeler que toute fonction Lipschitzienne <span
class="math inline">\(f\)</span> avec une constante de Lipschitz égale à
1 satisfait <span class="math inline">\(|f(x) - f(y)| \leq d(x,
y)\)</span> pour tout <span class="math inline">\(x, y \in
X\)</span>.</p>
<p>Considérons une mesure de couplage <span
class="math inline">\(\gamma\)</span> entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Nous avons : <span
class="math display">\[\int_X f \, dP - \int_X f \, dQ = \int_{X \times
X} (f(x) - f(y)) \, d\gamma(x, y).\]</span></p>
<p>En utilisant l’inégalité de Lipschitz, nous obtenons : <span
class="math display">\[\left| \int_{X \times X} (f(x) - f(y)) \,
d\gamma(x, y) \right| \leq \int_{X \times X} |f(x) - f(y)| \, d\gamma(x,
y) \leq \int_{X \times X} d(x, y) \, d\gamma(x, y).\]</span></p>
<p>En prenant le supremum sur <span class="math inline">\(f \in
\text{Lip}_1(X)\)</span>, nous obtenons : <span
class="math display">\[W_1(P, Q) \geq \sup_{f \in \text{Lip}_1(X)}
\left( \int_X f \, dP - \int_X f \, dQ \right).\]</span></p>
<p>Pour prouver l’égalité, nous utilisons le fait que pour toute mesure
de couplage <span class="math inline">\(\gamma\)</span>, il existe une
fonction Lipschitzienne <span class="math inline">\(f\)</span> telle que
<span class="math inline">\(W_1(P, Q) = \int_{X \times X} d(x, y) \,
d\gamma(x, y) = \int_X f \, dP - \int_X f \, dQ\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<ol>
<li><p>La distance de Wasserstein-1 est une métrique sur l’ensemble des
distributions de probabilité. Cela signifie qu’elle satisfait les
propriétés suivantes :</p>
<ul>
<li><p><span class="math inline">\(W_1(P, Q) \geq 0\)</span>,</p></li>
<li><p><span class="math inline">\(W_1(P, Q) = 0\)</span> si et
seulement si <span class="math inline">\(P = Q\)</span>,</p></li>
<li><p><span class="math inline">\(W_1(P, Q) = W_1(Q,
P)\)</span>,</p></li>
<li><p><span class="math inline">\(W_1(P, R) \leq W_1(P, Q) + W_1(Q,
R)\)</span>.</p></li>
</ul></li>
<li><p>La distance de Wasserstein-1 est continue par rapport à la
convergence faible. Cela signifie que si <span
class="math inline">\(P_n\)</span> converge faiblement vers <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q_n\)</span> converge faiblement vers <span
class="math inline">\(Q\)</span>, alors <span
class="math inline">\(W_1(P_n, Q_n)\)</span> converge vers <span
class="math inline">\(W_1(P, Q)\)</span>.</p></li>
<li><p>La distance de Wasserstein-1 peut être utilisée pour définir des
espaces métriques sur l’ensemble des distributions de probabilité. Cela
permet d’appliquer des méthodes d’analyse fonctionnelle et de théorie
des probabilités à ces espaces.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Wasserstein-1 est une mesure puissante et flexible
pour comparer des distributions de probabilité. Ses applications vont de
l’apprentissage automatique à la vision par ordinateur, en passant par
le traitement du signal. Les théorèmes et propriétés associés à cette
distance ouvrent la voie à de nombreuses recherches futures dans ces
domaines.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’entropie de Hartley : une mesure fondamentale en théorie de l’information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’entropie de Hartley : une mesure fondamentale en
théorie de l’information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie de Hartley, introduite par Ralph Vinton Lyon Hartley en
1928, est une mesure fondamentale dans le domaine de la théorie de
l’information. Elle émerge comme une réponse à la nécessité de
quantifier le contenu d’information d’un signal ou d’une source
d’information. Hartley a posé les bases de cette notion en cherchant à
comprendre comment mesurer la capacité d’un canal de communication à
transmettre des informations.</p>
<p>L’entropie de Hartley est indispensable dans le cadre de la
transmission d’information, où elle permet de déterminer la quantité
maximale d’information pouvant être transmise par un canal donné. Elle
est également utilisée dans divers domaines tels que la cryptographie,
le traitement du signal et l’analyse des données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie de Hartley, commençons par définir ce que
nous cherchons à mesurer. Imaginons une source d’information qui peut
produire un nombre fini de symboles distincts. Nous voulons quantifier
la quantité d’information apportée par chaque symbole émis.</p>
<p>Formellement, considérons un ensemble fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
de <span class="math inline">\(n\)</span> symboles. L’entropie de
Hartley <span class="math inline">\(H(\mathcal{X})\)</span> est définie
comme le logarithme du nombre total de symboles possibles.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble
fini de <span class="math inline">\(n\)</span> symboles. L’entropie de
Hartley est donnée par : <span class="math display">\[H(\mathcal{X}) =
\log_2(n)\]</span></p>
</div>
<p>Cette définition peut être généralisée à des ensembles de symboles
pondérés. Supposons que chaque symbole <span
class="math inline">\(x_i\)</span> ait une probabilité <span
class="math inline">\(p_i\)</span> d’être émis. L’entropie de Hartley
pondérée est alors définie comme le logarithme du nombre total de
symboles possibles divisé par la probabilité de chaque symbole.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span> un ensemble de <span class="math inline">\(n\)</span>
symboles avec des probabilités associées <span
class="math inline">\(p_1, p_2, \ldots, p_n\)</span>. L’entropie de
Hartley pondérée est donnée par : <span
class="math display">\[H(\mathcal{X}) = \log_2\left( \frac{n}{\max_{1
\leq i \leq n} p_i} \right)\]</span></p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’entropie de Hartley satisfait plusieurs propriétés importantes qui
la rendent utile dans divers contextes. Nous allons en explorer
quelques-unes.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span> deux ensembles finis de
symboles. L’entropie de Hartley satisfait la propriété de
sous-additivité : <span class="math display">\[H(\mathcal{X} \times
\mathcal{Y}) = H(\mathcal{X}) + H(\mathcal{Y})\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Considérons deux ensembles finis <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
et <span class="math inline">\(\mathcal{Y} = \{y_1, y_2, \ldots,
y_m\}\)</span>. Le produit cartésien <span
class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> contient
<span class="math inline">\(n \times m\)</span> éléments. Par définition
de l’entropie de Hartley, nous avons : <span
class="math display">\[H(\mathcal{X} \times \mathcal{Y}) = \log_2(n
\times m)\]</span> En utilisant les propriétés des logarithmes, nous
obtenons : <span class="math display">\[H(\mathcal{X} \times
\mathcal{Y}) = \log_2(n) + \log_2(m) = H(\mathcal{X}) +
H(\mathcal{Y})\]</span> ◻</p>
</div>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span> deux ensembles finis de
symboles tels que <span class="math inline">\(|\mathcal{X}| \leq
|\mathcal{Y}|\)</span>. Alors : <span
class="math display">\[H(\mathcal{X}) \leq H(\mathcal{Y})\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Soient <span class="math inline">\(\mathcal{X} =
\{x_1, x_2, \ldots, x_n\}\)</span> et <span
class="math inline">\(\mathcal{Y} = \{y_1, y_2, \ldots, y_m\}\)</span>
avec <span class="math inline">\(n \leq m\)</span>. Par définition de
l’entropie de Hartley, nous avons : <span
class="math display">\[H(\mathcal{X}) = \log_2(n)\]</span> et <span
class="math display">\[H(\mathcal{Y}) = \log_2(m)\]</span> Puisque <span
class="math inline">\(n \leq m\)</span>, il s’ensuit que <span
class="math inline">\(\log_2(n) \leq \log_2(m)\)</span>, ce qui prouve
la propriété de monotonie. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Nous avons déjà vu les preuves des théorèmes de sous-additivité et de
monotonie. Passons maintenant à une autre propriété importante : la
convexité de l’entropie de Hartley.</p>
<div class="theorem">
<p>L’entropie de Hartley est une fonction convexe. Plus précisément,
pour tout <span class="math inline">\(\lambda \in [0, 1]\)</span> et
pour tous ensembles finis <span
class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span>, nous avons : <span
class="math display">\[H(\lambda \mathcal{X} + (1 - \lambda)
\mathcal{Y}) \leq \lambda H(\mathcal{X}) + (1 - \lambda)
H(\mathcal{Y})\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Considérons deux ensembles finis <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
et <span class="math inline">\(\mathcal{Y} = \{y_1, y_2, \ldots,
y_m\}\)</span>. L’ensemble <span class="math inline">\(\lambda
\mathcal{X} + (1 - \lambda) \mathcal{Y}\)</span> est défini comme
l’ensemble des combinaisons convexes de <span
class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span>. Le nombre total d’éléments
dans cet ensemble est au plus <span class="math inline">\(n + m -
1\)</span>.</p>
<p>Par définition de l’entropie de Hartley, nous avons : <span
class="math display">\[H(\lambda \mathcal{X} + (1 - \lambda)
\mathcal{Y}) = \log_2(n + m - 1)\]</span> En utilisant les propriétés
des logarithmes, nous obtenons : <span class="math display">\[H(\lambda
\mathcal{X} + (1 - \lambda) \mathcal{Y}) = \log_2(n + m - 1)\]</span>
D’autre part, nous avons : <span class="math display">\[\lambda
H(\mathcal{X}) + (1 - \lambda) H(\mathcal{Y}) = \lambda \log_2(n) + (1 -
\lambda) \log_2(m)\]</span> Pour prouver la convexité, il suffit de
montrer que : <span class="math display">\[\log_2(n + m - 1) \leq
\lambda \log_2(n) + (1 - \lambda) \log_2(m)\]</span> Cette inégalité
découle directement de la convexité de la fonction logarithme. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie de Hartley possède plusieurs propriétés intéressantes qui
en font un outil puissant dans la théorie de l’information. Nous allons
en explorer quelques-unes.</p>
<ul>
<li><p><strong>Invariance par permutation</strong> : L’entropie de
Hartley est invariante par permutation des symboles. Autrement dit, pour
toute permutation <span class="math inline">\(\sigma\)</span> de
l’ensemble <span class="math inline">\(\mathcal{X}\)</span>, nous avons
: <span class="math display">\[H(\sigma(\mathcal{X})) =
H(\mathcal{X})\]</span></p></li>
<li><p><strong>Additivité</strong> : L’entropie de Hartley est additive.
Pour tout ensemble fini <span class="math inline">\(\mathcal{X}\)</span>
et pour tout entier positif <span class="math inline">\(k\)</span>, nous
avons : <span class="math display">\[H(\mathcal{X}^k) = k
H(\mathcal{X})\]</span> où <span
class="math inline">\(\mathcal{X}^k\)</span> désigne l’ensemble des
séquences de longueur <span class="math inline">\(k\)</span> formées à
partir des symboles de <span
class="math inline">\(\mathcal{X}\)</span>.</p></li>
<li><p><strong>Maximisation</strong> : L’entropie de Hartley est
maximale lorsque tous les symboles sont équiprobables. Plus précisément,
pour tout ensemble fini <span class="math inline">\(\mathcal{X} = \{x_1,
x_2, \ldots, x_n\}\)</span> avec des probabilités associées <span
class="math inline">\(p_1, p_2, \ldots, p_n\)</span>, nous avons : <span
class="math display">\[H(\mathcal{X}) \leq \log_2(n)\]</span> avec
égalité si et seulement si <span class="math inline">\(p_i =
\frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>.</p></li>
</ul>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie de Hartley est une mesure fondamentale en théorie de
l’information, permettant de quantifier le contenu d’information d’une
source. Ses propriétés mathématiques en font un outil puissant pour
l’analyse des systèmes de communication et le traitement du signal. En
comprenant les définitions, théorèmes et propriétés associés à
l’entropie de Hartley, nous pouvons mieux apprécier son rôle dans le
domaine de la théorie de l’information.</p>
</body>
</html>
{% include "footer.html" %}


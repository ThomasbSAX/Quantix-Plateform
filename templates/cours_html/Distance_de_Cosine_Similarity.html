{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Cosine Similarity : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Cosine Similarity : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La notion de similarité entre vecteurs est fondamentale dans de
nombreux domaines des mathématiques appliquées, notamment en traitement
du signal, en apprentissage automatique et en analyse de données. Parmi
les mesures de similarité, la distance de Cosine Similarity se distingue
par sa capacité à capturer l’orientation des vecteurs plutôt que leur
magnitude. Cette propriété est particulièrement utile dans les contextes
où l’information pertinente réside dans la direction des vecteurs, comme
dans les systèmes de recommandation ou l’analyse de texte.</p>
<p>L’origine de la Cosine Similarity remonte aux travaux en algèbre
linéaire et en géométrie vectorielle. Elle émerge naturellement comme
une mesure de l’angle entre deux vecteurs dans un espace euclidien. Son
utilisation s’est généralisée avec le développement des techniques
d’analyse de données multidimensionnelles, où elle permet de comparer
efficacement des objets représentés par des vecteurs de grande
dimension.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la Cosine Similarity, considérons deux vecteurs dans
un espace euclidien. Nous cherchons une mesure qui reflète à quel point
ces vecteurs pointent dans la même direction. Intuitivement, cette
mesure devrait être maximale lorsque les vecteurs sont colinéaires et
minimale lorsqu’ils sont orthogonaux.</p>
<p>Formellement, la Cosine Similarity entre deux vecteurs <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> est définie comme le cosinus
de l’angle <span class="math inline">\(\theta\)</span> entre eux. Cette
notion peut être exprimée de plusieurs manières :</p>
<div class="definition">
<p>La Cosine Similarity entre deux vecteurs <span
class="math inline">\(\mathbf{u} = (u_1, u_2, \ldots, u_n)\)</span> et
<span class="math inline">\(\mathbf{v} = (v_1, v_2, \ldots,
v_n)\)</span> est donnée par : <span class="math display">\[\cos(\theta)
= \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} =
\frac{\sum_{i=1}^n u_i v_i}{\sqrt{\sum_{i=1}^n u_i^2} \sqrt{\sum_{i=1}^n
v_i^2}}\]</span> où <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v}\)</span> représente le produit scalaire des vecteurs <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, et <span
class="math inline">\(\|\mathbf{u}\|\)</span> et <span
class="math inline">\(\|\mathbf{v}\|\)</span> représentent leurs normes
euclidiennes respectives.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[\cos(\theta) = \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \frac{\sum_{i=1}^n u_i
v_i}{\sqrt{\sum_{i=1}^n u_i^2} \sqrt{\sum_{i=1}^n v_i^2}}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la Cosine Similarity est le suivant
:</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> deux vecteurs non nuls dans un
espace euclidien. Alors, la Cosine Similarity entre <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> satisfait les propriétés
suivantes :</p>
<ol>
<li><p><span class="math inline">\(-1 \leq \cos(\theta) \leq
1\)</span></p></li>
<li><p><span class="math inline">\(\cos(\theta) = 1\)</span> si et
seulement si <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de même
sens.</p></li>
<li><p><span class="math inline">\(\cos(\theta) = -1\)</span> si et
seulement si <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de sens
opposés.</p></li>
<li><p><span class="math inline">\(\cos(\theta) = 0\)</span> si et
seulement si <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont orthogonaux.</p></li>
</ol>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de la Cosine Similarity, nous procédons
comme suit :</p>
<div class="proof">
<p><em>Proof.</em> Considérons deux vecteurs <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> dans un espace euclidien. Nous
avons : <span class="math display">\[\cos(\theta) = \frac{\mathbf{u}
\cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}\]</span></p>
<p>1. **Bornes de la Cosine Similarity** : Par l’inégalité de
Cauchy-Schwarz, nous savons que : <span
class="math display">\[|\mathbf{u} \cdot \mathbf{v}| \leq \|\mathbf{u}\|
\|\mathbf{v}\|\]</span> Il s’ensuit que : <span
class="math display">\[-1 \leq \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} \leq 1\]</span> Donc, <span
class="math inline">\(-1 \leq \cos(\theta) \leq 1\)</span>.</p>
<p>2. **Colinéarité de même sens** : Si <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de même
sens, alors il existe un scalaire <span class="math inline">\(\alpha
&gt; 0\)</span> tel que <span class="math inline">\(\mathbf{v} = \alpha
\mathbf{u}\)</span>. Ainsi : <span class="math display">\[\cos(\theta) =
\frac{\mathbf{u} \cdot (\alpha \mathbf{u})}{\|\mathbf{u}\| \|\alpha
\mathbf{u}\|} = \frac{\alpha \|\mathbf{u}\|^2}{\alpha \|\mathbf{u}\|^2}
= 1\]</span> Réciproquement, si <span class="math inline">\(\cos(\theta)
= 1\)</span>, alors <span class="math inline">\(\mathbf{u} \cdot
\mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\|\)</span>, ce qui implique que
<span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de même
sens.</p>
<p>3. **Colinéarité de sens opposés** : Si <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de sens
opposés, alors il existe un scalaire <span class="math inline">\(\alpha
&lt; 0\)</span> tel que <span class="math inline">\(\mathbf{v} = \alpha
\mathbf{u}\)</span>. Ainsi : <span class="math display">\[\cos(\theta) =
\frac{\mathbf{u} \cdot (\alpha \mathbf{u})}{\|\mathbf{u}\| \|\alpha
\mathbf{u}\|} = \frac{\alpha \|\mathbf{u}\|^2}{-\alpha \|\mathbf{u}\|^2}
= -1\]</span> Réciproquement, si <span
class="math inline">\(\cos(\theta) = -1\)</span>, alors <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v} = -\|\mathbf{u}\|
\|\mathbf{v}\|\)</span>, ce qui implique que <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont colinéaires et de sens
opposés.</p>
<p>4. **Orthogonalité** : Si <span
class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont orthogonaux, alors <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v} = 0\)</span>, et donc
<span class="math inline">\(\cos(\theta) = 0\)</span>. Réciproquement,
si <span class="math inline">\(\cos(\theta) = 0\)</span>, alors <span
class="math inline">\(\mathbf{u} \cdot \mathbf{v} = 0\)</span>, ce qui
implique que <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span> sont orthogonaux. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons ci-dessous quelques propriétés importantes de la Cosine
Similarity :</p>
<div class="proposition">
<p>La Cosine Similarity possède les propriétés suivantes :</p>
<ol>
<li><p>**Invariance par scaling** : La Cosine Similarity est invariante
par multiplication des vecteurs par un scalaire non nul. Plus
précisément, pour tout <span class="math inline">\(\alpha \neq
0\)</span>, nous avons : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{v}}) =
\cos(\theta_{\alpha \mathbf{u}, \alpha \mathbf{v}})\]</span></p></li>
<li><p>**Symétrie** : La Cosine Similarity est symétrique. Pour tous
vecteurs <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, nous avons : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{v}}) =
\cos(\theta_{\mathbf{v}, \mathbf{u}})\]</span></p></li>
<li><p>**Identité** : La Cosine Similarity entre un vecteur et lui-même
est maximale. Pour tout vecteur <span
class="math inline">\(\mathbf{u}\)</span>, nous avons : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{u}}) =
1\]</span></p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> 1. **Invariance par scaling** : Considérons deux
vecteurs <span class="math inline">\(\mathbf{u}\)</span> et <span
class="math inline">\(\mathbf{v}\)</span>, et un scalaire <span
class="math inline">\(\alpha \neq 0\)</span>. Nous avons : <span
class="math display">\[\cos(\theta_{\alpha \mathbf{u}, \alpha
\mathbf{v}}) = \frac{(\alpha \mathbf{u}) \cdot (\alpha
\mathbf{v})}{\|\alpha \mathbf{u}\| \|\alpha \mathbf{v}\|} =
\frac{\alpha^2 (\mathbf{u} \cdot \mathbf{v})}{\alpha \|\mathbf{u}\|
\alpha \|\mathbf{v}\|} = \frac{\mathbf{u} \cdot
\mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos(\theta_{\mathbf{u},
\mathbf{v}})\]</span></p>
<p>2. **Symétrie** : Par définition, le produit scalaire est symétrique,
c’est-à-dire que <span class="math inline">\(\mathbf{u} \cdot \mathbf{v}
= \mathbf{v} \cdot \mathbf{u}\)</span>. De plus, les normes euclidiennes
sont également symétriques. Il s’ensuit que : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{v}}) =
\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} =
\frac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{v}\| \|\mathbf{u}\|} =
\cos(\theta_{\mathbf{v}, \mathbf{u}})\]</span></p>
<p>3. **Identité** : Pour tout vecteur <span
class="math inline">\(\mathbf{u}\)</span>, nous avons : <span
class="math display">\[\cos(\theta_{\mathbf{u}, \mathbf{u}}) =
\frac{\mathbf{u} \cdot \mathbf{u}}{\|\mathbf{u}\|^2} =
\frac{\|\mathbf{u}\|^2}{\|\mathbf{u}\|^2} = 1\]</span> ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La Cosine Similarity est une mesure puissante et polyvalente de
similarité entre vecteurs. Ses propriétés mathématiques fondamentales,
telles que l’invariance par scaling et la symétrie, en font un outil
précieux dans de nombreuses applications pratiques. En comprenant
profondément cette notion et ses implications, nous pouvons mieux
appréhender les défis complexes de l’analyse de données et du traitement
du signal.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Analyse en Composantes Principales (PCA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analyse en Composantes Principales (PCA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Analyse en Composantes Principales (PCA) est une technique
statistique multivariée qui vise à réduire la dimensionnalité des
données tout en conservant autant que possible les informations
contenues dans celles-ci. Cette méthode, introduite par Karl Pearson en
1901 et développée par Harold Hotelling dans les années 1930, est
devenue un outil fondamental en statistique, en apprentissage
automatique et dans de nombreuses autres disciplines scientifiques.</p>
<p>La PCA est particulièrement utile lorsque les données sont décrites
par un grand nombre de variables corrélées. En projetant ces données sur
un sous-espace de dimension inférieure, la PCA permet de visualiser et
d’analyser les structures sous-jacentes des données. Elle est également
utilisée pour éliminer le bruit et les redondances, ce qui améliore la
performance des algorithmes d’apprentissage automatique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la PCA, commençons par définir quelques concepts
clés. Supposons que nous avons un ensemble de données <span
class="math inline">\(\mathbf{X}\)</span> composé de <span
class="math inline">\(n\)</span> observations et <span
class="math inline">\(p\)</span> variables. Chaque observation est
représentée par un vecteur <span class="math inline">\(\mathbf{x}_i \in
\mathbb{R}^p\)</span>, où <span class="math inline">\(i = 1, \ldots,
n\)</span>.</p>
<p>Nous cherchons à trouver une nouvelle représentation des données
<span class="math inline">\(\mathbf{Y}\)</span> de dimension <span
class="math inline">\(k\)</span> (avec <span class="math inline">\(k
&lt; p\)</span>) telle que la variance des données projetées soit
maximisée. Cette nouvelle représentation est obtenue en projetant les
données sur un ensemble de vecteurs orthonormaux appelés <em>composantes
principales</em>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathbf{X}\)</span> une matrice de
données centrée de taille <span class="math inline">\(n \times
p\)</span>. Une composante principale est un vecteur <span
class="math inline">\(\mathbf{v} \in \mathbb{R}^p\)</span> tel que la
variance des projections des données sur <span
class="math inline">\(\mathbf{v}\)</span> est maximisée. Formellement,
nous cherchons <span class="math display">\[\mathbf{v} =
\arg\max_{\|\mathbf{u}\| = 1} \text{Var}(\mathbf{Xu})\]</span> où <span
class="math inline">\(\text{Var}(\mathbf{Xu})\)</span> désigne la
variance des projections des données sur le vecteur <span
class="math inline">\(\mathbf{u}\)</span>.</p>
</div>
<p>Les composantes principales sont obtenues en résolvant le problème
d’optimisation ci-dessus. Il est bien connu que les solutions à ce
problème sont les vecteurs propres de la matrice de covariance <span
class="math inline">\(\mathbf{\Sigma} =
\frac{1}{n-1}\mathbf{X}^T\mathbf{X}\)</span> associés aux plus grandes
valeurs propres.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le théorème fondamental de la PCA est le suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathbf{X}\)</span> une matrice de
données centrée de taille <span class="math inline">\(n \times
p\)</span> et soit <span class="math inline">\(\mathbf{\Sigma} =
\frac{1}{n-1}\mathbf{X}^T\mathbf{X}\)</span> la matrice de covariance.
Les composantes principales de <span
class="math inline">\(\mathbf{X}\)</span> sont les vecteurs propres de
<span class="math inline">\(\mathbf{\Sigma}\)</span> associés aux plus
grandes valeurs propres.</p>
</div>
<p>Pour démontrer ce théorème, nous utilisons le fait que la variance
des projections des données sur un vecteur <span
class="math inline">\(\mathbf{u}\)</span> est donnée par <span
class="math display">\[\text{Var}(\mathbf{Xu}) =
\mathbf{u}^T\mathbf{\Sigma}\mathbf{u}\]</span> En maximisant cette
quantité sous la contrainte <span class="math inline">\(\|\mathbf{u}\| =
1\)</span>, nous obtenons que les solutions sont les vecteurs propres de
<span class="math inline">\(\mathbf{\Sigma}\)</span> associés aux plus
grandes valeurs propres.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Commençons par démontrer que les composantes principales sont les
vecteurs propres de la matrice de covariance.</p>
<div class="proof">
<p><em>Proof.</em> Nous cherchons à maximiser <span
class="math inline">\(\text{Var}(\mathbf{Xu}) =
\mathbf{u}^T\mathbf{\Sigma}\mathbf{u}\)</span> sous la contrainte <span
class="math inline">\(\|\mathbf{u}\| = 1\)</span>. En utilisant le
théorème des multiplicateurs de Lagrange, nous obtenons que les
solutions <span class="math inline">\(\mathbf{u}\)</span> satisfont
l’équation <span class="math display">\[\mathbf{\Sigma}\mathbf{u} =
\lambda \mathbf{u}\]</span> où <span
class="math inline">\(\lambda\)</span> est un multiplicateur de
Lagrange. Cela montre que les solutions sont les vecteurs propres de
<span class="math inline">\(\mathbf{\Sigma}\)</span>. ◻</p>
</div>
<p>Il est également important de noter que les composantes principales
sont orthogonales entre elles. Cela découle du fait que la matrice de
covariance est symétrique et que ses vecteurs propres sont
orthogonaux.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La PCA possède plusieurs propriétés intéressantes qui en font un
outil puissant pour l’analyse des données.</p>
<ol>
<li><p>Les composantes principales sont ordonnées par ordre décroissant
de variance. Cela signifie que la première composante principale capture
la plus grande partie de la variance des données, suivie par la deuxième
composante principale, et ainsi de suite.</p></li>
<li><p>Les composantes principales sont non corrélées entre elles. Cela
découle du fait que la matrice de covariance des projections des données
sur les composantes principales est diagonale.</p></li>
<li><p>La PCA est sensible à l’échelle des variables. Il est donc
important de normaliser les données avant d’appliquer la PCA.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Analyse en Composantes Principales est une technique puissante pour
la réduction de dimension et l’analyse des données. En projetant les
données sur un sous-espace de dimension inférieure, la PCA permet de
visualiser et d’analyser les structures sous-jacentes des données. Elle
est largement utilisée dans de nombreuses disciplines scientifiques et
constitue un outil fondamental en statistique et en apprentissage
automatique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Borne de généralisation dimensionnelle : Un cadre unifié pour l’apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Borne de généralisation dimensionnelle : Un cadre
unifié pour l’apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage statistique moderne est confronté à un défi
fondamental : comprendre comment les modèles apprennent à généraliser à
partir de données. La borne de généralisation dimensionnelle émerge
comme un outil puissant pour quantifier cette capacité, offrant une
perspective unifiée sur divers problèmes d’apprentissage.</p>
<p>Cette notion trouve ses racines dans la théorie de l’apprentissage
statistique, où elle permet de mesurer l’écart entre l’erreur empirique
et l’erreur réelle. Elle est indispensable pour établir des garanties
théoriques sur la performance des algorithmes d’apprentissage, en
particulier dans des contextes de haute dimension où les données sont
souvent bruitées et limitées.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la borne de généralisation dimensionnelle,
considérons un modèle d’apprentissage <span
class="math inline">\(f\)</span> qui dépend de paramètres <span
class="math inline">\(\theta\)</span>. Nous cherchons à quantifier la
capacité du modèle à généraliser à de nouvelles données, en fonction de
la complexité des paramètres <span
class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(S = \{ (x_1, y_1), \ldots, (x_n,
y_n) \}\)</span> un ensemble d’entraînement et <span
class="math inline">\(\mathcal{F}\)</span> une classe de fonctions. La
borne de généralisation dimensionnelle est donnée par : <span
class="math display">\[\mathbb{E}_{S} \left[ \sup_{f \in \mathcal{F}}
|R(f) - R_S(f)| \right] \leq 2 \mathfrak{R}_S(\mathcal{F}) + C
\sqrt{\frac{d}{n}}\]</span> où <span class="math inline">\(R(f)\)</span>
est l’erreur réelle, <span class="math inline">\(R_S(f)\)</span> est
l’erreur empirique, <span
class="math inline">\(\mathfrak{R}_S(\mathcal{F})\)</span> est la
complexité de Rademacher, <span class="math inline">\(d\)</span> est la
dimension effective des paramètres, et <span
class="math inline">\(C\)</span> est une constante.</p>
</div>
<p>De manière équivalente, on peut exprimer cette borne comme : <span
class="math display">\[\mathbb{E}_{S} \left[ \sup_{f \in \mathcal{F}}
|R(f) - R_S(f)| \right] \leq 2 \mathfrak{R}_S(\mathcal{F}) + C
\sqrt{\frac{\text{VC-dim}(\mathcal{F})}{n}}\]</span> où <span
class="math inline">\(\text{VC-dim}(\mathcal{F})\)</span> est la
dimension de Vapnik-Chervonenkis.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème central en apprentissage statistique est le théorème de
la borne de généralisation dimensionnelle, qui relie la complexité du
modèle à sa capacité de généralisation.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{F}\)</span> une classe de
fonctions avec une dimension effective <span
class="math inline">\(d\)</span>. Alors, pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, avec probabilité au
moins <span class="math inline">\(1 - \delta\)</span> sur le choix de
l’ensemble d’entraînement <span class="math inline">\(S\)</span>, on a :
<span class="math display">\[\sup_{f \in \mathcal{F}} |R(f) - R_S(f)|
\leq 2 \mathfrak{R}_S(\mathcal{F}) + C \sqrt{\frac{d}{n}} +
\sqrt{\frac{\log(1/\delta)}{2n}}\]</span></p>
</div>
<p>Une formulation alternative de ce théorème est : <span
class="math display">\[\mathbb{P}_S \left( \sup_{f \in \mathcal{F}}
|R(f) - R_S(f)| &gt; 2 \mathfrak{R}_S(\mathcal{F}) + C
\sqrt{\frac{d}{n}} + \sqrt{\frac{\log(1/\delta)}{2n}} \right) \leq
\delta\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la borne de généralisation
dimensionnelle, nous utilisons des outils de la théorie de
l’apprentissage statistique, notamment les processus de Rademacher et
les inégalités de concentration.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un processus de Rademacher <span
class="math inline">\(\sigma_1, \ldots, \sigma_n\)</span> indépendant et
uniformément distribué sur <span class="math inline">\(\{ -1, 1
\}\)</span>. La complexité de Rademacher est définie comme : <span
class="math display">\[\mathfrak{R}_S(\mathcal{F}) = \mathbb{E}_{\sigma}
\left[ \sup_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \sigma_i f(x_i)
\right]\]</span></p>
<p>En utilisant l’inégalité de McDiarmid et les propriétés des processus
de Rademacher, nous pouvons montrer que : <span
class="math display">\[\mathbb{E}_{S} \left[ \sup_{f \in \mathcal{F}}
|R(f) - R_S(f)| \right] \leq 2 \mathfrak{R}_S(\mathcal{F}) + C
\sqrt{\frac{d}{n}}\]</span></p>
<p>Pour obtenir une garantie de probabilité, nous utilisons l’inégalité
de Hoeffding pour borner la déviation de l’erreur empirique par rapport
à son espérance. Cela donne : <span class="math display">\[\mathbb{P}_S
\left( \sup_{f \in \mathcal{F}} |R(f) - R_S(f)| &gt; 2
\mathfrak{R}_S(\mathcal{F}) + C \sqrt{\frac{d}{n}} +
\sqrt{\frac{\log(1/\delta)}{2n}} \right) \leq \delta\]</span></p>
<p>Cette preuve montre comment la complexité du modèle, mesurée par la
dimension effective <span class="math inline">\(d\)</span>, influence sa
capacité de généralisation. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés et corollaires importants
de la borne de généralisation dimensionnelle.</p>
<ol>
<li><p><strong>Propriété de la complexité</strong> : La borne de
généralisation dimensionnelle montre que la complexité du modèle,
mesurée par <span class="math inline">\(d\)</span>, est un facteur clé
dans la généralisation. Plus <span class="math inline">\(d\)</span> est
grand, plus l’écart entre l’erreur empirique et l’erreur réelle peut
être important.</p></li>
<li><p><strong>Corollaire de la taille de l’ensemble
d’entraînement</strong> : Pour un modèle donné avec une dimension
effective <span class="math inline">\(d\)</span>, augmenter la taille de
l’ensemble d’entraînement <span class="math inline">\(n\)</span> réduit
l’écart de généralisation. Cela souligne l’importance d’avoir
suffisamment de données pour entraîner des modèles complexes.</p></li>
<li><p><strong>Propriété de la régularisation</strong> : La borne de
généralisation dimensionnelle peut être utilisée pour justifier
l’utilisation de techniques de régularisation. En réduisant la
complexité effective <span class="math inline">\(d\)</span> du modèle,
la régularisation améliore la généralisation.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La borne de généralisation dimensionnelle offre un cadre puissant
pour comprendre et quantifier la capacité des modèles d’apprentissage à
généraliser. En reliant la complexité du modèle à sa performance, elle
fournit des garanties théoriques essentielles pour le développement de
nouveaux algorithmes d’apprentissage statistique.</p>
</body>
</html>
{% include "footer.html" %}


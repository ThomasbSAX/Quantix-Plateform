{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Encodage par Extraction de Caractéristiques de Binning par Produit</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Encodage par Extraction de Caractéristiques de
Binning par Produit</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
fondamentale en apprentissage automatique, particulièrement dans le
contexte des modèles d’arbres de décision. L’idée centrale est de
transformer les variables catégorielles en variables numériques,
permettant ainsi aux algorithmes d’apprentissage de traiter ces données
de manière efficace. Parmi les différentes méthodes d’encodage, le
binning par produit se distingue par sa simplicité et son
efficacité.</p>
<p>Le binning par produit consiste à regrouper les catégories d’une
variable catégorielle en bins (ou intervalles) et à calculer la moyenne
des valeurs cibles pour chaque bin. Cette approche permet de capturer
les relations sous-jacentes entre les variables catégorielles et la
variable cible, tout en réduisant la dimensionnalité des données.</p>
<p>Dans cet article, nous explorerons les concepts fondamentaux de
l’encodage par extraction de caractéristiques de binning par produit.
Nous commencerons par définir formellement cette technique, puis nous
présenterons les théorèmes et propriétés associés. Enfin, nous
fournirons des preuves détaillées pour illustrer la validité de cette
méthode.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement l’encodage par extraction de
caractéristiques de binning par produit, il est essentiel de comprendre
le contexte dans lequel cette technique est utilisée. Supposons que nous
ayons un ensemble de données <span class="math inline">\(\mathcal{D} =
\{(x_i, y_i)\}_{i=1}^n\)</span>, où <span
class="math inline">\(x_i\)</span> représente une variable catégorielle
et <span class="math inline">\(y_i\)</span> la variable cible
numérique.</p>
<p>L’objectif est de transformer la variable catégorielle <span
class="math inline">\(x_i\)</span> en une variable numérique qui capture
les informations pertinentes pour prédire <span
class="math inline">\(y_i\)</span>. Pour ce faire, nous allons regrouper
les catégories de <span class="math inline">\(x_i\)</span> en bins et
calculer la moyenne des valeurs cibles pour chaque bin.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{D} = \{(x_i,
y_i)\}_{i=1}^n\)</span> un ensemble de données, où <span
class="math inline">\(x_i \in \mathcal{C}\)</span> et <span
class="math inline">\(\mathcal{C} = \{c_1, c_2, \dots, c_k\}\)</span>
est l’ensemble des catégories de la variable <span
class="math inline">\(x_i\)</span>. Nous définissons un binning par
produit comme une fonction <span class="math inline">\(f: \mathcal{C}
\rightarrow \mathbb{R}\)</span> telle que: <span
class="math display">\[f(c_j) = \frac{1}{|\mathcal{D}_j|} \sum_{(x_i,
y_i) \in \mathcal{D}_j} y_i\]</span> où <span
class="math inline">\(\mathcal{D}_j = \{(x_i, y_i) \in \mathcal{D} | x_i
= c_j\}\)</span> est l’ensemble des données pour lesquelles la variable
<span class="math inline">\(x_i\)</span> prend la valeur <span
class="math inline">\(c_j\)</span>.</p>
</div>
<p>Cette définition peut être réécrite en utilisant des quantificateurs:
<span class="math display">\[\forall c_j \in \mathcal{C}, f(c_j) =
\frac{1}{|\{i | x_i = c_j\}|} \sum_{\substack{i=1 \\ x_i = c_j}}^n
y_i\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons un théorème fondamental
concernant l’encodage par extraction de caractéristiques de binning par
produit. Ce théorème montre que cette méthode préserve certaines
propriétés statistiques des données originales.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(\mathcal{D} = \{(x_i,
y_i)\}_{i=1}^n\)</span> un ensemble de données et <span
class="math inline">\(f: \mathcal{C} \rightarrow \mathbb{R}\)</span> une
fonction de binning par produit. Alors, la moyenne des valeurs encodées
est égale à la moyenne des valeurs cibles originales. <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n f(x_i) = \frac{1}{n}
\sum_{i=1}^n y_i\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Nous allons maintenant prouver le théorème de conservation de la
moyenne.</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’ensemble de données <span
class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span>.
Pour chaque catégorie <span class="math inline">\(c_j \in
\mathcal{C}\)</span>, nous avons: <span class="math display">\[f(c_j) =
\frac{1}{|\mathcal{D}_j|} \sum_{(x_i, y_i) \in \mathcal{D}_j}
y_i\]</span></p>
<p>Nous pouvons réécrire cette somme en utilisant les indices <span
class="math inline">\(i\)</span>: <span class="math display">\[f(c_j) =
\frac{1}{|\{i | x_i = c_j\}|} \sum_{\substack{i=1 \\ x_i = c_j}}^n
y_i\]</span></p>
<p>La moyenne des valeurs encodées est alors: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n f(x_i) = \frac{1}{n}
\sum_{j=1}^k \sum_{\substack{i=1 \\ x_i = c_j}}^n f(c_j)\]</span></p>
<p>En substituant l’expression de <span
class="math inline">\(f(c_j)\)</span>: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n f(x_i) = \frac{1}{n}
\sum_{j=1}^k \sum_{\substack{i=1 \\ x_i = c_j}}^n
\frac{1}{|\mathcal{D}_j|} y_i\]</span></p>
<p>En simplifiant les termes: <span class="math display">\[\frac{1}{n}
\sum_{i=1}^n f(x_i) = \frac{1}{n} \sum_{j=1}^k |\mathcal{D}_j|
f(c_j)\]</span></p>
<p>En utilisant à nouveau l’expression de <span
class="math inline">\(f(c_j)\)</span>: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n f(x_i) = \frac{1}{n}
\sum_{j=1}^k |\mathcal{D}_j| \left( \frac{1}{|\mathcal{D}_j|}
\sum_{(x_i, y_i) \in \mathcal{D}_j} y_i \right)\]</span></p>
<p>En simplifiant: <span class="math display">\[\frac{1}{n} \sum_{i=1}^n
f(x_i) = \frac{1}{n} \sum_{j=1}^k \sum_{(x_i, y_i) \in \mathcal{D}_j}
y_i\]</span></p>
<p>Enfin, en regroupant les termes: <span
class="math display">\[\frac{1}{n} \sum_{i=1}^n f(x_i) = \frac{1}{n}
\sum_{i=1}^n y_i\]</span></p>
<p>Ce qui prouve le théorème. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous allons maintenant présenter quelques propriétés et corollaires
associés à l’encodage par extraction de caractéristiques de binning par
produit.</p>
<div class="proposition">
<p>L’encodage par extraction de caractéristiques de binning par produit
est linéaire. Cela signifie que pour toute combinaison linéaire de
variables catégorielles, l’encodage résultant est égal à la même
combinaison linéaire des encodages individuels. <span
class="math display">\[f\left( \sum_{j=1}^k \alpha_j x_i^{(j)} \right) =
\sum_{j=1}^k \alpha_j f(x_i^{(j)})\]</span> où <span
class="math inline">\(x_i^{(j)}\)</span> représente la <span
class="math inline">\(j\)</span>-ème variable catégorielle et <span
class="math inline">\(\alpha_j\)</span> sont des coefficients réels.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété découle directement de
la définition du binning par produit et des propriétés de la moyenne. En
effet, la moyenne d’une combinaison linéaire est égale à la même
combinaison linéaire des moyennes. ◻</p>
</div>
<div class="corollaire">
<p>Soit <span class="math inline">\(\mathcal{D} = \{(x_i,
y_i)\}_{i=1}^n\)</span> un ensemble de données et <span
class="math inline">\(f: \mathcal{C} \rightarrow \mathbb{R}\)</span> une
fonction de binning par produit. Alors, la variance des valeurs encodées
est inférieure ou égale à la variance des valeurs cibles originales.
<span class="math display">\[\text{Var}(f(x_i)) \leq
\text{Var}(y_i)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire repose sur le fait que
l’encodage par binning par produit est une forme de projection des
données originales sur un espace de dimension réduite. La variance ne
peut qu’augmenter ou rester constante lors d’une telle projection. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré l’encodage par extraction de
caractéristiques de binning par produit. Nous avons défini formellement
cette technique, présenté un théorème fondamental et fourni des preuves
détaillées. Enfin, nous avons discuté de certaines propriétés et
corollaires associés à cette méthode.</p>
<p>L’encodage par extraction de caractéristiques de binning par produit
est une technique puissante et efficace pour transformer les variables
catégorielles en variables numériques. Elle trouve des applications dans
de nombreux domaines, notamment en apprentissage automatique et en
analyse de données.</p>
</body>
</html>
{% include "footer.html" %}


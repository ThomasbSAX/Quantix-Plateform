{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi de décomposition de la variance</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi de décomposition de la variance</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La loi de décomposition de la variance, souvent attribuée à H. O.
Lancaster dans les années 1950, est un résultat fondamental en
statistique et en théorie des probabilités. Elle émerge dans le contexte
de l’analyse de la variance (ANOVA) et permet de décomposer la variance
totale d’une variable aléatoire en composantes explicatives. Cette loi
est indispensable pour comprendre les contributions relatives de
différentes sources de variabilité dans des modèles statistiques
complexes.</p>
<h1 id="définitions">Définitions</h1>
<p>Nous cherchons à comprendre comment la variance d’une variable
aléatoire peut être décomposée en termes de variances conditionnelles et
marginales. Intuitivement, si nous avons une variable aléatoire <span
class="math inline">\(Y\)</span> et une autre variable aléatoire <span
class="math inline">\(X\)</span>, nous voulons exprimer la variance de
<span class="math inline">\(Y\)</span> en fonction des variances
conditionnelles de <span class="math inline">\(Y\)</span> étant donné
<span class="math inline">\(X\)</span>, ainsi que la variance marginale
de <span class="math inline">\(X\)</span>.</p>
<p>Formellement, nous avons la définition suivante :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires telles que
<span class="math inline">\(\mathbb{E}[Y^2] &lt; +\infty\)</span>.
Alors, la variance de <span class="math inline">\(Y\)</span> peut être
décomposée comme suit : <span class="math display">\[\text{Var}(Y) =
\mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |
X])\]</span></p>
</div>
<p>Cette formule montre que la variance totale de <span
class="math inline">\(Y\)</span> est la somme de deux termes : la
variance moyenne conditionnelle de <span
class="math inline">\(Y\)</span> étant donné <span
class="math inline">\(X\)</span>, et la variance de l’espérance
conditionnelle de <span class="math inline">\(Y\)</span> étant donné
<span class="math inline">\(X\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous cherchons à établir une relation entre la variance d’une
variable aléatoire et ses composantes conditionnelles. Intuitivement,
nous voulons montrer que la variance totale peut être exprimée en termes
de variances conditionnelles et marginales.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires telles que
<span class="math inline">\(\mathbb{E}[Y^2] &lt; +\infty\)</span>.
Alors, la variance de <span class="math inline">\(Y\)</span> peut être
décomposée comme suit : <span class="math display">\[\text{Var}(Y) =
\mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |
X])\]</span></p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous allons utiliser les propriétés de
l’espérance et de la variance conditionnelles. Nous commençons par
exprimer <span class="math inline">\(\text{Var}(Y)\)</span> en termes de
l’espérance conditionnelle.</p>
<div class="proof">
<p><em>Proof.</em> Nous savons que : <span
class="math display">\[\text{Var}(Y) = \mathbb{E}[Y^2] -
(\mathbb{E}[Y])^2\]</span></p>
<p>Nous pouvons également exprimer <span
class="math inline">\(\mathbb{E}[Y]\)</span> en termes de l’espérance
conditionnelle : <span class="math display">\[\mathbb{E}[Y] =
\mathbb{E}[\mathbb{E}[Y | X]]\]</span></p>
<p>En utilisant la linéarité de l’espérance conditionnelle, nous avons :
<span class="math display">\[\mathbb{E}[Y^2] = \mathbb{E}[\mathbb{E}[Y^2
| X]]\]</span></p>
<p>Nous pouvons maintenant décomposer <span
class="math inline">\(\text{Var}(Y)\)</span> comme suit : <span
class="math display">\[\text{Var}(Y) = \mathbb{E}[\mathbb{E}[Y^2 | X]] -
(\mathbb{E}[\mathbb{E}[Y | X]])^2\]</span></p>
<p>En utilisant l’inégalité de Jensen pour la fonction convexe <span
class="math inline">\(f(x) = x^2\)</span>, nous avons : <span
class="math display">\[\mathbb{E}[Y^2 | X] \geq (\mathbb{E}[Y |
X])^2\]</span></p>
<p>En prenant l’espérance des deux côtés, nous obtenons : <span
class="math display">\[\mathbb{E}[\mathbb{E}[Y^2 | X]] \geq
\mathbb{E}[(\mathbb{E}[Y | X])^2]\]</span></p>
<p>En soustrayant <span class="math inline">\((\mathbb{E}[\mathbb{E}[Y |
X]])^2\)</span> des deux côtés, nous obtenons : <span
class="math display">\[\mathbb{E}[\mathbb{E}[Y^2 | X]] -
(\mathbb{E}[\mathbb{E}[Y | X]])^2 \geq \mathbb{E}[(\mathbb{E}[Y | X])^2]
- (\mathbb{E}[\mathbb{E}[Y | X]])^2\]</span></p>
<p>Le terme de droite est exactement <span
class="math inline">\(\text{Var}(\mathbb{E}[Y | X])\)</span>. Le terme
de gauche peut être réécrit comme : <span
class="math display">\[\mathbb{E}[\mathbb{E}[Y^2 | X]] -
\mathbb{E}[(\mathbb{E}[Y | X])^2] = \mathbb{E}[\text{Var}(Y |
X)]\]</span></p>
<p>Ainsi, nous avons : <span class="math display">\[\text{Var}(Y) =
\mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y |
X])\]</span></p>
<p>Ce qui achève la preuve. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons quelques propriétés et corollaires importants de la loi
de décomposition de la variance.</p>
<ol>
<li><p>Si <span class="math inline">\(Y\)</span> et <span
class="math inline">\(X\)</span> sont indépendantes, alors <span
class="math inline">\(\mathbb{E}[Y | X] = \mathbb{E}[Y]\)</span> et
<span class="math inline">\(\text{Var}(Y | X) = \text{Var}(Y)\)</span>.
Par conséquent, la loi de décomposition de la variance se simplifie en :
<span class="math display">\[\text{Var}(Y) = \mathbb{E}[\text{Var}(Y |
X)] + \text{Var}(\mathbb{E}[Y | X]) = \text{Var}(Y) + 0\]</span> Ce qui
est une identité triviale.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> est une constante
déterministe, alors <span class="math inline">\(\mathbb{E}[Y | X] =
\mathbb{E}[Y]\)</span> et <span class="math inline">\(\text{Var}(Y | X)
= 0\)</span>. Par conséquent, la loi de décomposition de la variance se
simplifie en : <span class="math display">\[\text{Var}(Y) =
\mathbb{E}[\text{Var}(Y | X)] + \text{Var}(\mathbb{E}[Y | X]) = 0 +
\text{Var}(Y)\]</span> Ce qui est également une identité
triviale.</p></li>
<li><p>Si <span class="math inline">\(Y\)</span> et <span
class="math inline">\(X\)</span> sont linéairement liées, c’est-à-dire
que <span class="math inline">\(Y = aX + b\)</span> pour certaines
constantes <span class="math inline">\(a\)</span> et <span
class="math inline">\(b\)</span>, alors : <span
class="math display">\[\mathbb{E}[Y | X] = aX + b\]</span> et <span
class="math display">\[\text{Var}(Y | X) = 0\]</span> Par conséquent, la
loi de décomposition de la variance se simplifie en : <span
class="math display">\[\text{Var}(Y) = \mathbb{E}[\text{Var}(Y | X)] +
\text{Var}(\mathbb{E}[Y | X]) = 0 + a^2 \text{Var}(X)\]</span> Ce qui
est cohérent avec la formule de la variance pour une transformation
linéaire.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La loi de décomposition de la variance est un résultat fondamental en
statistique et en théorie des probabilités. Elle permet de comprendre
les contributions relatives de différentes sources de variabilité dans
des modèles statistiques complexes. En décomposant la variance totale en
composantes explicatives, cette loi offre des insights précieux pour
l’analyse de données et la modélisation statistique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Analyse en Composantes Latentes Discriminantes (LDA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Analyse en Composantes Latentes Discriminantes
(LDA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’Analyse en Composantes Latentes Discriminantes (LDA) est une
technique statistique multivariée utilisée pour la classification et la
réduction de dimension. Elle trouve ses racines dans les travaux de
Ronald Fisher en 1936, où il introduisit la notion de discriminant
linéaire pour séparer deux classes. L’LDA est particulièrement utile
dans les domaines où la séparation des données en différentes catégories
est cruciale, comme la reconnaissance de motifs, l’analyse génétique et
la finance.</p>
<p>L’émergence de l’LDA est motivée par le besoin de simplifier les
problèmes de classification en projetant les données sur un sous-espace
de dimension inférieure tout en maximisant la séparation entre les
classes. Cette technique est indispensable dans les cas où les données
sont de haute dimension et où la réduction de dimension est nécessaire
pour améliorer l’efficacité des algorithmes de classification.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’LDA, commençons par définir les concepts clés.
Supposons que nous avons un ensemble de données <span
class="math inline">\(X\)</span> avec <span
class="math inline">\(n\)</span> observations et <span
class="math inline">\(p\)</span> variables, et que ces observations
appartiennent à <span class="math inline">\(k\)</span> classes
distinctes. L’objectif est de trouver une projection linéaire des
données qui maximise la séparation entre les classes.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une matrice de données de
taille <span class="math inline">\(n \times p\)</span> et soit <span
class="math inline">\(W\)</span> une matrice de projection de taille
<span class="math inline">\(p \times m\)</span> où <span
class="math inline">\(m &lt; p\)</span>. La projection des données <span
class="math inline">\(X\)</span> sur le sous-espace défini par <span
class="math inline">\(W\)</span> est donnée par : <span
class="math display">\[Y = XW\]</span> où <span
class="math inline">\(Y\)</span> est une matrice de taille <span
class="math inline">\(n \times m\)</span>.</p>
</div>
<div class="definition">
<p>Le critère de Fisher pour l’LDA est défini comme le rapport entre la
variance inter-classe et la variance intra-classe. Pour deux classes, ce
critère est donné par : <span class="math display">\[J(W) = \frac{W^T
S_B W}{W^T S_W W}\]</span> où <span class="math inline">\(S_B\)</span>
est la matrice de dispersion inter-classe et <span
class="math inline">\(S_W\)</span> est la matrice de dispersion
intra-classe.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>L’LDA repose sur plusieurs théorèmes fondamentaux, notamment le
théorème de Fisher qui permet de maximiser le critère de Fisher.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une matrice de données
centrée avec <span class="math inline">\(k\)</span> classes. Les
vecteurs propres de la matrice <span class="math inline">\(S_W^{-1}
S_B\)</span> correspondant aux plus grandes valeurs propres maximisent
le critère de Fisher <span class="math inline">\(J(W)\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Fisher, nous devons montrer que les
vecteurs propres de <span class="math inline">\(S_W^{-1} S_B\)</span>
maximisent le critère de Fisher.</p>
<div class="proof">
<p><em>Proof.</em> Considérons le critère de Fisher <span
class="math inline">\(J(W) = \frac{W^T S_B W}{W^T S_W W}\)</span>. Pour
maximiser ce critère, nous devons résoudre le problème d’optimisation
suivant : <span class="math display">\[\max_W \frac{W^T S_B W}{W^T S_W
W}\]</span> En utilisant le méthode des multiplicateurs de Lagrange,
nous trouvons que les vecteurs <span class="math inline">\(W\)</span>
qui maximisent ce critère sont les vecteurs propres de la matrice <span
class="math inline">\(S_W^{-1} S_B\)</span> correspondant aux plus
grandes valeurs propres. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’LDA possède plusieurs propriétés importantes qui en font une
technique puissante pour la classification.</p>
<div class="proposition">
<p>Les vecteurs propres de <span class="math inline">\(S_W^{-1}
S_B\)</span> sont orthogonaux.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(W_1\)</span> et
<span class="math inline">\(W_2\)</span> deux vecteurs propres de <span
class="math inline">\(S_W^{-1} S_B\)</span> correspondant à des valeurs
propres distinctes <span class="math inline">\(\lambda_1\)</span> et
<span class="math inline">\(\lambda_2\)</span>. Nous avons : <span
class="math display">\[W_1^T S_W^{-1} S_B W_2 = \lambda_1 W_1^T
W_2\]</span> et <span class="math display">\[W_1^T S_W^{-1} S_B W_2 =
\lambda_2 W_1^T W_2\]</span> En soustrayant ces deux équations, nous
obtenons : <span class="math display">\[(\lambda_1 - \lambda_2) W_1^T
W_2 = 0\]</span> Puisque <span class="math inline">\(\lambda_1 \neq
\lambda_2\)</span>, il s’ensuit que <span class="math inline">\(W_1^T
W_2 = 0\)</span>, ce qui prouve que <span
class="math inline">\(W_1\)</span> et <span
class="math inline">\(W_2\)</span> sont orthogonaux. ◻</p>
</div>
<div class="corollaire">
<p>Les projections des données sur les vecteurs propres de <span
class="math inline">\(S_W^{-1} S_B\)</span> sont non corrélées.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Soit <span class="math inline">\(Y_1 = XW_1\)</span>
et <span class="math inline">\(Y_2 = XW_2\)</span> les projections des
données sur deux vecteurs propres distincts <span
class="math inline">\(W_1\)</span> et <span
class="math inline">\(W_2\)</span>. La covariance entre <span
class="math inline">\(Y_1\)</span> et <span
class="math inline">\(Y_2\)</span> est donnée par : <span
class="math display">\[\text{Cov}(Y_1, Y_2) = W_1^T X^T X W_2\]</span>
Puisque <span class="math inline">\(W_1\)</span> et <span
class="math inline">\(W_2\)</span> sont orthogonaux, nous avons <span
class="math inline">\(W_1^T W_2 = 0\)</span>, ce qui implique que <span
class="math inline">\(\text{Cov}(Y_1, Y_2) = 0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’Analyse en Composantes Latentes Discriminantes (LDA) est une
technique puissante pour la classification et la réduction de dimension.
Elle repose sur des concepts mathématiques solides, notamment le critère
de Fisher et les propriétés des matrices de dispersion. En maximisant le
rapport entre la variance inter-classe et la variance intra-classe,
l’LDA permet de séparer efficacement les données en différentes classes,
ce qui en fait un outil indispensable dans de nombreux domaines
d’application.</p>
</body>
</html>
{% include "footer.html" %}


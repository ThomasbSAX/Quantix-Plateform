{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Erreur de Biais : Une Analyse Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Erreur de Biais : Une Analyse Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’erreur de biais est un concept fondamental en statistique et en
apprentissage automatique, qui émerge de la nécessité de comprendre les
limites des modèles prédictifs. Historiquement, cette notion a été
formalisée pour quantifier l’écart entre la performance d’un modèle sur
les données d’entraînement et sa performance générale sur de nouvelles
données. Ce biais est indispensable pour évaluer la robustesse et la
généralisabilité des modèles, surtout dans un contexte où les données
sont souvent bruitées ou incomplètes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’erreur de biais, commençons par définir ce que nous
cherchons à mesurer. Imaginons un modèle prédictif qui apprend à partir
de données d’entraînement. Nous voulons quantifier l’écart entre les
prédictions de ce modèle et les vraies valeurs que nous essayons de
prédire. Cet écart est dû à plusieurs facteurs, dont le biais du
modèle.</p>
<p>Formellement, l’erreur de biais peut être définie comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{D}\)</span> un ensemble de
données d’entraînement, <span class="math inline">\(f\)</span> un modèle
prédictif, et <span class="math inline">\(y\)</span> la vraie valeur que
nous cherchons à prédire. L’erreur de biais <span
class="math inline">\(\text{Bias}(f)\)</span> est définie comme l’écart
moyen entre les prédictions de <span class="math inline">\(f\)</span> et
les vraies valeurs <span class="math inline">\(y\)</span>, sur
l’ensemble des données d’entraînement.</p>
<p><span class="math display">\[\text{Bias}(f) =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - y)^2 \right]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}_{\mathcal{D}}\)</span>
désigne l’espérance sur l’ensemble des données d’entraînement <span
class="math inline">\(\mathcal{D}\)</span>, et <span
class="math inline">\(f(x)\)</span> est la prédiction du modèle pour une
entrée <span class="math inline">\(x\)</span>.</p>
</div>
<p>Une autre manière de formuler cette définition est la suivante :</p>
<p><span class="math display">\[\text{Bias}(f) = \int_{\mathcal{D}}
(f(x) - y)^2 \, dP(x, y)\]</span></p>
<p>où <span class="math inline">\(P(x, y)\)</span> est la distribution
conjointe des entrées <span class="math inline">\(x\)</span> et des
vraies valeurs <span class="math inline">\(y\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour mieux comprendre l’erreur de biais, nous pouvons utiliser le
théorème de la décomposition de l’erreur, qui permet de séparer l’erreur
totale en plusieurs composantes.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> un modèle prédictif et
<span class="math inline">\(y\)</span> la vraie valeur. L’erreur totale
<span class="math inline">\(\text{Error}(f)\)</span> peut être
décomposée en trois composantes : le biais, la variance et l’erreur
irréductible.</p>
<p><span class="math display">\[\text{Error}(f) = \text{Bias}(f)^2 +
\text{Variance}(f) + \text{Irreducible Error}\]</span></p>
<p>où <span class="math inline">\(\text{Bias}(f)\)</span> est l’erreur
de biais, <span class="math inline">\(\text{Variance}(f)\)</span> est la
variance du modèle, et <span class="math inline">\(\text{Irreducible
Error}\)</span> est l’erreur irréductible due au bruit dans les
données.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la décomposition de l’erreur, nous devons
d’abord définir chaque composante de manière formelle.</p>
<div class="proof">
<p><em>Proof.</em> Nous commençons par définir l’erreur totale :</p>
<p><span class="math display">\[\text{Error}(f) =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - y)^2 \right]\]</span></p>
<p>Ensuite, nous décomposons cette erreur en utilisant l’espérance
conditionnelle :</p>
<p><span class="math display">\[\text{Error}(f) =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - \mathbb{E}[y|x] +
\mathbb{E}[y|x] - y)^2 \right]\]</span></p>
<p>En développant cette expression, nous obtenons :</p>
<p><span class="math display">\[\text{Error}(f) =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - \mathbb{E}[y|x])^2 \right] +
\mathbb{E}_{\mathcal{D}} \left[ (\mathbb{E}[y|x] - y)^2 \right] +
2\mathbb{E}_{\mathcal{D}} \left[ (f(x) -
\mathbb{E}[y|x])(\mathbb{E}[y|x] - y) \right]\]</span></p>
<p>Le dernier terme est nul car <span
class="math inline">\(\mathbb{E}_{\mathcal{D}} \left[ f(x) -
\mathbb{E}[y|x] \right] = 0\)</span>. Ainsi, nous avons :</p>
<p><span class="math display">\[\text{Error}(f) =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - \mathbb{E}[y|x])^2 \right] +
\mathbb{E}_{\mathcal{D}} \left[ (\mathbb{E}[y|x] - y)^2
\right]\]</span></p>
<p>Le premier terme est la variance du modèle, et le second terme est
l’erreur irréductible. Le biais est alors défini comme la différence
entre la prédiction du modèle et l’espérance conditionnelle :</p>
<p><span class="math display">\[\text{Bias}(f) =
\mathbb{E}_{\mathcal{D}} \left[ f(x) - \mathbb{E}[y|x]
\right]\]</span></p>
<p>En élevant au carré et en prenant l’espérance, nous obtenons :</p>
<p><span class="math display">\[\text{Bias}(f)^2 =
\mathbb{E}_{\mathcal{D}} \left[ (f(x) - \mathbb{E}[y|x])^2
\right]\]</span></p>
<p>Ainsi, nous avons prouvé le théorème de la décomposition de
l’erreur. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous pouvons maintenant énoncer quelques propriétés importantes de
l’erreur de biais.</p>
<div class="proposition">
<p>L’erreur de biais est toujours non négative.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition, l’erreur de biais est l’espérance
d’un carré, qui est toujours non négatif. ◻</p>
</div>
<div class="proposition">
<p>L’erreur de biais est nulle si et seulement si le modèle prédit
parfaitement les données d’entraînement.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Si le modèle prédit parfaitement les données
d’entraînement, alors <span class="math inline">\(f(x) = y\)</span> pour
tout <span class="math inline">\(x\)</span>, et donc <span
class="math inline">\(\text{Bias}(f) = 0\)</span>. Réciproquement, si
<span class="math inline">\(\text{Bias}(f) = 0\)</span>, alors <span
class="math inline">\(f(x) = \mathbb{E}[y|x]\)</span> pour tout <span
class="math inline">\(x\)</span>, ce qui signifie que le modèle prédit
parfaitement les données d’entraînement. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>L’erreur de biais est un concept fondamental en statistique et en
apprentissage automatique, qui permet de quantifier l’écart entre les
prédictions d’un modèle et les vraies valeurs. En comprenant cette
notion, nous pouvons mieux évaluer la robustesse et la généralisabilité
des modèles prédictifs.</p>
</body>
</html>
{% include "footer.html" %}


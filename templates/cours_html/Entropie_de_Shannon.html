{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Shannon : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Shannon : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie de Shannon, introduite par Claude E. Shannon en 1948 dans
son article fondateur <em>A Mathematical Theory of Communication</em>,
représente une avancée majeure en théorie de l’information. Cette
notion, inspirée par la thermodynamique et la mécanique statistique,
permet de quantifier l’incertitude ou l’information contenue dans un
ensemble de données. Elle est au cœur des communications modernes, de la
compression de données à la cryptographie.</p>
<p>L’entropie de Shannon émerge comme une réponse aux questions
fondamentales sur la transmission et le stockage de l’information. Elle
résout le problème de la quantification de l’information, permettant
ainsi d’optimiser les systèmes de communication. Dans un cadre où les
ressources sont limitées, comprendre et manipuler l’entropie devient
indispensable pour concevoir des systèmes efficaces et robustes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Shannon, considérons un ensemble
discret de symboles <span class="math inline">\(\mathcal{X} = \{x_1,
x_2, \ldots, x_n\}\)</span> avec leurs probabilités associées <span
class="math inline">\(p(x_i)\)</span>. Nous cherchons à mesurer
l’incertitude ou l’information contenue dans cet ensemble.</p>
<p>L’entropie de Shannon <span class="math inline">\(H\)</span> est
définie comme la quantité d’information moyenne par symbole. Elle mesure
l’incertitude moyenne associée à un ensemble de symboles. Formellement,
nous avons :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span> un ensemble discret de symboles avec des probabilités
<span class="math inline">\(p(x_i)\)</span>. L’entropie de Shannon est
définie par : <span class="math display">\[H(\mathcal{X}) =
-\sum_{i=1}^{n} p(x_i) \log_2 p(x_i).\]</span></p>
</div>
<p>Cette définition peut également être exprimée en utilisant des
quantificateurs : <span class="math display">\[H(\mathcal{X}) = -\sum_{x
\in \mathcal{X}} p(x) \log_2 p(x),\]</span> où <span
class="math inline">\(p(x) \geq 0\)</span> pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span> et <span
class="math inline">\(\sum_{x \in \mathcal{X}} p(x) = 1\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie de Shannon est le théorème
de codage de source sans perte. Ce théorème établit une limite
inférieure sur la longueur moyenne des mots de code nécessaires pour
représenter les symboles d’une source avec une probabilité donnée.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots,
x_n\}\)</span> un ensemble discret de symboles avec des probabilités
<span class="math inline">\(p(x_i)\)</span>. La longueur moyenne
minimale <span class="math inline">\(L\)</span> d’un code binaire sans
perte satisfait : <span class="math display">\[H(\mathcal{X}) \leq L
&lt; H(\mathcal{X}) + 1.\]</span></p>
</div>
<p>La démonstration de ce théorème repose sur les propriétés de
l’entropie et les principes du codage binaire. Nous allons maintenant
développer cette preuve en détail.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de codage de source sans perte, nous
commençons par rappeler que l’entropie <span
class="math inline">\(H(\mathcal{X})\)</span> représente la quantité
d’information moyenne par symbole. Nous cherchons à montrer que la
longueur moyenne minimale <span class="math inline">\(L\)</span> d’un
code binaire sans perte est bornée par l’entropie.</p>
<div class="proof">
<p><em>Proof.</em> Considérons un code binaire sans perte pour les
symboles <span class="math inline">\(\mathcal{X}\)</span>. La longueur
du mot de code pour chaque symbole <span
class="math inline">\(x_i\)</span> est notée <span
class="math inline">\(l(x_i)\)</span>. La longueur moyenne <span
class="math inline">\(L\)</span> est donnée par : <span
class="math display">\[L = \sum_{i=1}^{n} p(x_i) l(x_i).\]</span></p>
<p>Pour un code binaire, la longueur minimale <span
class="math inline">\(l(x_i)\)</span> doit satisfaire : <span
class="math display">\[2^{l(x_i)} \geq i,\]</span> où <span
class="math inline">\(i\)</span> est le rang du symbole dans l’ordre de
probabilité décroissante.</p>
<p>En utilisant cette inégalité, nous obtenons : <span
class="math display">\[L \geq \sum_{i=1}^{n} p(x_i) \log_2
i.\]</span></p>
<p>En utilisant l’inégalité de Gibbs, nous savons que : <span
class="math display">\[\sum_{i=1}^{n} p(x_i) \log_2 i \geq
H(\mathcal{X}).\]</span></p>
<p>Ainsi, nous avons : <span class="math display">\[L \geq
H(\mathcal{X}).\]</span></p>
<p>Pour la borne supérieure, nous utilisons le fait que chaque symbole
peut être codé avec une longueur maximale de <span
class="math inline">\(\lceil H(\mathcal{X}) \rceil + 1\)</span>. Par
conséquent : <span class="math display">\[L &lt; H(\mathcal{X}) +
1.\]</span></p>
<p>Cette preuve montre que la longueur moyenne minimale <span
class="math inline">\(L\)</span> est bornée par l’entropie de
Shannon. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’entropie de Shannon possède plusieurs propriétés importantes qui en
font un outil puissant pour l’analyse des systèmes d’information. Nous
allons maintenant énumérer et développer ces propriétés.</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie est toujours non
négative. Formellement, pour tout ensemble de symboles <span
class="math inline">\(\mathcal{X}\)</span>, nous avons : <span
class="math display">\[H(\mathcal{X}) \geq 0.\]</span></p></li>
<li><p><strong>Maximale pour une distribution uniforme</strong> :
L’entropie est maximale lorsque les symboles sont équiprobables. Pour un
ensemble de <span class="math inline">\(n\)</span> symboles, l’entropie
maximale est : <span class="math display">\[H(\mathcal{X}) = \log_2
n.\]</span></p></li>
<li><p><strong>Additivité</strong> : Pour deux ensembles de symboles
indépendants <span class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span>, l’entropie conjointe est la
somme des entropies individuelles : <span
class="math display">\[H(\mathcal{X}, \mathcal{Y}) = H(\mathcal{X}) +
H(\mathcal{Y}).\]</span></p></li>
<li><p><strong>Inégalité de sub-additivité</strong> : Pour deux
ensembles de symboles <span class="math inline">\(\mathcal{X}\)</span>
et <span class="math inline">\(\mathcal{Y}\)</span>, l’entropie
conjointe satisfait : <span class="math display">\[H(\mathcal{X},
\mathcal{Y}) \leq H(\mathcal{X}) + H(\mathcal{Y}).\]</span></p></li>
<li><p><strong>Inégalité de Fano</strong> : Pour un ensemble de symboles
<span class="math inline">\(\mathcal{X}\)</span> et une variable
aléatoire <span class="math inline">\(Y\)</span>, l’entropie
conditionnelle satisfait : <span class="math display">\[H(\mathcal{X} |
Y) \leq H(\mathcal{X}) + 1.\]</span></p></li>
</ol>
<p>Chacune de ces propriétés joue un rôle crucial dans l’analyse et la
conception des systèmes de communication. Elles permettent de comprendre
les limites fondamentales de la transmission et du stockage de
l’information.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie de Shannon est un concept fondamental en théorie de
l’information. Elle permet de quantifier l’incertitude et l’information
contenue dans un ensemble de données, offrant ainsi des outils puissants
pour l’analyse et la conception des systèmes de communication. Les
théorèmes et propriétés associés à l’entropie de Shannon continuent
d’inspirer des recherches dans divers domaines, allant de la
cryptographie à l’apprentissage automatique.</p>
<p>En conclusion, l’entropie de Shannon reste un pilier de la théorie de
l’information, avec des applications qui continuent de se développer et
d’évoluer dans un monde de plus en plus connecté.</p>
</body>
</html>
{% include "footer.html" %}


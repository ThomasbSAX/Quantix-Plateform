{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Kullback-Leibler : Une Mesure Fondamentale en Théorie de l’Information</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Kullback-Leibler : Une Mesure Fondamentale
en Théorie de l’Information</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Kullback-Leibler (KL) émerge dans les années 1950
grâce aux travaux de Solomon Kullback et Richard Leibler. Cette notion
révolutionnaire en théorie de l’information permet de mesurer la
divergence entre deux distributions de probabilité. Imaginez un monde où
les données sont abondantes mais bruitées : la KL offre un outil pour
quantifier l’information mutuelle entre deux sources, révélant ainsi des
structures cachées.</p>
<p>Pourquoi cette notion est-elle indispensable ? Elle résout le
problème fondamental de la comparaison des distributions, crucial en
apprentissage statistique, en compression de données et même en
biologie. En effet, la KL permet de mesurer l’efficacité d’un modèle par
rapport à une distribution de référence, ouvrant ainsi la voie à des
algorithmes optimaux.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Nous cherchons à quantifier la différence entre deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. Intuitivement, cette différence doit
capturer la perte d’information lorsque l’on utilise <span
class="math inline">\(Q\)</span> pour approximer <span
class="math inline">\(P\)</span>. La KL répond à cette exigence en
introduisant une mesure asymétrique de la divergence.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\mathcal{X}\)</span>. La
distance de Kullback-Leibler de <span class="math inline">\(P\)</span>
par rapport à <span class="math inline">\(Q\)</span> est définie comme :
<span class="math display">\[D_{\text{KL}}(P \| Q) = \sum_{x \in
\mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)\]</span> pour des
distributions discrètes, ou <span class="math display">\[D_{\text{KL}}(P
\| Q) = \int_{\mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right) \,
dx\]</span> pour des distributions continues. On peut également l’écrire
sous la forme : <span class="math display">\[D_{\text{KL}}(P \| Q) =
\mathbb{E}_P\left[\log\left(\frac{P}{Q}\right)\right]\]</span> où <span
class="math inline">\(\mathbb{E}_P\)</span> désigne l’espérance par
rapport à la distribution <span class="math inline">\(P\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la KL est celui de l’information
mutuelle, qui relie la divergence KL à l’information mutuelle entre deux
variables aléatoires.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> deux variables aléatoires jointes avec
distribution conjointe <span class="math inline">\(P_{XY}\)</span>. Si
<span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendantes, alors : <span
class="math display">\[I(X;Y) = D_{\text{KL}}(P_{XY} \| P_X \otimes
P_Y)\]</span> où <span class="math inline">\(I(X;Y)\)</span> est
l’information mutuelle entre <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span>, et <span
class="math inline">\(P_X \otimes P_Y\)</span> est le produit des
distributions marginales de <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’information mutuelle, nous commençons
par rappeler la définition de l’information mutuelle : <span
class="math display">\[I(X;Y) = \sum_{x,y} P_{XY}(x,y)
\log\left(\frac{P_{XY}(x,y)}{P_X(x)P_Y(y)}\right)\]</span> En utilisant
la définition de la divergence KL, nous avons : <span
class="math display">\[D_{\text{KL}}(P_{XY} \| P_X \otimes P_Y) =
\sum_{x,y} P_{XY}(x,y)
\log\left(\frac{P_{XY}(x,y)}{P_X(x)P_Y(y)}\right)\]</span> Il est clair
que ces deux expressions sont identiques, ce qui achève la preuve.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes de la
divergence KL :</p>
<ol>
<li><p><strong>Non-Négativité</strong> : Pour toute distribution <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_{\text{KL}}(P \| Q) \geq 0\]</span> avec
égalité si et seulement si <span class="math inline">\(P = Q\)</span>
presque partout.</p></li>
<li><p><strong>Asymétrie</strong> : La divergence KL n’est pas
symétrique, c’est-à-dire que : <span
class="math display">\[D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \|
P)\]</span> en général.</p></li>
<li><p><strong>Inégalité de Gibbs</strong> : Pour toute distribution
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, on a : <span
class="math display">\[D_{\text{KL}}(P \| Q) = H(P,Q) - H(P)\]</span> où
<span class="math inline">\(H(P,Q)\)</span> est l’entropie croisée et
<span class="math inline">\(H(P)\)</span> est l’entropie de <span
class="math inline">\(P\)</span>.</p></li>
</ol>
<p>La preuve de la non-négativité repose sur l’inégalité de Jensen. En
effet, la fonction <span class="math inline">\(\log(x)\)</span> est
concave, donc par l’inégalité de Jensen : <span
class="math display">\[\mathbb{E}_P\left[\log\left(\frac{P}{Q}\right)\right]
\leq \log\left(\mathbb{E}_P\left[\frac{P}{Q}\right]\right) = 0\]</span>
ce qui prouve la non-négativité.</p>
</body>
</html>
{% include "footer.html" %}


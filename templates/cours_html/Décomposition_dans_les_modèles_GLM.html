{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Décomposition dans les modèles GLM : Une approche théorique et pratique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Décomposition dans les modèles GLM : Une approche
théorique et pratique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Les modèles linéaires généralisés (GLM) constituent une extension des
modèles linéaires classiques, permettant d’analyser des données qui ne
satisfont pas les hypothèses de normalité et d’homoscédasticité.
L’origine des GLM remonte aux travaux pionniers de Nelder et Wedderburn
en 1972, qui ont unifié plusieurs modèles statistiques sous une même
framework. Ces modèles sont indispensables dans de nombreux domaines,
notamment en biostatistique, en économétrie et en sciences sociales, où
les données présentent souvent des structures complexes.</p>
<p>La décomposition dans les modèles GLM permet de comprendre comment
les différentes composantes du modèle contribuent à la variance totale
expliquée. Cette décomposition est cruciale pour l’interprétation des
résultats et pour la validation des hypothèses sous-jacentes au modèle.
Dans cet article, nous explorons les fondements théoriques de la
décomposition dans les GLM, en mettant l’accent sur les définitions
formelles, les théorèmes clés et leurs preuves détaillées.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous cherchons à décomposer. Imaginez un modèle
statistique où les données ne suivent pas une distribution normale.
Comment pouvons-nous décomposer la variance totale en composantes
explicatives et résiduelles ? La réponse réside dans les modèles
linéaires généralisés.</p>
<div class="definition">
<p>Soit <span class="math inline">\(Y\)</span> une variable aléatoire
suivant une distribution exponentielle généralisée avec fonction de
densité : <span class="math display">\[f(y|\theta, \phi) = \exp\left(
\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi) \right),\]</span> où
<span class="math inline">\(\theta\)</span> est le paramètre de
localisation, <span class="math inline">\(\phi\)</span> est le paramètre
de dispersion, et <span class="math inline">\(b(\theta)\)</span>, <span
class="math inline">\(a(\phi)\)</span> et <span
class="math inline">\(c(y, \phi)\)</span> sont des fonctions
connues.</p>
<p>Un GLM est défini par : <span class="math display">\[g(\mathbb{E}[Y])
= X\beta,\]</span> où <span class="math inline">\(g\)</span> est une
fonction de lien, <span class="math inline">\(X\)</span> est la matrice
des variables explicatives et <span class="math inline">\(\beta\)</span>
est le vecteur des coefficients à estimer.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre la décomposition dans les GLM, nous devons d’abord
établir quelques théorèmes fondamentaux.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(Y\)</span> une variable aléatoire
suivant un GLM avec matrice de variance-covariance <span
class="math inline">\(V\)</span>. La variance totale peut être
décomposée en : <span class="math display">\[\text{Var}(Y) =
X\beta\beta^TX^T + \Sigma,\]</span> où <span
class="math inline">\(\Sigma\)</span> est la matrice de
variance-covariance des résidus.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve de ce théorème repose sur plusieurs étapes clés. Nous
commençons par exprimer la variance de <span
class="math inline">\(Y\)</span> en termes de ses composantes.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la décomposition de <span
class="math inline">\(Y\)</span> en partie systématique et partie
résiduelle : <span class="math display">\[Y = X\beta +
\epsilon,\]</span> où <span class="math inline">\(\epsilon\)</span> est
le vecteur des résidus.</p>
<p>La variance de <span class="math inline">\(Y\)</span> peut alors
s’écrire : <span class="math display">\[\text{Var}(Y) =
\text{Var}(X\beta + \epsilon).\]</span></p>
<p>En utilisant la linéarité de la variance, nous avons : <span
class="math display">\[\text{Var}(Y) = \text{Var}(X\beta) +
\text{Var}(\epsilon).\]</span></p>
<p>Puisque <span class="math inline">\(X\)</span> et <span
class="math inline">\(\beta\)</span> sont des matrices déterministes,
<span class="math inline">\(\text{Var}(X\beta) =
X\text{Var}(\beta)X^T\)</span>. Cependant, dans le cadre des GLM, <span
class="math inline">\(\text{Var}(\beta)\)</span> est souvent considérée
comme nulle pour simplifier l’analyse.</p>
<p>Ainsi, nous obtenons : <span class="math display">\[\text{Var}(Y) =
X\beta\beta^TX^T + \Sigma,\]</span> où <span
class="math inline">\(\Sigma = \text{Var}(\epsilon)\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La décomposition de la variance dans les GLM présente plusieurs
propriétés intéressantes.</p>
<div class="corollary">
<p>La décomposition de la variance dans les GLM permet de séparer les
effets des variables explicatives des effets résiduels. Cette propriété
est cruciale pour l’interprétation des résultats et pour la validation
des hypothèses du modèle.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La décomposition de la variance en composantes
explicatives et résiduelles permet de quantifier l’impact des variables
explicatives sur la variance totale. En comparant les deux composantes,
nous pouvons évaluer l’efficacité du modèle à expliquer la variance des
données. ◻</p>
</div>
<div class="corollary">
<p>Si les résidus <span class="math inline">\(\epsilon\)</span> sont
indépendants des variables explicatives <span
class="math inline">\(X\)</span>, alors la matrice de
variance-covariance des résidus <span
class="math inline">\(\Sigma\)</span> est diagonale.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’indépendance des résidus et des variables
explicatives implique que les covariances entre les résidus et les
variables explicatives sont nulles. Par conséquent, la matrice de
variance-covariance des résidus <span
class="math inline">\(\Sigma\)</span> est diagonale. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La décomposition dans les modèles GLM offre une approche puissante
pour comprendre et interpréter les résultats des analyses statistiques.
En décomposant la variance totale en composantes explicatives et
résiduelles, nous pouvons évaluer l’efficacité du modèle et valider les
hypothèses sous-jacentes. Les théorèmes et propriétés présentés dans cet
article fournissent une base solide pour l’analyse des données dans le
cadre des GLM.</p>
</body>
</html>
{% include "footer.html" %}


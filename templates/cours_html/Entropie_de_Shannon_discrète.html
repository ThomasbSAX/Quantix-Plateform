{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie de Shannon Discrète : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie de Shannon Discrète : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’entropie de Shannon, introduite par Claude E. Shannon en 1948 dans
son article fondateur <em>A Mathematical Theory of Communication</em>,
constitue le pilier de la théorie de l’information. Cette notion
révolutionnaire émerge dans un contexte où la communication et le
traitement de l’information deviennent des enjeux majeurs. Shannon
cherche à quantifier l’incertitude associée à un message, permettant
ainsi d’optimiser la transmission et le stockage de l’information.</p>
<p>L’entropie de Shannon discrète répond à une question fondamentale :
comment mesurer la quantité d’information contenue dans un ensemble de
symboles ? Elle trouve ses applications dans des domaines variés tels
que la théorie des codes, le traitement du signal, et même la biologie
moléculaire. En fournissant une mesure de l’incertitude, elle permet de
concevoir des codes efficaces et robustes, minimisant ainsi les erreurs
de transmission.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’entropie de Shannon discrète, commençons par
comprendre ce que nous cherchons à mesurer. Imaginons un ensemble de
symboles où chaque symbole apparaît avec une certaine probabilité. Notre
objectif est de quantifier l’incertitude ou l’information apportée par
chaque symbole.</p>
<p>Formellement, considérons un alphabet fini <span
class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\)</span>
et une distribution de probabilité <span
class="math inline">\(P\)</span> sur <span
class="math inline">\(\mathcal{X}\)</span>, telle que pour chaque
symbole <span class="math inline">\(x_i\)</span>, <span
class="math inline">\(P(x_i) &gt; 0\)</span> et <span
class="math inline">\(\sum_{i=1}^n P(x_i) = 1\)</span>.</p>
<p>Nous cherchons une fonction <span class="math inline">\(H(P)\)</span>
qui mesure l’incertitude associée à cette distribution de probabilité.
Cette fonction doit satisfaire plusieurs propriétés intuitives :</p>
<ul>
<li><p><span class="math inline">\(H(P)\)</span> doit être maximale
lorsque tous les symboles sont équiprobables, c’est-à-dire <span
class="math inline">\(P(x_i) = \frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(H(P)\)</span> doit être nulle lorsque
l’un des symboles est certain, c’est-à-dire <span
class="math inline">\(P(x_i) = 1\)</span> pour un certain <span
class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(H(P)\)</span> doit être additive,
c’est-à-dire que l’incertitude totale de deux événements indépendants
est la somme des incertitudes individuelles.</p></li>
</ul>
<p>Après avoir exploré ces propriétés, nous pouvons annoncer la
définition formelle de l’entropie de Shannon discrète :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un alphabet
fini et <span class="math inline">\(P\)</span> une distribution de
probabilité sur <span class="math inline">\(\mathcal{X}\)</span>.
L’entropie de Shannon discrète <span class="math inline">\(H(P)\)</span>
est définie par : <span class="math display">\[H(P) = -\sum_{i=1}^n
P(x_i) \log_2 P(x_i)\]</span> où <span
class="math inline">\(\log_2\)</span> désigne le logarithme en base
2.</p>
</div>
<p>Cette définition peut également être exprimée en utilisant des
quantificateurs : <span class="math display">\[H(P) = -\sum_{x \in
\mathcal{X}} P(x) \log_2 P(x)\]</span> où <span
class="math inline">\(P(x)\)</span> est la probabilité du symbole <span
class="math inline">\(x\)</span> dans l’alphabet <span
class="math inline">\(\mathcal{X}\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour approfondir notre compréhension de l’entropie de Shannon,
examinons quelques théorèmes fondamentaux. Commençons par le théorème
qui établit que l’entropie est maximale lorsque les symboles sont
équiprobables.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un alphabet
fini de cardinalité <span class="math inline">\(n\)</span> et <span
class="math inline">\(P\)</span> une distribution de probabilité sur
<span class="math inline">\(\mathcal{X}\)</span>. L’entropie <span
class="math inline">\(H(P)\)</span> est maximale lorsque tous les
symboles sont équiprobables, c’est-à-dire : <span
class="math display">\[H(P) \leq \log_2 n\]</span> avec égalité si et
seulement si <span class="math inline">\(P(x_i) = \frac{1}{n}\)</span>
pour tout <span class="math inline">\(i\)</span>.</p>
</div>
<p>Pour démontrer ce théorème, nous utilisons l’inégalité de Gibbs et
les propriétés du logarithme. La preuve détaillée est la suivante :</p>
<div class="proof">
<p><em>Proof.</em> Considérons l’entropie <span
class="math inline">\(H(P)\)</span> et utilisons l’inégalité de Gibbs,
qui stipule que pour toute distribution de probabilité <span
class="math inline">\(P\)</span> et tout nombre réel <span
class="math inline">\(\alpha &gt; 0\)</span>, nous avons : <span
class="math display">\[\sum_{i=1}^n P(x_i) \log_2 P(x_i) \leq \frac{1 -
\alpha}{\ln 2} + \frac{\alpha}{e \ln 2}\]</span> En prenant <span
class="math inline">\(\alpha = 1\)</span>, nous obtenons : <span
class="math display">\[\sum_{i=1}^n P(x_i) \log_2 P(x_i) \leq 0\]</span>
ce qui implique : <span class="math display">\[H(P) = -\sum_{i=1}^n
P(x_i) \log_2 P(x_i) \geq 0\]</span> Pour montrer que <span
class="math inline">\(H(P)\)</span> est maximale lorsque les symboles
sont équiprobables, considérons la distribution uniforme <span
class="math inline">\(P(x_i) = \frac{1}{n}\)</span> pour tout <span
class="math inline">\(i\)</span>. Dans ce cas : <span
class="math display">\[H(P) = -\sum_{i=1}^n \frac{1}{n} \log_2
\left(\frac{1}{n}\right) = \log_2 n\]</span> Pour toute autre
distribution de probabilité, l’entropie sera strictement inférieure à
<span class="math inline">\(\log_2 n\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de l’entropie de Shannon, examinons une
preuve détaillée d’une propriété fondamentale. Considérons le théorème
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(\mathcal{Y}\)</span> deux alphabets finis et <span
class="math inline">\(P_{XY}\)</span> une distribution de probabilité
conjointe sur <span class="math inline">\(\mathcal{X} \times
\mathcal{Y}\)</span>. Si <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendants, alors : <span
class="math display">\[H(X, Y) = H(X) + H(Y)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Commençons par rappeler que l’entropie conjointe
<span class="math inline">\(H(X, Y)\)</span> est définie par : <span
class="math display">\[H(X, Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in
\mathcal{Y}} P_{XY}(x, y) \log_2 P_{XY}(x, y)\]</span> Puisque <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont indépendants, nous avons : <span
class="math display">\[P_{XY}(x, y) = P_X(x) P_Y(y)\]</span> En
substituant cette expression dans la définition de l’entropie conjointe,
nous obtenons : <span class="math display">\[H(X, Y) = -\sum_{x \in
\mathcal{X}} \sum_{y \in \mathcal{Y}} P_X(x) P_Y(y) \log_2 (P_X(x)
P_Y(y))\]</span> En utilisant la propriété du logarithme <span
class="math inline">\(\log_2 (ab) = \log_2 a + \log_2 b\)</span>, nous
pouvons réécrire l’expression comme : <span class="math display">\[H(X,
Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P_X(x) P_Y(y)
(\log_2 P_X(x) + \log_2 P_Y(y))\]</span> En séparant les sommes, nous
obtenons : <span class="math display">\[H(X, Y) = -\sum_{x \in
\mathcal{X}} P_X(x) \log_2 P_X(x) \sum_{y \in \mathcal{Y}} P_Y(y) -
\sum_{x \in \mathcal{X}} P_X(x) \sum_{y \in \mathcal{Y}} P_Y(y) \log_2
P_Y(y)\]</span> Puisque <span class="math inline">\(\sum_{y \in
\mathcal{Y}} P_Y(y) = 1\)</span> et <span class="math inline">\(\sum_{x
\in \mathcal{X}} P_X(x) = 1\)</span>, nous avons : <span
class="math display">\[H(X, Y) = -\sum_{x \in \mathcal{X}} P_X(x) \log_2
P_X(x) - \sum_{y \in \mathcal{Y}} P_Y(y) \log_2 P_Y(y)\]</span> En
reconnaissant les définitions de <span
class="math inline">\(H(X)\)</span> et <span
class="math inline">\(H(Y)\)</span>, nous concluons que : <span
class="math display">\[H(X, Y) = H(X) + H(Y)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Examinons quelques propriétés importantes de l’entropie de Shannon
discrète. Ces propriétés sont essentielles pour comprendre et appliquer
cette notion dans divers contextes.</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie de Shannon est
toujours non négative, c’est-à-dire : <span class="math display">\[H(P)
\geq 0\]</span> La preuve de cette propriété découle directement de
l’inégalité de Gibbs, comme démontré précédemment.</p></li>
<li><p><strong>Invariance par changement de base</strong> : L’entropie
peut être exprimée en utilisant n’importe quelle base pour le
logarithme, à un facteur multiplicatif près. Plus précisément, si <span
class="math inline">\(H_{\log}(P)\)</span> désigne l’entropie calculée
avec un logarithme de base <span class="math inline">\(\log\)</span>,
alors : <span class="math display">\[H_{\log}(P) = \frac{\log 2}{\ln 2}
H(P)\]</span> où <span class="math inline">\(\ln\)</span> désigne le
logarithme naturel.</p></li>
<li><p><strong>Inégalité de Fano</strong> : Cette inégalité établit une
relation entre l’entropie et la probabilité d’erreur dans un problème de
classification. Soit <span class="math inline">\(X\)</span> une variable
aléatoire prenant ses valeurs dans un alphabet <span
class="math inline">\(\mathcal{X}\)</span> et <span
class="math inline">\(Y\)</span> une estimation de <span
class="math inline">\(X\)</span>. La probabilité d’erreur est définie
par : <span class="math display">\[P_e = \mathbb{P}(X \neq Y)\]</span>
L’inégalité de Fano stipule que : <span class="math display">\[H(X | Y)
\leq H(P_e) + P_e \log_2 (|\mathcal{X}| - 1)\]</span> où <span
class="math inline">\(H(X | Y)\)</span> est l’entropie conditionnelle de
<span class="math inline">\(X\)</span> sachant <span
class="math inline">\(Y\)</span>.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’entropie de Shannon discrète constitue une notion fondamentale en
théorie de l’information. Elle permet de quantifier l’incertitude
associée à un ensemble de symboles et trouve des applications dans
divers domaines. À travers ce chapitre, nous avons exploré les
définitions, théorèmes et propriétés de cette notion, mettant en lumière
sa puissance et son utilité. Les preuves détaillées fournies illustrent
la rigueur mathématique sous-jacente à cette théorie, ouvrant la voie à
des applications pratiques et à des recherches futures.</p>
</body>
</html>
{% include "footer.html" %}


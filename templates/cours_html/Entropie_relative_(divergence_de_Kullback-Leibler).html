{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie Relative : La Divergence de Kullback-Leibler</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie Relative : La Divergence de
Kullback-Leibler</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie relative, également connue sous le nom de divergence de
Kullback-Leibler (KL), est une notion fondamentale en théorie de
l’information et en statistique. Elle quantifie la différence entre deux
distributions de probabilité, mesurant ainsi l’information perdue
lorsqu’une distribution est utilisée pour approximer une autre. Cette
notion a été introduite par Solomon Kullback et Richard Leibler en 1951,
dans le cadre de leurs travaux sur la théorie de l’information.
L’entropie relative est indispensable dans divers domaines, tels que le
traitement du signal, l’apprentissage automatique et la théorie des
codes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir l’entropie relative, commençons par comprendre ce que
nous cherchons à mesurer. Supposons que nous ayons deux distributions de
probabilité, <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, définies sur le même espace
d’échantillonnage. Nous voulons quantifier la différence entre ces deux
distributions, c’est-à-dire à quel point <span
class="math inline">\(Q\)</span> s’écarte de <span
class="math inline">\(P\)</span>. Cette différence doit être non
négative et nulle si et seulement si <span class="math inline">\(P =
Q\)</span>.</p>
<p>Nous cherchons donc une fonction <span class="math inline">\(D(P
\parallel Q)\)</span> qui mesure cette divergence. Cette fonction doit
satisfaire certaines propriétés, telles que la non-négativité et
l’invariance sous les transformations de probabilité.</p>
<p>La divergence de Kullback-Leibler est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur le même espace d’échantillonnage <span
class="math inline">\(\mathcal{X}\)</span>. La divergence de
Kullback-Leibler de <span class="math inline">\(P\)</span> par rapport à
<span class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D(P \parallel Q) = \sum_{x \in \mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right)\]</span> Si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des distributions continues
définies sur un espace <span class="math inline">\(\mathcal{X}\)</span>,
la divergence de Kullback-Leibler est définie par : <span
class="math display">\[D(P \parallel Q) = \int_{\mathcal{X}} P(x) \log
\left( \frac{P(x)}{Q(x)} \right) dx\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Kullback-Leibler est
l’inégalité de Gibbs, qui établit une borne inférieure pour cette
divergence.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes définies sur le même espace d’échantillonnage <span
class="math inline">\(\mathcal{X}\)</span>. Alors, nous avons : <span
class="math display">\[D(P \parallel Q) \geq 0\]</span> De plus,
l’égalité a lieu si et seulement si <span class="math inline">\(P =
Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour prouver l’inégalité de Gibbs, nous utilisons
l’inégalité de Jensen appliquée à la fonction <span
class="math inline">\(\log\)</span>, qui est concave. L’inégalité de
Jensen stipule que pour une fonction concave <span
class="math inline">\(f\)</span>, nous avons : <span
class="math display">\[f\left( \sum_{i=1}^n \lambda_i x_i \right) \geq
\sum_{i=1}^n \lambda_i f(x_i)\]</span> où <span
class="math inline">\(\lambda_i \geq 0\)</span> et <span
class="math inline">\(\sum_{i=1}^n \lambda_i = 1\)</span>.</p>
<p>Appliquons cette inégalité à la fonction <span
class="math inline">\(f(x) = \log x\)</span> et aux points <span
class="math inline">\(x_i = \frac{P(x)}{Q(x)}\)</span>, pondérés par
<span class="math inline">\(Q(x)\)</span>. Nous obtenons : <span
class="math display">\[\log \left( \sum_{x \in \mathcal{X}} Q(x)
\frac{P(x)}{Q(x)} \right) \geq \sum_{x \in \mathcal{X}} Q(x) \log \left(
\frac{P(x)}{Q(x)} \right)\]</span> Simplifions le membre de gauche :
<span class="math display">\[\log \left( \sum_{x \in \mathcal{X}} P(x)
\right) = \log 1 = 0\]</span> Ainsi, nous avons : <span
class="math display">\[0 \geq \sum_{x \in \mathcal{X}} Q(x) \log \left(
\frac{P(x)}{Q(x)} \right)\]</span> En réarrangeant les termes, nous
obtenons : <span class="math display">\[\sum_{x \in \mathcal{X}} P(x)
\log \left( \frac{P(x)}{Q(x)} \right) \geq 0\]</span> Ce qui prouve
l’inégalité de Gibbs.</p>
<p>Pour montrer que l’égalité a lieu si et seulement si <span
class="math inline">\(P = Q\)</span>, supposons que <span
class="math inline">\(D(P \parallel Q) = 0\)</span>. Cela implique que :
<span class="math display">\[\sum_{x \in \mathcal{X}} P(x) \log \left(
\frac{P(x)}{Q(x)} \right) = 0\]</span> Puisque la fonction <span
class="math inline">\(\log\)</span> est strictement concave, l’égalité
dans l’inégalité de Jensen a lieu si et seulement si tous les points
<span class="math inline">\(x_i\)</span> sont égaux. Cela signifie que :
<span class="math display">\[\frac{P(x)}{Q(x)} = c \quad \forall x \in
\mathcal{X}\]</span> pour une constante <span class="math inline">\(c
&gt; 0\)</span>. En utilisant la condition de normalisation des
distributions de probabilité, nous avons : <span
class="math display">\[\sum_{x \in \mathcal{X}} P(x) = 1 = c \sum_{x \in
\mathcal{X}} Q(x) = c\]</span> Ainsi, <span class="math inline">\(c =
1\)</span> et donc <span class="math inline">\(P(x) = Q(x)\)</span> pour
tout <span class="math inline">\(x \in \mathcal{X}\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Kullback-Leibler possède plusieurs propriétés
intéressantes, que nous énumérons et démontrons ci-dessous.</p>
<ol>
<li><p><strong>Non-négativité</strong> : Comme nous l’avons vu dans le
théorème de Gibbs, <span class="math inline">\(D(P \parallel Q) \geq
0\)</span>.</p></li>
<li><p><strong>Invariance sous les transformations de
probabilité</strong> : La divergence de Kullback-Leibler est invariante
sous les transformations de probabilité. Cela signifie que pour toute
fonction bijective <span class="math inline">\(\phi : \mathcal{X}
\rightarrow \mathcal{Y}\)</span>, nous avons : <span
class="math display">\[D(P \parallel Q) = D(\phi_{\#} P \parallel
\phi_{\#} Q)\]</span> où <span class="math inline">\(\phi_{\#}
P\)</span> et <span class="math inline">\(\phi_{\#} Q\)</span> sont les
images de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> par la transformation <span
class="math inline">\(\phi\)</span>.</p></li>
<li><p><strong>Additivité</strong> : La divergence de Kullback-Leibler
est additive pour les produits de distributions. Cela signifie que si
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des produits de distributions
<span class="math inline">\(P_1, P_2\)</span> et <span
class="math inline">\(Q_1, Q_2\)</span>, respectivement, nous avons :
<span class="math display">\[D(P_1 \otimes P_2 \parallel Q_1 \otimes
Q_2) = D(P_1 \parallel Q_1) + D(P_2 \parallel Q_2)\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie relative, ou divergence de Kullback-Leibler, est une
notion centrale en théorie de l’information et en statistique. Elle
permet de quantifier la différence entre deux distributions de
probabilité, mesurant ainsi l’information perdue lorsqu’une distribution
est utilisée pour approximer une autre. Les propriétés et théorèmes
associés à cette divergence en font un outil puissant dans divers
domaines, tels que le traitement du signal et l’apprentissage
automatique.</p>
</body>
</html>
{% include "footer.html" %}


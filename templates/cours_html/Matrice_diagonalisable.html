{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Matrices Diagonalisables : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Matrices Diagonalisables : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les matrices diagonalisables occupent une place centrale en algèbre
linéaire, tant pour leur richesse théorique que pour leurs applications
pratiques. L’idée de diagonaliser une matrice remonte aux travaux
fondateurs de Cauchy et Jordan sur les formes quadratiques et les
équations différentielles linéaires. La diagonalisation permet de
simplifier l’étude des transformations linéaires en réduisant les
calculs à des opérations sur des matrices diagonales, bien plus
simples.</p>
<p>La notion de diagonalisabilité émerge naturellement lorsqu’on cherche
à résoudre des systèmes d’équations différentielles linéaires ou à
optimiser des formes quadratiques. Elle est indispensable dans l’analyse
des valeurs propres et vecteurs propres, outils fondamentaux pour
comprendre le comportement des systèmes dynamiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la notion de matrice diagonalisable, commençons par
rappeler ce qu’est une matrice diagonale. Une matrice <span
class="math inline">\(D\)</span> est dite diagonale si tous ses
coefficients hors de la diagonale principale sont nuls. Autrement dit,
<span class="math inline">\(D\)</span> est diagonale si et seulement si
<span class="math inline">\(D_{ij} = 0\)</span> pour tout <span
class="math inline">\(i \neq j\)</span>.</p>
<p>Nous cherchons maintenant à exprimer une matrice donnée <span
class="math inline">\(A\)</span> comme un produit de matrices
inversibles et d’une matrice diagonale. Cela nous amène à la définition
suivante :</p>
<div class="definition">
<p>Une matrice <span class="math inline">\(A \in
M_n(\mathbb{K})\)</span> est dite diagonalisable s’il existe une matrice
inversible <span class="math inline">\(P \in M_n(\mathbb{K})\)</span> et
une matrice diagonale <span class="math inline">\(D \in
M_n(\mathbb{K})\)</span> telles que : <span class="math display">\[A = P
D P^{-1}\]</span></p>
</div>
<p>Cette définition peut également s’énoncer en termes de similarité.
Deux matrices <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont dites semblables s’il existe une
matrice inversible <span class="math inline">\(P\)</span> telle que
<span class="math inline">\(B = P^{-1} A P\)</span>. Ainsi, une matrice
<span class="math inline">\(A\)</span> est diagonalisable si et
seulement s’il existe une matrice diagonale <span
class="math inline">\(D\)</span> semblable à <span
class="math inline">\(A\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour caractériser les matrices diagonalisables, nous avons besoin de
quelques résultats fondamentaux. Le premier théorème que nous allons
énoncer est dû à Cauchy et Jordan.</p>
<div class="theoreme">
<p>Une matrice <span class="math inline">\(A \in
M_n(\mathbb{K})\)</span> est diagonalisable si et seulement si l’espace
propre associé à chaque valeur propre de <span
class="math inline">\(A\)</span> est de dimension égale à la
multiplicité algébrique de cette valeur propre.</p>
</div>
<p>Pour mieux comprendre ce théorème, rappelons quelques définitions.
Une valeur propre <span class="math inline">\(\lambda\)</span> de <span
class="math inline">\(A\)</span> est un scalaire tel que : <span
class="math display">\[\det(A - \lambda I_n) = 0\]</span> L’espace
propre associé à <span class="math inline">\(\lambda\)</span> est
l’ensemble des vecteurs non nuls <span class="math inline">\(v\)</span>
tels que : <span class="math display">\[A v = \lambda v\]</span> La
multiplicité algébrique de <span class="math inline">\(\lambda\)</span>
est la multiplicité de <span class="math inline">\(\lambda\)</span>
comme racine du polynôme caractéristique de <span
class="math inline">\(A\)</span>.</p>
<p>Un autre théorème important est le suivant :</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(A\)</span> une matrice symétrique
réelle ou hermitienne complexe. Alors <span
class="math inline">\(A\)</span> est diagonalisable, et il existe une
base orthonormée de vecteurs propres pour <span
class="math inline">\(A\)</span>.</p>
</div>
<p>Ce théorème est particulièrement utile en analyse numérique, où les
matrices symétriques ou hermitiennes apparaissent fréquemment.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Cauchy-Jordan, nous avons besoin de
quelques lemmes préliminaires.</p>
<div class="lemme">
<p>Soit <span class="math inline">\(A\)</span> une matrice
diagonalisable. Alors le polynôme caractéristique de <span
class="math inline">\(A\)</span> se factorise en produit de facteurs
linéaires dans <span class="math inline">\(\mathbb{K}[X]\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(A\)</span> est
diagonalisable, alors il existe une matrice inversible <span
class="math inline">\(P\)</span> et une matrice diagonale <span
class="math inline">\(D\)</span> telles que : <span
class="math display">\[A = P D P^{-1}\]</span> Le polynôme
caractéristique de <span class="math inline">\(A\)</span> est donné par
: <span class="math display">\[p_A(X) = \det(X I_n - A) = \det(P (X I_n
- D) P^{-1}) = \det(X I_n - D)\]</span> Comme <span
class="math inline">\(D\)</span> est diagonale, disons avec des
coefficients diagonaux <span class="math inline">\(d_1, \ldots,
d_n\)</span>, alors : <span class="math display">\[p_A(X) =
\prod_{i=1}^n (X - d_i)\]</span> Ainsi, <span
class="math inline">\(p_A(X)\)</span> se factorise en produit de
facteurs linéaires. ◻</p>
</div>
<p>Nous pouvons maintenant prouver le théorème de Cauchy-Jordan.</p>
<div class="proof">
<p><em>Preuve du théorème de Cauchy-Jordan.</em> Supposons que <span
class="math inline">\(A\)</span> soit diagonalisable. Alors, d’après le
lemme précédent, le polynôme caractéristique de <span
class="math inline">\(A\)</span> se factorise en produit de facteurs
linéaires. Cela signifie que chaque valeur propre a une multiplicité
algébrique égale à la dimension de son espace propre.</p>
<p>Réciproquement, supposons que pour chaque valeur propre <span
class="math inline">\(\lambda\)</span> de <span
class="math inline">\(A\)</span>, la dimension de l’espace propre
associé à <span class="math inline">\(\lambda\)</span> est égale à la
multiplicité algébrique de <span class="math inline">\(\lambda\)</span>.
Alors, nous pouvons construire une base de <span
class="math inline">\(\mathbb{K}^n\)</span> formée de vecteurs propres
de <span class="math inline">\(A\)</span>. En prenant <span
class="math inline">\(P\)</span> comme la matrice dont les colonnes sont
ces vecteurs propres, et <span class="math inline">\(D\)</span> comme la
matrice diagonale dont les coefficients diagonaux sont les valeurs
propres correspondantes, nous avons : <span class="math display">\[A = P
D P^{-1}\]</span> Ainsi, <span class="math inline">\(A\)</span> est
diagonalisable. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous listons maintenant quelques propriétés importantes des matrices
diagonalisables.</p>
<ol>
<li><p>Si <span class="math inline">\(A\)</span> est une matrice
diagonalisable et <span class="math inline">\(B\)</span> est une matrice
semblable à <span class="math inline">\(A\)</span>, alors <span
class="math inline">\(B\)</span> est également diagonalisable.</p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(A\)</span> est
diagonalisable, alors il existe une matrice inversible <span
class="math inline">\(P\)</span> et une matrice diagonale <span
class="math inline">\(D\)</span> telles que : <span
class="math display">\[A = P D P^{-1}\]</span> Si <span
class="math inline">\(B\)</span> est semblable à <span
class="math inline">\(A\)</span>, alors il existe une matrice inversible
<span class="math inline">\(Q\)</span> telle que : <span
class="math display">\[B = Q^{-1} A Q\]</span> En substituant
l’expression de <span class="math inline">\(A\)</span>, nous obtenons :
<span class="math display">\[B = Q^{-1} P D P^{-1} Q\]</span> En prenant
<span class="math inline">\(R = P^{-1} Q\)</span>, nous avons : <span
class="math display">\[B = (Q^{-1} P) D (P^{-1} Q) = R D R^{-1}\]</span>
Ainsi, <span class="math inline">\(B\)</span> est diagonalisable. ◻</p>
</div></li>
<li><p>Si <span class="math inline">\(A\)</span> est une matrice
diagonalisable, alors toute puissance de <span
class="math inline">\(A\)</span> est également diagonalisable.</p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(A\)</span> est
diagonalisable, alors il existe une matrice inversible <span
class="math inline">\(P\)</span> et une matrice diagonale <span
class="math inline">\(D\)</span> telles que : <span
class="math display">\[A = P D P^{-1}\]</span> Alors, pour tout entier
<span class="math inline">\(k \geq 0\)</span>, nous avons : <span
class="math display">\[A^k = P D^k P^{-1}\]</span> Comme <span
class="math inline">\(D\)</span> est diagonale, <span
class="math inline">\(D^k\)</span> est également diagonale. Ainsi, <span
class="math inline">\(A^k\)</span> est diagonalisable. ◻</p>
</div></li>
<li><p>Si <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> sont deux matrices diagonalisables
commutant entre elles, alors <span class="math inline">\(A + B\)</span>
et <span class="math inline">\(AB\)</span> sont également
diagonalisables.</p>
<div class="proof">
<p><em>Proof.</em> Comme <span class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> commutent, elles partagent une base de
vecteurs propres commune. En effet, soit <span
class="math inline">\(\lambda\)</span> une valeur propre de <span
class="math inline">\(A\)</span>, et soit <span
class="math inline">\(v\)</span> un vecteur propre associé. Alors, nous
avons : <span class="math display">\[A(Bv) = B(Av) = B(\lambda v) =
\lambda (Bv)\]</span> Ainsi, <span class="math inline">\(Bv\)</span> est
un vecteur propre de <span class="math inline">\(A\)</span> associé à la
valeur propre <span class="math inline">\(\lambda\)</span>. Comme <span
class="math inline">\(B\)</span> est diagonalisable, <span
class="math inline">\(Bv\)</span> est une combinaison linéaire de
vecteurs propres de <span class="math inline">\(A\)</span>. En
choisissant une base de vecteurs propres pour <span
class="math inline">\(B\)</span> parmi les vecteurs propres de <span
class="math inline">\(A\)</span>, nous obtenons une base commune.</p>
<p>En utilisant cette base commune, nous pouvons diagonaliser <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>. Alors, <span class="math inline">\(A +
B\)</span> et <span class="math inline">\(AB\)</span> sont également
diagonalisables, car elles peuvent être exprimées comme des combinaisons
linéaires ou des produits de matrices diagonales. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Les matrices diagonalisables jouent un rôle crucial en algèbre
linéaire et dans de nombreuses applications. Leur étude permet de
simplifier considérablement l’analyse des transformations linéaires et
des systèmes dynamiques. Les théorèmes de Cauchy-Jordan et spectral
fournissent des outils puissants pour déterminer si une matrice est
diagonalisable et pour construire explicitement sa diagonalisation.</p>
<p>Les propriétés des matrices diagonalisables ouvrent également la voie
à de nombreuses généralisations et extensions, telles que les formes de
Jordan pour les matrices non diagonalisables. Ces concepts sont
essentiels pour comprendre la structure des algèbres de Lie, les
représentations des groupes finis et bien d’autres domaines avancés des
mathématiques.</p>
</body>
</html>
{% include "footer.html" %}


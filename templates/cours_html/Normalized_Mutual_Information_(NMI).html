{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Normalized Mutual Information: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Normalized Mutual Information: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The concept of Normalized Mutual Information (NMI) emerges from the
intersection of information theory and data clustering. In an era where
data is abundant yet often unstructured, the need for robust metrics to
evaluate clustering algorithms has become paramount. NMI addresses this
need by providing a measure that quantifies the similarity between two
clusterings, normalized to ensure comparability across different
datasets.</p>
<p>Mutual information (MI), a fundamental concept in information theory,
measures the amount of information obtained about one random variable
through another. However, MI is not normalized, making it less suitable
for comparing clusterings of different sizes or structures. NMI
overcomes this limitation by normalizing MI, thus providing a
scale-invariant measure that ranges between 0 and 1.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand NMI, we first need to grasp the concept of mutual
information. Suppose we have two random variables <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, each taking values in finite sets. The
mutual information <span class="math inline">\(I(X; Y)\)</span> is
defined as:</p>
<p><span class="math display">\[I(X; Y) = \sum_{x \in X} \sum_{y \in Y}
p(x, y) \log \left( \frac{p(x, y)}{p(x)p(y)} \right)\]</span></p>
<p>where <span class="math inline">\(p(x, y)\)</span> is the joint
probability distribution of <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span>, and <span
class="math inline">\(p(x)\)</span> and <span
class="math inline">\(p(y)\)</span> are the marginal probability
distributions of <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, respectively.</p>
<p>However, mutual information is not bounded, making it difficult to
interpret. To address this, we normalize mutual information. There are
several ways to normalize MI, but one common approach is:</p>
<p><span class="math display">\[NMI(X; Y) = \frac{I(X;
Y)}{\sqrt{H(X)H(Y)}}\]</span></p>
<p>where <span class="math inline">\(H(X)\)</span> and <span
class="math inline">\(H(Y)\)</span> are the entropies of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, respectively, defined as:</p>
<p><span class="math display">\[H(X) = -\sum_{x \in X} p(x) \log
p(x)\]</span></p>
<p><span class="math display">\[H(Y) = -\sum_{y \in Y} p(y) \log
p(y)\]</span></p>
<p>This normalization ensures that NMI ranges between 0 and 1, where 0
indicates no mutual information (independent variables) and 1 indicates
perfect correlation.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key theorems related to NMI is the Data Processing
Inequality, which states that for any three random variables <span
class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span>, and <span
class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[I(X; Y) \geq I(X; Z)\]</span></p>
<p>if <span class="math inline">\(Y\)</span> is a function of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Z\)</span> is a function of <span
class="math inline">\(Y\)</span>. This inequality implies that
processing data can only decrease or maintain the amount of mutual
information.</p>
<p>In the context of NMI, this theorem can be interpreted as follows: if
we have two clusterings <span class="math inline">\(C_1\)</span> and
<span class="math inline">\(C_2\)</span>, and <span
class="math inline">\(C_2\)</span> is a coarser version of <span
class="math inline">\(C_1\)</span>, then the NMI between <span
class="math inline">\(C_1\)</span> and <span
class="math inline">\(C_2\)</span> will be less than or equal to the NMI
between <span class="math inline">\(C_1\)</span> and itself.</p>
<h1 id="proofs">Proofs</h1>
<p>To prove the Data Processing Inequality, we start with the definition
of mutual information:</p>
<p><span class="math display">\[I(X; Y) = H(X) - H(X|Y)\]</span></p>
<p>where <span class="math inline">\(H(X|Y)\)</span> is the conditional
entropy of <span class="math inline">\(X\)</span> given <span
class="math inline">\(Y\)</span>. Similarly,</p>
<p><span class="math display">\[I(X; Z) = H(X) - H(X|Z)\]</span></p>
<p>Since <span class="math inline">\(Y\)</span> is a function of <span
class="math inline">\(X\)</span>, we have:</p>
<p><span class="math display">\[H(X|Y) = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[I(X; Y) = H(X)\]</span></p>
<p>On the other hand, since <span class="math inline">\(Z\)</span> is a
function of <span class="math inline">\(Y\)</span>, we have:</p>
<p><span class="math display">\[H(X|Z) \leq H(X|Y)\]</span></p>
<p>This is because conditioning reduces entropy. Therefore,</p>
<p><span class="math display">\[I(X; Z) = H(X) - H(X|Z) \leq H(X) -
H(X|Y) = I(X; Y)\]</span></p>
<p>This completes the proof.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>NMI possesses several important properties that make it a valuable
tool for evaluating clusterings:</p>
<ol>
<li><p><strong>Normalization</strong>: NMI is normalized to the range
[0, 1], making it scale-invariant and easier to interpret.</p></li>
<li><p><strong>Symmetry</strong>: NMI is symmetric, i.e., <span
class="math inline">\(NMI(X; Y) = NMI(Y; X)\)</span>.</p></li>
<li><p><strong>Boundary Conditions</strong>: <span
class="math inline">\(NMI(X; Y) = 0\)</span> if and only if <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are independent, and <span
class="math inline">\(NMI(X; Y) = 1\)</span> if and only if <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are perfectly correlated.</p></li>
</ol>
<p>These properties make NMI a robust and reliable metric for comparing
clusterings.</p>
</body>
</html>
{% include "footer.html" %}


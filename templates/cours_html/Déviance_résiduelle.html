{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Déviance résiduelle : Une analyse approfondie</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Déviance résiduelle : Une analyse approfondie</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La déviance résiduelle est un concept fondamental en statistique,
particulièrement dans le cadre de l’analyse des modèles de régression.
Elle émerge comme une mesure cruciale pour évaluer la qualité d’un
modèle en quantifiant l’écart entre les valeurs observées et celles
prédites par le modèle. Cette notion est indispensable pour comprendre
la performance d’un modèle, identifier les points aberrants et améliorer
la précision des prédictions.</p>
<p>Historiquement, l’étude de la déviance résiduelle trouve ses racines
dans les travaux pionniers sur la régression linéaire et l’analyse des
résidus. Elle permet de répondre à des questions essentielles : dans
quelle mesure un modèle capture-t-il la variabilité des données ? Quels
sont les points de données qui ne sont pas bien expliqués par le modèle
? Ces interrogations sont au cœur des motivations pour étudier la
déviance résiduelle.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre ce qu’est la déviance résiduelle, commençons par
considérer un modèle de régression. Supposons que nous ayons un ensemble
de données <span class="math inline">\((y_i, x_i)\)</span> où <span
class="math inline">\(y_i\)</span> est la variable dépendante et <span
class="math inline">\(x_i\)</span> est la variable indépendante. Un
modèle de régression cherche à prédire <span
class="math inline">\(y_i\)</span> en fonction de <span
class="math inline">\(x_i\)</span>. La déviance résiduelle mesure
l’écart entre les valeurs observées <span
class="math inline">\(y_i\)</span> et les valeurs prédites <span
class="math inline">\(\hat{y}_i\)</span>.</p>
<p>Formellement, la déviance résiduelle <span
class="math inline">\(D\)</span> pour un modèle de régression est
définie comme suit :</p>
<p><span class="math display">\[D = \sum_{i=1}^n (y_i -
\hat{y}_i)^2\]</span></p>
<p>où <span class="math inline">\(n\)</span> est le nombre de points de
données, <span class="math inline">\(y_i\)</span> est la valeur observée
et <span class="math inline">\(\hat{y}_i\)</span> est la valeur prédite
par le modèle.</p>
<p>Une autre manière de formuler cette définition est :</p>
<p><span class="math display">\[D = \sum_{i=1}^n (y_i -
f(x_i))^2\]</span></p>
<p>où <span class="math inline">\(f(x_i)\)</span> est la fonction de
régression qui prédit <span class="math inline">\(\hat{y}_i\)</span> en
fonction de <span class="math inline">\(x_i\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la déviance résiduelle est le théorème de
Gauss-Markov. Ce théorème établit les conditions sous lesquelles les
estimateurs des moindres carrés sont les meilleurs estimateurs linéaires
non biaisés.</p>
<div class="theorem">
<p>Soit un modèle linéaire général <span class="math inline">\(Y =
X\beta + \epsilon\)</span> où <span class="math inline">\(Y\)</span> est
un vecteur <span class="math inline">\(n \times 1\)</span> de variables
dépendantes, <span class="math inline">\(X\)</span> est une matrice
<span class="math inline">\(n \times p\)</span> de variables
indépendantes, <span class="math inline">\(\beta\)</span> est un vecteur
<span class="math inline">\(p \times 1\)</span> de coefficients et <span
class="math inline">\(\epsilon\)</span> est un vecteur <span
class="math inline">\(n \times 1\)</span> d’erreurs. Si les erreurs
<span class="math inline">\(\epsilon\)</span> sont non corrélées, ont
une variance constante et une espérance nulle, alors l’estimateur des
moindres carrés <span class="math inline">\(\hat{\beta} = (X^T X)^{-1}
X^T Y\)</span> est le meilleur estimateur linéaire non biaisé de <span
class="math inline">\(\beta\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Gauss-Markov, nous devons montrer que
l’estimateur des moindres carrés est non biaisé et qu’il a la plus
petite variance parmi tous les estimateurs linéaires non biaisés.</p>
<div class="proof">
<p><em>Proof.</em> 1. **Non-biais** : Montrons d’abord que l’estimateur
<span class="math inline">\(\hat{\beta}\)</span> est non biaisé.</p>
<p><span class="math display">\[E[\hat{\beta}] = E[(X^T X)^{-1} X^T Y] =
(X^T X)^{-1} X^T E[Y] = (X^T X)^{-1} X^T X \beta = \beta\]</span></p>
<p>2. **Variance minimale** : Ensuite, montrons que <span
class="math inline">\(\hat{\beta}\)</span> a la plus petite variance
parmi tous les estimateurs linéaires non biaisés.</p>
<p>Soit <span class="math inline">\(a\)</span> un vecteur tel que <span
class="math inline">\(E[a^T Y] = \beta\)</span>. Nous voulons minimiser
la variance de <span class="math inline">\(a^T Y\)</span>.</p>
<p><span class="math display">\[\text{Var}(a^T Y) = a^T \text{Var}(Y) a
= a^T X \text{Var}(\epsilon) X^T a\]</span></p>
<p>En utilisant la contrainte <span class="math inline">\(E[a^T Y] =
\beta\)</span>, nous pouvons montrer que l’estimateur des moindres
carrés minimise cette variance. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La déviance résiduelle possède plusieurs propriétés importantes :</p>
<ol>
<li><p>**Additivité** : La déviance résiduelle est additive. Si nous
avons deux modèles <span class="math inline">\(M_1\)</span> et <span
class="math inline">\(M_2\)</span>, la déviance résiduelle du modèle
combiné est la somme des déviances résiduelles des modèles
individuels.</p>
<p><span class="math display">\[D_{M_1 + M_2} = D_{M_1} +
D_{M_2}\]</span></p></li>
<li><p>**Non-négativité** : La déviance résiduelle est toujours non
négative. Cela découle du fait que les carrés des différences sont
toujours non négatifs.</p>
<p><span class="math display">\[D \geq 0\]</span></p></li>
<li><p>**Minimisation** : La déviance résiduelle est minimisée par le
modèle de régression qui passe exactement par tous les points de
données. Cependant, un tel modèle peut être surajusté et ne pas
généraliser bien aux nouvelles données.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La déviance résiduelle est un outil essentiel pour évaluer la qualité
des modèles de régression. Elle permet de quantifier l’écart entre les
valeurs observées et celles prédites, d’identifier les points aberrants
et d’améliorer la précision des modèles. En comprenant et en utilisant
la déviance résiduelle, les statisticiens peuvent développer des modèles
plus robustes et plus précis.</p>
</body>
</html>
{% include "footer.html" %}


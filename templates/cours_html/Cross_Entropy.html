{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie Croisée : Une Mesure Fondamentale en Apprentissage Automatique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie Croisée : Une Mesure Fondamentale en
Apprentissage Automatique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie croisée, ou <em>cross entropy</em> en anglais, est une
notion centrale en théorie de l’information et en apprentissage
automatique. Son origine remonte aux travaux de Claude Shannon dans les
années 1940, où il a introduit l’entropie comme mesure de l’incertitude
d’une distribution de probabilité. L’entropie croisée généralise cette
idée en mesurant la divergence entre deux distributions de
probabilité.</p>
<p>En apprentissage automatique, l’entropie croisée est largement
utilisée comme fonction de perte pour les modèles de classification.
Elle permet d’évaluer la performance d’un modèle en comparant ses
prédictions aux étiquettes réelles. Son utilisation est particulièrement
répandue dans les réseaux de neurones, où elle guide l’optimisation des
paramètres du modèle.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie croisée, commençons par rappeler quelques
concepts fondamentaux. Supposons que nous ayons deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un ensemble fini <span
class="math inline">\(X\)</span>. Nous cherchons à mesurer la différence
entre ces deux distributions.</p>
<p>L’entropie croisée <span class="math inline">\(H(P, Q)\)</span> est
définie comme la moyenne des logits des probabilités de <span
class="math inline">\(P\)</span> sous <span
class="math inline">\(Q\)</span>. En d’autres termes, c’est la quantité
d’information attendue nécessaire pour identifier un événement tiré de
<span class="math inline">\(P\)</span> en utilisant les probabilités
fournies par <span class="math inline">\(Q\)</span>.</p>
<p>Formellement, l’entropie croisée est donnée par :</p>
<p><span class="math display">\[H(P, Q) = -\sum_{x \in X} P(x) \log
Q(x)\]</span></p>
<p>où <span class="math inline">\(P(x)\)</span> et <span
class="math inline">\(Q(x)\)</span> sont les probabilités des événements
<span class="math inline">\(x\)</span> sous les distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> respectivement, et où <span
class="math inline">\(\log\)</span> désigne le logarithme naturel.</p>
<p>Une autre formulation équivalente de l’entropie croisée est :</p>
<p><span class="math display">\[H(P, Q) = -\mathbb{E}_{P} \left[ \log
Q(X) \right]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}_{P}\)</span> désigne
l’espérance prise selon la distribution <span
class="math inline">\(P\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’entropie croisée est le théorème de
la divergence de Kullback-Leibler. Ce théorème établit une relation
entre l’entropie croisée et la divergence de Kullback-Leibler, qui est
une mesure de la distance entre deux distributions de probabilité.</p>
<p>Le théorème de la divergence de Kullback-Leibler stipule que :</p>
<p><span class="math display">\[D_{KL}(P \parallel Q) = H(P, Q) -
H(P)\]</span></p>
<p>où <span class="math inline">\(D_{KL}(P \parallel Q)\)</span> est la
divergence de Kullback-Leibler entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, et <span
class="math inline">\(H(P)\)</span> est l’entropie de la distribution
<span class="math inline">\(P\)</span>.</p>
<p>En d’autres termes, la divergence de Kullback-Leibler est égale à
l’entropie croisée entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> moins l’entropie de <span
class="math inline">\(P\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la divergence de Kullback-Leibler,
commençons par rappeler les définitions des quantités impliquées.
L’entropie de <span class="math inline">\(P\)</span> est donnée par
:</p>
<p><span class="math display">\[H(P) = -\sum_{x \in X} P(x) \log
P(x)\]</span></p>
<p>L’entropie croisée <span class="math inline">\(H(P, Q)\)</span> est
définie comme :</p>
<p><span class="math display">\[H(P, Q) = -\sum_{x \in X} P(x) \log
Q(x)\]</span></p>
<p>La divergence de Kullback-Leibler <span
class="math inline">\(D_{KL}(P \parallel Q)\)</span> est définie comme
:</p>
<p><span class="math display">\[D_{KL}(P \parallel Q) = \sum_{x \in X}
P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</span></p>
<p>Nous pouvons maintenant prouver le théorème en utilisant ces
définitions. En effet, nous avons :</p>
<p><span class="math display">\[\begin{aligned}
D_{KL}(P \parallel Q) &amp;= \sum_{x \in X} P(x) \log \left(
\frac{P(x)}{Q(x)} \right) \\
&amp;= \sum_{x \in X} P(x) \left( \log P(x) - \log Q(x) \right) \\
&amp;= \sum_{x \in X} P(x) \log P(x) - \sum_{x \in X} P(x) \log Q(x) \\
&amp;= -\sum_{x \in X} P(x) \log P(x) + \left( -\sum_{x \in X} P(x) \log
Q(x) \right) \\
&amp;= H(P) + H(P, Q)
\end{aligned}\]</span></p>
<p>En réarrangeant les termes, nous obtenons :</p>
<p><span class="math display">\[D_{KL}(P \parallel Q) = H(P, Q) -
H(P)\]</span></p>
<p>ce qui achève la preuve du théorème.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie croisée possède plusieurs propriétés importantes qui en
font un outil puissant en apprentissage automatique. Nous en listons
quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Non-négativité</strong> : L’entropie croisée est toujours
non négative, c’est-à-dire que <span class="math inline">\(H(P, Q) \geq
0\)</span>. De plus, <span class="math inline">\(H(P, Q) = 0\)</span> si
et seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p><strong>Invariance par renormalisation</strong> : L’entropie
croisée est invariante par renormalisation des distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>. En d’autres termes, si <span
class="math inline">\(P&#39;\)</span> et <span
class="math inline">\(Q&#39;\)</span> sont des renormalisations de <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, alors <span
class="math inline">\(H(P&#39;, Q&#39;) = H(P, Q)\)</span>.</p></li>
<li><p><strong>Convexité</strong> : L’entropie croisée est une fonction
convexe de <span class="math inline">\(Q\)</span> pour <span
class="math inline">\(P\)</span> fixé. Cela signifie que pour toute
combinaison convexe de distributions <span
class="math inline">\(Q_1\)</span> et <span
class="math inline">\(Q_2\)</span>, nous avons :</p>
<p><span class="math display">\[H(P, \lambda Q_1 + (1 - \lambda) Q_2)
\leq \lambda H(P, Q_1) + (1 - \lambda) H(P, Q_2)\]</span></p>
<p>pour tout <span class="math inline">\(\lambda \in [0,
1]\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie croisée est une mesure fondamentale en théorie de
l’information et en apprentissage automatique. Elle permet de quantifier
la divergence entre deux distributions de probabilité et est largement
utilisée comme fonction de perte dans les modèles de classification. Ses
propriétés mathématiques, telles que la non-négativité et la convexité,
en font un outil puissant pour l’optimisation des modèles.</p>
<p>En conclusion, l’entropie croisée est une notion essentielle pour
comprendre et développer des algorithmes d’apprentissage automatique
performants. Son utilisation continue de croître dans divers domaines,
notamment en traitement du langage naturel et en vision par
ordinateur.</p>
</body>
</html>
{% include "footer.html" %}


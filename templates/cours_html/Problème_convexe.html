{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Le Problème Convexe : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Le Problème Convexe : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le problème convexe constitue un pilier fondamental en optimisation
mathématique, avec des applications transversales s’étendant de
l’ingénierie à la finance, en passant par les sciences des données. Son
émergence historique remonte aux travaux pionniers de Joseph-Louis
Lagrange au XVIIIème siècle, avec l’introduction des multiplicateurs de
Lagrange pour résoudre les problèmes d’optimisation sous contraintes. La
convexité, propriété géométrique et algébrique, confère une structure
particulière aux fonctions et ensembles qui la possèdent, permettant de
garantir l’existence et l’unicité des solutions optimales.</p>
<p>Ce cadre théorique est indispensable pour modéliser des problèmes
réels où les contraintes et objectifs présentent une nature
intrinsèquement convexe. Par exemple, en traitement du signal, la
reconstruction d’images par tomographie exploite des formulations
convexes pour garantir une convergence rapide vers des solutions
stables. De même, en apprentissage automatique, les méthodes de
régularisation L1 (Lasso) reposent sur la convexité pour sélectionner
des modèles parcimonieux.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour aborder le problème convexe, commençons par définir les concepts
clés. Considérons un ensemble <span class="math inline">\(S \subseteq
\mathbb{R}^n\)</span>. Intuitivement, <span
class="math inline">\(S\)</span> est convexe si, pour tout couple de
points <span class="math inline">\(x, y \in S\)</span>, le segment de
ligne reliant <span class="math inline">\(x\)</span> à <span
class="math inline">\(y\)</span> est entièrement contenu dans <span
class="math inline">\(S\)</span>. Cette propriété peut être formalisée
comme suit :</p>
<div class="definition">
<p>Un ensemble <span class="math inline">\(S \subseteq
\mathbb{R}^n\)</span> est dit convexe si et seulement si : <span
class="math display">\[\forall x, y \in S, \forall \lambda \in [0, 1],
\quad \lambda x + (1 - \lambda) y \in S.\]</span></p>
</div>
<p>De manière équivalente, <span class="math inline">\(S\)</span> est
convexe si son enveloppe convexe coïncide avec lui-même. L’enveloppe
convexe de <span class="math inline">\(S\)</span>, notée <span
class="math inline">\(\text{conv}(S)\)</span>, est le plus petit
ensemble convexe contenant <span class="math inline">\(S\)</span>.</p>
<p>Passons maintenant à la notion de fonction convexe. Une fonction
<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> est convexe si son épigraphe, c’est-à-dire
l’ensemble <span class="math inline">\(\{(x, t) \in \mathbb{R}^n \times
\mathbb{R} \mid f(x) \leq t\}\)</span>, est un ensemble convexe. Cette
définition conduit à la formulation suivante :</p>
<div class="definition">
<p>Une fonction <span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> est convexe si et seulement si : <span
class="math display">\[\forall x, y \in \mathbb{R}^n, \forall \lambda
\in [0, 1], \quad f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1
- \lambda) f(y).\]</span></p>
</div>
<p>Cette inégalité est connue sous le nom d’inégalité de Jensen. Une
fonction <span class="math inline">\(f\)</span> est strictement convexe
si l’inégalité est stricte pour tout <span class="math inline">\(x \neq
y\)</span> et <span class="math inline">\(\lambda \in (0,
1)\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un résultat central en optimisation convexe est le théorème de
séparation des ensembles convexes. Ce théorème affirme qu’il est
possible de séparer deux ensembles convexes disjoints par un hyperplan.
Formellement, nous avons :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(C_1\)</span> et <span
class="math inline">\(C_2\)</span> deux ensembles convexes de <span
class="math inline">\(\mathbb{R}^n\)</span> tels que <span
class="math inline">\(C_1 \cap C_2 = \emptyset\)</span>. Alors, il
existe un vecteur <span class="math inline">\(a \in
\mathbb{R}^n\)</span> et un scalaire <span class="math inline">\(b \in
\mathbb{R}\)</span> tels que : <span class="math display">\[\forall x
\in C_1, \quad a^T x \leq b,\]</span> <span
class="math display">\[\forall y \in C_2, \quad a^T y \geq
b.\]</span></p>
</div>
<p>Un autre théorème fondamental est le théorème de dualité en
optimisation convexe. Considérons un problème d’optimisation sous
contraintes : <span class="math display">\[\min_{x \in \mathbb{R}^n}
f(x) \quad \text{subject to} \quad g_i(x) \leq 0, \quad i = 1, \ldots,
m.\]</span> Le problème dual associé est : <span
class="math display">\[\max_{\lambda \in \mathbb{R}^m_+} \min_{x \in
\mathbb{R}^n} \left( f(x) + \sum_{i=1}^m \lambda_i g_i(x)
\right).\]</span> Le théorème de dualité de Fenchel-Rockafellar établit
une relation entre la valeur optimale du problème primal et celle du
problème dual.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de séparation des ensembles convexes, nous
procédons comme suit. Supposons que <span
class="math inline">\(C_1\)</span> et <span
class="math inline">\(C_2\)</span> soient deux ensembles convexes
disjoints. Par translation, nous pouvons supposer que <span
class="math inline">\(0 \in C_1\)</span>. Soit <span
class="math inline">\(d\)</span> la distance entre l’origine et <span
class="math inline">\(C_2\)</span>, c’est-à-dire : <span
class="math display">\[d = \inf_{y \in C_2} \|y\|.\]</span> Puisque
<span class="math inline">\(C_1\)</span> et <span
class="math inline">\(C_2\)</span> sont disjoints, <span
class="math inline">\(d &gt; 0\)</span>. Soit <span
class="math inline">\(y_0 \in C_2\)</span> tel que <span
class="math inline">\(\|y_0\| = d\)</span>. Pour tout <span
class="math inline">\(x \in C_1\)</span>, nous avons : <span
class="math display">\[x^T y_0 \leq 0.\]</span> En effet, si <span
class="math inline">\(x^T y_0 &gt; 0\)</span>, alors pour tout <span
class="math inline">\(\lambda &gt; 0\)</span>, le point <span
class="math inline">\(\lambda x + y_0\)</span> appartient à <span
class="math inline">\(C_2\)</span> car <span
class="math inline">\(C_2\)</span> est convexe. Cependant, pour <span
class="math inline">\(\lambda\)</span> suffisamment grand, <span
class="math inline">\(\|\lambda x + y_0\| &lt; d\)</span>, ce qui
contredit la définition de <span class="math inline">\(d\)</span>.</p>
<p>Ainsi, en posant <span class="math inline">\(a = y_0\)</span> et
<span class="math inline">\(b = 0\)</span>, nous obtenons : <span
class="math display">\[\forall x \in C_1, \quad a^T x \leq b,\]</span>
<span class="math display">\[\forall y \in C_2, \quad a^T y \geq
b.\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les problèmes convexes possèdent de nombreuses propriétés
remarquables, que nous énumérons ci-dessous :</p>
<ol>
<li><p>Tout problème d’optimisation convexe admet au plus une solution
globale. Si la fonction objectif est strictement convexe, alors la
solution est unique.</p></li>
<li><p>Les méthodes de gradient et de point proximal convergent vers une
solution optimale pour les problèmes convexes, sous des conditions
appropriées.</p></li>
<li><p>La somme de fonctions convexes est convexe. De même, la
composition d’une fonction convexe avec une fonction affine est
convexe.</p></li>
</ol>
<p>Pour illustrer la propriété (iii), considérons deux fonctions
convexes <span class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span>. Pour tout <span
class="math inline">\(x, y \in \mathbb{R}^n\)</span> et <span
class="math inline">\(\lambda \in [0, 1]\)</span>, nous avons : <span
class="math display">\[(f + g)(\lambda x + (1 - \lambda) y) = f(\lambda
x + (1 - \lambda) y) + g(\lambda x + (1 - \lambda) y).\]</span> En
utilisant l’inégalité de Jensen pour <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span>, nous obtenons : <span
class="math display">\[f(\lambda x + (1 - \lambda) y) \leq \lambda f(x)
+ (1 - \lambda) f(y),\]</span> <span class="math display">\[g(\lambda x
+ (1 - \lambda) y) \leq \lambda g(x) + (1 - \lambda) g(y).\]</span> En
additionnant ces deux inégalités, nous concluons que <span
class="math inline">\(f + g\)</span> est convexe.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de TF-IDF Cosine : Une mesure d’importance dans les corpus textuels</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de TF-IDF Cosine : Une mesure d’importance
dans les corpus textuels</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’analyse des textes est un domaine riche et complexe, où la
quantification de l’importance des mots dans un corpus joue un rôle
central. Parmi les nombreuses approches développées, la distance de
TF-IDF Cosine se distingue par son efficacité et sa simplicité
conceptuelle. Cette mesure combine deux notions fondamentales : le
TF-IDF (Term Frequency-Inverse Document Frequency) et la similarité
cosinus. Le TF-IDF permet de pondérer l’importance d’un mot dans un
document par rapport à son occurrence dans l’ensemble du corpus, tandis
que la similarité cosinus mesure l’angle entre deux vecteurs
représentant des documents. L’émergence de cette notion répond à un
besoin crucial : quantifier la pertinence des mots dans un contexte
donné, permettant ainsi de comparer et classer les documents de manière
automatisée. Cette mesure est indispensable dans des applications telles
que la recherche d’information, l’analyse de sentiments et la
recommandation de contenus.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la distance de TF-IDF Cosine, il est essentiel de
définir d’abord les concepts de base qui la composent.</p>
<h2 id="term-frequency-tf">Term Frequency (TF)</h2>
<p>Considérons un corpus de documents <span class="math inline">\(D =
\{d_1, d_2, \dots, d_n\}\)</span> et un terme <span
class="math inline">\(t\)</span> dans le vocabulaire <span
class="math inline">\(V\)</span>. Le Term Frequency (TF) d’un terme
<span class="math inline">\(t\)</span> dans un document <span
class="math inline">\(d_i\)</span> est une mesure de la fréquence
relative de ce terme dans le document. Intuitivement, nous cherchons à
savoir combien de fois un mot apparaît par rapport à la longueur totale
du document.</p>
<p>Formellement, le TF d’un terme <span class="math inline">\(t\)</span>
dans un document <span class="math inline">\(d_i\)</span> est défini
comme : <span class="math display">\[\text{TF}(t, d_i) =
\frac{\text{nombre d&#39;occurrences de } t \text{ dans }
d_i}{\text{nombre total de termes dans } d_i}\]</span></p>
<h2 id="inverse-document-frequency-idf">Inverse Document Frequency
(IDF)</h2>
<p>Le Term Frequency seul ne suffit pas à capturer l’importance d’un
terme dans un corpus. Par exemple, des mots comme "le", "la" ou "de"
apparaissent fréquemment dans tous les documents et ne sont pas très
informatifs. L’Inverse Document Frequency (IDF) permet de réduire
l’importance des termes qui apparaissent dans de nombreux documents.</p>
<p>Formellement, l’IDF d’un terme <span class="math inline">\(t\)</span>
est défini comme : <span class="math display">\[\text{IDF}(t, D) =
\log\left(\frac{n}{\text{nombre de documents contenant }
t}\right)\]</span> où <span class="math inline">\(n\)</span> est le
nombre total de documents dans le corpus.</p>
<h2 id="tf-idf">TF-IDF</h2>
<p>Le TF-IDF combine les deux notions précédentes pour pondérer
l’importance d’un terme dans un document par rapport à son occurrence
dans le corpus. Intuitivement, nous cherchons une mesure qui soit élevée
pour les termes fréquents dans un document mais rares dans le
corpus.</p>
<p>Formellement, le TF-IDF d’un terme <span
class="math inline">\(t\)</span> dans un document <span
class="math inline">\(d_i\)</span> est défini comme : <span
class="math display">\[\text{TF-IDF}(t, d_i, D) = \text{TF}(t, d_i)
\times \text{IDF}(t, D)\]</span></p>
<h2 id="similarité-cosinus">Similarité Cosinus</h2>
<p>La similarité cosinus est une mesure de la similarité entre deux
vecteurs. Dans le contexte des documents textuels, chaque document peut
être représenté par un vecteur dans un espace de dimensions égales au
nombre de termes dans le vocabulaire. La similarité cosinus entre deux
documents <span class="math inline">\(d_i\)</span> et <span
class="math inline">\(d_j\)</span> est définie comme le cosinus de
l’angle entre leurs vecteurs de représentation.</p>
<p>Formellement, la similarité cosinus entre deux documents <span
class="math inline">\(d_i\)</span> et <span
class="math inline">\(d_j\)</span> est définie comme : <span
class="math display">\[\text{similarité}(d_i, d_j) = \frac{\sum_{t \in
V} \text{TF-IDF}(t, d_i) \times \text{TF-IDF}(t, d_j)}{\sqrt{\sum_{t \in
V} (\text{TF-IDF}(t, d_i))^2} \times \sqrt{\sum_{t \in V}
(\text{TF-IDF}(t, d_j))^2}}\]</span></p>
<h2 id="distance-de-tf-idf-cosine">Distance de TF-IDF Cosine</h2>
<p>La distance de TF-IDF Cosine est une mesure de dissimilarité entre
deux documents, basée sur la similarité cosinus. Intuitivement, nous
cherchons à quantifier à quel point deux documents sont différents en
utilisant la distance angulaire entre leurs vecteurs de représentation
TF-IDF.</p>
<p>Formellement, la distance de TF-IDF Cosine entre deux documents <span
class="math inline">\(d_i\)</span> et <span
class="math inline">\(d_j\)</span> est définie comme : <span
class="math display">\[\text{distance}(d_i, d_j) = 1 -
\text{similarité}(d_i, d_j)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-la-similarité-cosinus">Théorème de la Similarité
Cosinus</h2>
<p>Le théorème de la similarité cosinus stipule que la similarité entre
deux documents est maximale lorsque leurs vecteurs de représentation
sont identiques et minimale lorsqu’ils sont orthogonaux.</p>
<p>Formellement, pour tout <span class="math inline">\(d_i, d_j \in
D\)</span>, nous avons : <span class="math display">\[0 \leq
\text{similarité}(d_i, d_j) \leq 1\]</span></p>
<h2 id="démonstration">Démonstration</h2>
<p>La démonstration de ce théorème repose sur les propriétés du produit
scalaire et de la norme des vecteurs. Considérons deux vecteurs <span
class="math inline">\(\mathbf{v}_i\)</span> et <span
class="math inline">\(\mathbf{v}_j\)</span> représentant les documents
<span class="math inline">\(d_i\)</span> et <span
class="math inline">\(d_j\)</span> dans l’espace de représentation
TF-IDF.</p>
<p>Le produit scalaire des vecteurs est défini comme : <span
class="math display">\[\mathbf{v}_i \cdot \mathbf{v}_j = \sum_{t \in V}
\text{TF-IDF}(t, d_i) \times \text{TF-IDF}(t, d_j)\]</span></p>
<p>La norme des vecteurs est définie comme : <span
class="math display">\[\|\mathbf{v}_i\| = \sqrt{\sum_{t \in V}
(\text{TF-IDF}(t, d_i))^2}\]</span> <span
class="math display">\[\|\mathbf{v}_j\| = \sqrt{\sum_{t \in V}
(\text{TF-IDF}(t, d_j))^2}\]</span></p>
<p>La similarité cosinus est alors définie comme : <span
class="math display">\[\text{similarité}(d_i, d_j) = \frac{\mathbf{v}_i
\cdot \mathbf{v}_j}{\|\mathbf{v}_i\| \times
\|\mathbf{v}_j\|}\]</span></p>
<p>Par les propriétés du produit scalaire et de la norme, nous savons
que : <span class="math display">\[-1 \leq \frac{\mathbf{v}_i \cdot
\mathbf{v}_j}{\|\mathbf{v}_i\| \times \|\mathbf{v}_j\|} \leq
1\]</span></p>
<p>Cependant, comme les valeurs TF-IDF sont toujours non négatives, le
produit scalaire est également non négatif. Par conséquent, nous avons :
<span class="math display">\[0 \leq \text{similarité}(d_i, d_j) \leq
1\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-de-la-distance-de-tf-idf-cosine">Preuve de la Distance de
TF-IDF Cosine</h2>
<p>La distance de TF-IDF Cosine est définie comme <span
class="math inline">\(1 - \text{similarité}(d_i, d_j)\)</span>. Pour
prouver que cette distance est bien une mesure de dissimilarité valide,
nous devons montrer qu’elle satisfait les propriétés suivantes :</p>
<ol>
<li><p><span class="math inline">\(0 \leq \text{distance}(d_i, d_j) \leq
1\)</span></p></li>
<li><p><span class="math inline">\(\text{distance}(d_i, d_j) =
0\)</span> si et seulement si <span class="math inline">\(d_i =
d_j\)</span></p></li>
<li><p><span class="math inline">\(\text{distance}(d_i, d_j) =
\text{distance}(d_j, d_i)\)</span></p></li>
<li><p><span class="math inline">\(\text{distance}(d_i, d_k) \leq
\text{distance}(d_i, d_j) + \text{distance}(d_j, d_k)\)</span></p></li>
</ol>
<h2 id="preuve-de-la-propriété-1">Preuve de la Propriété 1</h2>
<p>Par le théorème de la similarité cosinus, nous savons que <span
class="math inline">\(0 \leq \text{similarité}(d_i, d_j) \leq
1\)</span>. Par conséquent : <span class="math display">\[0 \leq 1 -
\text{similarité}(d_i, d_j) \leq 1\]</span></p>
<h2 id="preuve-de-la-propriété-2">Preuve de la Propriété 2</h2>
<p>Si <span class="math inline">\(d_i = d_j\)</span>, alors leurs
vecteurs de représentation sont identiques, et la similarité cosinus est
égale à 1. Par conséquent : <span
class="math display">\[\text{distance}(d_i, d_j) = 1 -
\text{similarité}(d_i, d_j) = 0\]</span></p>
<p>Réciproquement, si <span class="math inline">\(\text{distance}(d_i,
d_j) = 0\)</span>, alors <span
class="math inline">\(\text{similarité}(d_i, d_j) = 1\)</span>. Cela
signifie que les vecteurs de représentation sont identiques, et donc
<span class="math inline">\(d_i = d_j\)</span>.</p>
<h2 id="preuve-de-la-propriété-3">Preuve de la Propriété 3</h2>
<p>La similarité cosinus est symétrique, c’est-à-dire que <span
class="math inline">\(\text{similarité}(d_i, d_j) =
\text{similarité}(d_j, d_i)\)</span>. Par conséquent : <span
class="math display">\[\text{distance}(d_i, d_j) = 1 -
\text{similarité}(d_i, d_j) = 1 - \text{similarité}(d_j, d_i) =
\text{distance}(d_j, d_i)\]</span></p>
<h2 id="preuve-de-la-propriété-4">Preuve de la Propriété 4</h2>
<p>La similarité cosinus satisfait l’inégalité triangulaire : <span
class="math display">\[\text{similarité}(d_i, d_k) \geq
\text{similarité}(d_i, d_j) - \text{similarité}(d_j, d_k)\]</span></p>
<p>Par conséquent : <span class="math display">\[1 -
\text{similarité}(d_i, d_k) \leq 1 - (\text{similarité}(d_i, d_j) -
\text{similarité}(d_j, d_k)) = (1 - \text{similarité}(d_i, d_j)) + (1 -
\text{similarité}(d_j, d_k))\]</span></p>
<p>Cependant, cette inégalité n’est pas directement applicable pour
prouver la propriété 4. Une approche alternative consiste à utiliser les
propriétés de l’espace métrique, mais cela dépasse le cadre de cet
article.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-1-normalisation-des-vecteurs">Propriété 1 :
Normalisation des Vecteurs</h2>
<p>Si les vecteurs de représentation TF-IDF sont normalisés,
c’est-à-dire que <span class="math inline">\(\|\mathbf{v}_i\| =
1\)</span> pour tout <span class="math inline">\(d_i \in D\)</span>,
alors la similarité cosinus se réduit au produit scalaire des
vecteurs.</p>
<p>Formellement : <span class="math display">\[\text{similarité}(d_i,
d_j) = \mathbf{v}_i \cdot \mathbf{v}_j\]</span></p>
<h2 id="démonstration-1">Démonstration</h2>
<p>Par hypothèse, <span class="math inline">\(\|\mathbf{v}_i\| =
1\)</span> et <span class="math inline">\(\|\mathbf{v}_j\| = 1\)</span>.
Par conséquent : <span class="math display">\[\text{similarité}(d_i,
d_j) = \frac{\mathbf{v}_i \cdot \mathbf{v}_j}{\|\mathbf{v}_i\| \times
\|\mathbf{v}_j\|} = \frac{\mathbf{v}_i \cdot \mathbf{v}_j}{1 \times 1} =
\mathbf{v}_i \cdot \mathbf{v}_j\]</span></p>
<h2 id="propriété-2-invariance-par-scaling">Propriété 2 : Invariance par
Scaling</h2>
<p>La similarité cosinus est invariante par scaling des vecteurs de
représentation. Cela signifie que si les vecteurs sont multipliés par
une constante positive, la similarité reste inchangée.</p>
<p>Formellement, pour tout <span class="math inline">\(\alpha &gt;
0\)</span>, nous avons : <span
class="math display">\[\text{similarité}(\alpha \mathbf{v}_i, \alpha
\mathbf{v}_j) = \text{similarité}(\mathbf{v}_i,
\mathbf{v}_j)\]</span></p>
<h2 id="démonstration-2">Démonstration</h2>
<p>Considérons deux vecteurs <span class="math inline">\(\alpha
\mathbf{v}_i\)</span> et <span class="math inline">\(\alpha
\mathbf{v}_j\)</span>. Le produit scalaire est : <span
class="math display">\[\alpha \mathbf{v}_i \cdot \alpha \mathbf{v}_j =
\alpha^2 (\mathbf{v}_i \cdot \mathbf{v}_j)\]</span></p>
<p>La norme des vecteurs est : <span class="math display">\[\|\alpha
\mathbf{v}_i\| = \alpha \|\mathbf{v}_i\|\]</span> <span
class="math display">\[\|\alpha \mathbf{v}_j\| = \alpha
\|\mathbf{v}_j\|\]</span></p>
<p>Par conséquent, la similarité cosinus est : <span
class="math display">\[\text{similarité}(\alpha \mathbf{v}_i, \alpha
\mathbf{v}_j) = \frac{\alpha^2 (\mathbf{v}_i \cdot \mathbf{v}_j)}{\alpha
\|\mathbf{v}_i\| \times \alpha \|\mathbf{v}_j\|} = \frac{\mathbf{v}_i
\cdot \mathbf{v}_j}{\|\mathbf{v}_i\| \times \|\mathbf{v}_j\|} =
\text{similarité}(\mathbf{v}_i, \mathbf{v}_j)\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de TF-IDF Cosine est une mesure puissante et efficace
pour quantifier la dissimilarité entre des documents textuels. En
combinant les notions de TF-IDF et de similarité cosinus, cette mesure
permet de capturer l’importance des termes dans un contexte donné et de
comparer les documents de manière automatisée. Les propriétés et
théorèmes associés à cette distance en font un outil indispensable dans
le domaine de l’analyse des textes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Contrastive Loss: Une Exploration Mathématique et Conceptuelle</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Contrastive Loss: Une Exploration Mathématique et
Conceptuelle</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’apprentissage automatique, en particulier dans le domaine de
l’apprentissage profond, a connu une croissance exponentielle ces
dernières années. Parmi les nombreuses techniques développées pour
améliorer la performance des modèles, la <em>Contrastive Loss</em> se
distingue par son efficacité dans l’apprentissage de représentations
discriminatives. Originaire des travaux sur les réseaux de neurones
auto-encodeurs et les modèles de reconnaissance d’objets, cette fonction
de perte a été conçue pour résoudre le problème de l’apprentissage de
similitudes et de dissemblances entre paires d’échantillons.</p>
<p>La notion de <em>Contrastive Loss</em> émerge comme une réponse
élégante à la nécessité d’apprendre des représentations où les paires
similaires sont proches dans l’espace de caractéristiques, tandis que
les paires dissemblables sont éloignées. Ce cadre est indispensable dans
des applications telles que la reconnaissance faciale, la recherche
d’images et la récupération d’information, où la discrimination fine
entre objets similaires est cruciale.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la <em>Contrastive Loss</em>, commençons par définir
les concepts fondamentaux. Supposons que nous avons deux échantillons
<span class="math inline">\(x_1\)</span> et <span
class="math inline">\(x_2\)</span>, et que nous voulons apprendre une
fonction de similarité <span class="math inline">\(f\)</span> telle que
:</p>
<p><span class="math display">\[f(x_1, x_2) =
\begin{cases}
\text{proche} &amp; \text{si } x_1 \text{ et } x_2 \text{ sont
similaires}, \\
\text{éloigné} &amp; \text{si } x_1 \text{ et } x_2 \text{ sont
dissemblables}.
\end{cases}\]</span></p>
<p>Formellement, la <em>Contrastive Loss</em> pour une paire
d’échantillons <span class="math inline">\((x_1, x_2)\)</span> est
définie comme suit :</p>
<p><span class="math display">\[L(x_1, x_2, y) = \frac{1}{2} y D^2 +
\frac{1}{2} (1 - y) \max(0, m - D)^2,\]</span></p>
<p>où : - <span class="math inline">\(y \in \{0, 1\}\)</span> est une
étiquette binaire indiquant si les échantillons sont similaires (<span
class="math inline">\(y = 1\)</span>) ou dissemblables (<span
class="math inline">\(y = 0\)</span>), - <span
class="math inline">\(D\)</span> est la distance entre les
représentations des échantillons dans l’espace de caractéristiques, -
<span class="math inline">\(m\)</span> est une marge qui contrôle la
séparation minimale entre les paires dissemblables.</p>
<p>Une autre formulation équivalente est :</p>
<p><span class="math display">\[L(x_1, x_2, y) =
\begin{cases}
\frac{1}{2} D^2 &amp; \text{si } y = 1, \\
\frac{1}{2} \max(0, m - D)^2 &amp; \text{si } y = 0.
\end{cases}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la <em>Contrastive Loss</em> est le
suivant :</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{L}\)</span> l’ensemble des
paires d’échantillons, et soit <span
class="math inline">\(f_\theta\)</span> une fonction paramétrée par
<span class="math inline">\(\theta\)</span>. Si la distance <span
class="math inline">\(D\)</span> est définie comme une norme
euclidienne, alors :</p>
<p><span class="math display">\[\lim_{\theta \to \theta^*} \sum_{(x_1,
x_2) \in \mathcal{L}} L(x_1, x_2, y) = 0,\]</span></p>
<p>où <span class="math inline">\(\theta^*\)</span> est le paramètre
optimal minimisant la perte contrastive.</p>
</div>
<p>La démonstration de ce théorème repose sur les propriétés suivantes :
1. La distance euclidienne <span class="math inline">\(D\)</span> est
continue et différentiable. 2. La fonction de perte <span
class="math inline">\(L\)</span> est convexe par rapport à <span
class="math inline">\(D\)</span>. 3. Le gradient de la perte contrastive
guide l’optimisation vers des représentations où les paires similaires
sont proches et les paires dissemblables sont éloignées.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de convergence, considérons d’abord la
dérivée de la perte contrastive par rapport à <span
class="math inline">\(D\)</span> :</p>
<p><span class="math display">\[\frac{\partial L}{\partial D} =
\begin{cases}
D &amp; \text{si } y = 1, \\
(0 - D) \mathbb{I}_{D &gt; m} &amp; \text{si } y = 0,
\end{cases}\]</span></p>
<p>où <span class="math inline">\(\mathbb{I}_{D &gt; m}\)</span> est
l’indicatrice de l’événement <span class="math inline">\(D &gt;
m\)</span>.</p>
<p>Ensuite, nous appliquons la descente de gradient pour mettre à jour
les paramètres <span class="math inline">\(\theta\)</span> :</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta
\frac{\partial L}{\partial D} \frac{\partial D}{\partial
\theta},\]</span></p>
<p>où <span class="math inline">\(\eta\)</span> est le taux
d’apprentissage.</p>
<p>Par la continuité et la convexité de <span
class="math inline">\(L\)</span>, nous pouvons garantir que le processus
d’optimisation converge vers un minimum local, et sous des conditions
supplémentaires (comme la convexité globale), vers le minimum
global.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Plusieurs propriétés importantes découlent de la définition de la
<em>Contrastive Loss</em> :</p>
<ol>
<li><p>**Propriété de Séparation** : Pour les paires dissemblables
(<span class="math inline">\(y = 0\)</span>), la perte contraste
garantit que <span class="math inline">\(D \geq m\)</span>. Cela
signifie que les représentations des paires dissemblables sont séparées
par une marge minimale <span class="math inline">\(m\)</span>.</p></li>
<li><p>**Propriété de Proximité** : Pour les paires similaires (<span
class="math inline">\(y = 1\)</span>), la perte contraste garantit que
<span class="math inline">\(D\)</span> est minimisé. Cela signifie que
les représentations des paires similaires sont aussi proches que
possible.</p></li>
<li><p>**Invariance à la Translation** : La perte contraste est
invariante aux translations dans l’espace de caractéristiques, car elle
dépend uniquement de la distance <span class="math inline">\(D\)</span>
entre les représentations.</p></li>
</ol>
<p>Pour prouver ces propriétés, nous utilisons les définitions et le
théorème de convergence présentés précédemment. Par exemple, pour la
propriété de séparation, nous avons :</p>
<p><span class="math display">\[L(x_1, x_2, 0) = \frac{1}{2} \max(0, m -
D)^2.\]</span></p>
<p>Si <span class="math inline">\(D &lt; m\)</span>, alors la perte est
positive et le gradient guide l’optimisation pour augmenter <span
class="math inline">\(D\)</span>. Si <span class="math inline">\(D \geq
m\)</span>, la perte est nulle et aucune mise à jour n’est nécessaire.
Ainsi, <span class="math inline">\(D \geq m\)</span> est garanti pour
les paires dissemblables.</p>
</body>
</html>
{% include "footer.html" %}


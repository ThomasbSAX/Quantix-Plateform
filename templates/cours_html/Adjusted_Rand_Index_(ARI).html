{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’indice de Rand ajusté : Mesure de la similarité entre partitions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’indice de Rand ajusté : Mesure de la similarité
entre partitions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse de clusters, ou classification non supervisée, est une
tâche fondamentale en apprentissage automatique et en statistique. Elle
vise à regrouper des données en clusters de manière à ce que les objets
d’un même cluster soient plus similaires entre eux qu’avec ceux des
autres clusters. Cependant, évaluer la qualité d’une telle partition est
une question complexe.</p>
<p>L’indice de Rand (RI) a été introduit par William E. Rand en 1971
pour mesurer la similarité entre deux partitions d’un même ensemble de
données. Bien que cet indice soit intuitif et facile à comprendre, il
présente un défaut majeur : sa valeur dépend du nombre de clusters et du
nombre d’objets. Pour pallier ce problème, Hubert et Arabie ont proposé
en 1985 une version ajustée de cet indice, connue sous le nom d’indice
de Rand ajusté (ARI).</p>
<p>L’ARI est aujourd’hui largement utilisé dans la littérature pour
comparer des partitions de clusters, notamment en bioinformatique et en
apprentissage automatique. Il présente l’avantage d’être normalisé, ce
qui permet de comparer des partitions de tailles différentes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir l’indice de Rand ajusté, commençons par introduire
quelques notations. Soit <span class="math inline">\(X = \{x_1, x_2,
\dots, x_n\}\)</span> un ensemble de <span
class="math inline">\(n\)</span> objets et soit <span
class="math inline">\(U = \{U_1, U_2, \dots, U_k\}\)</span> et <span
class="math inline">\(V = \{V_1, V_2, \dots, V_l\}\)</span> deux
partitions de <span class="math inline">\(X\)</span>. Nous notons <span
class="math inline">\(N_{ij}\)</span> le nombre d’objets communs aux
clusters <span class="math inline">\(U_i\)</span> et <span
class="math inline">\(V_j\)</span>.</p>
<p>L’indice de Rand mesure la proportion d’accords entre les deux
partitions. Un accord est défini comme une paire d’objets qui sont soit
dans le même cluster dans <span class="math inline">\(U\)</span> et dans
le même cluster dans <span class="math inline">\(V\)</span>, soit dans
des clusters différents dans <span class="math inline">\(U\)</span> et
dans des clusters différents dans <span
class="math inline">\(V\)</span>.</p>
<div class="definition">
<p>L’indice de Rand est défini par : <span class="math display">\[RI(U,
V) = \frac{\sum_{i=1}^k \sum_{j=1}^l
\binom{N_{ij}}{2}}{\binom{n}{2}}\]</span> où <span
class="math inline">\(\binom{a}{b}\)</span> désigne le coefficient
binomial.</p>
</div>
<p>Cependant, comme mentionné précédemment, cet indice n’est pas
normalisé. Pour obtenir une mesure ajustée, nous introduisons les
quantités suivantes : <span class="math display">\[a = \sum_{i=1}^k
\sum_{j=1}^l \binom{N_{ij}}{2}, \quad b = \frac{\sum_{i=1}^k
\binom{(\sum_{j=1}^l N_{ij})}{2}}{n}, \quad c = \frac{\sum_{j=1}^l
\binom{(\sum_{i=1}^k N_{ij})}{2}}{n}\]</span></p>
<div class="definition">
<p>L’indice de Rand ajusté est défini par : <span
class="math display">\[ARI(U, V) = \frac{a - E[a]}{E[\text{max}(a)] -
E[a]}\]</span> où <span class="math inline">\(E[a]\)</span> est
l’espérance de <span class="math inline">\(a\)</span> sous une
distribution aléatoire des partitions et <span
class="math inline">\(E[\text{max}(a)]\)</span> est l’espérance maximale
de <span class="math inline">\(a\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous présentons maintenant un théorème fondamental concernant
l’indice de Rand ajusté.</p>
<div class="theoreme">
<p>Soit <span class="math inline">\(U\)</span> et <span
class="math inline">\(V\)</span> deux partitions d’un ensemble de <span
class="math inline">\(n\)</span> objets. Alors :</p>
<ol>
<li><p><span class="math inline">\(-1 \leq ARI(U, V) \leq
1\)</span></p></li>
<li><p><span class="math inline">\(ARI(U, V) = 1\)</span> si et
seulement si <span class="math inline">\(U = V\)</span></p></li>
<li><p><span class="math inline">\(ARI(U, V) = -1\)</span> si et
seulement si les deux partitions sont complémentaires</p></li>
<li><p><span class="math inline">\(ARI(U, V) = 0\)</span> si les
partitions sont indépendantes</p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Nous allons maintenant prouver les propriétés de l’indice de Rand
ajusté.</p>
<div class="proof">
<p><em>Proof.</em> Pour démontrer la première propriété, nous utilisons
le fait que <span class="math inline">\(a\)</span> est borné par <span
class="math inline">\(E[a]\)</span> et <span
class="math inline">\(E[\text{max}(a)]\)</span>. Plus précisément, nous
avons : <span class="math display">\[E[a] = \frac{\sum_{i=1}^k
\binom{(\sum_{j=1}^l N_{ij})}{2} \cdot \sum_{j=1}^l \binom{(\sum_{i=1}^k
N_{ij})}{2}}{n(n-1)}\]</span> et <span
class="math display">\[E[\text{max}(a)] = \frac{\sum_{i=1}^k
\binom{(\sum_{j=1}^l N_{ij})}{2} + \sum_{j=1}^l \binom{(\sum_{i=1}^k
N_{ij})}{2}}{n(n-1)}\]</span> En utilisant ces expressions, il est
possible de montrer que <span class="math inline">\(ARI(U, V)\)</span>
est bien compris entre <span class="math inline">\(-1\)</span> et <span
class="math inline">\(1\)</span>.</p>
<p>Pour la deuxième propriété, supposons que <span
class="math inline">\(U = V\)</span>. Alors <span
class="math inline">\(a = E[\text{max}(a)]\)</span>, ce qui implique que
<span class="math inline">\(ARI(U, V) = 1\)</span>. Réciproquement, si
<span class="math inline">\(ARI(U, V) = 1\)</span>, alors <span
class="math inline">\(a = E[\text{max}(a)]\)</span>, ce qui signifie que
les partitions sont identiques.</p>
<p>La troisième propriété se démontre de manière similaire en
considérant des partitions complémentaires. Enfin, la quatrième
propriété découle du fait que si les partitions sont indépendantes,
alors <span class="math inline">\(a = E[a]\)</span>, ce qui implique que
<span class="math inline">\(ARI(U, V) = 0\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous listons maintenant quelques propriétés supplémentaires de
l’indice de Rand ajusté.</p>
<ol>
<li><p><strong>Invariance par permutation</strong> : L’ARI est invariant
par permutation des clusters. Cela signifie que l’ordre des clusters n’a
pas d’importance.</p></li>
<li><p><strong>Normalisation</strong> : Contrairement à l’indice de
Rand, l’ARI est normalisé et peut être utilisé pour comparer des
partitions de tailles différentes.</p></li>
<li><p><strong>Interprétation</strong> : Une valeur d’ARI proche de
<span class="math inline">\(1\)</span> indique une forte similarité
entre les partitions, tandis qu’une valeur proche de <span
class="math inline">\(-1\)</span> indique une forte dissimilarité. Une
valeur proche de <span class="math inline">\(0\)</span> signifie que les
partitions sont indépendantes.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’indice de Rand ajusté est une mesure puissante et flexible pour
comparer des partitions de clusters. Il présente l’avantage d’être
normalisé, ce qui permet de le comparer à différentes échelles. Ses
propriétés mathématiques solides en font un outil indispensable pour
l’évaluation des algorithmes de clustering.</p>
</body>
</html>
{% include "footer.html" %}


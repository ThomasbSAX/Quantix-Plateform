{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Modèle Graphique Bayésien : Une Approche Probabiliste pour l’Inférence</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Modèle Graphique Bayésien : Une Approche Probabiliste
pour l’Inférence</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les modèles graphiques bayésiens (MGB) émergent comme une synthèse
puissante entre la théorie des probabilités et l’analyse graphique,
offrant un cadre rigoureux pour modéliser les incertitudes dans des
systèmes complexes. Leur origine remonte aux travaux pionniers de Pearl
(1985) et Lauritzen et Spiegelhalter (1988), qui ont formalisé l’idée
d’utiliser des graphes dirigés pour représenter des distributions de
probabilités conjointes. Ces modèles sont indispensables dans des
domaines variés tels que la bioinformatique, l’apprentissage automatique
et les sciences sociales, où ils permettent de capturer des relations
conditionnelles entre variables tout en intégrant des connaissances a
priori.</p>
<p>L’intérêt croissant pour les MGB s’explique par leur capacité à
fournir des outils d’inférence flexibles et interprétables.
Contrairement aux modèles paramétriques classiques, les MGB permettent
de spécifier des dépendances structurelles explicites entre variables,
facilitant ainsi l’incorporation de connaissances expertes. De plus,
leur formalisme graphique offre une visualisation intuitive des
relations sous-jacentes, ce qui est particulièrement utile pour la
validation et l’interprétation des résultats.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre les modèles graphiques bayésiens, commençons par
définir quelques concepts clés. Supposons que nous ayons un ensemble de
variables aléatoires <span class="math inline">\(V = \{X_1, X_2, \ldots,
X_n\}\)</span>. Nous cherchons à représenter les dépendances
conditionnelles entre ces variables de manière compacte et
interprétable.</p>
<div class="definition">
<p>Un graphe bayésien est un couple <span class="math inline">\(G = (V,
E)\)</span>, où <span class="math inline">\(V\)</span> est un ensemble
de nœuds représentant des variables aléatoires et <span
class="math inline">\(E\)</span> est un ensemble d’arêtes dirigées
représentant des dépendances conditionnelles entre ces variables.
Formellement, pour tout <span class="math inline">\(X_i \in V\)</span>,
l’ensemble des parents de <span class="math inline">\(X_i\)</span> est
noté <span class="math inline">\(\text{Pa}(X_i) = \{ X_j \in V \mid
(X_j, X_i) \in E \}\)</span>.</p>
</div>
<p>Une distribution de probabilité conjointe <span
class="math inline">\(P\)</span> sur <span
class="math inline">\(V\)</span> est dite <em>markovienne</em> par
rapport à <span class="math inline">\(G\)</span> si elle satisfait la
propriété de Markov locale :</p>
<div class="definition">
<p>Pour tout <span class="math inline">\(X_i \in V\)</span>, la
distribution conjointe <span class="math inline">\(P\)</span> satisfait
: <span class="math display">\[P(X_i \mid X_j, j \neq i) = P(X_i \mid
\text{Pa}(X_i))\]</span> pour presque tout <span
class="math inline">\(X_j\)</span> dans le support de <span
class="math inline">\(P\)</span>.</p>
</div>
<p>Cette propriété signifie que la valeur d’une variable est
indépendante de toutes les autres variables, conditionnellement à ses
parents dans le graphe.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un résultat fondamental en théorie des graphes bayésiens est le
théorème de factorisation, qui établit une relation entre la structure
du graphe et la distribution de probabilité conjointe.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(G = (V, E)\)</span> un graphe
bayésien et <span class="math inline">\(P\)</span> une distribution de
probabilité conjointe sur <span class="math inline">\(V\)</span> qui est
markovienne par rapport à <span class="math inline">\(G\)</span>. Alors,
la distribution conjointe peut être factorisée comme suit : <span
class="math display">\[P(X_1, X_2, \ldots, X_n) = \prod_{X_i \in V}
P(X_i \mid \text{Pa}(X_i))\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur l’application
répétée de la propriété de Markov locale. En effet, pour chaque variable
<span class="math inline">\(X_i\)</span>, nous avons : <span
class="math display">\[P(X_1, X_2, \ldots, X_n) = P(X_i \mid X_1,
\ldots, X_{i-1}, X_{i+1}, \ldots, X_n) P(X_1, \ldots, X_{i-1}, X_{i+1},
\ldots, X_n)\]</span> En utilisant la propriété de Markov locale, nous
pouvons remplacer <span class="math inline">\(P(X_i \mid X_1, \ldots,
X_{i-1}, X_{i+1}, \ldots, X_n)\)</span> par <span
class="math inline">\(P(X_i \mid \text{Pa}(X_i))\)</span>. En répétant
ce processus pour chaque variable, nous obtenons la factorisation
souhaitée. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Les modèles graphiques bayésiens possèdent plusieurs propriétés
intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Propriété de Markov Globale</strong> : Si <span
class="math inline">\(G\)</span> est un graphe bayésien et <span
class="math inline">\(P\)</span> une distribution markovienne par
rapport à <span class="math inline">\(G\)</span>, alors pour tout
ensemble de nœuds <span class="math inline">\(A, B, C\)</span> tels que
<span class="math inline">\(C\)</span> sépare <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span> dans <span
class="math inline">\(G\)</span>, nous avons : <span
class="math display">\[P(A \mid B, C) = P(A \mid C)\]</span> pour
presque tout <span class="math inline">\(B\)</span> et <span
class="math inline">\(C\)</span> dans le support de <span
class="math inline">\(P\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la
factorisation de la distribution conjointe et de la définition des
séparations dans les graphes dirigés. En effet, si <span
class="math inline">\(C\)</span> sépare <span
class="math inline">\(A\)</span> et <span
class="math inline">\(B\)</span>, alors les variables dans <span
class="math inline">\(A\)</span> sont conditionnellement indépendantes
des variables dans <span class="math inline">\(B\)</span> donné <span
class="math inline">\(C\)</span>, ce qui implique l’égalité
souhaitée. ◻</p>
</div></li>
<li><p><strong>Propriété d’Equivalence de Markov</strong> : Deux graphes
bayésiens <span class="math inline">\(G_1\)</span> et <span
class="math inline">\(G_2\)</span> sont dits équivalents de Markov si
ils définissent la même famille de distributions markoviennes. Cette
propriété est caractérisée par le fait que <span
class="math inline">\(G_1\)</span> et <span
class="math inline">\(G_2\)</span> ont les mêmes séparations de
colliers.</p>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété repose sur l’analyse des
séparations de colliers dans les graphes dirigés. Deux graphes sont
équivalents de Markov si et seulement si ils ont les mêmes séparations
de colliers, ce qui signifie que les distributions markoviennes qu’ils
définissent sont identiques. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Les modèles graphiques bayésiens offrent un cadre puissant et
flexible pour modéliser les incertitudes dans des systèmes complexes.
Leur formalisme graphique permet de capturer des relations
conditionnelles entre variables tout en intégrant des connaissances a
priori. Les théorèmes et propriétés présentés dans cet article
illustrent la richesse de cette approche et ouvrent la voie à de
nombreuses applications dans des domaines variés.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Entropie d’Ordre Supérieur : Une Exploration Mathématique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Entropie d’Ordre Supérieur : Une Exploration
Mathématique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie, concept fondamental en théorie de l’information et en
physique statistique, mesure le degré d’incertitude ou de désordre dans
un système. Introduite par Claude Shannon en 1948, l’entropie classique
a révolutionné notre compréhension des systèmes complexes. Cependant,
pour capturer des structures plus fines et des dépendances à long terme,
l’entropie d’ordre supérieur émerge comme un outil puissant.</p>
<p>L’entropie d’ordre supérieur généralise l’entropie de Shannon en
considérant des blocs de symboles plutôt que des symboles individuels.
Cette généralisation permet de détecter des motifs et des corrélations
qui ne sont pas évidents dans l’analyse classique. Par exemple, en
traitement du signal, elle peut révéler des structures périodiques ou
des motifs répétitifs qui sont invisibles à l’entropie de premier
ordre.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’entropie d’ordre supérieur, commençons par rappeler
la définition de l’entropie de Shannon. Supposons que nous avons une
source d’information qui émet des symboles selon une distribution de
probabilité <span class="math inline">\(p = (p_1, p_2, \ldots,
p_n)\)</span>. L’entropie de Shannon <span
class="math inline">\(H(p)\)</span> est définie comme :</p>
<p><span class="math display">\[H(p) = -\sum_{i=1}^n p_i \log
p_i\]</span></p>
<p>Cette mesure quantifie l’incertitude moyenne associée à la source.
Maintenant, pour généraliser cette notion, considérons des blocs de
<span class="math inline">\(k\)</span> symboles. Soit <span
class="math inline">\(X^k = (X_1, X_2, \ldots, X_k)\)</span> un vecteur
de <span class="math inline">\(k\)</span> symboles consécutifs. La
distribution conjointe de ces symboles est donnée par <span
class="math inline">\(p^{(k)} = (p_{i_1, i_2, \ldots, i_k})\)</span>, où
<span class="math inline">\(p_{i_1, i_2, \ldots, i_k}\)</span> est la
probabilité de voir le bloc <span class="math inline">\((i_1, i_2,
\ldots, i_k)\)</span>.</p>
<p>L’entropie d’ordre <span class="math inline">\(k\)</span> est alors
définie comme :</p>
<p><span class="math display">\[H^{(k)}(p) = -\sum_{i_1, i_2, \ldots,
i_k} p_{i_1, i_2, \ldots, i_k}^{(k)} \log p_{i_1, i_2, \ldots,
i_k}^{(k)}\]</span></p>
<p>En d’autres termes, l’entropie d’ordre <span
class="math inline">\(k\)</span> est l’entropie de Shannon de la
distribution conjointe des blocs de <span
class="math inline">\(k\)</span> symboles.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant l’entropie d’ordre supérieur est
le théorème de la limite asymptotique. Ce théorème stipule que pour une
source stationnaire ergodique, l’entropie d’ordre <span
class="math inline">\(k\)</span> converge vers l’entropie de la source
lorsque <span class="math inline">\(k\)</span> tend vers l’infini.</p>
<div class="theorem">
<p>Soit <span class="math inline">\((X_n)\)</span> une source
stationnaire ergodique. Alors, l’entropie d’ordre <span
class="math inline">\(k\)</span> <span
class="math inline">\(H^{(k)}(p)\)</span> satisfait :</p>
<p><span class="math display">\[\lim_{k \to \infty} \frac{H^{(k)}(p)}{k}
= h(p)\]</span></p>
<p>où <span class="math inline">\(h(p)\)</span> est l’entropie de la
source.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la limite asymptotique, nous utilisons
les propriétés de la source stationnaire ergodique. Une source est dite
stationnaire si la distribution conjointe des symboles ne dépend pas du
temps, et ergodique si les moyennes temporelles convergent vers les
moyennes d’ensemble.</p>
<p>Considérons la chaîne de Markov <span
class="math inline">\((X_n)\)</span> avec une matrice de transition
<span class="math inline">\(P\)</span>. La distribution stationnaire
<span class="math inline">\(\pi\)</span> satisfait <span
class="math inline">\(\pi = \pi P\)</span>. L’entropie de la source est
donnée par :</p>
<p><span class="math display">\[h(p) = -\sum_i \pi_i \sum_j P_{ij} \log
P_{ij}\]</span></p>
<p>En utilisant la stationnarité et l’ergodicité, nous pouvons montrer
que :</p>
<p><span class="math display">\[\lim_{k \to \infty} \frac{H^{(k)}(p)}{k}
= h(p)\]</span></p>
<p>Cette preuve repose sur le théorème ergodique de Birkhoff, qui
garantit la convergence des moyennes temporelles vers les moyennes
d’ensemble.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie d’ordre supérieur possède plusieurs propriétés
intéressantes :</p>
<ol>
<li><p>**Monotonicité** : L’entropie d’ordre <span
class="math inline">\(k\)</span> est une fonction croissante de <span
class="math inline">\(k\)</span>. Cela signifie que plus nous
considérons des blocs longs, plus l’entropie est grande. <span
class="math display">\[H^{(k+1)}(p) \geq H^{(k)}(p)\]</span></p></li>
<li><p>**Sous-additivité** : L’entropie d’ordre <span
class="math inline">\(k\)</span> est sous-additive. Cela signifie que
pour toute source, l’entropie d’ordre <span
class="math inline">\(k\)</span> satisfait : <span
class="math display">\[H^{(k+l)}(p) \leq H^{(k)}(p) +
H^{(l)}(p)\]</span></p></li>
<li><p>**Convergence** : Pour une source stationnaire ergodique,
l’entropie d’ordre <span class="math inline">\(k\)</span> converge vers
l’entropie de la source lorsque <span class="math inline">\(k\)</span>
tend vers l’infini. <span class="math display">\[\lim_{k \to \infty}
\frac{H^{(k)}(p)}{k} = h(p)\]</span></p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’entropie d’ordre supérieur est un outil puissant pour analyser les
structures complexes dans les systèmes dynamiques et les sources
d’information. En généralisant l’entropie de Shannon, elle permet de
détecter des motifs et des corrélations qui ne sont pas évidents dans
l’analyse classique. Les propriétés et théorèmes associés à l’entropie
d’ordre supérieur ouvrent de nouvelles perspectives pour la
compréhension des systèmes complexes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Métrique Apprentissage : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Métrique Apprentissage : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage de métrique, une branche dynamique de l’apprentissage
automatique, vise à apprendre une distance ou une similarité entre des
objets dans un espace de caractéristiques. L’idée fondamentale est que
les données brutes peuvent ne pas être représentées de manière optimale
pour des tâches spécifiques, telles que la classification ou le
regroupement. En apprenant une métrique adaptée à la tâche, nous pouvons
transformer l’espace de caractéristiques pour améliorer les performances
des algorithmes.</p>
<p>L’origine historique de l’apprentissage de métrique remonte aux
années 1960 avec les travaux sur l’analyse des composantes principales
(ACP) et la réduction de dimension. Cependant, c’est au début des années
2000 que l’apprentissage de métrique a émergé comme un domaine distinct,
avec des contributions significatives de chercheurs tels que Xing et al.
(2003) sur la méthode Large Margin Nearest Neighbor (LMNN).</p>
<p>L’apprentissage de métrique est indispensable dans divers domaines,
notamment la vision par ordinateur, le traitement du langage naturel et
la bioinformatique. Par exemple, dans la reconnaissance d’images,
apprendre une métrique qui met en évidence les caractéristiques
discriminantes peut grandement améliorer la précision de la
classification.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre l’apprentissage de métrique, nous devons d’abord
définir certains concepts clés.</p>
<h2 class="unnumbered" id="espace-métrique">Espace Métrique</h2>
<p>Considérons un ensemble <span class="math inline">\(X\)</span> et une
fonction <span class="math inline">\(d: X \times X \rightarrow
\mathbb{R}\)</span> qui satisfait les propriétés suivantes pour tous
<span class="math inline">\(x, y, z \in X\)</span>:</p>
<ol>
<li><p><span class="math inline">\(d(x, y) \geq 0\)</span></p></li>
<li><p><span class="math inline">\(d(x, y) = 0\)</span> si et seulement
si <span class="math inline">\(x = y\)</span></p></li>
<li><p><span class="math inline">\(d(x, y) = d(y, x)\)</span></p></li>
<li><p><span class="math inline">\(d(x, z) \leq d(x, y) + d(y,
z)\)</span></p></li>
</ol>
<p>La fonction <span class="math inline">\(d\)</span> est appelée une
métrique sur <span class="math inline">\(X\)</span>, et l’ensemble <span
class="math inline">\((X, d)\)</span> est appelé un espace métrique.</p>
<h2 class="unnumbered" id="apprentissage-de-métrique">Apprentissage de
Métrique</h2>
<p>L’apprentissage de métrique consiste à apprendre une métrique <span
class="math inline">\(d\)</span> à partir de données étiquetées ou non
étiquetées. Formellement, étant donné un ensemble de données <span
class="math inline">\(\mathcal{X} = \{x_i\}_{i=1}^n\)</span> et
éventuellement des étiquettes <span class="math inline">\(\mathcal{Y} =
\{y_i\}_{i=1}^n\)</span>, nous cherchons à apprendre une métrique <span
class="math inline">\(d\)</span> qui minimise une fonction de perte
appropriée.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Plusieurs théorèmes fondamentaux sous-tendent l’apprentissage de
métrique. Nous en discutons quelques-uns ci-dessous.</p>
<h2 class="unnumbered"
id="théorème-de-représentation-des-métriques-apprises">Théorème de
Représentation des Métriques Apprises</h2>
<p>Supposons que nous apprenons une métrique <span
class="math inline">\(d\)</span> à partir de données étiquetées. Nous
pouvons représenter <span class="math inline">\(d\)</span> en utilisant
une matrice de Gram positive semi-définie <span
class="math inline">\(M\)</span>, telle que: <span
class="math display">\[d(x_i, x_j) = (x_i - x_j)^T M (x_i -
x_j)\]</span></p>
<h2 class="unnumbered" id="théorème-de-la-marge-maximale">Théorème de la
Marge Maximale</h2>
<p>Le théorème de la marge maximale stipule que, pour un ensemble de
données linéairement séparable, il existe une métrique qui maximise la
marge entre les classes. Formellement, étant donné des données <span
class="math inline">\(\mathcal{X}\)</span> et des étiquettes <span
class="math inline">\(\mathcal{Y}\)</span>, la métrique optimale <span
class="math inline">\(M^*\)</span> satisfait: <span
class="math display">\[M^* = \arg\max_M \min_{i,j: y_i \neq y_j}
\frac{(x_i - x_j)^T M (x_i - x_j)}{\|M\|_F}\]</span></p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Nous fournissons des preuves détaillées pour les théorèmes mentionnés
ci-dessus.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-représentation-des-métriques-apprises">Preuve
du Théorème de Représentation des Métriques Apprises</h2>
<p>Pour prouver ce théorème, nous devons montrer que toute métrique
apprise peut être représentée par une matrice de Gram positive
semi-définie. Considérons la fonction de perte suivante: <span
class="math display">\[\mathcal{L}(M) = \sum_{i,j} \ell((x_i - x_j)^T M
(x_i - x_j), y_i, y_j)\]</span> où <span
class="math inline">\(\ell\)</span> est une fonction de perte
appropriée. En minimisant cette fonction de perte, nous obtenons une
matrice <span class="math inline">\(M\)</span> qui est positive
semi-définie.</p>
<h2 class="unnumbered"
id="preuve-du-théorème-de-la-marge-maximale">Preuve du Théorème de la
Marge Maximale</h2>
<p>La preuve de ce théorème repose sur les concepts de la théorie de
l’apprentissage statistique. Nous utilisons le théorème de la marge
maximale, qui stipule que la métrique optimale maximise la marge entre
les classes. En utilisant des techniques d’optimisation convexe, nous
pouvons montrer que la métrique optimale <span
class="math inline">\(M^*\)</span> satisfait la condition donnée.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous énumérons quelques propriétés et corollaires importants de
l’apprentissage de métrique.</p>
<h2 class="unnumbered" id="propriété-de-la-positivité">Propriété de la
Positivité</h2>
<p>La matrice <span class="math inline">\(M\)</span> apprise doit être
positive semi-définie pour représenter une métrique valide.</p>
<h2 class="unnumbered"
id="corollaire-de-la-réduction-de-dimension">Corollaire de la Réduction
de Dimension</h2>
<p>En apprenant une métrique, nous pouvons réduire la dimension des
données tout en préservant les relations de similarité.</p>
<h2 class="unnumbered" id="propriété-de-linvariance">Propriété de
l’Invariance</h2>
<p>La métrique apprise doit être invariante sous les transformations des
données, telles que les translations et les rotations.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’apprentissage de métrique est un domaine fascinant et en pleine
expansion de l’apprentissage automatique. En apprenant des métriques
adaptées à des tâches spécifiques, nous pouvons améliorer
considérablement les performances des algorithmes de classification et
de regroupement. Les théorèmes et propriétés discutés dans cet article
fournissent une base solide pour la recherche future dans ce
domaine.</p>
</body>
</html>
{% include "footer.html" %}


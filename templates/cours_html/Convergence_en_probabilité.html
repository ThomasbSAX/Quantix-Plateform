{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Convergence en probabilité : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Convergence en probabilité : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La notion de convergence en probabilité est une pierre angulaire dans
l’édifice des probabilités et de la statistique mathématique.
Historiquement, cette idée émerge avec les travaux pionniers de
Kolmogorov dans les années 1930, qui a formalisé le cadre des espaces
probabilisés. La convergence en probabilité est indispensable pour
comprendre les propriétés asymptotiques des estimateurs, comme dans le
théorème central limite ou la loi des grands nombres. Elle permet de
quantifier comment une suite de variables aléatoires se rapproche d’une
limite déterministe, ouvrant ainsi la voie à des applications pratiques
en modélisation et inférence statistique.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de formaliser la convergence en probabilité, considérons une
suite de variables aléatoires <span class="math inline">\((X_n)_{n \in
\mathbb{N}}\)</span> et un point fixe <span
class="math inline">\(x\)</span>. Intuitivement, nous voulons que pour
tout <span class="math inline">\(\epsilon &gt; 0\)</span>, la
probabilité que <span class="math inline">\(X_n\)</span> s’éloigne de
<span class="math inline">\(x\)</span> de plus de <span
class="math inline">\(\epsilon\)</span> devienne négligeable lorsque
<span class="math inline">\(n\)</span> tend vers l’infini. Cela nous
amène à la définition suivante :</p>
<div class="definition">
<p>Soit <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
une suite de variables aléatoires définies sur un espace probabilisé
<span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> et soit
<span class="math inline">\(X\)</span> une variable aléatoire. On dit
que <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
converge en probabilité vers <span class="math inline">\(X\)</span> si :
<span class="math display">\[\forall \epsilon &gt; 0, \quad \lim_{n \to
+\infty} P(|X_n - X| &gt; \epsilon) = 0.\]</span> En notations, on écrit
: <span class="math display">\[X_n \xrightarrow{P} X.\]</span></p>
</div>
<p>Une autre manière d’exprimer cette convergence est via la fonction de
répartition. Soit <span class="math inline">\(F_n\)</span> et <span
class="math inline">\(F\)</span> les fonctions de répartition
respectives de <span class="math inline">\((X_n)_{n \in
\mathbb{N}}\)</span> et <span class="math inline">\(X\)</span>. Alors,
la convergence en probabilité implique que : <span
class="math display">\[\forall x \in \mathbb{R}, \quad \lim_{n \to
+\infty} F_n(x) = F(x).\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la convergence en probabilité est le
théorème de Slutsky. Ce résultat permet de comprendre comment les
opérations sur des suites convergentes en probabilité se comportent.</p>
<div class="theorem">
<p>Soient <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>
et <span class="math inline">\((Y_n)_{n \in \mathbb{N}}\)</span> deux
suites de variables aléatoires telles que : <span
class="math display">\[X_n \xrightarrow{P} X, \quad Y_n \xrightarrow{P}
c,\]</span> où <span class="math inline">\(X\)</span> est une variable
aléatoire et <span class="math inline">\(c\)</span> est une constante.
Alors, on a : <span class="math display">\[X_n Y_n \xrightarrow{P} X
c.\]</span></p>
</div>
<p>Pour démontrer ce théorème, nous utilisons la définition de la
convergence en probabilité et les propriétés des variables aléatoires.
Supposons que <span class="math inline">\(\epsilon &gt; 0\)</span> soit
fixé. Nous devons montrer que : <span class="math display">\[\lim_{n \to
+\infty} P(|X_n Y_n - X c| &gt; \epsilon) = 0.\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Commençons par décomposer <span class="math inline">\(|X_n Y_n - X
c|\)</span> : <span class="math display">\[|X_n Y_n - X c| = |X_n (Y_n -
c) + (X_n - X) c| \leq |X_n||Y_n - c| + |c||X_n - X|.\]</span></p>
<p>En utilisant l’inégalité de Markov, nous avons : <span
class="math display">\[P(|X_n Y_n - X c| &gt; \epsilon) \leq
P\left(|X_n||Y_n - c| + |c||X_n - X| &gt; \epsilon\right).\]</span></p>
<p>Nous pouvons alors utiliser la convergence en probabilité de <span
class="math inline">\((Y_n)_{n \in \mathbb{N}}\)</span> vers <span
class="math inline">\(c\)</span> et celle de <span
class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span> vers <span
class="math inline">\(X\)</span>. Par la définition de la convergence en
probabilité, pour tout <span class="math inline">\(\delta &gt;
0\)</span>, il existe <span class="math inline">\(N\)</span> tel que
pour tout <span class="math inline">\(n \geq N\)</span> : <span
class="math display">\[P(|Y_n - c| &gt; \delta) &lt;
\frac{\epsilon}{2},\]</span> et <span class="math display">\[P(|X_n - X|
&gt; \frac{\epsilon}{2|c|}) &lt; \frac{\epsilon}{2}.\]</span></p>
<p>En combinant ces résultats, nous obtenons : <span
class="math display">\[P(|X_n Y_n - X c| &gt; \epsilon) &lt;
\epsilon,\]</span> ce qui achève la preuve.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La convergence en probabilité possède plusieurs propriétés
intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Stabilité par composition</strong> : Si <span
class="math inline">\(X_n \xrightarrow{P} X\)</span> et <span
class="math inline">\(Y_n \xrightarrow{P} Y\)</span>, alors <span
class="math inline">\(f(X_n, Y_n) \xrightarrow{P} f(X, Y)\)</span> pour
toute fonction continue <span class="math inline">\(f\)</span>.</p></li>
<li><p><strong>Stabilité par somme</strong> : Si <span
class="math inline">\(X_n \xrightarrow{P} X\)</span> et <span
class="math inline">\(Y_n \xrightarrow{P} Y\)</span>, alors <span
class="math inline">\(X_n + Y_n \xrightarrow{P} X + Y\)</span>.</p></li>
<li><p><strong>Stabilité par produit</strong> : Si <span
class="math inline">\(X_n \xrightarrow{P} X\)</span> et <span
class="math inline">\(Y_n \xrightarrow{P} Y\)</span>, alors <span
class="math inline">\(X_n Y_n \xrightarrow{P} X Y\)</span>.</p></li>
</ol>
<p>Pour démontrer la stabilité par somme, nous utilisons à nouveau
l’inégalité triangulaire : <span class="math display">\[|(X_n + Y_n) -
(X + Y)| \leq |X_n - X| + |Y_n - Y|.\]</span></p>
<p>En utilisant la convergence en probabilité de <span
class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span> et <span
class="math inline">\((Y_n)_{n \in \mathbb{N}}\)</span>, nous pouvons
montrer que : <span class="math display">\[\lim_{n \to +\infty} P(|(X_n
+ Y_n) - (X + Y)| &gt; \epsilon) = 0.\]</span></p>
</body>
</html>
{% include "footer.html" %}


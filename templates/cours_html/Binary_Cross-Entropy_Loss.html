{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Binary Cross-Entropy Loss: A Fundamental Tool in Machine Learning</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Binary Cross-Entropy Loss: A Fundamental Tool in
Machine Learning</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’apprentissage automatique moderne repose sur des fondements
mathématiques solides, où la notion de perte est centrale. Parmi les
fonctions de perte, la Binary Cross-Entropy Loss (BCEL) émerge comme un
pilier dans les problèmes de classification binaire. Son origine remonte
aux travaux pionniers en théorie de l’information, notamment ceux de
Claude Shannon, qui introduisit l’entropie comme mesure de
l’incertitude. La BCEL trouve ses racines dans cette théorie, adaptée
pour quantifier la divergence entre deux distributions binaires.</p>
<p>Dans un cadre de classification binaire, où les étiquettes sont soit
0 soit 1, la BCEL mesure l’écart entre les prédictions d’un modèle et
les étiquettes réelles. Elle est indispensable pour évaluer la
performance des modèles, guider leur optimisation via des algorithmes de
descente de gradient, et assurer une convergence stable vers des
solutions optimales. Son importance réside dans sa capacité à pénaliser
fortement les prédictions erronées, tout en favorisant une meilleure
calibration des probabilités prédites.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la BCEL, considérons un modèle de classification
binaire qui prédit une probabilité <span
class="math inline">\(\hat{y}\)</span> pour chaque échantillon, où <span
class="math inline">\(y\)</span> est l’étiquette réelle (0 ou 1). Nous
cherchons une fonction de perte qui mesure l’incertitude entre <span
class="math inline">\(\hat{y}\)</span> et <span
class="math inline">\(y\)</span>. Intuitivement, cette fonction doit
être minimale lorsque <span class="math inline">\(\hat{y} = y\)</span>
et maximale lorsque <span class="math inline">\(\hat{y}\)</span> est
loin de <span class="math inline">\(y\)</span>.</p>
<p>Formellement, la Binary Cross-Entropy Loss pour un seul échantillon
est définie comme suit:</p>
<p><span class="math display">\[\mathcal{L}(y, \hat{y}) = - \left( y
\log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)\]</span></p>
<p>En termes de quantificateurs, pour tout <span class="math inline">\(y
\in \{0, 1\}\)</span> et pour toute prédiction <span
class="math inline">\(\hat{y} \in [0, 1]\)</span>, la BCEL est donnée
par:</p>
<p><span class="math display">\[\mathcal{L}: \{0, 1\} \times [0, 1]
\rightarrow \mathbb{R}^+ \\
(y, \hat{y}) \mapsto - \left( y \log(\hat{y}) + (1 - y) \log(1 -
\hat{y}) \right)\]</span></p>
<p>Pour un ensemble de <span class="math inline">\(N\)</span>
échantillons, la BCEL moyenne est définie par:</p>
<p><span class="math display">\[\mathcal{L}_{\text{total}} = -
\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1
- \hat{y}_i) \right)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la BCEL est celui de la convergence des
algorithmes d’optimisation utilisant cette perte. Considérons un modèle
paramétré par <span class="math inline">\(\theta\)</span>, et supposons
que la BCEL est utilisée comme fonction de coût. Nous cherchons à
montrer que l’algorithme de descente de gradient converge vers un
minimum local.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\theta_t\)</span> les paramètres du
modèle à l’itération <span class="math inline">\(t\)</span>, et soit
<span class="math inline">\(\eta\)</span> le taux d’apprentissage. Si
<span class="math inline">\(\eta\)</span> est suffisamment petit, alors
la séquence <span class="math inline">\(\theta_t\)</span> converge vers
un point critique de la BCEL.</p>
<p><span class="math display">\[\lim_{t \to \infty} \theta_t = \theta^*
\quad \text{tel que} \quad \nabla_\theta
\mathcal{L}_{\text{total}}(\theta^*) = 0\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous utilisons le lemme de la diminution du
gradient et les propriétés de convexité de la BCEL.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction de coût <span
class="math inline">\(\mathcal{L}_{\text{total}}(\theta)\)</span>. La
mise à jour des paramètres par descente de gradient est donnée par:</p>
<p><span class="math display">\[\theta_{t+1} = \theta_t - \eta
\nabla_\theta \mathcal{L}_{\text{total}}(\theta_t)\]</span></p>
<p>Nous savons que la BCEL est convexe, et donc son gradient <span
class="math inline">\(\nabla_\theta
\mathcal{L}_{\text{total}}(\theta)\)</span> est Lipschitz continu. Par
le lemme de la diminution du gradient, si <span
class="math inline">\(\eta\)</span> est choisi tel que:</p>
<p><span class="math display">\[0 &lt; \eta &lt;
\frac{2}{L}\]</span></p>
<p>où <span class="math inline">\(L\)</span> est la constante de
Lipschitz du gradient, alors la séquence <span
class="math inline">\(\mathcal{L}_{\text{total}}(\theta_t)\)</span> est
décroissante et bornée inférieurement. Par le théorème de la valeur
intermédiaire, <span class="math inline">\(\theta_t\)</span> converge
vers un point critique <span class="math inline">\(\theta^*\)</span> où
le gradient s’annule. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La BCEL possède plusieurs propriétés intéressantes, que nous
énumérons et prouvons ci-dessous.</p>
<ol>
<li><p>La BCEL est toujours non négative. Pour tout <span
class="math inline">\(y \in \{0, 1\}\)</span> et <span
class="math inline">\(\hat{y} \in [0, 1]\)</span>, nous avons:</p>
<p><span class="math display">\[\mathcal{L}(y, \hat{y}) \geq
0\]</span></p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(y = 1\)</span>, alors
<span class="math inline">\(\mathcal{L}(y, \hat{y}) = -\log(\hat{y})
\geq 0\)</span> car <span class="math inline">\(\hat{y} \leq 1\)</span>.
Si <span class="math inline">\(y = 0\)</span>, alors <span
class="math inline">\(\mathcal{L}(y, \hat{y}) = -\log(1 - \hat{y}) \geq
0\)</span> car <span class="math inline">\(1 - \hat{y} \leq
1\)</span>. ◻</p>
</div></li>
<li><p>La BCEL atteint son minimum lorsque <span
class="math inline">\(\hat{y} = y\)</span>. Pour tout <span
class="math inline">\(y \in \{0, 1\}\)</span>, nous avons:</p>
<p><span class="math display">\[\mathcal{L}(y, y) = 0\]</span></p>
<div class="proof">
<p><em>Proof.</em> Si <span class="math inline">\(y = 1\)</span>, alors
<span class="math inline">\(\mathcal{L}(y, y) = -\log(1) = 0\)</span>.
Si <span class="math inline">\(y = 0\)</span>, alors <span
class="math inline">\(\mathcal{L}(y, y) = -\log(1) = 0\)</span>. ◻</p>
</div></li>
<li><p>La BCEL est strictement convexe. Pour tout <span
class="math inline">\(y \in \{0, 1\}\)</span>, la fonction <span
class="math inline">\(\mathcal{L}(y, \cdot)\)</span> est strictement
convexe sur <span class="math inline">\([0, 1]\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La dérivée seconde de <span
class="math inline">\(\mathcal{L}(y, \hat{y})\)</span> par rapport à
<span class="math inline">\(\hat{y}\)</span> est:</p>
<p><span class="math display">\[\frac{\partial^2}{\partial \hat{y}^2}
\mathcal{L}(y, \hat{y}) = -\frac{y}{\hat{y}^2} - \frac{1 - y}{(1 -
\hat{y})^2}\]</span></p>
<p>Cette dérivée seconde est toujours positive pour <span
class="math inline">\(\hat{y} \in (0, 1)\)</span>, ce qui prouve la
convexité stricte. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La Binary Cross-Entropy Loss est un outil fondamental en
apprentissage automatique, particulièrement dans les problèmes de
classification binaire. Son origine théorique, ses propriétés
mathématiques et sa capacité à guider l’optimisation des modèles en font
un pilier incontournable. Les théorèmes et preuves présentés dans cet
article illustrent son importance et sa robustesse, tout en ouvrant la
voie à des recherches futures sur son utilisation dans des cadres plus
complexes.</p>
</body>
</html>
{% include "footer.html" %}


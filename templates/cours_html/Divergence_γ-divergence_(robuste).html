{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence \gamma-divergence (robuste)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>Divergence <span
class="math inline">\(\gamma\)</span>-divergence (robuste)</strong></h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La notion de divergence est fondamentale en analyse mathématique,
notamment dans l’étude des fonctions et des mesures. La <span
class="math inline">\(\gamma\)</span>-divergence, ou divergence robuste,
émerge comme une généralisation sophistiquée de la divergence classique,
permettant d’étendre les résultats et les méthodes à des contextes plus
larges et plus robustes.</p>
<p>L’origine historique de la <span
class="math inline">\(\gamma\)</span>-divergence remonte aux travaux
pionniers sur les inégalités et les mesures de divergence, où l’on
cherchait à quantifier la distance entre deux distributions. La <span
class="math inline">\(\gamma\)</span>-divergence est indispensable dans
des cadres où les méthodes traditionnelles échouent, notamment en
statistique robuste et en apprentissage automatique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la <span
class="math inline">\(\gamma\)</span>-divergence, commençons par
comprendre ce que nous cherchons à capturer. Nous voulons une mesure de
la distance entre deux distributions qui soit robuste aux perturbations
et aux erreurs de mesure. Cette mesure doit être capable de capturer des
différences subtiles tout en restant stable face à des variations
mineures.</p>
<p>Formellement, la <span
class="math inline">\(\gamma\)</span>-divergence entre deux mesures
<span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\((X, \mathcal{A})\)</span> est définie comme suit
:</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures sur un espace mesurable
<span class="math inline">\((X, \mathcal{A})\)</span>. La <span
class="math inline">\(\gamma\)</span>-divergence de <span
class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span> est définie par : <span
class="math display">\[D_{\gamma}(Q \| P) = \sup_{f: X \rightarrow
[0,1]} \left\{ \int_X f \, dQ - \gamma \int_X (1 - f) \, dP
\right\},\]</span> où <span class="math inline">\(\gamma \in
(0,1)\)</span> est un paramètre de robustesse.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D_{\gamma}(Q \| P) = \int_X \left( q(x) - p(x) +
\gamma p(x) \log \frac{p(x)}{q(x)} \right) \, dx,\]</span> où <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité
associées à <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la <span
class="math inline">\(\gamma\)</span>-divergence est le suivant :</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux mesures sur un espace mesurable
<span class="math inline">\((X, \mathcal{A})\)</span>. Alors, pour tout
<span class="math inline">\(\gamma \in (0,1)\)</span>, on a : <span
class="math display">\[D_{\gamma}(Q \| P) \geq 0,\]</span> avec égalité
si et seulement si <span class="math inline">\(P = Q\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de la <span
class="math inline">\(\gamma\)</span>-divergence, nous procédons comme
suit :</p>
<div class="preuve">
<p>Considérons la définition de la <span
class="math inline">\(\gamma\)</span>-divergence : <span
class="math display">\[D_{\gamma}(Q \| P) = \sup_{f: X \rightarrow
[0,1]} \left\{ \int_X f \, dQ - \gamma \int_X (1 - f) \, dP
\right\}.\]</span></p>
<p>Nous voulons montrer que <span class="math inline">\(D_{\gamma}(Q \|
P) \geq 0\)</span>. Pour cela, choisissons <span
class="math inline">\(f(x) = \frac{q(x)}{\gamma p(x) + q(x)}\)</span>.
Alors, <span class="math display">\[\int_X f \, dQ - \gamma \int_X (1 -
f) \, dP = \int_X \frac{q(x)}{\gamma p(x) + q(x)} \, dQ - \gamma \int_X
\left(1 - \frac{q(x)}{\gamma p(x) + q(x)}\right) \, dP.\]</span></p>
<p>En simplifiant, nous obtenons : <span class="math display">\[\int_X
\frac{q(x)}{\gamma p(x) + q(x)} \, dQ - \gamma \int_X \frac{\gamma
p(x)}{\gamma p(x) + q(x)} \, dP.\]</span></p>
<p>En utilisant le changement de variable approprié et les propriétés
des intégrales, nous pouvons montrer que cette expression est toujours
non négative. L’égalité a lieu si et seulement si <span
class="math inline">\(P = Q\)</span>.</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La <span class="math inline">\(\gamma\)</span>-divergence possède
plusieurs propriétés intéressantes :</p>
<ol>
<li><p><strong>Invariance par transformation</strong> : La <span
class="math inline">\(\gamma\)</span>-divergence est invariante sous les
transformations mesurables.</p></li>
<li><p><strong>Convexité</strong> : La <span
class="math inline">\(\gamma\)</span>-divergence est une fonction
convexe de <span class="math inline">\(Q\)</span> pour <span
class="math inline">\(P\)</span> fixé.</p></li>
<li><p><strong>Continuité</strong> : La <span
class="math inline">\(\gamma\)</span>-divergence est continue par
rapport à la convergence faible des mesures.</p></li>
</ol>
<p>Pour prouver la convexité de la <span
class="math inline">\(\gamma\)</span>-divergence, nous utilisons les
propriétés des fonctions convexes et les inégalités de Jensen.</p>
<div class="preuve">
<p>Soient <span class="math inline">\(Q_1\)</span> et <span
class="math inline">\(Q_2\)</span> deux mesures, et <span
class="math inline">\(\lambda \in [0,1]\)</span>. Nous voulons montrer
que : <span class="math display">\[D_{\gamma}(\lambda Q_1 + (1 -
\lambda) Q_2 \| P) \leq \lambda D_{\gamma}(Q_1 \| P) + (1 - \lambda)
D_{\gamma}(Q_2 \| P).\]</span></p>
<p>En utilisant la définition de la <span
class="math inline">\(\gamma\)</span>-divergence et les propriétés des
intégrales, nous pouvons montrer que cette inégalité tient par
convexité.</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La <span class="math inline">\(\gamma\)</span>-divergence est une
généralisation puissante et robuste de la divergence classique, offrant
des outils précieux pour l’analyse des mesures et des distributions. Ses
propriétés et ses applications en font un sujet de recherche actif et
prometteur.</p>
</body>
</html>
{% include "footer.html" %}


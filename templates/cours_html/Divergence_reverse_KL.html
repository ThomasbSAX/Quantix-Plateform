{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Kullback-Leibler inverse : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Kullback-Leibler inverse : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Kullback-Leibler (KL), introduite par Solomon
Kullback et Richard Leibler en 1951, est une mesure fondamentale de
l’information dans la théorie des probabilités et de l’apprentissage
automatique. Elle quantifie la différence entre deux distributions de
probabilité, souvent utilisée pour évaluer la performance des modèles
statistiques.</p>
<p>Cependant, dans certains contextes, notamment en apprentissage
profond et en optimisation stochastique, une variante de la divergence
KL, appelée divergence reverse KL (ou KL inverse), s’est révélée
particulièrement utile. Cette divergence inverse présente des propriétés
uniques qui la rendent adaptée à certaines tâches d’apprentissage
automatique, notamment dans les réseaux de neurones génératifs et les
méthodes de Monte Carlo.</p>
<p>Dans cet article, nous explorons en profondeur la divergence reverse
KL, ses définitions mathématiques, ses théorèmes associés et ses
applications pratiques. Nous commençons par une introduction historique
et conceptuelle, suivie d’une formalisation rigoureuse de la divergence.
Ensuite, nous présentons les théorèmes clés liés à cette divergence et
fournissons des preuves détaillées. Enfin, nous discutons des propriétés
et corollaires importants qui émergent de cette théorie.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence reverse KL, il est essentiel de
commencer par la définition classique de la divergence de
Kullback-Leibler. La divergence KL entre deux distributions de
probabilité <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes ou continues définies sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. La divergence de
Kullback-Leibler de <span class="math inline">\(Q\)</span> par rapport à
<span class="math inline">\(P\)</span> est définie par : <span
class="math display">\[D_{\text{KL}}(Q \| P) = \sum_{x \in \mathcal{X}}
Q(x) \log \left( \frac{Q(x)}{P(x)} \right)\]</span> pour des
distributions discrètes, et <span class="math display">\[D_{\text{KL}}(Q
\| P) = \int_{\mathcal{X}} Q(x) \log \left( \frac{Q(x)}{P(x)} \right)
dx\]</span> pour des distributions continues.</p>
</div>
<p>La divergence reverse KL, quant à elle, inverse l’ordre des
distributions dans la définition classique. Elle est définie comme suit
:</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes ou continues définies sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. La divergence reverse KL de
<span class="math inline">\(Q\)</span> par rapport à <span
class="math inline">\(P\)</span> est définie par : <span
class="math display">\[D_{\text{reverse KL}}(P \| Q) = \sum_{x \in
\mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</span> pour
des distributions discrètes, et <span
class="math display">\[D_{\text{reverse KL}}(P \| Q) =
\int_{\mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
dx\]</span> pour des distributions continues.</p>
</div>
<p>Il est important de noter que la divergence reverse KL n’est pas
symétrique, c’est-à-dire que <span
class="math inline">\(D_{\text{reverse KL}}(P \| Q) \neq
D_{\text{reverse KL}}(Q \| P)\)</span>. De plus, elle n’est pas non plus
une distance au sens mathématique, car elle ne satisfait pas l’inégalité
triangulaire.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons quelques théorèmes clés liés à la
divergence reverse KL. Ces théorèmes sont essentiels pour comprendre les
propriétés et les applications de cette divergence.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. Alors, pour toute fonction
mesurable et bornée <span class="math inline">\(f : \mathcal{X}
\rightarrow \mathbb{R}\)</span>, on a : <span
class="math display">\[\mathbb{E}_P[f] - \log \left( \mathbb{E}_Q[e^f]
\right) \leq D_{\text{reverse KL}}(P \| Q)\]</span> où <span
class="math inline">\(\mathbb{E}_P\)</span> et <span
class="math inline">\(\mathbb{E}_Q\)</span> désignent les espérances
sous les distributions <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur l’inégalité de
Gibbs, qui est une conséquence directe de la définition de la divergence
reverse KL. En effet, en utilisant les propriétés des logarithmes et des
espérances, on peut montrer que : <span
class="math display">\[\mathbb{E}_P[f] - \log \left( \mathbb{E}_Q[e^f]
\right) = \int_{\mathcal{X}} P(x) f(x) dx - \log \left(
\int_{\mathcal{X}} Q(x) e^{f(x)} dx \right)\]</span> En appliquant
l’inégalité de Jensen à la fonction <span
class="math inline">\(\log\)</span>, on obtient : <span
class="math display">\[\log \left( \int_{\mathcal{X}} Q(x) e^{f(x)} dx
\right) \leq \int_{\mathcal{X}} Q(x) f(x) dx\]</span> En réarrangeant
les termes, on obtient : <span class="math display">\[\mathbb{E}_P[f] -
\log \left( \mathbb{E}_Q[e^f] \right) \leq \int_{\mathcal{X}} P(x) f(x)
dx - \int_{\mathcal{X}} Q(x) f(x) dx\]</span> En utilisant la définition
de la divergence reverse KL, on peut montrer que : <span
class="math display">\[\int_{\mathcal{X}} P(x) f(x) dx -
\int_{\mathcal{X}} Q(x) f(x) dx = D_{\text{reverse KL}}(P \| Q)\]</span>
Ce qui achève la preuve. ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(P\)</span> une distribution de
probabilité fixe et soit <span class="math inline">\(Q\)</span> une
distribution de probabilité paramétrée par un vecteur <span
class="math inline">\(\theta\)</span>. La divergence reverse KL <span
class="math inline">\(D_{\text{reverse KL}}(P \| Q)\)</span> est
minimisée lorsque <span class="math inline">\(Q\)</span> est égal à
<span class="math inline">\(P\)</span>, c’est-à-dire lorsque <span
class="math inline">\(\theta = \theta^*\)</span> où <span
class="math inline">\(Q_{\theta^*} = P\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur les propriétés de
convexité de la divergence reverse KL. En effet, en utilisant le fait
que la fonction <span class="math inline">\(f(x) = x \log x\)</span> est
strictement convexe, on peut montrer que la divergence reverse KL est
une fonction convexe de <span class="math inline">\(Q\)</span>. Par
conséquent, elle admet un minimum unique.</p>
<p>Pour montrer que ce minimum est atteint lorsque <span
class="math inline">\(Q = P\)</span>, il suffit de vérifier que <span
class="math inline">\(D_{\text{reverse KL}}(P \| P) = 0\)</span>. En
effet, en utilisant la définition de la divergence reverse KL, on a :
<span class="math display">\[D_{\text{reverse KL}}(P \| P) =
\int_{\mathcal{X}} P(x) \log \left( \frac{P(x)}{P(x)} \right) dx =
\int_{\mathcal{X}} P(x) \log(1) dx = 0\]</span> Puisque la divergence
reverse KL est convexe et atteint sa valeur minimale lorsque <span
class="math inline">\(Q = P\)</span>, le théorème est prouvé. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés et
corollaires importants liés à la divergence reverse KL. Ces résultats
sont essentiels pour comprendre les applications pratiques de cette
divergence.</p>
<div class="proposition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace mesurable <span
class="math inline">\(\mathcal{X}\)</span>. Les propriétés suivantes
sont satisfaites :</p>
<ol>
<li><p>Non-négativité : <span class="math inline">\(D_{\text{reverse
KL}}(P \| Q) \geq 0\)</span>.</p></li>
<li><p>Minimisation : <span class="math inline">\(D_{\text{reverse
KL}}(P \| Q) = 0\)</span> si et seulement si <span
class="math inline">\(P = Q\)</span>.</p></li>
<li><p>Invariance par transformation : Si <span class="math inline">\(T
: \mathcal{X} \rightarrow \mathcal{Y}\)</span> est une transformation
mesurable et bijective, alors <span
class="math inline">\(D_{\text{reverse KL}}(P \| Q) = D_{\text{reverse
KL}}(T_\# P \| T_\# Q)\)</span>, où <span class="math inline">\(T_\#
P\)</span> et <span class="math inline">\(T_\# Q\)</span> désignent les
images de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> par la transformation <span
class="math inline">\(T\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ces propriétés repose sur les
définitions et les théorèmes précédents. Nous allons les prouver une par
une.</p>
<ol>
<li><p>Non-négativité : La non-négativité de la divergence reverse KL
découle directement de l’inégalité de Gibbs. En effet, en prenant <span
class="math inline">\(f(x) = 0\)</span>, on obtient : <span
class="math display">\[D_{\text{reverse KL}}(P \| Q) \geq 0\]</span> Ce
qui prouve la non-négativité.</p></li>
<li><p>Minimisation : La minimisation de la divergence reverse KL a été
prouvée dans le théorème précédent. En effet, nous avons montré que
<span class="math inline">\(D_{\text{reverse KL}}(P \| Q) = 0\)</span>
si et seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p>Invariance par transformation : L’invariance par transformation
découle du fait que la divergence reverse KL est définie en termes
d’intégrales et de logarithmes, qui sont invariants par transformations
mesurables. En effet, si <span class="math inline">\(T : \mathcal{X}
\rightarrow \mathcal{Y}\)</span> est une transformation mesurable et
bijective, alors : <span class="math display">\[D_{\text{reverse KL}}(P
\| Q) = \int_{\mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx
= \int_{\mathcal{Y}} T_\# P(y) \log \left( \frac{T_\# P(y)}{T_\# Q(y)}
\right) dy = D_{\text{reverse KL}}(T_\# P \| T_\# Q)\]</span> Ce qui
prouve l’invariance par transformation.</p></li>
</ol>
<p> ◻</p>
</div>
<div class="corollaire">
<p>La divergence reverse KL est souvent utilisée dans les algorithmes
d’apprentissage automatique pour minimiser la différence entre une
distribution cible <span class="math inline">\(P\)</span> et une
distribution paramétrée <span class="math inline">\(Q\)</span>. En
particulier, elle est utilisée dans les réseaux de neurones génératifs
et les méthodes de Monte Carlo pour améliorer la stabilité et la
convergence des algorithmes.</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré en profondeur la divergence
reverse KL, ses définitions mathématiques, ses théorèmes associés et ses
applications pratiques. Nous avons commencé par une introduction
historique et conceptuelle, suivie d’une formalisation rigoureuse de la
divergence. Ensuite, nous avons présenté les théorèmes clés liés à cette
divergence et fourni des preuves détaillées. Enfin, nous avons discuté
des propriétés et corollaires importants qui émergent de cette
théorie.</p>
<p>La divergence reverse KL est un outil puissant dans le domaine de
l’apprentissage automatique et des probabilités. Ses propriétés uniques
en font une mesure précieuse pour évaluer la performance des modèles
statistiques et optimiser les algorithmes d’apprentissage. Nous espérons
que cet article a fourni une compréhension approfondie de cette
divergence et de ses applications potentielles.</p>
</body>
</html>
{% include "footer.html" %}


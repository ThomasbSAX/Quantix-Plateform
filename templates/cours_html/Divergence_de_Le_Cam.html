{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Le Cam : Une mesure de la distance entre lois de probabilité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Le Cam : Une mesure de la distance entre
lois de probabilité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Le Cam émerge dans le cadre des statistiques
asymptotiques, où elle joue un rôle crucial dans la théorie de
l’estimation et du test d’hypothèses. Introduite par Lucien Le Cam,
cette notion permet de mesurer la distance entre deux lois de
probabilité, offrant ainsi un outil puissant pour comparer des modèles
statistiques.</p>
<p>L’importance de la divergence de Le Cam réside dans sa capacité à
capturer les différences essentielles entre deux lois, même lorsque
celles-ci sont proches. Elle est particulièrement utile dans les
contextes où les méthodes classiques de comparaison, comme la divergence
de Kullback-Leibler, peuvent être insuffisantes ou difficiles à
calculer.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Le Cam, commençons par comprendre ce
que nous cherchons à mesurer. Imaginons deux lois de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un espace mesurable <span
class="math inline">\((\mathcal{X}, \mathcal{A})\)</span>. Nous voulons
quantifier à quel point ces deux lois sont différentes, en tenant compte
non seulement de leurs valeurs ponctuelles mais aussi de leur
comportement global.</p>
<p>La divergence de Le Cam est définie comme suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux lois de probabilité sur un espace
mesurable <span class="math inline">\((\mathcal{X},
\mathcal{A})\)</span>. La divergence de Le Cam entre <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_L(P, Q) = \sup_{A \in \mathcal{A}} \left( P(A)
- Q(A) \right).\]</span></p>
</div>
<p>Cette définition peut être réécrite de plusieurs manières
équivalentes :</p>
<p><span class="math display">\[D_L(P, Q) = \sup_{A \in \mathcal{A}}
P(A) - \inf_{A \in \mathcal{A}} Q(A),\]</span></p>
<p>ou encore,</p>
<p><span class="math display">\[D_L(P, Q) = \sup_{A \in \mathcal{A}}
P(A) - \inf_{B \in \mathcal{A}} Q(B).\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Le Cam est le suivant
:</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(P_n\)</span> et <span
class="math inline">\(Q_n\)</span> deux suites de lois de probabilité
sur un espace mesurable <span class="math inline">\((\mathcal{X},
\mathcal{A})\)</span>. Si la divergence de Le Cam entre <span
class="math inline">\(P_n\)</span> et <span
class="math inline">\(Q_n\)</span> tend vers zéro lorsque <span
class="math inline">\(n\)</span> tend vers l’infini, alors les suites
<span class="math inline">\(P_n\)</span> et <span
class="math inline">\(Q_n\)</span> sont asymptotiquement
équivalentes.</p>
</div>
<p>Ce théorème peut être formulé de manière plus précise comme suit
:</p>
<p><span class="math display">\[\lim_{n \to \infty} D_L(P_n, Q_n) = 0
\implies P_n \text{ et } Q_n \text{ sont asymptotiquement
équivalentes.}\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Le Cam, nous procédons par étapes :</p>
<div class="proof">
<p><em>Proof.</em> 1. Supposons que <span class="math inline">\(\lim_{n
\to \infty} D_L(P_n, Q_n) = 0\)</span>. Cela signifie que pour tout
<span class="math inline">\(\epsilon &gt; 0\)</span>, il existe un
entier <span class="math inline">\(N\)</span> tel que pour tout <span
class="math inline">\(n \geq N\)</span>, nous avons <span
class="math inline">\(D_L(P_n, Q_n) &lt; \epsilon\)</span>.</p>
<p>2. Considérons un ensemble <span class="math inline">\(A \in
\mathcal{A}\)</span>. Nous avons : <span class="math display">\[P_n(A) -
Q_n(A) \leq D_L(P_n, Q_n) &lt; \epsilon.\]</span></p>
<p>3. De même, pour tout ensemble <span class="math inline">\(B \in
\mathcal{A}\)</span>, nous avons : <span class="math display">\[Q_n(B) -
P_n(B) \leq D_L(Q_n, P_n) &lt; \epsilon.\]</span></p>
<p>4. En combinant ces deux inégalités, nous obtenons : <span
class="math display">\[|P_n(A) - Q_n(A)| &lt; \epsilon.\]</span></p>
<p>5. Puisque <span class="math inline">\(A\)</span> est arbitraire,
cela montre que les suites <span class="math inline">\(P_n\)</span> et
<span class="math inline">\(Q_n\)</span> sont asymptotiquement proches
pour toute mesure de probabilité. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence de Le Cam possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p>La divergence de Le Cam est toujours comprise entre 0 et 1 :
<span class="math display">\[0 \leq D_L(P, Q) \leq 1.\]</span></p></li>
<li><p>La divergence de Le Cam est symétrique : <span
class="math display">\[D_L(P, Q) = D_L(Q, P).\]</span></p></li>
<li><p>Si <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des lois discrètes, la divergence
de Le Cam peut être exprimée en termes des probabilités ponctuelles :
<span class="math display">\[D_L(P, Q) = \max_{x \in \mathcal{X}} (P(x)
- Q(x)).\]</span></p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Le Cam est un outil puissant pour comparer des lois
de probabilité, offrant une mesure de la distance entre deux
distributions. Son utilisation dans les statistiques asymptotiques
permet de mieux comprendre les propriétés des estimateurs et des tests
d’hypothèses, rendant cette notion indispensable dans le domaine de la
théorie statistique.</p>
</body>
</html>
{% include "footer.html" %}


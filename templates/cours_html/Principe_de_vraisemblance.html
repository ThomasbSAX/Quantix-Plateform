{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Le Principe de Vraisemblance : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Le Principe de Vraisemblance : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le principe de vraisemblance émerge comme une réponse élégante aux
défis posés par l’estimation statistique. Dans un contexte où les
données sont soumises à des aléas, ce principe propose une méthode pour
identifier les paramètres qui maximisent la probabilité d’observer ces
données. Historiquement, ce concept a été formalisé par Ronald A. Fisher
dans les années 1920, révolutionnant ainsi la théorie statistique.</p>
<p>Pourquoi ce principe est-il indispensable ? Il offre un cadre
rigoureux pour l’inférence statistique, permettant de tirer des
conclusions robustes à partir de données souvent bruitées. Dans ce
chapitre, nous explorerons les fondements mathématiques du principe de
vraisemblance, ses théorèmes clés et ses applications pratiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant d’énoncer formellement le principe de vraisemblance, il est
essentiel de comprendre ce que nous cherchons à maximiser. Supposons que
nous disposions d’un échantillon de données <span
class="math inline">\(X = \{x_1, x_2, \ldots, x_n\}\)</span> et que nous
soupçonnons que ces données proviennent d’une distribution de
probabilité <span class="math inline">\(P_\theta\)</span> paramétrée par
un vecteur <span class="math inline">\(\theta\)</span>. Notre objectif
est de déterminer la valeur de <span
class="math inline">\(\theta\)</span> qui rend les données observées les
plus probables.</p>
<p>Formellement, nous cherchons à maximiser la fonction de vraisemblance
<span class="math inline">\(L(\theta; X)\)</span>, définie comme suit
:</p>
<p><span class="math display">\[L(\theta; X) = P_\theta(X) =
\prod_{i=1}^n p_\theta(x_i)\]</span></p>
<p>où <span class="math inline">\(p_\theta(x_i)\)</span> représente la
densité de probabilité de <span class="math inline">\(x_i\)</span> sous
le paramètre <span class="math inline">\(\theta\)</span>.</p>
<p>Pour simplifier les calculs, il est souvent préférable de travailler
avec le logarithme de la fonction de vraisemblance, appelé score :</p>
<p><span class="math display">\[\ell(\theta; X) = \log L(\theta; X) =
\sum_{i=1}^n \log p_\theta(x_i)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux liés au principe de vraisemblance est
le théorème de la consistance de l’estimateur du maximum de
vraisemblance. Ce théorème garantit que sous certaines conditions,
l’estimateur du maximum de vraisemblance converge vers le vrai paramètre
lorsque la taille de l’échantillon tend vers l’infini.</p>
<p><strong>Théorème 1 (Consistance de l’estimateur du maximum de
vraisemblance)</strong></p>
<p>Soit <span class="math inline">\(\theta_0\)</span> le vrai paramètre
et <span class="math inline">\(\hat{\theta}_n\)</span> l’estimateur du
maximum de vraisemblance basé sur un échantillon de taille <span
class="math inline">\(n\)</span>. Si les conditions régularité sont
satisfaites, alors :</p>
<p><span class="math display">\[\hat{\theta}_n \xrightarrow{P} \theta_0
\quad \text{quand} \quad n \to \infty\]</span></p>
<p>où <span class="math inline">\(\xrightarrow{P}\)</span> désigne la
convergence en probabilité.</p>
<p><strong>Démonstration</strong></p>
<p>La démonstration de ce théorème repose sur plusieurs étapes clés.
Tout d’abord, nous utilisons le fait que la fonction de vraisemblance
<span class="math inline">\(L(\theta; X)\)</span> est une suite de
variables aléatoires. Ensuite, nous appliquons le théorème central
limite pour montrer que la distribution de <span
class="math inline">\(\hat{\theta}_n\)</span> devient de plus en plus
concentrée autour de <span class="math inline">\(\theta_0\)</span>
lorsque <span class="math inline">\(n\)</span> augmente. Enfin, nous
utilisons des arguments de continuité pour conclure la convergence en
probabilité.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer les preuves détaillées, considérons un exemple simple
où <span class="math inline">\(X\)</span> suit une distribution normale
de moyenne <span class="math inline">\(\mu\)</span> et de variance
connue <span class="math inline">\(\sigma^2\)</span>. La fonction de
vraisemblance s’écrit :</p>
<p><span class="math display">\[L(\mu; X) = \prod_{i=1}^n
\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i -
\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>Le score est alors :</p>
<p><span class="math display">\[\ell(\mu; X) = -\frac{n}{2}
\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i -
\mu)^2\]</span></p>
<p>Pour maximiser ce score, nous cherchons la valeur de <span
class="math inline">\(\mu\)</span> qui minimise la somme des carrés des
écarts. En prenant la dérivée par rapport à <span
class="math inline">\(\mu\)</span> et en égalant à zéro, nous obtenons
:</p>
<p><span class="math display">\[\frac{d\ell}{d\mu} = \frac{1}{\sigma^2}
\sum_{i=1}^n (x_i - \mu) = 0\]</span></p>
<p>Ce qui conduit à l’estimateur du maximum de vraisemblance :</p>
<p><span class="math display">\[\hat{\mu} = \frac{1}{n} \sum_{i=1}^n
x_i\]</span></p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’estimateur du maximum de vraisemblance possède plusieurs propriétés
remarquables :</p>
<ol>
<li><p><strong>Invariance</strong> : Si <span
class="math inline">\(g\)</span> est une fonction différentiable, alors
l’estimateur du maximum de vraisemblance pour <span
class="math inline">\(g(\theta)\)</span> est <span
class="math inline">\(g(\hat{\theta})\)</span>.</p></li>
<li><p><strong>Efficacité</strong> : Sous certaines conditions,
l’estimateur du maximum de vraisemblance atteint la borne de Cramér-Rao,
ce qui signifie qu’il est asymptotiquement efficace.</p></li>
<li><p><strong>Normalité Asymptotique</strong> : L’estimateur du maximum
de vraisemblance est asymptotiquement normal, c’est-à-dire que :</p>
<p><span class="math display">\[\sqrt{n} (\hat{\theta}_n - \theta_0)
\xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})\]</span></p>
<p>où <span class="math inline">\(I(\theta_0)\)</span> est l’information
de Fisher.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le principe de vraisemblance est un outil puissant et polyvalent en
statistique. Il fournit une méthode systématique pour estimer les
paramètres d’un modèle à partir de données observées. Ses propriétés
théoriques robustes et ses applications pratiques en font un pilier de
la théorie statistique moderne.</p>
</body>
</html>
{% include "footer.html" %}


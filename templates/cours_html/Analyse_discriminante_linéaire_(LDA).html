{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Analyse Discriminante Linéaire (LDA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analyse Discriminante Linéaire (LDA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’analyse discriminante linéaire (LDA) est une technique statistique
puissante utilisée pour la classification et la réduction de dimension.
Son origine remonte aux travaux de Ronald Fisher en 1936, où il
introduisit une méthode pour discriminer entre plusieurs classes de
données. Le LDA est indispensable dans des domaines tels que la
reconnaissance de motifs, le traitement d’images, et l’apprentissage
automatique.</p>
<p>Le LDA émerge comme une solution pour répondre à des problèmes de
classification où les données sont représentées par plusieurs variables.
Il cherche à trouver une projection linéaire des données qui maximise la
séparation entre les classes tout en minimisant la variance
intra-classe. Cette approche est particulièrement utile lorsque les
données sont de haute dimension et que l’on souhaite réduire leur
complexité tout en conservant les informations discriminantes.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de formaliser le LDA, il est important de comprendre ce que
l’on cherche à accomplir. Supposons que nous ayons un ensemble de
données avec plusieurs classes. Notre objectif est de trouver une
projection linéaire qui maximise la séparation entre ces classes.</p>
<p>Pour ce faire, nous devons définir quelques notions clés. Soit <span
class="math inline">\(X\)</span> une matrice de données de taille <span
class="math inline">\(n \times p\)</span>, où <span
class="math inline">\(n\)</span> est le nombre d’observations et <span
class="math inline">\(p\)</span> est le nombre de variables. Soit <span
class="math inline">\(y\)</span> un vecteur de taille <span
class="math inline">\(n\)</span> contenant les étiquettes des
classes.</p>
<p>La matrice de covariance intra-classe <span
class="math inline">\(S_W\)</span> est définie comme la somme des
matrices de covariance de chaque classe. Elle mesure la variance des
données au sein de chaque classe.</p>
<p><span class="math display">\[S_W = \sum_{i=1}^{c} S_i\]</span></p>
<p>où <span class="math inline">\(S_i\)</span> est la matrice de
covariance de la classe <span class="math inline">\(i\)</span>, et <span
class="math inline">\(c\)</span> est le nombre total de classes.</p>
<p>La matrice de covariance inter-classe <span
class="math inline">\(S_B\)</span> est définie comme la somme des
matrices de covariance entre les classes. Elle mesure la variance des
données entre les différentes classes.</p>
<p><span class="math display">\[S_B = \sum_{i=1}^{c} n_i (\mu_i -
\mu)(\mu_i - \mu)^T\]</span></p>
<p>où <span class="math inline">\(n_i\)</span> est le nombre
d’observations dans la classe <span class="math inline">\(i\)</span>,
<span class="math inline">\(\mu_i\)</span> est le vecteur moyen de la
classe <span class="math inline">\(i\)</span>, et <span
class="math inline">\(\mu\)</span> est le vecteur moyen global.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Le théorème fondamental du LDA stipule que la projection linéaire
optimale qui maximise le rapport de la variance inter-classe à la
variance intra-classe est donnée par les vecteurs propres de la matrice
<span class="math inline">\(S_W^{-1} S_B\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une matrice de données et
<span class="math inline">\(y\)</span> un vecteur d’étiquettes. La
projection linéaire optimale <span class="math inline">\(W\)</span> qui
maximise le rapport <span class="math inline">\(\frac{\text{tr}(W^T S_B
W)}{\text{tr}(W^T S_W W)}\)</span> est donnée par les vecteurs propres
de la matrice <span class="math inline">\(S_W^{-1} S_B\)</span>.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous devons montrer que la projection <span
class="math inline">\(W\)</span> qui maximise le rapport de la variance
inter-classe à la variance intra-classe est effectivement donnée par les
vecteurs propres de <span class="math inline">\(S_W^{-1}
S_B\)</span>.</p>
<p>Considérons le rapport <span class="math inline">\(J(W) =
\frac{\text{tr}(W^T S_B W)}{\text{tr}(W^T S_W W)}\)</span>. Nous voulons
maximiser ce rapport par rapport à <span
class="math inline">\(W\)</span>.</p>
<p>En utilisant le théorème des multiplicateurs de Lagrange, nous
pouvons montrer que la solution optimale <span
class="math inline">\(W\)</span> satisfait l’équation :</p>
<p><span class="math display">\[S_B W = \lambda S_W W\]</span></p>
<p>où <span class="math inline">\(\lambda\)</span> est une
constante.</p>
<p>En réarrangeant cette équation, nous obtenons :</p>
<p><span class="math display">\[S_W^{-1} S_B W = \lambda W\]</span></p>
<p>Cela montre que <span class="math inline">\(W\)</span> est un vecteur
propre de la matrice <span class="math inline">\(S_W^{-1}
S_B\)</span>.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Le LDA possède plusieurs propriétés importantes qui en font une
méthode puissante pour la classification et la réduction de
dimension.</p>
<ol>
<li><p>Le LDA est une méthode supervisée, ce qui signifie qu’il
nécessite des étiquettes de classe pour les données
d’entraînement.</p></li>
<li><p>Le LDA suppose que les données suivent une distribution normale
multivariée et que les matrices de covariance des classes sont
égales.</p></li>
<li><p>Le LDA peut être utilisé pour la réduction de dimension en
projetant les données sur un sous-espace de dimension
inférieure.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>L’analyse discriminante linéaire est une technique puissante pour la
classification et la réduction de dimension. Elle repose sur des
concepts mathématiques solides et possède plusieurs propriétés
intéressantes qui en font une méthode incontournable dans de nombreux
domaines.</p>
</body>
</html>
{% include "footer.html" %}


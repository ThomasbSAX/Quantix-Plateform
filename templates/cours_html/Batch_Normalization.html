{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Batch Normalization : Une Révolution dans l’Entraînement des Réseaux de Neurones</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Batch Normalization : Une Révolution dans
l’Entraînement des Réseaux de Neurones</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entraînement des réseaux de neurones profonds a toujours été un
défi majeur dans le domaine de l’apprentissage automatique. Les
problèmes de disparition et d’explosion des gradients, ainsi que la
sensibilité aux hyperparamètres, ont longtemps limité les performances
de ces modèles. C’est dans ce contexte que la <em>Batch
Normalization</em> (BN), introduite par Sergey Ioffe et Christian
Szegedy en 2015, a révolutionné le paysage de l’apprentissage
profond.</p>
<p>La BN vise à résoudre ces problèmes en normalisant les activations
des couches intermédiaires du réseau. En d’autres termes, elle permet de
centrer et réduire les activations pour chaque mini-batch de données, ce
qui stabilise l’entraînement et accélère la convergence. Cette technique
s’est avérée si efficace qu’elle est devenue un composant standard dans
de nombreux architectures modernes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant d’aborder la formulation mathématique de la BN, il est
essentiel de comprendre son objectif principal. Supposons que nous ayons
un réseau de neurones avec plusieurs couches cachées. Pendant
l’entraînement, les activations des neurones dans ces couches peuvent
varier considérablement en fonction de la distribution des données
d’entrée et des poids du réseau. Cela peut entraîner des problèmes de
convergence lente ou même de non-convergence.</p>
<p>Pour résoudre ce problème, nous cherchons à normaliser les
activations de chaque couche de sorte que leur distribution moyenne soit
nulle et leur variance unitaire. Cela permet d’accélérer la convergence
du réseau en rendant l’entraînement moins sensible aux initialisations
des poids et aux hyperparamètres.</p>
<p>Formellement, pour une couche donnée avec un mini-batch de taille
<span class="math inline">\(m\)</span>, les activations <span
class="math inline">\(x^{(k)}\)</span> pour le <span
class="math inline">\(k\)</span>-ième exemple du mini-batch sont
normalisées comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{B} = \{x^{(1)}, x^{(2)},
\ldots, x^{(m)}\}\)</span> un mini-batch de taille <span
class="math inline">\(m\)</span>. La normalisation des activations pour
chaque composante <span class="math inline">\(x^{(k)}_j\)</span> (où
<span class="math inline">\(j\)</span> est l’index de la composante) est
donnée par :</p>
<p><span class="math display">\[\hat{x}^{(k)}_j = \frac{x^{(k)}_j -
\mu_\mathcal{B}}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}}\]</span></p>
<p>où <span class="math inline">\(\mu_\mathcal{B} = \frac{1}{m}
\sum_{i=1}^m x^{(i)}_j\)</span> est la moyenne du mini-batch, <span
class="math inline">\(\sigma^2_\mathcal{B} = \frac{1}{m} \sum_{i=1}^m
(x^{(i)}_j - \mu_\mathcal{B})^2\)</span> est la variance du mini-batch,
et <span class="math inline">\(\epsilon\)</span> est une petite
constante pour éviter la division par zéro.</p>
</div>
<p>Après la normalisation, nous introduisons des paramètres d’échelle
<span class="math inline">\(\gamma\)</span> et de décalage <span
class="math inline">\(\beta\)</span> pour permettre au réseau
d’apprendre les transformations nécessaires :</p>
<p><span class="math display">\[y^{(k)}_j = \gamma_j \hat{x}^{(k)}_j +
\beta_j\]</span></p>
<p>où <span class="math inline">\(\gamma_j\)</span> et <span
class="math inline">\(\beta_j\)</span> sont des paramètres appris
pendant l’entraînement.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>La BN repose sur plusieurs théorèmes et propriétés mathématiques qui
garantissent son efficacité. L’un des principaux avantages de la BN est
qu’elle permet au réseau de converger plus rapidement en réduisant les
problèmes d’optimisation.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> une fonction de perte et
<span class="math inline">\(W\)</span> les poids du réseau. La BN permet
de réduire la dépendance de <span class="math inline">\(f(W)\)</span>
par rapport à l’initialisation des poids, ce qui accélère la convergence
de l’algorithme d’optimisation.</p>
<p>Formellement, pour un mini-batch <span
class="math inline">\(\mathcal{B}\)</span>, la normalisation des
activations réduit la variance de la fonction de perte <span
class="math inline">\(f(W)\)</span> par rapport aux variations des poids
<span class="math inline">\(W\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver ce théorème, nous devons montrer que la BN réduit
effectivement la variance de la fonction de perte. Commençons par
rappeler que la normalisation des activations permet de centrer et
réduire les données.</p>
<p>Soit <span class="math inline">\(\mathcal{B} = \{x^{(1)}, x^{(2)},
\ldots, x^{(m)}\}\)</span> un mini-batch de taille <span
class="math inline">\(m\)</span>. La normalisation des activations est
donnée par :</p>
<p><span class="math display">\[\hat{x}^{(k)}_j = \frac{x^{(k)}_j -
\mu_\mathcal{B}}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}}\]</span></p>
<p>où <span class="math inline">\(\mu_\mathcal{B} = \frac{1}{m}
\sum_{i=1}^m x^{(i)}_j\)</span> et <span
class="math inline">\(\sigma^2_\mathcal{B} = \frac{1}{m} \sum_{i=1}^m
(x^{(i)}_j - \mu_\mathcal{B})^2\)</span>.</p>
<p>En appliquant cette transformation, nous obtenons des activations
normalisées avec une moyenne nulle et une variance unitaire. Cela permet
de réduire la dépendance des activations par rapport aux poids du
réseau, ce qui stabilise l’entraînement.</p>
<p>Pour prouver que la BN réduit la variance de la fonction de perte,
nous devons montrer que les gradients de la fonction de perte par
rapport aux poids sont plus stables. Cela peut être démontré en
utilisant le théorème central limite et les propriétés des gradients
dans les réseaux de neurones.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La BN possède plusieurs propriétés intéressantes qui en font une
technique puissante pour l’entraînement des réseaux de neurones.</p>
<ol>
<li><p><strong>Stabilisation des Gradients</strong> : La BN permet de
stabiliser les gradients pendant l’entraînement, ce qui réduit les
problèmes de disparition et d’explosion des gradients.</p></li>
<li><p><strong>Réduction de la Sensibilité aux Hyperparamètres</strong>
: La BN permet au réseau d’être moins sensible aux hyperparamètres, tels
que le taux d’apprentissage et l’initialisation des poids.</p></li>
<li><p><strong>Accélération de la Convergence</strong> : La BN permet au
réseau de converger plus rapidement en réduisant les problèmes
d’optimisation.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
résultats théoriques et empiriques présentés dans la littérature.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La Batch Normalization a révolutionné l’entraînement des réseaux de
neurones profonds en introduisant une méthode simple mais puissante pour
normaliser les activations des couches intermédiaires. Cette technique a
permis de résoudre plusieurs problèmes majeurs, tels que la disparition
et l’explosion des gradients, ainsi que la sensibilité aux
hyperparamètres. En conséquence, la BN est devenue un composant standard
dans de nombreuses architectures modernes.</p>
</body>
</html>
{% include "footer.html" %}


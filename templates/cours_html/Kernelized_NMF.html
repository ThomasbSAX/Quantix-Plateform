{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized Non-negative Matrix Factorization: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized Non-negative Matrix Factorization: A
Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Non-negative Matrix Factorization (NMF) has emerged as a powerful
tool for dimensionality reduction and feature extraction in various
domains, particularly in text mining, image processing, and
bioinformatics. The fundamental idea behind NMF is to decompose a
non-negative matrix into two lower-dimensional non-negative matrices,
capturing the underlying structure of the data. However, traditional NMF
suffers from limitations when dealing with non-linear relationships
within the data.</p>
<p>Kernel methods have been widely used to address non-linearity in
various machine learning tasks. By implicitly mapping the data into a
higher-dimensional feature space, kernel methods enable linear
operations in this transformed space to capture non-linear relationships
in the original input space. The marriage of NMF and kernel methods,
known as Kernelized NMF (KNMF), aims to harness the strengths of both
approaches.</p>
<p>In this article, we delve into the theoretical foundations of KNMF,
exploring its definitions, key theorems, and properties. We provide
detailed proofs and discuss the practical implications of this powerful
technique.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>To understand KNMF, we first need to establish some key
definitions.</p>
<h2 class="unnumbered"
id="non-negative-matrix-factorization-nmf">Non-negative Matrix
Factorization (NMF)</h2>
<p>Consider a non-negative matrix <span class="math inline">\(V \in
\mathbb{R}^{m \times n}\)</span>. The goal of NMF is to find two
non-negative matrices <span class="math inline">\(W \in \mathbb{R}^{m
\times k}\)</span> and <span class="math inline">\(H \in \mathbb{R}^{k
\times n}\)</span> such that: <span class="math display">\[V \approx
WH\]</span></p>
<p>This approximation is typically sought by minimizing the Frobenius
norm of the difference between <span class="math inline">\(V\)</span>
and <span class="math inline">\(WH\)</span>: <span
class="math display">\[\min_{W, H} \|V - WH\|_F^2\]</span> subject to
<span class="math inline">\(W \geq 0\)</span> and <span
class="math inline">\(H \geq 0\)</span>.</p>
<h2 class="unnumbered" id="kernel-methods">Kernel Methods</h2>
<p>A kernel function <span class="math inline">\(\kappa: \mathcal{X}
\times \mathcal{X} \rightarrow \mathbb{R}\)</span> is a function that
computes the inner product of two vectors in some (possibly
high-dimensional) feature space <span
class="math inline">\(\mathcal{F}\)</span>: <span
class="math display">\[\kappa(x, y) = \langle \phi(x), \phi(y)
\rangle\]</span> where <span class="math inline">\(\phi: \mathcal{X}
\rightarrow \mathcal{F}\)</span> is a feature map.</p>
<h2 class="unnumbered" id="kernelized-nmf-knmf">Kernelized NMF
(KNMF)</h2>
<p>The idea behind KNMF is to apply NMF in the feature space induced by
a kernel function. Given a kernel matrix <span class="math inline">\(K
\in \mathbb{R}^{n \times n}\)</span> where <span
class="math inline">\(K_{ij} = \kappa(x_i, x_j)\)</span>, we seek
non-negative matrices <span class="math inline">\(A \in \mathbb{R}^{n
\times k}\)</span> and <span class="math inline">\(B \in \mathbb{R}^{k
\times n}\)</span> such that: <span class="math display">\[K \approx
AB\]</span></p>
<p>This can be formulated as the optimization problem: <span
class="math display">\[\min_{A, B} \|K - AB\|_F^2\]</span> subject to
<span class="math inline">\(A \geq 0\)</span> and <span
class="math inline">\(B \geq 0\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>In this section, we present some key theorems related to KNMF.</p>
<h2 class="unnumbered"
id="theorem-1-existence-of-non-negative-solutions">Theorem 1: Existence
of Non-negative Solutions</h2>
<p>Under certain conditions, the optimization problem for KNMF admits
non-negative solutions.</p>
<div class="theorem">
<p>Let <span class="math inline">\(K\)</span> be a positive
semi-definite kernel matrix. Then, there exist non-negative matrices
<span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> such that: <span
class="math display">\[K = AB\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof follows from the fact that <span
class="math inline">\(K\)</span> is positive semi-definite and can be
decomposed into a product of non-negative matrices. This is a
consequence of the spectral theorem for positive semi-definite matrices
and the non-negativity constraints. ◻</p>
</div>
<h2 class="unnumbered"
id="theorem-2-convergence-of-multiplicative-updates">Theorem 2:
Convergence of Multiplicative Updates</h2>
<p>The multiplicative update rules for KNMF converge to a stationary
point.</p>
<div class="theorem">
<p>Let <span class="math inline">\(A^{(t)}\)</span> and <span
class="math inline">\(B^{(t)}\)</span> denote the matrices at iteration
<span class="math inline">\(t\)</span>. The multiplicative update rules:
<span class="math display">\[A_{ik}^{(t+1)} = A_{ik}^{(t)} \frac{(K
B^{(t)})_{ik}}{(A^{(t)} (B^{(t)})^T B^{(t)})_{ik}}\]</span> <span
class="math display">\[B_{kj}^{(t+1)} = B_{kj}^{(t)} \frac{(A^{(t)T}
K)_{kj}}{(A^{(t)T} A^{(t)} B^{(t)})_{kj}}\]</span> converge to a
stationary point of the objective function.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof relies on showing that each update step
reduces the objective function and that the sequence of updates is
bounded. This follows from the properties of multiplicative update rules
in NMF and their extension to KNMF. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>In this section, we provide detailed proofs for the key theorems
presented earlier.</p>
<h2 class="unnumbered" id="proof-of-theorem-1">Proof of Theorem 1</h2>
<p>To prove the existence of non-negative solutions for KNMF, we start
by noting that <span class="math inline">\(K\)</span> is a positive
semi-definite matrix. By the spectral theorem, there exists an
orthogonal matrix <span class="math inline">\(U\)</span> and a diagonal
matrix <span class="math inline">\(\Sigma\)</span> with non-negative
entries such that: <span class="math display">\[K = U \Sigma
U^T\]</span></p>
<p>We can then express <span class="math inline">\(U\)</span> and <span
class="math inline">\(\Sigma\)</span> in terms of non-negative matrices.
Let <span class="math inline">\(U = Q R\)</span>, where <span
class="math inline">\(Q\)</span> is a non-negative matrix obtained by
taking the absolute values of the columns of <span
class="math inline">\(U\)</span>, and <span
class="math inline">\(R\)</span> is a diagonal matrix with the signs of
the entries of <span class="math inline">\(U\)</span>. Then: <span
class="math display">\[K = Q R \Sigma R^T Q^T\]</span></p>
<p>Since <span class="math inline">\(R \Sigma R^T\)</span> is a
non-negative matrix, we can write: <span class="math display">\[K = (Q R
\sqrt{\Sigma}) (\sqrt{\Sigma} R^T Q^T)\]</span></p>
<p>Let <span class="math inline">\(A = Q R \sqrt{\Sigma}\)</span> and
<span class="math inline">\(B = \sqrt{\Sigma} R^T Q^T\)</span>. Then,
<span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are non-negative matrices such that:
<span class="math display">\[K = AB\]</span></p>
<h2 class="unnumbered" id="proof-of-theorem-2">Proof of Theorem 2</h2>
<p>To prove the convergence of the multiplicative update rules, we
consider the auxiliary function approach. For each entry <span
class="math inline">\((A B)_{ij}\)</span>, we define an auxiliary
function <span class="math inline">\(G(A, A&#39;)\)</span> such that:
<span class="math display">\[G(A, A&#39;) \geq (A B)_{ij}\]</span> and
<span class="math display">\[G(A, A) = (A B)_{ij}\]</span></p>
<p>The multiplicative update rule ensures that: <span
class="math display">\[G(A^{(t+1)}, A^{(t)}) \leq G(A^{(t)},
A^{(t)})\]</span> which implies that the objective function is
non-increasing. Since the objective function is bounded below, the
sequence of updates must converge to a stationary point.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>In this section, we list some key properties and corollaires of
KNMF.</p>
<h2 class="unnumbered" id="property-1-dimensionality-reduction">Property
1: Dimensionality Reduction</h2>
<p>KNMF can be used for dimensionality reduction by projecting the data
into a lower-dimensional space spanned by the columns of <span
class="math inline">\(A\)</span>.</p>
<h2 class="unnumbered" id="property-2-feature-extraction">Property 2:
Feature Extraction</h2>
<p>The rows of <span class="math inline">\(B\)</span> can be interpreted
as features extracted from the kernel matrix, capturing the underlying
structure of the data.</p>
<h2 class="unnumbered" id="property-3-sparsity">Property 3:
Sparsity</h2>
<p>By imposing additional constraints, such as <span
class="math inline">\(l_1\)</span>-norm regularization, KNMF can produce
sparse solutions, which are often more interpretable.</p>
<h2 class="unnumbered"
id="corollaire-1-generalization-to-other-kernels">Corollaire 1:
Generalization to Other Kernels</h2>
<p>The framework of KNMF can be generalized to other kernel functions,
including polynomial kernels and radial basis function (RBF)
kernels.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Kernelized Non-negative Matrix Factorization is a powerful technique
for capturing non-linear relationships in data while preserving the
interpretability of NMF. By leveraging kernel methods, KNMF extends the
applicability of NMF to a wider range of problems. The theoretical
foundations, as presented in this article, provide a solid basis for
further research and practical applications.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Random Fourier Features: A Bridge Between Kernel Methods and Neural Networks</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Random Fourier Features: A Bridge Between Kernel
Methods and Neural Networks</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-and-motivations">Introduction
and Motivations</h1>
<p>The field of machine learning has witnessed a remarkable evolution
over the past decades, with kernel methods and neural networks emerging
as two prominent paradigms. Kernel methods, rooted in statistical
learning theory, offer a principled approach to nonlinear classification
and regression tasks. On the other hand, neural networks, inspired by
biological systems, have demonstrated unprecedented success in various
domains, particularly in deep learning.</p>
<p>The Random Fourier Features (RFF) method, introduced by Rahimi and
Recht <span class="citation" data-cites="rahimi2007random"></span>,
bridges these two paradigms. It provides a way to efficiently
approximate shift-invariant kernel functions using random projections,
thereby enabling the scalability of kernel methods to large datasets.
This approach is particularly appealing as it allows for the use of
linear learning algorithms in a high-dimensional feature space, induced
by the kernel.</p>
<p>In this article, we delve into the mathematical foundations of Random
Fourier Features. We begin by exploring the motivations behind this
method, followed by a rigorous definition and an analysis of its
properties. We then proceed to discuss the theoretical guarantees
provided by this approach, culminating in a detailed proof of its
effectiveness.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>Before delving into the Random Fourier Features method, it is
essential to establish some foundational concepts.</p>
<h2 class="unnumbered" id="shift-invariant-kernels">Shift-Invariant
Kernels</h2>
<p>Consider a kernel function <span class="math inline">\(k:
\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\)</span>. A
kernel is said to be shift-invariant if there exists a function <span
class="math inline">\(\kappa: \mathbb{R}^d \rightarrow
\mathbb{R}\)</span> such that for all <span class="math inline">\(x, y
\in \mathbb{R}^d\)</span>,</p>
<p><span class="math display">\[k(x, y) = \kappa(x - y).\]</span></p>
<p>This property implies that the kernel’s value depends only on the
difference between its inputs, making it invariant to translations.</p>
<h2 class="unnumbered" id="fourier-transform-of-kernels">Fourier
Transform of Kernels</h2>
<p>For a shift-invariant kernel <span class="math inline">\(k\)</span>,
the Fourier transform of its corresponding function <span
class="math inline">\(\kappa\)</span> plays a crucial role. The Fourier
transform <span class="math inline">\(\hat{\kappa}: \mathbb{R}^d
\rightarrow \mathbb{C}\)</span> is defined as:</p>
<p><span class="math display">\[\hat{\kappa}(\omega) =
\int_{\mathbb{R}^d} \kappa(x) e^{-i \omega^T x} \, dx,\]</span></p>
<p>where <span class="math inline">\(\omega \in \mathbb{R}^d\)</span> is
the frequency variable. The Fourier transform provides a way to analyze
the kernel in the frequency domain, revealing its spectral
properties.</p>
<h2 class="unnumbered" id="random-fourier-features">Random Fourier
Features</h2>
<p>The Random Fourier Features method leverages the Fourier transform to
approximate shift-invariant kernels. Specifically, for a given kernel
<span class="math inline">\(k\)</span> with corresponding function <span
class="math inline">\(\kappa\)</span>, the method approximates <span
class="math inline">\(k(x, y)\)</span> using a finite set of random
features.</p>
<p>Let <span class="math inline">\(\{ \omega_j \}_{j=1}^m\)</span> be a
set of random frequencies drawn from the distribution <span
class="math inline">\(p(\omega) =
\frac{\hat{\kappa}(\omega)}{\pi}\)</span>, and let <span
class="math inline">\(\{ b_j \}_{j=1}^m\)</span> be a set of random
phases drawn uniformly from <span class="math inline">\([0,
2\pi]\)</span>. The Random Fourier Features approximation of the kernel
is then given by:</p>
<p><span class="math display">\[k(x, y) \approx \phi(x)^T
\phi(y),\]</span></p>
<p>where the feature map <span class="math inline">\(\phi: \mathbb{R}^d
\rightarrow \mathbb{R}^m\)</span> is defined as:</p>
<p><span class="math display">\[\phi(x) = \frac{1}{\sqrt{m}} \left[
\cos(\omega_1^T x + b_1), \cos(\omega_2^T x + b_2), \ldots,
\cos(\omega_m^T x + b_m) \right]^T.\]</span></p>
<p>This approximation allows for the efficient computation of kernel
functions, enabling the scalability of kernel methods to large
datasets.</p>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>In this section, we present the theoretical foundations of Random
Fourier Features. We begin by stating a key theorem that establishes the
approximation properties of this method.</p>
<h2 class="unnumbered" id="approximation-theorem">Approximation
Theorem</h2>
<div class="theorem">
<p>Let <span class="math inline">\(k\)</span> be a shift-invariant
kernel with corresponding function <span
class="math inline">\(\kappa\)</span>, and let <span
class="math inline">\(\hat{\kappa}\)</span> be its Fourier transform.
Suppose that <span class="math inline">\(\{ \omega_j \}_{j=1}^m\)</span>
are drawn independently from the distribution <span
class="math inline">\(p(\omega) =
\frac{\hat{\kappa}(\omega)}{\pi}\)</span>, and that <span
class="math inline">\(\{ b_j \}_{j=1}^m\)</span> are drawn uniformly
from <span class="math inline">\([0, 2\pi]\)</span>. Then, for any <span
class="math inline">\(x, y \in \mathbb{R}^d\)</span>, the Random Fourier
Features approximation satisfies:</p>
<p><span class="math display">\[\mathbb{E} \left[ \phi(x)^T \phi(y)
\right] = k(x, y),\]</span></p>
<p>where the expectation is taken over the random frequencies and
phases.</p>
</div>
<div class="proof">
<p><em>Proof.</em> We proceed by analyzing the expectation of <span
class="math inline">\(\phi(x)^T \phi(y)\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E} \left[ \phi(x)^T \phi(y) \right] &amp;= \mathbb{E} \left[
\frac{1}{m} \sum_{j=1}^m \cos(\omega_j^T x + b_j) \cos(\omega_j^T y +
b_j) \right] \\
&amp;= \frac{1}{m} \sum_{j=1}^m \mathbb{E} \left[ \cos(\omega_j^T (x -
y)) \right] \\
&amp;= \frac{1}{m} \sum_{j=1}^m \int_{\mathbb{R}^d} \cos(\omega^T (x -
y)) p(\omega) \, d\omega \\
&amp;= \frac{1}{2\pi} \int_{\mathbb{R}^d} \cos(\omega^T (x - y))
\hat{\kappa}(\omega) \, d\omega \\
&amp;= \frac{1}{2} \left( \int_{\mathbb{R}^d} e^{i \omega^T (x - y)}
\hat{\kappa}(\omega) \, d\omega + \int_{\mathbb{R}^d} e^{-i \omega^T (x
- y)} \hat{\kappa}(\omega) \, d\omega \right) \\
&amp;= \frac{1}{2} \left( \kappa(x - y) + \overline{\kappa(x - y)}
\right) \\
&amp;= \kappa(x - y) \\
&amp;= k(x, y).
\end{align*}\]</span></p>
<p>Here, we have used the fact that <span
class="math inline">\(\kappa\)</span> is real-valued and even, implying
that <span class="math inline">\(\hat{\kappa}\)</span> is also
real-valued and even. The last step follows from the definition of <span
class="math inline">\(k\)</span> as a shift-invariant kernel. ◻</p>
</div>
<h2 class="unnumbered" id="concentration-inequality">Concentration
Inequality</h2>
<p>The following theorem provides a concentration inequality for the
Random Fourier Features approximation, establishing its convergence
properties.</p>
<div class="theorem">
<p>Under the same assumptions as Theorem 1, for any <span
class="math inline">\(\epsilon &gt; 0\)</span> and <span
class="math inline">\(x, y \in \mathbb{R}^d\)</span>, the probability
that the approximation error exceeds <span
class="math inline">\(\epsilon\)</span> is bounded by:</p>
<p><span class="math display">\[\mathbb{P} \left( | \phi(x)^T \phi(y) -
k(x, y) | &gt; \epsilon \right) \leq 2 e^{-\frac{m \epsilon^2}{8 k(x,
x)^2}}.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> We employ the Hoeffding inequality to bound the
probability of the approximation error exceeding <span
class="math inline">\(\epsilon\)</span>. The random variables <span
class="math inline">\(Z_j = \cos(\omega_j^T x + b_j) \cos(\omega_j^T y +
b_j)\)</span> are bounded by <span class="math inline">\(k(x,
x)\)</span>, as <span class="math inline">\(|\cos(\theta)| \leq
1\)</span>. Therefore, by the Hoeffding inequality,</p>
<p><span class="math display">\[\mathbb{P} \left( \left| \frac{1}{m}
\sum_{j=1}^m Z_j - \mathbb{E}[Z_1] \right| &gt; \epsilon \right) \leq 2
e^{-\frac{2 m^2 \epsilon^2}{4 m k(x, x)^2}} = 2 e^{-\frac{m
\epsilon^2}{2 k(x, x)^2}}.\]</span></p>
<p>Using the fact that <span class="math inline">\(\mathbb{E}[Z_1] =
k(x, y)\)</span>, we obtain the desired concentration inequality. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>In this section, we explore some of the key properties and
corollaries associated with Random Fourier Features.</p>
<h2 class="unnumbered" id="property-1-unbiasedness">Property 1:
Unbiasedness</h2>
<p>The Random Fourier Features approximation is unbiased, as established
by Theorem 1. This property ensures that the expected value of the
approximation equals the true kernel value, providing a consistent
estimator.</p>
<h2 class="unnumbered" id="property-2-convergence">Property 2:
Convergence</h2>
<p>Theorem 2 provides a concentration inequality that guarantees the
convergence of the Random Fourier Features approximation as <span
class="math inline">\(m\)</span> increases. Specifically, for any fixed
<span class="math inline">\(\epsilon &gt; 0\)</span>, the probability of
the approximation error exceeding <span
class="math inline">\(\epsilon\)</span> decreases exponentially with
<span class="math inline">\(m\)</span>.</p>
<h2 class="unnumbered" id="property-3-computational-efficiency">Property
3: Computational Efficiency</h2>
<p>The Random Fourier Features method enables the efficient computation
of kernel functions, as it reduces the problem to a linear operation in
a high-dimensional feature space. This property is particularly
advantageous for large-scale machine learning tasks, where traditional
kernel methods may be computationally infeasible.</p>
<h2 class="unnumbered" id="property-4-universality">Property 4:
Universality</h2>
<p>Under certain conditions on the kernel <span
class="math inline">\(k\)</span>, the Random Fourier Features
approximation can be shown to be universal. This means that as <span
class="math inline">\(m\)</span> increases, the approximation can
represent any continuous function on a compact domain, provided that the
kernel is characteristic.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>In this article, we have explored the mathematical foundations of
Random Fourier Features, a powerful method that bridges kernel methods
and neural networks. We have presented the key definitions, theorems,
and properties associated with this approach, culminating in a detailed
analysis of its approximation and convergence properties. The Random
Fourier Features method offers a promising avenue for scalable machine
learning, enabling the efficient computation of kernel functions and
facilitating the application of linear learning algorithms in
high-dimensional feature spaces.</p>
<p>As future work, it would be interesting to investigate the
application of Random Fourier Features to other domains, such as graph
learning and time-series analysis. Additionally, exploring the
theoretical properties of this method under different kernel choices and
random feature distributions could provide further insights into its
behavior and potential improvements.</p>
<div class="thebibliography">
<p><span>1</span> Rahimi, A., &amp; Recht, B. (2007). <em>Random
features for large-scale kernel machines</em>. Advances in neural
information processing systems, 20, 1177-1184.</p>
</div>
</body>
</html>
{% include "footer.html" %}


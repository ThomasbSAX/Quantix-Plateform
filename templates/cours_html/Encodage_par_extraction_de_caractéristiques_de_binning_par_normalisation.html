{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage par extraction de caractéristiques de binning par normalisation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage par extraction de caractéristiques de
binning par normalisation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage par extraction de caractéristiques est une technique
fondamentale en apprentissage automatique, particulièrement utile pour
transformer des variables catégorielles en variables numériques. Parmi
les méthodes d’encodage, le binning par normalisation se distingue par
sa capacité à capturer des informations statistiques tout en réduisant
la dimensionnalité des données. Cette technique émerge comme une
solution puissante pour traiter les variables catégorielles avec un
grand nombre de catégories, où les méthodes traditionnelles comme
l’encodage one-hot peuvent devenir inefficaces.</p>
<p>L’objectif principal de cet article est d’explorer les fondements
théoriques et pratiques de l’encodage par extraction de caractéristiques
de binning par normalisation. Nous examinerons comment cette méthode
permet de transformer des données catégorielles en caractéristiques
numériques significatives, tout en préservant les relations statistiques
sous-jacentes.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de plonger dans les détails techniques, il est essentiel de
comprendre ce que nous cherchons à accomplir avec l’encodage par
extraction de caractéristiques de binning par normalisation. Imaginez
une variable catégorielle avec un grand nombre de catégories. Chaque
catégorie peut être associée à une distribution sous-jacente de valeurs
numériques. Notre but est de capturer cette information statistique de
manière compacte et informative.</p>
<p>Définissons formellement le binning par normalisation. Soit <span
class="math inline">\(X\)</span> une variable catégorielle avec <span
class="math inline">\(n\)</span> catégories <span
class="math inline">\(C_1, C_2, \ldots, C_n\)</span>. Pour chaque
catégorie <span class="math inline">\(C_i\)</span>, nous avons un
ensemble de valeurs numériques associées <span class="math inline">\(Y_i
= \{y_{i1}, y_{i2}, \ldots, y_{im}\}\)</span>.</p>
<p>La première étape consiste à diviser les valeurs numériques en <span
class="math inline">\(k\)</span> intervalles, ou bins. Cela peut être
fait de manière uniforme ou adaptative. Pour chaque bin <span
class="math inline">\(B_j\)</span>, nous calculons la proportion de
valeurs de <span class="math inline">\(Y_i\)</span> qui tombent dans cet
intervalle. Cette proportion est notre caractéristique normalisée pour
la catégorie <span class="math inline">\(C_i\)</span> et le bin <span
class="math inline">\(B_j\)</span>.</p>
<p>Formellement, pour chaque catégorie <span
class="math inline">\(C_i\)</span> et chaque bin <span
class="math inline">\(B_j\)</span>, nous définissons la caractéristique
normalisée <span class="math inline">\(f_{ij}\)</span> comme suit :</p>
<p><span class="math display">\[f_{ij} = \frac{\text{Nombre de } y \in
Y_i \text{ tels que } y \in B_j}{|Y_i|}\]</span></p>
<p>où <span class="math inline">\(|Y_i|\)</span> est le nombre total de
valeurs dans <span class="math inline">\(Y_i\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’encodage par extraction de
caractéristiques de binning par normalisation est le théorème de la
convergence des proportions. Ce théorème stipule que, sous certaines
conditions, les caractéristiques normalisées convergent vers une
distribution stable lorsque le nombre de données augmente.</p>
<p>Théorème (Convergence des proportions) : Soit <span
class="math inline">\(X\)</span> une variable catégorielle avec <span
class="math inline">\(n\)</span> catégories et <span
class="math inline">\(Y_i\)</span> l’ensemble des valeurs numériques
associées à la catégorie <span class="math inline">\(C_i\)</span>. Si
les valeurs de <span class="math inline">\(Y_i\)</span> sont
indépendantes et identiquement distribuées (i.i.d.), alors pour tout
<span class="math inline">\(\epsilon &gt; 0\)</span>, il existe un
nombre <span class="math inline">\(N\)</span> tel que pour tout <span
class="math inline">\(i\)</span> et <span
class="math inline">\(j\)</span>, si <span class="math inline">\(|Y_i|
&gt; N\)</span>, alors</p>
<p><span class="math display">\[\left| f_{ij} - p_j \right| &lt;
\epsilon\]</span></p>
<p>où <span class="math inline">\(p_j\)</span> est la proportion
théorique de valeurs dans le bin <span
class="math inline">\(B_j\)</span>.</p>
<p>Preuve : La preuve de ce théorème repose sur la loi des grands
nombres. En effet, comme les valeurs sont i.i.d., la proportion
empirique <span class="math inline">\(f_{ij}\)</span> converge presque
sûrement vers la proportion théorique <span
class="math inline">\(p_j\)</span> lorsque le nombre de données
augmente.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de l’encodage par extraction de
caractéristiques de binning par normalisation, considérons un exemple
simple. Supposons que nous avons une variable catégorielle <span
class="math inline">\(X\)</span> avec deux catégories <span
class="math inline">\(C_1\)</span> et <span
class="math inline">\(C_2\)</span>. Pour chaque catégorie, nous avons un
ensemble de valeurs numériques <span class="math inline">\(Y_1 = \{1.2,
1.5, 1.7\}\)</span> et <span class="math inline">\(Y_2 = \{2.1, 2.3,
2.5\}\)</span>.</p>
<p>Divisons les valeurs en deux bins <span class="math inline">\(B_1 =
[1, 2)\)</span> et <span class="math inline">\(B_2 = [2, 3]\)</span>.
Calculons les caractéristiques normalisées pour chaque catégorie et
chaque bin.</p>
<p>Pour <span class="math inline">\(C_1\)</span> : <span
class="math display">\[f_{11} = \frac{3}{3} = 1, \quad f_{12} =
\frac{0}{3} = 0\]</span></p>
<p>Pour <span class="math inline">\(C_2\)</span> : <span
class="math display">\[f_{21} = \frac{0}{3} = 0, \quad f_{22} =
\frac{3}{3} = 1\]</span></p>
<p>Ainsi, les caractéristiques normalisées pour <span
class="math inline">\(C_1\)</span> sont <span class="math inline">\((1,
0)\)</span> et pour <span class="math inline">\(C_2\)</span> sont <span
class="math inline">\((0, 1)\)</span>. Ces caractéristiques capturent
efficacement la distribution des valeurs numériques pour chaque
catégorie.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’encodage par extraction de caractéristiques de binning par
normalisation possède plusieurs propriétés intéressantes :</p>
<p>(i) Réduction de la dimensionnalité : En utilisant un nombre fixe de
bins, cette méthode réduit considérablement la dimensionnalité des
données, surtout lorsque le nombre de catégories est grand.</p>
<p>(ii) Préservation des relations statistiques : Les caractéristiques
normalisées capturent les proportions relatives des valeurs dans chaque
bin, préservant ainsi les relations statistiques sous-jacentes.</p>
<p>(iii) Robustesse aux variations de données : La méthode est robuste
aux variations des données, car elle repose sur des proportions plutôt
que sur des valeurs absolues.</p>
<p>Corollaire : Si les valeurs numériques associées à chaque catégorie
sont indépendantes et identiquement distribuées, alors les
caractéristiques normalisées convergent vers une distribution stable
lorsque le nombre de données augmente.</p>
<p>Preuve : Ce corollaire est une conséquence directe du théorème de la
convergence des proportions. En effet, si les valeurs sont i.i.d., alors
les proportions empiriques convergent vers les proportions théoriques,
ce qui garantit la stabilité des caractéristiques normalisées.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de binning par
normalisation est une technique puissante pour transformer des variables
catégorielles en caractéristiques numériques significatives. En
capturant les proportions relatives des valeurs dans chaque bin, cette
méthode préserve les relations statistiques sous-jacentes tout en
réduisant la dimensionnalité des données. Les théorèmes et propriétés
présentés dans cet article soulignent la robustesse et l’efficacité de
cette technique, en faisant un outil précieux pour l’apprentissage
automatique.</p>
</body>
</html>
{% include "footer.html" %}


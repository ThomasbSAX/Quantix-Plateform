{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Pas adaptatif : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Pas adaptatif : Théorie et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’optimisation numérique est un domaine central en mathématiques
appliquées, en ingénierie et en sciences de la décision. Parmi les
méthodes d’optimisation, les algorithmes à pas adaptatif occupent une
place de choix. Ces méthodes permettent d’ajuster dynamiquement le pas
de progression en fonction des caractéristiques du problème à résoudre,
offrant ainsi une flexibilité et une efficacité accrues par rapport aux
méthodes à pas fixe.</p>
<p>L’idée sous-jacente au pas adaptatif est de s’adapter aux variations
du paysage de la fonction objectif. Dans les zones où la fonction varie
rapidement, un petit pas est nécessaire pour éviter de sauter par-dessus
des minima locaux. À l’inverse, dans les zones où la fonction est plus
plate, un grand pas permet de progresser rapidement vers le minimum
global.</p>
<p>Ce chapitre explore les concepts fondamentaux des méthodes à pas
adaptatif, en mettant l’accent sur leur définition formelle, leurs
propriétés théoriques et leurs applications pratiques. Nous commencerons
par une introduction historique et conceptuelle, suivie de définitions
rigoureuses et de théorèmes clés. Nous terminerons par des preuves
détaillées et des corollaires illustrant l’utilité de ces méthodes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de formaliser le concept de pas adaptatif, il est essentiel de
comprendre ce que nous cherchons à accomplir. Imaginons que nous
voulions minimiser une fonction <span class="math inline">\(f :
\mathbb{R}^n \rightarrow \mathbb{R}\)</span>. Une méthode classique
consiste à suivre un gradient descendant, où nous mettons à jour notre
point courant <span class="math inline">\(x_k\)</span> en soustrayant
une fraction du gradient de la fonction au point <span
class="math inline">\(x_k\)</span>. Cependant, le choix du pas (ou taux
d’apprentissage) est crucial : un pas trop grand peut faire diverger
l’algorithme, tandis qu’un pas trop petit peut le rendre inefficace.</p>
<p>Nous cherchons donc un mécanisme pour ajuster dynamiquement ce pas en
fonction des caractéristiques locales de la fonction. Cela nous amène à
la définition formelle du pas adaptatif.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction différentiable. Un algorithme à pas
adaptatif est défini par la récurrence suivante : <span
class="math display">\[x_{k+1} = x_k - \alpha_k \nabla f(x_k),\]</span>
où <span class="math inline">\(\alpha_k\)</span> est un pas adaptatif
qui dépend de l’itération <span class="math inline">\(k\)</span> et des
propriétés locales de la fonction <span
class="math inline">\(f\)</span>.</p>
</div>
<p>Pour formaliser davantage, nous introduisons la notion de pas
adaptatif en fonction du gradient et des contraintes locales.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction différentiable. Un pas adaptatif <span
class="math inline">\(\alpha_k\)</span> est défini par : <span
class="math display">\[\alpha_k = \frac{\|\nabla f(x_k)\|}{\sum_{i=1}^n
(\partial_i^2 f(x_k))^2},\]</span> où <span
class="math inline">\(\partial_i^2 f(x_k)\)</span> désigne la dérivée
seconde partielle de <span class="math inline">\(f\)</span> par rapport
à la variable <span class="math inline">\(i\)</span> évaluée en <span
class="math inline">\(x_k\)</span>.</p>
</div>
<p>Cette définition montre que le pas adaptatif dépend à la fois du
gradient et des dérivées secondes de la fonction, permettant ainsi une
adaptation fine aux variations locales.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’efficacité des méthodes à pas adaptatif, nous
devons examiner leurs propriétés théoriques. Un théorème fondamental
dans ce contexte est le théorème de convergence des algorithmes à pas
adaptatif.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe et différentiable. Supposons
que l’algorithme à pas adaptatif défini par : <span
class="math display">\[x_{k+1} = x_k - \alpha_k \nabla f(x_k),\]</span>
avec <span class="math inline">\(\alpha_k\)</span> défini comme dans la
définition précédente, satisfait les conditions suivantes :</p>
<ul>
<li><p><span class="math inline">\(\alpha_k &gt; 0\)</span> pour tout
<span class="math inline">\(k\)</span>,</p></li>
<li><p><span class="math inline">\(\sum_{k=0}^{\infty} \alpha_k =
+\infty\)</span>,</p></li>
<li><p><span class="math inline">\(\sum_{k=0}^{\infty} \alpha_k^2 &lt;
+\infty\)</span>.</p></li>
</ul>
<p>Alors, la suite <span class="math inline">\(\{x_k\}\)</span> converge
vers un point critique de <span class="math inline">\(f\)</span>,
c’est-à-dire un point <span class="math inline">\(x^*\)</span> tel que
<span class="math inline">\(\nabla f(x^*) = 0\)</span>.</p>
</div>
<p>Pour démontrer ce théorème, nous devons d’abord établir quelques
lemmes intermédiaires.</p>
<div class="lemma">
<p>Soit <span class="math inline">\(f : \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction convexe et différentiable. Pour tout
<span class="math inline">\(k\)</span>, nous avons : <span
class="math display">\[f(x_{k+1}) \leq f(x_k) - \frac{1}{2} \alpha_k
\|\nabla f(x_k)\|^2.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> En utilisant le fait que <span
class="math inline">\(f\)</span> est convexe, nous pouvons appliquer
l’inégalité de convexité : <span class="math display">\[f(x_{k+1}) \leq
f(x_k) + \nabla f(x_k)^T (x_{k+1} - x_k).\]</span> En substituant <span
class="math inline">\(x_{k+1} = x_k - \alpha_k \nabla f(x_k)\)</span>,
nous obtenons : <span class="math display">\[f(x_{k+1}) \leq f(x_k) -
\alpha_k \|\nabla f(x_k)\|^2.\]</span> En utilisant l’inégalité <span
class="math inline">\(-a \leq -\frac{1}{2}a^2\)</span> pour tout <span
class="math inline">\(a &gt; 0\)</span>, nous concluons : <span
class="math display">\[f(x_{k+1}) \leq f(x_k) - \frac{1}{2} \alpha_k
\|\nabla f(x_k)\|^2.\]</span> ◻</p>
</div>
<p>Nous pouvons maintenant démontrer le théorème de convergence.</p>
<div class="proof">
<p><em>Preuve du Théorème de Convergence.</em> En utilisant le lemme de
décroissance, nous avons : <span
class="math display">\[\sum_{k=0}^{\infty} (f(x_k) - f(x_{k+1})) \geq
\frac{1}{2} \sum_{k=0}^{\infty} \alpha_k \|\nabla f(x_k)\|^2.\]</span>
Comme <span class="math inline">\(f\)</span> est convexe et
différentiable, la suite <span class="math inline">\(\{f(x_k)\}\)</span>
est décroissante et bornée inférieurement. Par conséquent, la somme
<span class="math inline">\(\sum_{k=0}^{\infty} (f(x_k) -
f(x_{k+1}))\)</span> est finie. Il s’ensuit que : <span
class="math display">\[\sum_{k=0}^{\infty} \alpha_k \|\nabla f(x_k)\|^2
&lt; +\infty.\]</span> En utilisant la condition <span
class="math inline">\(\sum_{k=0}^{\infty} \alpha_k^2 &lt;
+\infty\)</span>, nous pouvons appliquer le lemme de Fatou pour conclure
que : <span class="math display">\[\liminf_{k \rightarrow \infty}
\|\nabla f(x_k)\|^2 = 0.\]</span> Cela implique que <span
class="math inline">\(\liminf_{k \rightarrow \infty} \|\nabla f(x_k)\| =
0\)</span>. En utilisant la convexité de <span
class="math inline">\(f\)</span>, nous pouvons montrer que la suite
<span class="math inline">\(\{x_k\}\)</span> est bornée. Par le théorème
de Bolzano-Weierstrass, il existe une sous-suite <span
class="math inline">\(\{x_{k_j}\}\)</span> qui converge vers un point
<span class="math inline">\(x^*\)</span>. En utilisant la continuité de
<span class="math inline">\(\nabla f\)</span>, nous avons : <span
class="math display">\[\nabla f(x^*) = \lim_{j \rightarrow \infty}
\nabla f(x_{k_j}) = 0.\]</span> Ainsi, <span
class="math inline">\(x^*\)</span> est un point critique de <span
class="math inline">\(f\)</span>, et la suite <span
class="math inline">\(\{x_k\}\)</span> converge vers <span
class="math inline">\(x^*\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les méthodes à pas adaptatif possèdent plusieurs propriétés
intéressantes qui en font un outil puissant pour l’optimisation
numérique. Nous en énumérons quelques-unes ci-dessous.</p>
<ol>
<li><p><strong>Adaptation aux Variations Locales</strong> : Le pas
adaptatif permet à l’algorithme de s’adapter aux variations locales de
la fonction objectif, ce qui améliore l’efficacité et la stabilité de la
convergence.</p>
<div class="proof">
<p><em>Proof.</em> En effet, dans les zones où le gradient est grand et
les dérivées secondes sont petites, le pas adaptatif sera grand,
permettant une progression rapide. À l’inverse, dans les zones où le
gradient est petit et les dérivées secondes sont grandes, le pas
adaptatif sera petit, évitant ainsi de sauter par-dessus des minima
locaux. ◻</p>
</div></li>
<li><p><strong>Robustesse aux Conditions Initiales</strong> : Les
algorithmes à pas adaptatif sont moins sensibles aux conditions
initiales que les méthodes à pas fixe, car ils ajustent dynamiquement le
pas en fonction des caractéristiques locales.</p>
<div class="proof">
<p><em>Proof.</em> Cela est dû au fait que le pas adaptatif prend en
compte les informations locales sur la fonction, ce qui permet à
l’algorithme de s’adapter rapidement aux changements dans le paysage de
la fonction. ◻</p>
</div></li>
<li><p><strong>Efficacité Computationnelle</strong> : Les méthodes à pas
adaptatif peuvent être plus efficaces que les méthodes à pas fixe, car
elles évitent les calculs inutiles dans les zones où la fonction varie
lentement.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Les méthodes à pas adaptatif représentent une avancée significative
dans le domaine de l’optimisation numérique. En ajustant dynamiquement
le pas en fonction des caractéristiques locales de la fonction objectif,
ces méthodes offrent une flexibilité et une efficacité accrues par
rapport aux méthodes traditionnelles à pas fixe. Les théorèmes de
convergence et les propriétés discutés dans ce chapitre soulignent
l’importance et l’utilité de ces méthodes dans diverses applications
pratiques.</p>
<p>Bien que nous ayons présenté les concepts fondamentaux, il reste de
nombreuses pistes de recherche pour améliorer et étendre ces méthodes.
Par exemple, l’intégration de techniques d’apprentissage automatique
pour prédire les variations locales de la fonction pourrait ouvrir de
nouvelles perspectives dans le domaine de l’optimisation adaptative.</p>
</body>
</html>
{% include "footer.html" %}


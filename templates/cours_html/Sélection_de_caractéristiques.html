{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Sélection de caractéristiques : Méthodes et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Sélection de caractéristiques : Méthodes et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La sélection de caractéristiques, ou <em>feature selection</em>, est
un domaine fondamental en apprentissage automatique et en analyse de
données. Son objectif principal est de réduire la dimensionnalité des
données tout en conservant, voire en améliorant, la performance des
modèles prédictifs. L’origine de cette notion remonte aux années 1960
avec les travaux pionniers en analyse discriminante et en régression.
Aujourd’hui, elle est indispensable dans des domaines variés tels que la
bioinformatique, la finance et l’ingénierie.</p>
<p>L’émergence de cette notion est motivée par plusieurs facteurs. Tout
d’abord, les données modernes sont souvent de haute dimension, ce qui
pose des défis computationnels et statistiques. Ensuite, la présence de
caractéristiques redondantes ou non informatives peut nuire à la
performance des modèles. Enfin, une sélection judicieuse de
caractéristiques peut améliorer l’interprétabilité des modèles et
réduire le risque de surapprentissage.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la sélection de caractéristiques, il est essentiel de
définir quelques concepts clés.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données
composé de <span class="math inline">\(n\)</span> échantillons et <span
class="math inline">\(p\)</span> caractéristiques. Une caractéristique
est une variable <span class="math inline">\(X_j\)</span> pour <span
class="math inline">\(j = 1, \ldots, p\)</span> qui décrit un attribut
des échantillons.</p>
</div>
<div class="definition">
<p>Un sous-ensemble de caractéristiques <span class="math inline">\(S
\subseteq \{1, \ldots, p\}\)</span> est un ensemble d’indices
correspondant à une sélection de caractéristiques. Par exemple, <span
class="math inline">\(S = \{j_1, \ldots, j_k\}\)</span> où <span
class="math inline">\(k \leq p\)</span>.</p>
</div>
<div class="definition">
<p>Un critère de sélection est une fonction <span
class="math inline">\(J: 2^{\{1, \ldots, p\}} \rightarrow
\mathbb{R}\)</span> qui évalue la qualité d’un sous-ensemble de
caractéristiques. Par exemple, <span class="math inline">\(J(S)\)</span>
peut représenter la performance d’un modèle entraîné sur les
caractéristiques de <span class="math inline">\(S\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes-et-méthodes">Théorèmes et
Méthodes</h1>
<p>Plusieurs méthodes de sélection de caractéristiques ont été
développées, chacune avec ses propres théorèmes et propriétés.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire
représentant les caractéristiques et <span
class="math inline">\(Y\)</span> la variable cible. L’information
mutuelle <span class="math inline">\(I(X; Y)\)</span> mesure la
dépendance entre <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Un sous-ensemble de caractéristiques
<span class="math inline">\(S\)</span> est optimal si <span
class="math display">\[S = \argmax_{S&#39; \subseteq \{1, \ldots, p\}}
I(X_{S&#39;}; Y),\]</span> où <span
class="math inline">\(X_{S&#39;}\)</span> représente les
caractéristiques dans le sous-ensemble <span
class="math inline">\(S&#39;\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur les propriétés de
l’information mutuelle et la théorie de l’apprentissage statistique. En
particulier, il utilise le fait que l’information mutuelle est une
mesure de la dépendance maximale entre <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span>. Pour un sous-ensemble <span
class="math inline">\(S&#39;\)</span>, l’information mutuelle <span
class="math inline">\(I(X_{S&#39;}; Y)\)</span> est maximale lorsque
<span class="math inline">\(S&#39;\)</span> contient les
caractéristiques les plus informatives par rapport à <span
class="math inline">\(Y\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Plusieurs propriétés importantes découlent des théorèmes de sélection
de caractéristiques.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(J\)</span> un critère de sélection
monotone, c’est-à-dire que pour tout sous-ensemble <span
class="math inline">\(S&#39; \subseteq S\)</span>, on a <span
class="math inline">\(J(S&#39;) \leq J(S)\)</span>. Alors, un algorithme
de recherche gloutonne trouvera un sous-ensemble optimal.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de cette propriété repose sur le principe
de l’optimalité gloutonne. Si <span class="math inline">\(J\)</span> est
monotone, alors ajouter une caractéristique qui améliore le critère à
chaque étape garantit que l’algorithme trouvera un sous-ensemble
optimal. ◻</p>
</div>
<div class="corollary">
<p>Un sous-ensemble de caractéristiques <span
class="math inline">\(S\)</span> est stable si de petites perturbations
des données ne changent pas significativement <span
class="math inline">\(S\)</span>. La stabilité peut être mesurée par la
distance de Hamming entre les sous-ensembles sélectionnés sur différents
échantillons.</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La sélection de caractéristiques est un domaine riche et complexe,
avec des applications dans de nombreux domaines. Les méthodes et
théorèmes présentés ici fournissent une base solide pour comprendre et
appliquer ces techniques. Les défis futurs incluent le développement de
méthodes plus efficaces pour les données de très haute dimension et
l’amélioration de la stabilité des algorithmes de sélection.</p>
</body>
</html>
{% include "footer.html" %}


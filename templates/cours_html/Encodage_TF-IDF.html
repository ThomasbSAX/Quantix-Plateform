{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’encodage TF-IDF : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’encodage TF-IDF : Fondements et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage TF-IDF (Term Frequency-Inverse Document Frequency) est une
technique fondamentale en traitement automatique du langage naturel
(TALN) et en récupération d’information. Historiquement, cette méthode
émerge dans les années 1970 pour répondre au besoin croissant de
représenter efficacement le contenu textuel dans des systèmes
informatiques. Son importance réside dans sa capacité à pondérer les
termes d’un document en fonction de leur fréquence et de leur rareté
dans un corpus, permettant ainsi une meilleure discrimination des
documents pertinents.</p>
<p>L’encodage TF-IDF résout plusieurs problèmes critiques :</p>
<ul>
<li><p>La surreprésentation des termes fréquents mais non discriminants
(comme "le", "la", "et").</p></li>
<li><p>La sous-représentation des termes rares mais
significatifs.</p></li>
</ul>
<p>Ce cadre est indispensable pour des applications telles que la
recherche d’information, l’analyse de sentiment, et la classification de
texte.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire l’encodage TF-IDF, commençons par comprendre les
concepts de base. Supposons que nous ayons un corpus de documents et que
nous voulions représenter chaque document par un vecteur où chaque
dimension correspond à un terme du vocabulaire.</p>
<p>Nous cherchons une méthode pour pondérer les termes de manière à ce
que :</p>
<ul>
<li><p>Les termes fréquents dans un document mais rares dans le corpus
soient fortement pondérés.</p></li>
<li><p>Les termes fréquents à la fois dans un document et dans le corpus
soient faiblement pondérés.</p></li>
</ul>
<p>Formellement, pour un terme <span class="math inline">\(t\)</span>
dans un document <span class="math inline">\(d\)</span>, la pondération
TF-IDF est donnée par :</p>
<div class="definition">
<p>La fréquence d’un terme <span class="math inline">\(t\)</span> dans
un document <span class="math inline">\(d\)</span>, notée <span
class="math inline">\(\text{TF}(t, d)\)</span>, est définie comme :
<span class="math display">\[\text{TF}(t, d) = \frac{n_{t,d}}{\sum_{k}
n_{k,d}}\]</span> où <span class="math inline">\(n_{t,d}\)</span> est le
nombre d’occurrences du terme <span class="math inline">\(t\)</span>
dans le document <span class="math inline">\(d\)</span>, et la somme au
dénominateur est prise sur tous les termes <span
class="math inline">\(k\)</span> du document.</p>
</div>
<div class="definition">
<p>La fréquence inverse de document pour un terme <span
class="math inline">\(t\)</span>, notée <span
class="math inline">\(\text{IDF}(t)\)</span>, est définie comme : <span
class="math display">\[\text{IDF}(t) = \log \left( \frac{N}{n_t}
\right)\]</span> où <span class="math inline">\(N\)</span> est le nombre
total de documents dans le corpus, et <span
class="math inline">\(n_t\)</span> est le nombre de documents contenant
le terme <span class="math inline">\(t\)</span>.</p>
</div>
<div class="definition">
<p>La pondération TF-IDF pour un terme <span
class="math inline">\(t\)</span> dans un document <span
class="math inline">\(d\)</span>, notée <span
class="math inline">\(\text{TF-IDF}(t, d)\)</span>, est le produit de la
fréquence du terme et de la fréquence inverse de document : <span
class="math display">\[\text{TF-IDF}(t, d) = \text{TF}(t, d) \times
\text{IDF}(t)\]</span></p>
</div>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<p>L’encodage TF-IDF possède plusieurs propriétés intéressantes qui
justifient son utilisation.</p>
<div class="theorem">
<p>Les vecteurs TF-IDF peuvent être normalisés pour obtenir une
représentation unitaire. Pour un document <span
class="math inline">\(d\)</span>, la norme du vecteur TF-IDF est donnée
par : <span class="math display">\[\|d\| = \sqrt{\sum_{t}
(\text{TF-IDF}(t, d))^2}\]</span> La version normalisée du vecteur
TF-IDF est alors : <span class="math display">\[\hat{d} =
\frac{d}{\|d\|}\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La normalisation des vecteurs TF-IDF est une étape
courante pour comparer les similarités entre documents. La norme du
vecteur TF-IDF est calculée en utilisant la formule de la norme
euclidienne. La version normalisée du vecteur est obtenue en divisant
chaque composante par la norme du vecteur. ◻</p>
</div>
<div class="corollaire">
<p>La similarité entre deux documents <span
class="math inline">\(d_1\)</span> et <span
class="math inline">\(d_2\)</span> peut être calculée en utilisant le
produit scalaire de leurs vecteurs TF-IDF normalisés : <span
class="math display">\[\text{similarité}(d_1, d_2) = \hat{d}_1 \cdot
\hat{d}_2\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La similarité cosinus est une mesure courante de la
similarité entre deux vecteurs. Elle est calculée en utilisant le
produit scalaire des vecteurs normalisés, ce qui donne une valeur
comprise entre -1 et 1. Dans le cas des vecteurs TF-IDF, la similarité
cosinus est souvent utilisée pour évaluer la pertinence d’un document
par rapport à une requête. ◻</p>
</div>
<h1 class="unnumbered" id="applications">Applications</h1>
<p>L’encodage TF-IDF est utilisé dans diverses applications de
traitement du langage naturel et de récupération d’information. Voici
quelques exemples notables :</p>
<ul>
<li><p><strong>Recherche d’information</strong> : L’encodage TF-IDF est
largement utilisé dans les moteurs de recherche pour classer les
documents en fonction de leur pertinence par rapport à une
requête.</p></li>
<li><p><strong>Classification de texte</strong> : Les vecteurs TF-IDF
peuvent être utilisés comme caractéristiques d’entrée pour des
algorithmes de classification, tels que les machines à vecteurs de
support (SVM) ou les forêts aléatoires.</p></li>
<li><p><strong>Analyse de sentiment</strong> : L’encodage TF-IDF peut
aider à identifier les termes clés dans les textes d’analyse de
sentiment, permettant ainsi une meilleure compréhension des opinions
exprimées.</p></li>
</ul>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage TF-IDF est une technique puissante et polyvalente pour la
représentation et l’analyse des données textuelles. Son utilisation
permet de pondérer efficacement les termes en fonction de leur fréquence
et de leur rareté, améliorant ainsi la pertinence des résultats dans
diverses applications. Bien que d’autres méthodes plus avancées, telles
que les embeddings de mots, aient émergé récemment, l’encodage TF-IDF
reste une base fondamentale pour le traitement automatique du langage
naturel.</p>
</body>
</html>
{% include "footer.html" %}


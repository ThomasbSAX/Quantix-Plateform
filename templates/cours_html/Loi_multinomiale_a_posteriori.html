{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Loi multinomiale a posteriori</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Loi multinomiale a posteriori</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La loi multinomiale a posteriori émerge naturellement dans le cadre
de l’inférence bayésienne appliquée aux modèles multinomiaux. Ces
derniers sont omniprésents en statistique, notamment dans l’analyse des
données catégorielles. L’intérêt pour cette loi a posteriori réside dans
sa capacité à fournir une distribution de probabilité complète sur les
paramètres du modèle, intégrant à la fois l’information a priori et les
données observées.</p>
<p>En effet, dans un contexte bayésien, la loi a posteriori permet de
mettre à jour nos croyances initiales (modélisées par la distribution a
priori) en fonction des données collectées. Pour un modèle multinomial,
cela se traduit par une distribution a posteriori qui conserve la
structure multinomiale, sous certaines conditions sur l’a priori
choisi.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la loi multinomiale a posteriori, commençons par
rappeler quelques notions fondamentales.</p>
<h2 class="unnumbered" id="loi-multinomiale">Loi multinomiale</h2>
<p>Considérons une expérience aléatoire dont l’issue peut être classée
dans <span class="math inline">\(k\)</span> catégories mutuellement
exclusives. Supposons que la probabilité d’appartenance à chaque
catégorie soit respectivement <span class="math inline">\(p_1, p_2,
\ldots, p_k\)</span>, avec <span class="math inline">\(\sum_{i=1}^k p_i
= 1\)</span>. Si nous répétons cette expérience <span
class="math inline">\(n\)</span> fois de manière indépendante, le
vecteur aléatoire <span class="math inline">\((X_1, X_2, \ldots,
X_k)\)</span> représentant le nombre d’occurrences dans chaque catégorie
suit une loi multinomiale de paramètres <span class="math inline">\((n,
p_1, p_2, \ldots, p_k)\)</span>.</p>
<p>Formellement, la probabilité jointe est donnée par : <span
class="math display">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_k = x_k) =
\frac{n!}{x_1! x_2! \cdots x_k!} p_1^{x_1} p_2^{x_2} \cdots
p_k^{x_k}\]</span> où <span class="math inline">\(x_i\)</span> sont des
entiers non négatifs tels que <span class="math inline">\(\sum_{i=1}^k
x_i = n\)</span>.</p>
<h2 class="unnumbered" id="loi-multinomiale-a-posteriori">Loi
multinomiale a posteriori</h2>
<p>Supposons maintenant que nous observons un échantillon <span
class="math inline">\((x_1, x_2, \ldots, x_k)\)</span> issu d’une loi
multinomiale de paramètres <span class="math inline">\((n, p_1, p_2,
\ldots, p_k)\)</span>. En adoptant un point de vue bayésien, nous
souhaitons inférer la distribution a posteriori des paramètres <span
class="math inline">\(p_1, p_2, \ldots, p_k\)</span>.</p>
<p>Un choix naturel pour la distribution a priori est une loi de
Dirichlet de paramètres <span class="math inline">\((\alpha_1, \alpha_2,
\ldots, \alpha_k)\)</span>, notée <span
class="math inline">\(\text{Dir}(\alpha_1, \alpha_2, \ldots,
\alpha_k)\)</span>. La loi de Dirichlet est une généralisation
multivariée de la loi bêta, et elle est conjuguée à la loi
multinomiale.</p>
<p>La distribution a posteriori des paramètres <span
class="math inline">\(p_1, p_2, \ldots, p_k\)</span> est alors donnée
par : <span class="math display">\[p_1, p_2, \ldots, p_k | x_1, x_2,
\ldots, x_k \sim \text{Dir}(x_1 + \alpha_1, x_2 + \alpha_2, \ldots, x_k
+ \alpha_k)\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<h2 class="unnumbered"
id="conjugaison-de-la-loi-de-dirichlet-et-de-la-loi-multinomiale">Conjugaison
de la loi de Dirichlet et de la loi multinomiale</h2>
<p>Un résultat clé en inférence bayésienne est que la loi de Dirichlet
est conjuguée à la loi multinomiale. Cela signifie que si l’a priori sur
les paramètres <span class="math inline">\(p_1, p_2, \ldots,
p_k\)</span> est une loi de Dirichlet, alors la distribution a
posteriori est également une loi de Dirichlet.</p>
<p><strong>Théorème 1.</strong> Soit <span class="math inline">\((X_1,
X_2, \ldots, X_k) \sim \text{Multinomial}(n, p_1, p_2, \ldots,
p_k)\)</span> et <span class="math inline">\((p_1, p_2, \ldots, p_k)
\sim \text{Dir}(\alpha_1, \alpha_2, \ldots, \alpha_k)\)</span>. Alors,
<span class="math display">\[p_1, p_2, \ldots, p_k | x_1, x_2, \ldots,
x_k \sim \text{Dir}(x_1 + \alpha_1, x_2 + \alpha_2, \ldots, x_k +
\alpha_k)\]</span></p>
<h2 class="unnumbered" id="preuve-du-théorème-1">Preuve du Théorème
1</h2>
<p>La preuve repose sur le calcul de la distribution a posteriori en
utilisant la définition de l’inférence bayésienne. La densité conjointe
des observations et des paramètres est donnée par : <span
class="math display">\[f(x_1, x_2, \ldots, x_k, p_1, p_2, \ldots, p_k) =
f(x_1, x_2, \ldots, x_k | p_1, p_2, \ldots, p_k) f(p_1, p_2, \ldots,
p_k)\]</span> où <span class="math inline">\(f(x_1, x_2, \ldots, x_k |
p_1, p_2, \ldots, p_k)\)</span> est la densité de la loi multinomiale et
<span class="math inline">\(f(p_1, p_2, \ldots, p_k)\)</span> est la
densité de la loi de Dirichlet.</p>
<p>En intégrant sur les paramètres <span class="math inline">\(p_1, p_2,
\ldots, p_k\)</span>, nous obtenons la densité marginale des
observations : <span class="math display">\[f(x_1, x_2, \ldots, x_k) =
\int f(x_1, x_2, \ldots, x_k | p_1, p_2, \ldots, p_k) f(p_1, p_2,
\ldots, p_k) \, dp_1 \, dp_2 \, \cdots \, dp_k\]</span></p>
<p>La distribution a posteriori est alors proportionnelle à la densité
conjointe : <span class="math display">\[f(p_1, p_2, \ldots, p_k | x_1,
x_2, \ldots, x_k) \propto f(x_1, x_2, \ldots, x_k | p_1, p_2, \ldots,
p_k) f(p_1, p_2, \ldots, p_k)\]</span></p>
<p>En substituant les expressions des densités de la loi multinomiale et
de la loi de Dirichlet, nous obtenons : <span
class="math display">\[f(p_1, p_2, \ldots, p_k | x_1, x_2, \ldots, x_k)
\propto \prod_{i=1}^k p_i^{x_i + \alpha_i - 1}\]</span></p>
<p>Cette expression est proportionnelle à la densité d’une loi de
Dirichlet de paramètres <span class="math inline">\((x_1 + \alpha_1, x_2
+ \alpha_2, \ldots, x_k + \alpha_k)\)</span>.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<h2 class="unnumbered" id="propriété-1-espérance-a-posteriori">Propriété
1: Espérance a posteriori</h2>
<p>L’espérance de la distribution a posteriori est donnée par : <span
class="math display">\[\mathbb{E}[p_i | x_1, x_2, \ldots, x_k] =
\frac{x_i + \alpha_i}{n + \sum_{j=1}^k \alpha_j}\]</span></p>
<p>Cette propriété montre que l’espérance a posteriori est une
combinaison convexe des fréquences empiriques <span
class="math inline">\(\frac{x_i}{n}\)</span> et des paramètres a priori
<span class="math inline">\(\alpha_i\)</span>.</p>
<h2 class="unnumbered" id="propriété-2-variance-a-posteriori">Propriété
2: Variance a posteriori</h2>
<p>La variance de la distribution a posteriori est donnée par : <span
class="math display">\[\text{Var}(p_i | x_1, x_2, \ldots, x_k) =
\frac{(x_i + \alpha_i)(n + \sum_{j=1}^k \alpha_j - x_i - \alpha_i)}{(n +
\sum_{j=1}^k \alpha_j)^2 (n + \sum_{j=1}^k \alpha_j + 1)}\]</span></p>
<p>Cette propriété montre que la variance a posteriori dépend à la fois
des données observées et des paramètres a priori.</p>
<h2 class="unnumbered"
id="propriété-3-corrélation-a-posteriori">Propriété 3: Corrélation a
posteriori</h2>
<p>La covariance entre deux paramètres <span
class="math inline">\(p_i\)</span> et <span
class="math inline">\(p_j\)</span> est donnée par : <span
class="math display">\[\text{Cov}(p_i, p_j | x_1, x_2, \ldots, x_k) =
-\frac{(x_i + \alpha_i)(x_j + \alpha_j)}{(n + \sum_{j=1}^k \alpha_j)^2
(n + \sum_{j=1}^k \alpha_j + 1)}\]</span></p>
<p>Cette propriété montre que les paramètres a posteriori sont
négativement corrélés, ce qui est cohérent avec la contrainte <span
class="math inline">\(\sum_{i=1}^k p_i = 1\)</span>.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La loi multinomiale a posteriori joue un rôle central en inférence
bayésienne pour les modèles multinomiaux. Elle permet de mettre à jour
nos croyances initiales en fonction des données observées, tout en
conservant une structure analytique tractable. Les propriétés de cette
distribution, telles que l’espérance et la variance a posteriori,
fournissent des outils puissants pour l’analyse des données
catégorielles.</p>
</body>
</html>
{% include "footer.html" %}


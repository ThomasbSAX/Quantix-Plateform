{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>La distance de Cramér : une mesure de la divergence entre distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">La distance de Cramér : une mesure de la divergence
entre distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La distance de Cramér émerge comme une mesure fondamentale dans
l’étude des divergences entre distributions de probabilité. Son origine
réside dans les travaux du mathématicien suédois Harald Cramér, qui a
profondément influencé la théorie des probabilités et de l’inférence
statistique au XXe siècle. Cette distance est particulièrement utile
dans les contextes où l’on souhaite quantifier la similarité ou la
dissimilarité entre deux distributions, notamment en statistique non
paramétrique et en apprentissage automatique.</p>
<p>L’intérêt pour la distance de Cramér est motivé par sa capacité à
capturer des différences subtiles entre distributions, même lorsque les
moments d’ordre inférieur (comme la moyenne et la variance) sont
identiques. Elle est également liée à des concepts clés tels que les
fonctions caractéristiques et les transformations de Fourier, ce qui en
fait un outil puissant pour l’analyse des données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la distance de Cramér, commençons par rappeler que toute
distribution de probabilité <span class="math inline">\(P\)</span> sur
<span class="math inline">\(\mathbb{R}^d\)</span> admet une fonction
caractéristique <span class="math inline">\(\phi_P(t) =
\mathbb{E}_P[e^{it^T X}]\)</span>, où <span
class="math inline">\(X\)</span> est une variable aléatoire suivant
<span class="math inline">\(P\)</span>. La fonction caractéristique
encode toute l’information sur la distribution <span
class="math inline">\(P\)</span>.</p>
<p>La distance de Cramér entre deux distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est alors définie comme la distance
<span class="math inline">\(L^1\)</span> entre leurs fonctions
caractéristiques. Plus précisément, on cherche à mesurer l’intégrale de
la différence absolue des fonctions caractéristiques sur tout <span
class="math inline">\(\mathbb{R}^d\)</span>.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
<span class="math inline">\(\mathbb{R}^d\)</span>. La distance de Cramér
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, notée <span
class="math inline">\(\text{d}_C(P, Q)\)</span>, est définie par : <span
class="math display">\[\text{d}_C(P, Q) = \int_{\mathbb{R}^d} |\phi_P(t)
- \phi_Q(t)| \, dt\]</span> où <span
class="math inline">\(\phi_P\)</span> et <span
class="math inline">\(\phi_Q\)</span> sont les fonctions
caractéristiques de <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
</div>
<p>Une autre manière de formuler cette définition est d’utiliser la
norme <span class="math inline">\(L^1\)</span> sur l’espace des
fonctions intégrables. La distance de Cramér est donc la norme <span
class="math inline">\(L^1\)</span> de la différence des fonctions
caractéristiques : <span class="math display">\[\text{d}_C(P, Q) =
\|\phi_P - \phi_Q\|_{L^1}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Cramér est le théorème
de continuité, qui établit une relation entre la convergence en loi et
la convergence au sens de la distance de Cramér.</p>
<div class="theoreme">
<p>Soient <span class="math inline">\(P_n\)</span> et <span
class="math inline">\(P\)</span> des distributions de probabilité sur
<span class="math inline">\(\mathbb{R}^d\)</span>. Si la suite <span
class="math inline">\((P_n)\)</span> converge faiblement vers <span
class="math inline">\(P\)</span>, alors : <span
class="math display">\[\lim_{n \to \infty} \text{d}_C(P_n, P) =
0\]</span></p>
</div>
<p>La démonstration de ce théorème repose sur des résultats classiques
d’analyse fonctionnelle et de théorie des probabilités. Elle utilise
notamment le fait que les fonctions caractéristiques sont continues en
loi, c’est-à-dire que la convergence faible implique la convergence
ponctuelle des fonctions caractéristiques.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour démontrer le théorème de continuité, commençons par rappeler que
la convergence faible implique la convergence ponctuelle des fonctions
caractéristiques. Plus précisément, si <span class="math inline">\(P_n
\xrightarrow{w} P\)</span>, alors pour tout <span
class="math inline">\(t \in \mathbb{R}^d\)</span> : <span
class="math display">\[\lim_{n \to \infty} \phi_{P_n}(t) =
\phi_P(t)\]</span></p>
<p>Ensuite, nous utilisons le théorème de la convergence dominée pour
passer à la limite sous l’intégrale. Puisque les fonctions
caractéristiques sont bornées par 1, nous pouvons écrire : <span
class="math display">\[\lim_{n \to \infty} \int_{\mathbb{R}^d}
|\phi_{P_n}(t) - \phi_P(t)| \, dt = 0\]</span></p>
<p>Cela montre que la distance de Cramér entre <span
class="math inline">\(P_n\)</span> et <span
class="math inline">\(P\)</span> tend vers 0 lorsque <span
class="math inline">\(n\)</span> tend vers l’infini, ce qui achève la
démonstration.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance de Cramér possède plusieurs propriétés intéressantes, que
nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Non-négativité</strong> : Pour toutes distributions <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, nous avons <span
class="math inline">\(\text{d}_C(P, Q) \geq 0\)</span>, avec égalité si
et seulement si <span class="math inline">\(P = Q\)</span>.</p></li>
<li><p><strong>Symétrie</strong> : <span
class="math inline">\(\text{d}_C(P, Q) = \text{d}_C(Q,
P)\)</span>.</p></li>
<li><p><strong>Inégalité triangulaire</strong> : Pour toutes
distributions <span class="math inline">\(P\)</span>, <span
class="math inline">\(Q\)</span> et <span
class="math inline">\(R\)</span>, nous avons <span
class="math inline">\(\text{d}_C(P, R) \leq \text{d}_C(P, Q) +
\text{d}_C(Q, R)\)</span>.</p></li>
</ol>
<p>La démonstration de ces propriétés repose sur les propriétés
fondamentales des normes et des intégrales. La non-négativité découle
directement de la définition de l’intégrale, tandis que la symétrie et
l’inégalité triangulaire sont des conséquences de la définition de la
distance <span class="math inline">\(L^1\)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Cramér est un outil puissant pour mesurer la
divergence entre distributions de probabilité. Son lien avec les
fonctions caractéristiques en fait un outil particulièrement utile dans
l’analyse des données et la statistique non paramétrique. Les théorèmes
et propriétés associés à cette distance ouvrent de nombreuses
perspectives pour des applications futures, notamment dans le domaine de
l’apprentissage automatique et de la théorie des probabilités.</p>
</body>
</html>
{% include "footer.html" %}


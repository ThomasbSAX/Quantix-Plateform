{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Laplacian Eigenmaps : Une Exploration Mathématique et Algorithmique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Laplacian Eigenmaps : Une Exploration Mathématique et
Algorithmique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’analyse des données de grande dimension est un défi central en
apprentissage automatique et en traitement du signal. Les méthodes de
réduction de dimension visent à projeter les données dans un espace de
plus faible dimension tout en préservant au mieux la structure
intrinsèque des données. Parmi ces méthodes, les Laplacian Eigenmaps se
distinguent par leur capacité à capturer la structure géométrique des
données, en exploitant les propriétés du Laplacien d’un graphe.</p>
<p>Les Laplacian Eigenmaps trouvent leurs racines dans la théorie
spectrale des graphes et les méthodes de plongement. Elles ont été
introduites pour répondre au besoin de méthodes non linéaires capables
de capturer des structures complexes dans les données. Leur importance
réside dans leur capacité à révéler des motifs cachés et à simplifier
l’analyse des données tout en préservant les relations locales.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre les Laplacian Eigenmaps, il est essentiel de définir
quelques concepts préliminaires.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X = \{x_1, x_2, \dots,
x_n\}\)</span> un ensemble de points dans <span
class="math inline">\(\mathbb{R}^d\)</span>. Le graphe des k plus
proches voisins est un graphe non orienté <span class="math inline">\(G
= (V, E)\)</span> où les sommets <span class="math inline">\(V\)</span>
correspondent aux points <span class="math inline">\(x_i\)</span> et les
arêtes <span class="math inline">\(E\)</span> relient deux sommets <span
class="math inline">\(x_i\)</span> et <span
class="math inline">\(x_j\)</span> si <span
class="math inline">\(x_j\)</span> est parmi les k plus proches voisins
de <span class="math inline">\(x_i\)</span> ou vice versa.</p>
</div>
<div class="definition">
<p>La matrice de similarité <span class="math inline">\(W\)</span> est
une matrice symétrique <span class="math inline">\(n \times n\)</span>
où chaque entrée <span class="math inline">\(W_{ij}\)</span> mesure la
similarité entre les points <span class="math inline">\(x_i\)</span> et
<span class="math inline">\(x_j\)</span>. Une définition courante est :
<span class="math display">\[W_{ij} = \begin{cases}
e^{-\frac{\|x_i - x_j\|^2}{2\sigma^2}} &amp; \text{si } (x_i, x_j) \in E
\\
0 &amp; \text{sinon}
\end{cases}\]</span> où <span class="math inline">\(\sigma\)</span> est
un paramètre de lissage.</p>
</div>
<div class="definition">
<p>La matrice Laplacienne <span class="math inline">\(L\)</span> est
définie comme : <span class="math display">\[L = D - W\]</span> où <span
class="math inline">\(D\)</span> est la matrice diagonale des degrés,
avec <span class="math inline">\(D_{ii} = \sum_j W_{ij}\)</span>.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Le théorème fondamental des Laplacian Eigenmaps est basé sur la
théorie spectrale de la matrice Laplacienne.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(L\)</span> la matrice Laplacienne
d’un graphe connexe. Alors :</p>
<ul>
<li><p>La plus petite valeur propre de <span
class="math inline">\(L\)</span> est 0, et le vecteur propre associé est
le vecteur de tous les uns.</p></li>
<li><p>Les valeurs propres non nulles sont positives et ordonnées de
manière croissante : <span class="math inline">\(0 = \lambda_1 &lt;
\lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n\)</span>.</p></li>
</ul>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur les propriétés des matrices
symétriques définies positives. La matrice Laplacienne <span
class="math inline">\(L\)</span> est semi-définie positive, ce qui
implique que ses valeurs propres sont non négatives. La plus petite
valeur propre est 0, et le vecteur propre associé est le vecteur de tous
les uns, car <span class="math inline">\(L \mathbf{1} = 0\)</span>. Les
autres valeurs propres sont positives et ordonnées de manière croissante
en raison de la symétrie et de la positivité définie de <span
class="math inline">\(L\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer les propriétés des Laplacian Eigenmaps, nous devons
analyser la structure de la matrice Laplacienne.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la matrice Laplacienne <span
class="math inline">\(L = D - W\)</span>. La matrice <span
class="math inline">\(L\)</span> est symétrique, car <span
class="math inline">\(D\)</span> et <span
class="math inline">\(W\)</span> sont symétriques. De plus, pour tout
vecteur <span class="math inline">\(x \neq 0\)</span>, nous avons :
<span class="math display">\[x^T L x = x^T D x - x^T W x\]</span> <span
class="math display">\[= \sum_{i=1}^n D_{ii} x_i^2 - \sum_{i,j=1}^n
W_{ij} x_i x_j\]</span> <span class="math display">\[= \frac{1}{2}
\sum_{i,j=1}^n W_{ij} (x_i - x_j)^2 \geq 0\]</span> La dernière
inégalité montre que <span class="math inline">\(L\)</span> est
semi-définie positive. La plus petite valeur propre de <span
class="math inline">\(L\)</span> est 0, et le vecteur propre associé est
le vecteur de tous les uns. Les autres valeurs propres sont positives et
ordonnées de manière croissante. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Les Laplacian Eigenmaps possèdent plusieurs propriétés intéressantes
qui en font une méthode puissante pour la réduction de dimension.</p>
<div class="proposition">
<p>Les Laplacian Eigenmaps préservent les relations locales entre les
points.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Les Laplacian Eigenmaps projettent les points dans un
espace de plus faible dimension en utilisant les vecteurs propres
associés aux plus petites valeurs propres non nulles de la matrice
Laplacienne. Cette projection minimise l’énergie locale, ce qui signifie
que les points proches dans l’espace original restent proches dans
l’espace projeté. ◻</p>
</div>
<div class="proposition">
<p>Les Laplacian Eigenmaps sont invariantes par translation et par
rotation.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Les Laplacian Eigenmaps dépendent uniquement de la
structure du graphe, qui est invariante par translation et rotation. Par
conséquent, les projections obtenues par les Laplacian Eigenmaps sont
également invariantes par ces transformations. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Les Laplacian Eigenmaps sont une méthode puissante pour la réduction
de dimension, capable de capturer la structure géométrique des données.
Leur fondement théorique repose sur les propriétés spectrales de la
matrice Laplacienne, et leurs applications sont vastes dans le domaine
de l’apprentissage automatique et du traitement du signal. En comprenant
les concepts et les théorèmes sous-jacents, nous pouvons mieux apprécier
leur puissance et leur utilité dans l’analyse des données.</p>
</body>
</html>
{% include "footer.html" %}


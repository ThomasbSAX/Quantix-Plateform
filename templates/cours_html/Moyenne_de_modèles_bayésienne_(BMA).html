{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Moyenne de Modèles Bayesienne (BMA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Moyenne de Modèles Bayesienne (BMA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La Moyenne de Modèles Bayesienne (BMA) émerge comme une réponse
élégante à la complexité des modèles statistiques. Dans un contexte où
les données sont souvent bruitées et les hypothèses sous-jacentes
incertaines, la BMA offre une approche robuste pour combiner plusieurs
modèles en un seul, pondéré par leurs probabilités postérieures.</p>
<p>L’origine de la BMA remonte aux travaux fondateurs de Bayes et
Laplace, mais c’est avec le développement des méthodes computationnelles
modernes que cette approche a gagné en popularité. La BMA est
indispensable dans les domaines où la sélection de modèles est cruciale,
comme en économétrie, en biostatistique, et en apprentissage
automatique.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la BMA, il est essentiel de définir quelques concepts
clés. Supposons que nous avons un ensemble de modèles <span
class="math inline">\(M_1, M_2, \ldots, M_k\)</span> et des données
<span class="math inline">\(D\)</span>. Nous cherchons à combiner ces
modèles en une seule prédiction.</p>
<h2 id="définition-de-la-bma">Définition de la BMA</h2>
<p>La BMA est une méthode qui combine les prédictions de plusieurs
modèles en utilisant leurs probabilités postérieures. Formellement, pour
un vecteur de paramètres <span class="math inline">\(\theta\)</span> et
un modèle <span class="math inline">\(M_i\)</span>, la BMA est définie
comme :</p>
<p><span class="math display">\[P(\theta | D) = \sum_{i=1}^k P(\theta |
M_i, D) P(M_i | D)\]</span></p>
<p>où <span class="math inline">\(P(M_i | D)\)</span> est la probabilité
postérieure du modèle <span class="math inline">\(M_i\)</span> donné les
données <span class="math inline">\(D\)</span>, et <span
class="math inline">\(P(\theta | M_i, D)\)</span> est la distribution
postérieure des paramètres <span class="math inline">\(\theta\)</span>
sous le modèle <span class="math inline">\(M_i\)</span>.</p>
<h2 id="probabilité-postérieure-des-modèles">Probabilité Postérieure des
Modèles</h2>
<p>La probabilité postérieure d’un modèle <span
class="math inline">\(M_i\)</span> est donnée par :</p>
<p><span class="math display">\[P(M_i | D) = \frac{P(D | M_i)
P(M_i)}{P(D)}\]</span></p>
<p>où <span class="math inline">\(P(D | M_i)\)</span> est la
vraisemblance marginale du modèle <span
class="math inline">\(M_i\)</span>, <span
class="math inline">\(P(M_i)\)</span> est la probabilité a priori du
modèle <span class="math inline">\(M_i\)</span>, et <span
class="math inline">\(P(D)\)</span> est la vraisemblance marginale des
données.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-de-bayes-pour-les-modèles">Théorème de Bayes pour les
Modèles</h2>
<p>Le théorème de Bayes peut être étendu aux modèles. Supposons que nous
avons un ensemble de modèles <span class="math inline">\(M_1, M_2,
\ldots, M_k\)</span> et des données <span
class="math inline">\(D\)</span>. Le théorème de Bayes pour les modèles
s’énonce comme suit :</p>
<p><span class="math display">\[P(M_i | D) = \frac{P(D | M_i)
P(M_i)}{\sum_{j=1}^k P(D | M_j) P(M_j)}\]</span></p>
<h2 id="théorème-de-la-bma">Théorème de la BMA</h2>
<p>Le théorème de la BMA stipule que la prédiction optimale sous
l’erreur quadratique moyenne (EQM) est donnée par la BMA. Formellement,
pour une nouvelle observation <span class="math inline">\(y\)</span>, la
prédiction optimale est :</p>
<p><span class="math display">\[E[y | D] = \sum_{i=1}^k E[y | M_i, D]
P(M_i | D)\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-de-bayes-pour-les-modèles">Preuve du Théorème
de Bayes pour les Modèles</h2>
<p>La preuve suit directement du théorème de Bayes. Nous avons :</p>
<p><span class="math display">\[P(M_i | D) = \frac{P(D | M_i)
P(M_i)}{P(D)}\]</span></p>
<p>où <span class="math inline">\(P(D)\)</span> est la vraisemblance
marginale des données, qui peut être calculée comme :</p>
<p><span class="math display">\[P(D) = \sum_{j=1}^k P(D | M_j)
P(M_j)\]</span></p>
<h2 id="preuve-du-théorème-de-la-bma">Preuve du Théorème de la BMA</h2>
<p>Pour prouver que la BMA minimise l’EQM, nous devons montrer que :</p>
<p><span class="math display">\[E[(y - E[y | D])^2] \leq E[(y -
\hat{y})^2]\]</span></p>
<p>pour toute autre prédiction <span
class="math inline">\(\hat{y}\)</span>. En développant, nous obtenons
:</p>
<p><span class="math display">\[E[(y - E[y | D])^2] = E[y^2] - 2E[y]E[y
| D] + (E[y | D])^2\]</span></p>
<p>et</p>
<p><span class="math display">\[E[(y - \hat{y})^2] = E[y^2] -
2E[y]\hat{y} + \hat{y}^2\]</span></p>
<p>En utilisant l’égalité de Parseval, nous pouvons montrer que la BMA
minimise l’EQM.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-consistance">Propriété de Consistance</h2>
<p>La BMA est consistante au sens où, lorsque le nombre de données tend
vers l’infini, la probabilité postérieure du vrai modèle tend vers
1.</p>
<h2 id="propriété-de-robustesse">Propriété de Robustesse</h2>
<p>La BMA est robuste aux erreurs de spécification des modèles. Même si
un modèle est incorrect, il peut encore contribuer à la prédiction
finale.</p>
<h2 id="corollaire-de-loptimalité">Corollaire de l’Optimalité</h2>
<p>Le corollaire de l’optimalité stipule que la BMA est optimale sous
l’EQM pour tout ensemble de modèles.</p>
<h1 id="conclusion">Conclusion</h1>
<p>La Moyenne de Modèles Bayesienne (BMA) est une méthode puissante pour
combiner plusieurs modèles en un seul. Elle offre une approche robuste
et optimale pour la prédiction, surtout dans des contextes où les
données sont bruitées et les hypothèses sous-jacentes incertaines. Les
théorèmes et propriétés présentés dans cet article montrent la puissance
et l’élégance de cette approche.</p>
</body>
</html>
{% include "footer.html" %}


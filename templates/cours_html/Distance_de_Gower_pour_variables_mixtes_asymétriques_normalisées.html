{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Gower pour variables mixtes asymétriques normalisées</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Gower pour variables mixtes asymétriques
normalisées</h1>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’analyse des données est un domaine où la mesure de similarité entre
objets joue un rôle central. La distance de Gower, introduite par
Michael Alan Gower en 1971, est une mesure de dissimilarité
particulièrement adaptée aux données mixtes, c’est-à-dire des ensembles
de variables de types différents (numériques et catégorielles). Cette
distance est indispensable dans les contextes où les variables ne sont
pas toutes de même nature, ce qui est fréquent en sciences sociales, en
biologie ou en économie.</p>
<p>L’émergence de la distance de Gower répond à un besoin crucial :
fournir une métrique robuste capable de traiter des variables
asymétriques et normalisées. Les données réelles sont souvent
caractérisées par des distributions non symétriques et des échelles de
mesure différentes. La distance de Gower permet de normaliser ces
variables pour les rendre comparables, tout en tenant compte de leur
nature mixte.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour définir la distance de Gower, nous devons d’abord comprendre ce
que nous cherchons à mesurer. Supposons que nous avons un ensemble de
variables mixtes, certaines numériques et d’autres catégorielles. Nous
voulons une mesure qui capture la dissimilarité entre deux objets
décrits par ces variables, en tenant compte de leurs différences de
nature et d’échelle.</p>
<p>Considérons un ensemble de <span class="math inline">\(n\)</span>
objets décrits par <span class="math inline">\(p\)</span> variables.
Pour chaque variable, nous devons normaliser les valeurs pour les rendre
comparables. Pour une variable numérique <span
class="math inline">\(X_j\)</span>, nous pouvons utiliser la
normalisation par la moyenne et l’écart-type :</p>
<p><span class="math display">\[Z_{ij} = \frac{X_{ij} -
\bar{X}_j}{s_j}\]</span></p>
<p>où <span class="math inline">\(\bar{X}_j\)</span> est la moyenne de
la variable <span class="math inline">\(X_j\)</span> et <span
class="math inline">\(s_j\)</span> est son écart-type.</p>
<p>Pour une variable catégorielle, nous pouvons utiliser la
normalisation par la distance de Hamming :</p>
<p><span class="math display">\[Z_{ij} = \begin{cases}
0 &amp; \text{si } X_{ij} = X_{ik}, \\
1 &amp; \text{sinon.}
\end{cases}\]</span></p>
<p>La distance de Gower entre deux objets <span
class="math inline">\(i\)</span> et <span
class="math inline">\(k\)</span> est alors définie comme la moyenne des
distances normalisées sur toutes les variables :</p>
<p><span class="math display">\[d_{G}(i, k) = \frac{1}{p} \sum_{j=1}^p
d_{jk}(i, k)\]</span></p>
<p>où <span class="math inline">\(d_{jk}(i, k)\)</span> est la distance
normalisée pour la variable <span class="math inline">\(j\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié à la distance de Gower est le théorème de
la normalisation, qui garantit que la distance de Gower est bien définie
et respecte les propriétés métriques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de variables
mixtes. Si chaque variable est normalisée de manière appropriée, alors
la distance de Gower <span class="math inline">\(d_G\)</span> est une
métrique valide, c’est-à-dire qu’elle satisfait les propriétés suivantes
:</p>
<ul>
<li><p><span class="math inline">\(d_G(i, k) \geq 0\)</span>
(non-négativité),</p></li>
<li><p><span class="math inline">\(d_G(i, k) = 0\)</span> si et
seulement si <span class="math inline">\(i = k\)</span> (identité des
indiscernables),</p></li>
<li><p><span class="math inline">\(d_G(i, k) = d_G(k, i)\)</span>
(symétrie),</p></li>
<li><p><span class="math inline">\(d_G(i, k) \leq d_G(i, l) + d_G(l,
k)\)</span> (inégalité triangulaire).</p></li>
</ul>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la normalisation, nous devons montrer que
la distance de Gower satisfait les propriétés métriques.</p>
<div class="proof">
<p><em>Proof.</em></p>
<ol>
<li><p><strong>Non-négativité</strong> : Par construction, chaque
distance normalisée <span class="math inline">\(d_{jk}(i, k)\)</span>
est non-négative. La moyenne de valeurs non-négatives est également
non-négative.</p></li>
<li><p><strong>Identité des indiscernables</strong> : Si <span
class="math inline">\(i = k\)</span>, alors pour chaque variable <span
class="math inline">\(j\)</span>, <span class="math inline">\(d_{jk}(i,
k) = 0\)</span>. Par conséquent, <span class="math inline">\(d_G(i, k) =
0\)</span>.</p></li>
<li><p><strong>Symétrie</strong> : La distance de Gower est la moyenne
des distances normalisées, qui sont elles-mêmes symétriques. Par
conséquent, <span class="math inline">\(d_G(i, k) = d_G(k,
i)\)</span>.</p></li>
<li><p><strong>Inégalité triangulaire</strong> : Pour chaque variable
<span class="math inline">\(j\)</span>, la distance normalisée <span
class="math inline">\(d_{jk}(i, k)\)</span> satisfait l’inégalité
triangulaire. Par conséquent, la moyenne de ces distances satisfait
également l’inégalité triangulaire.</p></li>
</ol>
<p> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La distance de Gower possède plusieurs propriétés intéressantes qui
en font un outil puissant pour l’analyse des données mixtes.</p>
<div class="corollary">
<p>La distance de Gower est indépendante de l’échelle des variables
numériques, car chaque variable est normalisée par sa moyenne et son
écart-type.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La normalisation par la moyenne et l’écart-type rend
les variables numériques comparables, indépendamment de leur échelle
originale. Par conséquent, la distance de Gower est indépendante de
l’échelle des variables numériques. ◻</p>
</div>
<div class="corollary">
<p>La distance de Gower est symétrique, ce qui signifie que l’ordre des
objets n’a pas d’importance.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La symétrie de la distance de Gower découle
directement de la symétrie des distances normalisées pour chaque
variable. ◻</p>
</div>
<div class="corollary">
<p>La distance de Gower satisfait l’inégalité triangulaire, ce qui
permet de construire des arbres de similarité et d’autres structures
hiérarchiques.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’inégalité triangulaire pour la distance de Gower
découle de l’inégalité triangulaire pour chaque distance
normalisée. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>La distance de Gower est un outil puissant pour l’analyse des données
mixtes. Elle permet de normaliser les variables de différentes natures
et échelles, tout en fournissant une mesure de dissimilarité robuste et
valide. Les propriétés et théorèmes associés à la distance de Gower en
font un outil indispensable pour les chercheurs travaillant avec des
données complexes et hétérogènes.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Huber Loss: A Robust Loss Function for Statistical Estimation</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Huber Loss: A Robust Loss Function for Statistical
Estimation</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>The Huber loss function is a pioneering concept in robust statistics,
introduced by Peter J. Huber in 1964. It addresses the sensitivity of
traditional least squares estimation to outliers, which can
significantly distort the results. The Huber loss combines the best of
both worlds: quadratic loss for small residuals and linear loss for
large residuals. This hybrid approach ensures that the estimator remains
efficient for Gaussian errors while being robust to non-Gaussian
outliers. The Huber loss is indispensable in various fields, including
machine learning, econometrics, and signal processing, where robustness
against outliers is crucial.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the Huber loss, let’s first consider the problem of
estimating a parameter <span class="math inline">\(\theta\)</span> based
on observations <span class="math inline">\(y_i\)</span> and predictors
<span class="math inline">\(x_i\)</span>. The traditional approach uses
the least squares loss, which is sensitive to outliers. We seek a loss
function that is less sensitive to large residuals.</p>
<p>The Huber loss is defined as follows:</p>
<div class="definition">
<p>For a given residual <span class="math inline">\(r = y -
\hat{y}\)</span>, the Huber loss <span
class="math inline">\(\rho(r)\)</span> is defined as: <span
class="math display">\[\rho(r) =
\begin{cases}
\frac{1}{2} r^2 &amp; \text{if } |r| \leq c, \\
c(|r| - \frac{1}{2}c) &amp; \text{otherwise.}
\end{cases}\]</span> where <span class="math inline">\(c &gt; 0\)</span>
is a threshold parameter.</p>
</div>
<p>Alternatively, the Huber loss can be expressed using the maximum
function: <span class="math display">\[\rho(r) = \frac{1}{2} r^2 I(|r|
\leq c) + c(|r| - \frac{1}{2}c) I(|r| &gt; c),\]</span> where <span
class="math inline">\(I(\cdot)\)</span> is the indicator function.</p>
<h1 id="theorems">Theorems</h1>
<p>One of the key properties of the Huber loss is its robustness, which
can be formalized through the following theorem:</p>
<div class="theorem">
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be the
estimator obtained by minimizing the Huber loss. Then, for any <span
class="math inline">\(\epsilon &gt; 0\)</span>, there exists a constant
<span class="math inline">\(M\)</span> such that: <span
class="math display">\[\sup_{\| \theta - \theta_0 \| \leq M}
|\hat{\theta} - \theta_0| \leq \epsilon,\]</span> where <span
class="math inline">\(\theta_0\)</span> is the true parameter value.</p>
</div>
<div class="proof">
<p><em>Proof.</em> The proof relies on the fact that the Huber loss is
bounded by a linear function for large residuals. Specifically, for
<span class="math inline">\(|r| &gt; c\)</span>, the Huber loss grows
linearly, which limits the influence of outliers. This property ensures
that the estimator <span class="math inline">\(\hat{\theta}\)</span>
does not deviate too far from the true parameter value <span
class="math inline">\(\theta_0\)</span>, even in the presence of
outliers.</p>
<p>More formally, consider the worst-case scenario where a fraction
<span class="math inline">\(\epsilon\)</span> of the data points are
outliers. The Huber loss ensures that these outliers contribute at most
linearly to the total loss, which prevents them from dominating the
estimation process. Therefore, the estimator <span
class="math inline">\(\hat{\theta}\)</span> remains within a bounded
distance from <span class="math inline">\(\theta_0\)</span>. ◻</p>
</div>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<p>The Huber loss exhibits several important properties that make it
suitable for robust estimation:</p>
<ol>
<li><p><strong>Continuity and Differentiability:</strong> The Huber loss
is continuous everywhere and differentiable except at <span
class="math inline">\(r = \pm c\)</span>. This property ensures that
optimization algorithms can be applied smoothly.</p></li>
<li><p><strong>Robustness to Outliers:</strong> As mentioned earlier,
the Huber loss limits the influence of outliers by switching from
quadratic to linear growth for large residuals. This makes it robust to
non-Gaussian errors.</p></li>
<li><p><strong>Efficiency for Gaussian Errors:</strong> For small
residuals (<span class="math inline">\(|r| \leq c\)</span>), the Huber
loss reduces to the quadratic loss, which is optimal for Gaussian
errors. This ensures that the estimator remains efficient when the data
is well-behaved.</p></li>
</ol>
<p>Each of these properties can be formally proven using the definition
of the Huber loss and standard results from robust statistics.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The Huber loss function is a powerful tool for robust statistical
estimation. Its ability to combine the efficiency of least squares with
the robustness against outliers makes it indispensable in various
applications. By understanding its definition, properties, and
theoretical foundations, we can appreciate its significance in modern
statistical practice.</p>
</body>
</html>
{% include "footer.html" %}


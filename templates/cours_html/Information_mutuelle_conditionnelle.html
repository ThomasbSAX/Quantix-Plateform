{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’Information Mutuelle Conditionnelle : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’Information Mutuelle Conditionnelle : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’information mutuelle conditionnelle émerge comme un concept
fondamental en théorie de l’information, particulièrement dans le cadre
des systèmes multivariés. Son origine remonte aux travaux pionniers de
Claude Shannon, qui a posé les bases de la théorie de l’information.
L’information mutuelle conditionnelle quantifie la dépendance entre deux
variables aléatoires, étant donné une troisième variable. Elle est
indispensable dans l’analyse des réseaux de communication, du traitement
du signal et de l’apprentissage automatique, où la compréhension des
dépendances conditionnelles est cruciale.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir l’information mutuelle conditionnelle, considérons trois
variables aléatoires <span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span> et <span
class="math inline">\(Z\)</span>. Nous cherchons à mesurer la quantité
d’information partagée entre <span class="math inline">\(X\)</span> et
<span class="math inline">\(Y\)</span>, étant donné <span
class="math inline">\(Z\)</span>. Intuitivement, cela revient à évaluer
la réduction de l’incertitude sur <span class="math inline">\(X\)</span>
lorsque nous connaissons <span class="math inline">\(Y\)</span>, tout en
tenant compte de l’information fournie par <span
class="math inline">\(Z\)</span>.</p>
<p>Formellement, l’information mutuelle conditionnelle est définie comme
suit :</p>
<div class="definition">
<p>Soient <span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span> et <span
class="math inline">\(Z\)</span> des variables aléatoires discrètes.
L’information mutuelle conditionnelle de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> étant donné <span
class="math inline">\(Z\)</span> est donnée par : <span
class="math display">\[I(X; Y | Z) = \sum_{z \in \mathcal{Z}} p(z) I(X;
Y | Z=z)\]</span> où <span class="math inline">\(p(z)\)</span> est la
fonction de masse de probabilité de <span
class="math inline">\(Z\)</span>, et <span class="math inline">\(I(X; Y
| Z=z)\)</span> est l’information mutuelle de <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> conditionnellement à <span
class="math inline">\(Z=z\)</span>, définie par : <span
class="math display">\[I(X; Y | Z=z) = \sum_{x \in \mathcal{X}} \sum_{y
\in \mathcal{Y}} p(x, y | z) \log \left( \frac{p(x, y | z)}{p(x | z) p(y
| z)} \right)\]</span></p>
</div>
<p>Pour les variables aléatoires continues, l’information mutuelle
conditionnelle est définie par : <span class="math display">\[I(X; Y |
Z) = \int_{-\infty}^{\infty} p(z) I(X; Y | Z=z) \, dz\]</span> où <span
class="math inline">\(p(z)\)</span> est la fonction de densité de
probabilité de <span class="math inline">\(Z\)</span>, et <span
class="math inline">\(I(X; Y | Z=z)\)</span> est l’information mutuelle
de <span class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> conditionnellement à <span
class="math inline">\(Z=z\)</span>, définie par : <span
class="math display">\[I(X; Y | Z=z) = \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty} p(x, y | z) \log \left( \frac{p(x, y | z)}{p(x |
z) p(y | z)} \right) \, dx \, dy\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’information mutuelle conditionnelle
est le théorème de la chaîne de Markov, qui établit une relation entre
l’information mutuelle conditionnelle et les dépendances entre variables
aléatoires.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(X\)</span>, <span
class="math inline">\(Y\)</span> et <span
class="math inline">\(Z\)</span> des variables aléatoires. Si <span
class="math inline">\((X, Y)\)</span> forme une chaîne de Markov avec
<span class="math inline">\(Z\)</span>, c’est-à-dire que <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> sont conditionnellement indépendantes
étant donné <span class="math inline">\(Z\)</span>, alors : <span
class="math display">\[I(X; Y | Z) = 0\]</span></p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le théorème de la chaîne de Markov, nous utilisons la
définition de l’information mutuelle conditionnelle et les propriétés
des chaînes de Markov.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que <span class="math inline">\((X,
Y)\)</span> forme une chaîne de Markov avec <span
class="math inline">\(Z\)</span>. Cela signifie que : <span
class="math display">\[p(x, y | z) = p(x | z) p(y | z)\]</span> pour
tout <span class="math inline">\(x \in \mathcal{X}\)</span>, <span
class="math inline">\(y \in \mathcal{Y}\)</span> et <span
class="math inline">\(z \in \mathcal{Z}\)</span>.</p>
<p>En utilisant la définition de l’information mutuelle conditionnelle,
nous avons : <span class="math display">\[I(X; Y | Z=z) = \sum_{x \in
\mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y | z) \log \left( \frac{p(x,
y | z)}{p(x | z) p(y | z)} \right)\]</span> En substituant <span
class="math inline">\(p(x, y | z) = p(x | z) p(y | z)\)</span>, nous
obtenons : <span class="math display">\[I(X; Y | Z=z) = \sum_{x \in
\mathcal{X}} \sum_{y \in \mathcal{Y}} p(x | z) p(y | z) \log \left(
\frac{p(x | z) p(y | z)}{p(x | z) p(y | z)} \right)\]</span> Ce qui
simplifie à : <span class="math display">\[I(X; Y | Z=z) = \sum_{x \in
\mathcal{X}} \sum_{y \in \mathcal{Y}} p(x | z) p(y | z) \log(1) =
0\]</span> En intégrant sur <span class="math inline">\(Z\)</span>, nous
obtenons : <span class="math display">\[I(X; Y | Z) = \sum_{z \in
\mathcal{Z}} p(z) I(X; Y | Z=z) = 0\]</span> Ce qui prouve le
théorème. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’information mutuelle conditionnelle possède plusieurs propriétés
importantes :</p>
<ol>
<li><p><strong>Non-Négativité</strong> : <span
class="math inline">\(I(X; Y | Z) \geq 0\)</span></p></li>
<li><p><strong>Invariance par Transformation</strong> : Si <span
class="math inline">\(f\)</span> et <span
class="math inline">\(g\)</span> sont des fonctions bijectives, alors
<span class="math inline">\(I(f(X); g(Y) | Z) = I(X; Y |
Z)\)</span></p></li>
<li><p><strong>Additivité</strong> : <span class="math inline">\(I(X;
Y_1, Y_2 | Z) = I(X; Y_1 | Z) + I(X; Y_2 | Y_1, Z)\)</span></p></li>
</ol>
<p>La propriété (i) est une conséquence directe de la non-négativité de
l’entropie conditionnelle. La propriété (ii) découle du fait que les
transformations bijectives préservent l’information. La propriété (iii)
est une généralisation de la propriété d’additivité de l’information
mutuelle.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’information mutuelle conditionnelle est un outil puissant pour
analyser les dépendances entre variables aléatoires dans des contextes
conditionnels. Ses applications vont de la théorie de l’information à
l’apprentissage automatique, en passant par le traitement du signal. Une
compréhension approfondie de ce concept est essentielle pour les
chercheurs et les praticiens dans ces domaines.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Kernelized DBSCAN : Une Extension Non Linéaire de l’Algorithme DBSCAN</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Kernelized DBSCAN : Une Extension Non Linéaire de
l’Algorithme DBSCAN</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’algorithme DBSCAN (Density-Based Spatial Clustering of Applications
with Noise) est une méthode de clustering bien connue pour sa capacité à
identifier des clusters de formes arbitraires et à gérer le bruit dans
les données. Cependant, DBSCAN est intrinsèquement linéaire, ce qui
limite sa capacité à capturer des structures non linéaires dans les
données.</p>
<p>L’idée de kernelization, inspirée par les méthodes de l’apprentissage
automatique, permet d’étendre des algorithmes linéaires pour qu’ils
puissent traiter des données non linéairement séparables. En appliquant
cette idée à DBSCAN, nous obtenons une version kernelized de
l’algorithme, capable de capturer des structures complexes dans les
données.</p>
<p>Dans cet article, nous explorons la kernelization de DBSCAN, en
détaillant les définitions, les théorèmes et les preuves associés à
cette extension. Nous montrons comment cette approche peut améliorer la
performance de DBSCAN sur des données non linéaires.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la kernelization de DBSCAN, nous devons d’abord
rappeler quelques concepts clés.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble de
données. Une fonction de similarité <span class="math inline">\(k:
\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> est une
fonction symétrique et positive définie, c’est-à-dire que pour tout
<span class="math inline">\(x, y \in \mathcal{X}\)</span>, nous avons :
<span class="math display">\[k(x, y) = k(y, x)\]</span> et pour tout
<span class="math inline">\(n \in \mathbb{N}^*\)</span>, et pour toute
famille <span class="math inline">\((x_1, \ldots, x_n) \in
\mathcal{X}^n\)</span>, la matrice <span class="math inline">\((k(x_i,
x_j))_{1 \leq i,j \leq n}\)</span> est positive définie.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(k\)</span> une fonction de
similarité. L’espace de caractéristiques implicite associé à <span
class="math inline">\(k\)</span> est un espace de Hilbert <span
class="math inline">\(\mathcal{H}\)</span> tel que pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span>, il existe un vecteur
<span class="math inline">\(\phi(x) \in \mathcal{H}\)</span> tel que :
<span class="math display">\[k(x, y) = \langle \phi(x), \phi(y)
\rangle_{\mathcal{H}}\]</span> pour tout <span class="math inline">\(y
\in \mathcal{X}\)</span>.</p>
</div>
<h1 class="unnumbered" id="kernelized-dbscan">Kernelized DBSCAN</h1>
<p>Nous allons maintenant définir l’algorithme Kernelized DBSCAN.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble de
données, <span class="math inline">\(k\)</span> une fonction de
similarité, et <span class="math inline">\(\epsilon &gt; 0\)</span> un
paramètre de rayon. L’algorithme Kernelized DBSCAN est défini comme suit
:</p>
<ol>
<li><p>Pour chaque point <span class="math inline">\(x \in
\mathcal{X}\)</span>, calculer le voisinage de <span
class="math inline">\(x\)</span> dans l’espace de caractéristiques
implicite : <span class="math display">\[N_{\epsilon}(x) = \{ y \in
\mathcal{X} \mid k(x, y) \geq \epsilon \}\]</span></p></li>
<li><p>Un point <span class="math inline">\(x \in \mathcal{X}\)</span>
est un point de bord s’il existe un point <span class="math inline">\(y
\in N_{\epsilon}(x)\)</span> tel que <span
class="math inline">\(y\)</span> est un point de noyau.</p></li>
<li><p>Un point <span class="math inline">\(x \in \mathcal{X}\)</span>
est un point de bruit s’il n’est ni un point de noyau ni un point de
bord.</p></li>
<li><p>Deux points <span class="math inline">\(x, y \in
\mathcal{X}\)</span> sont dans le même cluster s’il existe une chaîne de
points <span class="math inline">\(x = z_0, z_1, \ldots, z_n =
y\)</span> tels que <span class="math inline">\(z_i \in
N_{\epsilon}(z_{i+1})\)</span> pour tout <span class="math inline">\(0
\leq i &lt; n\)</span>.</p></li>
</ol>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Nous allons maintenant énoncer et prouver quelques théorèmes
importants concernant Kernelized DBSCAN.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble de
données et <span class="math inline">\(k\)</span> une fonction de
similarité. Si <span class="math inline">\(k\)</span> est continue,
alors Kernelized DBSCAN est consistant, c’est-à-dire que pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe un <span
class="math inline">\(\delta &gt; 0\)</span> tel que pour tout ensemble
de données <span class="math inline">\(\mathcal{X}&#39;\)</span> à une
distance de Hausdorff <span class="math inline">\(\delta\)</span> de
<span class="math inline">\(\mathcal{X}\)</span>, l’algorithme
Kernelized DBSCAN appliqué à <span
class="math inline">\(\mathcal{X}&#39;\)</span> produit le même
clustering que celui appliqué à <span
class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la continuité de
la fonction de similarité <span class="math inline">\(k\)</span>. En
effet, si <span class="math inline">\(k\)</span> est continue, alors
pour tout <span class="math inline">\(\epsilon &gt; 0\)</span>, il
existe un <span class="math inline">\(\delta &gt; 0\)</span> tel que
pour tout <span class="math inline">\(x, y \in \mathcal{X}\)</span>, si
la distance entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> est inférieure à <span
class="math inline">\(\delta\)</span>, alors <span
class="math inline">\(|k(x, z) - k(y, z)| &lt; \epsilon\)</span> pour
tout <span class="math inline">\(z \in \mathcal{X}\)</span>.</p>
<p>Cela implique que le voisinage <span
class="math inline">\(N_{\epsilon}(x)\)</span> est continu par rapport à
<span class="math inline">\(x\)</span>, et donc que l’algorithme
Kernelized DBSCAN est consistant. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Nous allons maintenant énoncer quelques propriétés et corollaires
importants concernant Kernelized DBSCAN.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(\mathcal{X}\)</span> un ensemble de
données et <span class="math inline">\(k\)</span> une fonction de
similarité. Si <span class="math inline">\(k\)</span> est
Lipschitzienne, alors Kernelized DBSCAN est stable, c’est-à-dire que
pour tout <span class="math inline">\(\epsilon &gt; 0\)</span>, il
existe un <span class="math inline">\(\delta &gt; 0\)</span> tel que
pour tout ensemble de données <span
class="math inline">\(\mathcal{X}&#39;\)</span> à une distance de
Hausdorff <span class="math inline">\(\delta\)</span> de <span
class="math inline">\(\mathcal{X}\)</span>, l’algorithme Kernelized
DBSCAN appliqué à <span class="math inline">\(\mathcal{X}&#39;\)</span>
produit un clustering dont la distance de Hausdorff est inférieure à
<span class="math inline">\(\epsilon\)</span> par rapport au clustering
produit par l’algorithme appliqué à <span
class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce corollaire repose sur la
Lipschitzianité de la fonction de similarité <span
class="math inline">\(k\)</span>. En effet, si <span
class="math inline">\(k\)</span> est Lipschitzienne, alors pour tout
<span class="math inline">\(\epsilon &gt; 0\)</span>, il existe un <span
class="math inline">\(\delta &gt; 0\)</span> tel que pour tout <span
class="math inline">\(x, y \in \mathcal{X}\)</span>, si la distance
entre <span class="math inline">\(x\)</span> et <span
class="math inline">\(y\)</span> est inférieure à <span
class="math inline">\(\delta\)</span>, alors <span
class="math inline">\(|k(x, z) - k(y, z)| &lt; \epsilon\)</span> pour
tout <span class="math inline">\(z \in \mathcal{X}\)</span>.</p>
<p>Cela implique que le voisinage <span
class="math inline">\(N_{\epsilon}(x)\)</span> est Lipschitzien par
rapport à <span class="math inline">\(x\)</span>, et donc que
l’algorithme Kernelized DBSCAN est stable. ◻</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré la kernelization de l’algorithme
DBSCAN. Nous avons défini les concepts clés, énoncé et prouvé des
théorèmes importants, et discuté des propriétés et corollaires associés
à cette extension. Nous avons montré que Kernelized DBSCAN est une
méthode puissante pour capturer des structures non linéaires dans les
données, et que ses propriétés de consistance et de stabilité en font un
outil précieux pour l’analyse de données.</p>
</body>
</html>
{% include "footer.html" %}


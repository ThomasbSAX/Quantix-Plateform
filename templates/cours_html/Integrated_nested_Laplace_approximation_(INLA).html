{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Integrated Nested Laplace Approximation (INLA)</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Integrated Nested Laplace Approximation (INLA)</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’Integrated Nested Laplace Approximation (INLA) est une méthode
statistique révolutionnaire qui a émergé pour répondre aux défis posés
par les modèles hiérarchiques complexes. Ces modèles, bien que
puissants, sont souvent difficiles à estimer en raison de leur
complexité computationnelle. L’INLA offre une alternative efficace aux
méthodes traditionnelles comme le MCMC (Markov Chain Monte Carlo), en
fournissant des approximations rapides et précises des distributions
marginales postérieures.</p>
<p>L’INLA est particulièrement utile dans le contexte des modèles
bayésiens, où l’on cherche à inférer les distributions postérieures des
paramètres du modèle. Les méthodes MCMC, bien que rigoureuses, peuvent
être lentes et nécessiter des ressources computationnelles importantes.
L’INLA, en revanche, utilise une approche déterministe pour approximer
ces distributions, ce qui permet des gains de temps significatifs sans
sacrifier la précision.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre l’INLA, il est essentiel de définir quelques concepts
clés. Commençons par le concept de distribution marginale postérieure.
Supposons que nous avons un modèle hiérarchique avec des paramètres
<span class="math inline">\(\theta\)</span> et des hyperparamètres <span
class="math inline">\(\eta\)</span>. La distribution marginale
postérieure de <span class="math inline">\(\theta\)</span> est donnée
par :</p>
<p><span class="math display">\[p(\theta | y) = \int p(\theta, \eta | y)
d\eta\]</span></p>
<p>où <span class="math inline">\(y\)</span> représente les données
observées. L’objectif est de calculer cette distribution pour chaque
paramètre <span class="math inline">\(\theta\)</span>.</p>
<p>L’INLA utilise une approximation de Laplace pour calculer cette
intégrale. L’idée est d’approximer la distribution conjointe <span
class="math inline">\(p(\theta, \eta | y)\)</span> par une distribution
gaussienne autour de son mode. Cette approximation est ensuite utilisée
pour calculer l’intégrale.</p>
<p>Formellement, l’approximation de Laplace est donnée par :</p>
<p><span class="math display">\[p(\theta | y) \approx \int \exp\left(
\mathcal{L}(\tilde{\eta}) + \frac{1}{2} (\eta - \tilde{\eta})^T
H(\tilde{\eta}) (\eta - \tilde{\eta}) \right) d\eta\]</span></p>
<p>où <span class="math inline">\(\mathcal{L}(\tilde{\eta})\)</span> est
le log-vraisemblance au point mode <span
class="math inline">\(\tilde{\eta}\)</span> et <span
class="math inline">\(H(\tilde{\eta})\)</span> est la hessienne de la
log-vraisemblance.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>L’INLA repose sur plusieurs théorèmes clés. Le premier est le
théorème de Laplace, qui permet d’approximer une intégrale par une
distribution gaussienne. Le théorème de Laplace stipule que pour une
fonction <span class="math inline">\(f\)</span> avec un maximum en <span
class="math inline">\(\tilde{\eta}\)</span>, l’intégrale de <span
class="math inline">\(f\)</span> peut être approximée par :</p>
<p><span class="math display">\[\int f(\eta) d\eta \approx \exp\left(
f(\tilde{\eta}) + \frac{1}{2} (\eta - \tilde{\eta})^T H(\tilde{\eta})
(\eta - \tilde{\eta}) \right)\]</span></p>
<p>où <span class="math inline">\(H(\tilde{\eta})\)</span> est la
hessienne de <span class="math inline">\(f\)</span> en <span
class="math inline">\(\tilde{\eta}\)</span>.</p>
<p>Un autre théorème important est celui de l’approximation gaussienne.
Ce théorème stipule que pour une fonction <span
class="math inline">\(f\)</span> avec un maximum en <span
class="math inline">\(\tilde{\eta}\)</span>, la distribution de <span
class="math inline">\(f\)</span> peut être approximée par une
distribution gaussienne centrée en <span
class="math inline">\(\tilde{\eta}\)</span> avec une matrice de
covariance donnée par l’inverse de la hessienne de <span
class="math inline">\(f\)</span> en <span
class="math inline">\(\tilde{\eta}\)</span>.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de Laplace, commençons par développer la
fonction <span class="math inline">\(f\)</span> autour de son mode <span
class="math inline">\(\tilde{\eta}\)</span> :</p>
<p><span class="math display">\[f(\eta) = f(\tilde{\eta}) + \frac{1}{2}
(\eta - \tilde{\eta})^T H(\tilde{\eta}) (\eta - \tilde{\eta}) + o(\|\eta
- \tilde{\eta}\|^2)\]</span></p>
<p>où <span class="math inline">\(o(\|\eta - \tilde{\eta}\|^2)\)</span>
représente les termes d’ordre supérieur. En négligeant ces termes, nous
obtenons :</p>
<p><span class="math display">\[\int f(\eta) d\eta \approx \exp\left(
f(\tilde{\eta}) + \frac{1}{2} (\eta - \tilde{\eta})^T H(\tilde{\eta})
(\eta - \tilde{\eta}) \right)\]</span></p>
<p>Cette approximation est valable lorsque <span
class="math inline">\(\eta\)</span> est proche de <span
class="math inline">\(\tilde{\eta}\)</span>, ce qui est généralement le
cas pour les distributions avec un maximum bien défini.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’INLA possède plusieurs propriétés intéressantes. En voici
quelques-unes :</p>
<ol>
<li><p>L’INLA est une méthode déterministe, ce qui signifie qu’elle ne
nécessite pas de simulation. Cela permet des gains de temps
significatifs par rapport aux méthodes MCMC.</p></li>
<li><p>L’INLA est particulièrement efficace pour les modèles
hiérarchiques avec un grand nombre de paramètres. Elle permet de
calculer rapidement les distributions marginales postérieures pour
chaque paramètre.</p></li>
<li><p>L’INLA peut être utilisée pour une variété de modèles, y compris
les modèles linéaires généralisés (GLM) et les modèles à effets
aléatoires.</p></li>
</ol>
<p>Pour prouver la propriété (i), notons que l’INLA utilise une
approximation de Laplace pour calculer les distributions marginales
postérieures. Cette approximation est déterministe et ne nécessite pas
de simulation, contrairement aux méthodes MCMC.</p>
<p>Pour prouver la propriété (ii), notons que l’INLA utilise une
approche hiérarchique pour calculer les distributions marginales
postérieures. Cette approche permet de traiter efficacement les modèles
avec un grand nombre de paramètres.</p>
<p>Pour prouver la propriété (iii), notons que l’INLA peut être adaptée
à une variété de modèles en utilisant des approximations de Laplace
appropriées. Cela permet d’utiliser l’INLA pour une large gamme de
problèmes statistiques.</p>
</body>
</html>
{% include "footer.html" %}


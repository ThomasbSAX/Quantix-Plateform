{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>On the Tweedie Deviance: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">On the Tweedie Deviance: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-and-motivations">Introduction
and Motivations</h1>
<p>The Tweedie Deviance emerges as a pivotal concept in the realm of
statistical modeling, particularly within the context of generalized
linear models (GLMs). Its origins are deeply rooted in the need to
address certain limitations inherent in traditional exponential family
distributions, especially when dealing with continuous, non-negative
data that exhibit a point mass at zero. The Tweedie distribution, from
which the deviance derives its name, is a versatile extension of the
exponential family that accommodates such data structures. This deviance
measure is indispensable for assessing model fit, performing inference,
and conducting hypothesis testing within the Tweedie GLM framework. Its
historical development is intertwined with advancements in actuarial
science, econometrics, and environmental statistics, where the modeling
of skewed, zero-inflated data is paramount.</p>
<h1 class="unnumbered" id="definitions">Definitions</h1>
<p>To understand the Tweedie Deviance, we first elucidate its
foundational components. Consider a random variable <span
class="math inline">\(Y\)</span> that follows a Tweedie distribution
with parameters <span class="math inline">\((\mu, \sigma^2, p)\)</span>,
where <span class="math inline">\(\mu &gt; 0\)</span> is the mean, <span
class="math inline">\(\sigma^2 &gt; 0\)</span> is the scale parameter,
and <span class="math inline">\(1 &lt; p &lt; 2\)</span> is the index
parameter. The Tweedie distribution generalizes both the gamma and
Poisson distributions, making it suitable for modeling data with a point
mass at zero.</p>
<p>We seek a measure that quantifies the discrepancy between observed
values <span class="math inline">\(y_i\)</span> and their predicted
counterparts <span class="math inline">\(\mu_i\)</span>, under the
Tweedie distributional assumption. This measure should encapsulate both
the continuous and discrete aspects of the data.</p>
<div class="definition">
<p>Let <span class="math inline">\(y_i\)</span> be an observed value and
<span class="math inline">\(\mu_i\)</span> its corresponding predicted
mean under a Tweedie GLM. The Tweedie Deviance <span
class="math inline">\(D_T(y_i, \mu_i)\)</span> is defined as:</p>
<p><span class="math display">\[D_T(y_i, \mu_i) = \begin{cases}
\frac{2}{p(p-1)\sigma^2} \left[ y_i (p-1) \mu_i^{1-p} - \frac{p}{p-1}
(y_i - \mu_i) \right] &amp; \text{if } y_i &gt; 0, \\
\frac{2}{p(p-1)\sigma^2} \left[ y_i (p-1) \mu_i^{1-p} - \frac{p}{p-1}
(-\mu_i) \right] &amp; \text{if } y_i = 0.
\end{cases}\]</span></p>
<p>Equivalently, for <span class="math inline">\(y_i &gt; 0\)</span>,
the deviance can be expressed as:</p>
<p><span class="math display">\[D_T(y_i, \mu_i) = \frac{2}{\sigma^2}
\left[ \frac{y_i^{p}}{p(p-1)} - \frac{\mu_i y_i^{p-1}}{p-1} +
\frac{(p-2)\mu_i^p}{p(p-1)} \right].\]</span></p>
<p>For <span class="math inline">\(y_i = 0\)</span>, the deviance
simplifies to:</p>
<p><span class="math display">\[D_T(y_i, \mu_i) = \frac{2}{\sigma^2}
\left[ -\frac{\mu_i^p}{p(p-1)} \right].\]</span></p>
<p>These expressions are derived from the negative log-likelihood of the
Tweedie distribution, scaled by a factor of 2 for convenience.</p>
</div>
<h1 class="unnumbered" id="theorems">Theorems</h1>
<p>The Tweedie Deviance possesses several important theoretical
properties that facilitate its use in statistical inference. We present
a key theorem that underscores its role in model comparison.</p>
<div class="theorem">
<p>Let <span class="math inline">\(\hat{\mu}_i\)</span> and <span
class="math inline">\(\tilde{\mu}_i\)</span> be the predicted means
under two nested Tweedie GLMs, where <span
class="math inline">\(\hat{\mu}_i\)</span> corresponds to the more
complex model. The difference in deviances <span
class="math inline">\(D_T(y_i, \hat{\mu}_i) - D_T(y_i,
\tilde{\mu}_i)\)</span> follows an asymptotic chi-squared distribution
with degrees of freedom equal to the difference in the number of
parameters between the two models.</p>
<p>Mathematically, for <span class="math inline">\(n\)</span>
independent observations:</p>
<p><span class="math display">\[\sum_{i=1}^n \left( D_T(y_i,
\hat{\mu}_i) - D_T(y_i, \tilde{\mu}_i) \right) \sim
\chi^2_{\text{df}},\]</span></p>
<p>where <span class="math inline">\(\text{df} = \dim(\hat{\mu}) -
\dim(\tilde{\mu})\)</span>.</p>
</div>
<h1 class="unnumbered" id="proofs">Proofs</h1>
<p>The proof of the theorem relies on the properties of the Tweedie
distribution and the theory of generalized linear models. We outline the
key steps:</p>
<div class="proof">
<p><em>Proof.</em> 1. **Taylor Expansion**: Expand the negative
log-likelihood around the true parameter values, using a second-order
Taylor approximation. This yields a quadratic form in the
parameters.</p>
<p>2. **Asymptotic Normality**: Invoke the central limit theorem to
establish that the score function is asymptotically normal, with mean
zero and covariance matrix given by the Fisher information.</p>
<p>3. **Wald’s Theorem**: Apply Wald’s theorem to derive the asymptotic
distribution of the deviance difference, recognizing that it is a
quadratic form in the parameter estimates.</p>
<p>4. **Chi-Squared Distribution**: Show that the resulting quadratic
form follows a chi-squared distribution, with degrees of freedom equal
to the difference in model dimensions.</p>
<p>The detailed algebraic manipulations and theoretical justifications
are omitted for brevity but can be found in standard texts on
GLMs. ◻</p>
</div>
<h1 class="unnumbered" id="properties-and-corollaries">Properties and
Corollaries</h1>
<p>The Tweedie Deviance exhibits several properties that are crucial for
its practical application. We enumerate and discuss these properties
below.</p>
<ol>
<li><p>**Non-Negativity**: For all <span class="math inline">\(y_i,
\mu_i &gt; 0\)</span>, the Tweedie Deviance is non-negative. This
property ensures that it serves as a valid measure of discrepancy.</p>
<div class="proof">
<p><em>Proof.</em> The non-negativity follows from the fact that <span
class="math inline">\(D_T(y_i, \mu_i)\)</span> is derived from a
negative log-likelihood, which is always non-negative for exponential
family distributions. ◻</p>
</div></li>
<li><p>**Minimization Property**: The deviance is minimized when <span
class="math inline">\(\mu_i = y_i\)</span>. This property justifies its
use in estimating model parameters.</p>
<div class="proof">
<p><em>Proof.</em> To see this, consider the first derivative of <span
class="math inline">\(D_T(y_i, \mu_i)\)</span> with respect to <span
class="math inline">\(\mu_i\)</span>. Setting the derivative equal to
zero yields <span class="math inline">\(\mu_i = y_i\)</span>, which is
indeed a minimum. ◻</p>
</div></li>
<li><p>**Additivity**: The total deviance for <span
class="math inline">\(n\)</span> independent observations is the sum of
the individual deviances. This property facilitates the aggregation of
model fit measures across multiple data points.</p>
<div class="proof">
<p><em>Proof.</em> The additivity arises from the independence
assumption, which implies that the negative log-likelihood is additive.
Consequently, the deviance inherits this property. ◻</p>
</div></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>The Tweedie Deviance is a powerful tool for assessing model fit
within the framework of generalized linear models, particularly when
dealing with data that exhibit a point mass at zero. Its theoretical
properties and practical applications make it an indispensable component
of modern statistical modeling. Future research directions include the
development of robust variants of the deviance and its extension to more
complex data structures.</p>
</body>
</html>
{% include "footer.html" %}


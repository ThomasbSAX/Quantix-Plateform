{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par Feature Engineering: Techniques et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par Feature Engineering: Techniques et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par <em>feature engineering</em> est une étape cruciale
dans le processus de modélisation prédictive. À l’ère des données
massives et de l’apprentissage automatique, la qualité des
caractéristiques (features) utilisées peut faire la différence entre un
modèle performant et un modèle inefficace. L’origine de cette technique
remonte aux premiers travaux en statistique et en apprentissage
automatique, où l’on cherchait à transformer des variables catégorielles
en variables numériques pour les rendre exploitables par des
algorithmes.</p>
<p>L’importance du <em>feature engineering</em> réside dans sa capacité
à améliorer la performance des modèles en réduisant la dimensionnalité,
en éliminant les redondances et en capturant des informations
pertinentes. Dans ce chapitre, nous explorerons les différentes
techniques d’encodage, leurs définitions formelles et leurs applications
pratiques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Avant de plonger dans les techniques d’encodage, il est essentiel de
comprendre ce que nous cherchons à accomplir. Supposons que nous ayons
une variable catégorielle <span class="math inline">\(C\)</span> prenant
des valeurs dans un ensemble fini <span class="math inline">\(\{c_1,
c_2, \ldots, c_k\}\)</span>. Notre objectif est de transformer cette
variable en une ou plusieurs variables numériques qui préservent autant
que possible l’information originale.</p>
<div class="definition">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
prenant des valeurs dans <span class="math inline">\(\{c_1, c_2, \ldots,
c_k\}\)</span>. L’encodage par One-Hot consiste à créer <span
class="math inline">\(k\)</span> nouvelles variables binaires <span
class="math inline">\(X_1, X_2, \ldots, X_k\)</span> telles que : <span
class="math display">\[X_i = \begin{cases}
1 &amp; \text{si } C = c_i, \\
0 &amp; \text{sinon.}
\end{cases}\]</span> Autrement dit, pour chaque valeur <span
class="math inline">\(c_i\)</span> de <span
class="math inline">\(C\)</span>, nous créons une variable binaire qui
vaut 1 si la valeur est présente et 0 sinon.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
ordonnée prenant des valeurs dans <span class="math inline">\(\{c_1,
c_2, \ldots, c_k\}\)</span> avec un ordre naturel <span
class="math inline">\(c_1 &lt; c_2 &lt; \ldots &lt; c_k\)</span>.
L’encodage ordinal consiste à associer à chaque valeur <span
class="math inline">\(c_i\)</span> un nombre entier <span
class="math inline">\(i\)</span>, de sorte que : <span
class="math display">\[X = i \quad \text{si} \quad C = c_i.\]</span> Cet
encodage préserve l’ordre des catégories mais ne capture pas
nécessairement les distances entre elles.</p>
</div>
<h1 class="unnumbered" id="théorèmes-et-propriétés">Théorèmes et
Propriétés</h1>
<p>Dans cette section, nous explorons quelques propriétés importantes
des techniques d’encodage.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
et <span class="math inline">\(X_1, X_2, \ldots, X_k\)</span> ses
encodages One-Hot correspondants. Alors, pour toute réalisation <span
class="math inline">\(c_i\)</span> de <span
class="math inline">\(C\)</span>, nous avons : <span
class="math display">\[\sum_{j=1}^k X_j = 1.\]</span> Cette propriété
garantit que chaque observation est associée à une et une seule
catégorie.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition de l’encodage One-Hot, pour toute
réalisation <span class="math inline">\(c_i\)</span> de <span
class="math inline">\(C\)</span>, nous avons <span
class="math inline">\(X_j = 1\)</span> si et seulement si <span
class="math inline">\(j = i\)</span>. Par conséquent, la somme des
variables binaires est égale à 1. ◻</p>
</div>
<div class="theorem">
<p>Soit <span class="math inline">\(C\)</span> une variable catégorielle
ordonnée et <span class="math inline">\(X\)</span> son encodage ordinal
correspondant. Alors, pour toutes réalisations <span
class="math inline">\(c_i\)</span> et <span
class="math inline">\(c_j\)</span> de <span
class="math inline">\(C\)</span>, nous avons : <span
class="math display">\[c_i &lt; c_j \implies X(c_i) &lt;
X(c_j).\]</span> Cette propriété garantit que l’ordre des catégories est
préservé.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Par définition de l’encodage ordinal, <span
class="math inline">\(X(c_i) = i\)</span> et <span
class="math inline">\(X(c_j) = j\)</span>. Puisque <span
class="math inline">\(c_i &lt; c_j\)</span>, il s’ensuit que <span
class="math inline">\(i &lt; j\)</span> et donc <span
class="math inline">\(X(c_i) &lt; X(c_j)\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="applications-pratiques">Applications
Pratiques</h1>
<p>Les techniques d’encodage par <em>feature engineering</em> sont
largement utilisées dans divers domaines, notamment en apprentissage
automatique et en analyse de données.</p>
<div class="example">
<p>Considérons un modèle de régression linéaire où l’une des variables
explicatives est catégorielle, par exemple le sexe d’un individu
(Masculin, Féminin). En utilisant l’encodage One-Hot, nous créons deux
variables binaires : <span class="math display">\[X_{\text{Masculin}} =
\begin{cases}
1 &amp; \text{si sexe = Masculin}, \\
0 &amp; \text{sinon.}
\end{cases} \quad
X_{\text{Féminin}} = \begin{cases}
1 &amp; \text{si sexe = Féminin}, \\
0 &amp; \text{sinon.}
\end{cases}\]</span> Le modèle de régression peut alors estimer l’effet
de chaque catégorie sur la variable dépendante.</p>
</div>
<div class="example">
<p>Considérons un modèle de classement où l’une des variables
explicatives est une note de satisfaction (Faible, Moyenne, Élevée). En
utilisant l’encodage ordinal, nous associons à chaque note un nombre
entier : <span class="math display">\[X = \begin{cases}
1 &amp; \text{si satisfaction = Faible}, \\
2 &amp; \text{si satisfaction = Moyenne}, \\
3 &amp; \text{si satisfaction = Élevée}.
\end{cases}\]</span> Le modèle de classement peut alors utiliser cette
variable numérique pour prédire l’ordre des observations.</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par <em>feature engineering</em> est une technique
puissante pour transformer des variables catégorielles en variables
numériques, permettant ainsi leur utilisation dans divers modèles de
machine learning. Les techniques d’encodage One-Hot et ordinal sont
parmi les plus couramment utilisées, chacune ayant ses avantages et ses
inconvénients. En comprenant ces techniques et leurs propriétés, nous
pouvons améliorer la performance de nos modèles et extraire des
informations plus pertinentes de nos données.</p>
</body>
</html>
{% include "footer.html" %}


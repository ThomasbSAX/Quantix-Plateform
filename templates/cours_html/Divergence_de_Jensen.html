{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Jensen</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Jensen</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Jensen émerge dans le cadre de l’analyse des
inégalités et des fonctions convexes. Elle trouve ses racines dans les
travaux de Jensen sur les fonctions convexes et leur propriétés. Cette
notion est indispensable pour comprendre les limites des approximations
linéaires dans des contextes non-linéaires, notamment en théorie de
l’information et en apprentissage automatique.</p>
<h1 id="définitions">Définitions</h1>
<p>Considérons une fonction <span class="math inline">\(f\)</span>
convexe sur un ensemble <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons à mesurer
l’erreur commise en approximant <span class="math inline">\(f\)</span>
par une fonction affine. Cette erreur est précisément ce que capture la
divergence de Jensen.</p>
<div class="definition">
<p>Soit <span class="math inline">\(f: \mathcal{X} \rightarrow
\mathbb{R}\)</span> une fonction convexe et <span
class="math inline">\(p\)</span> une distribution de probabilité sur
<span class="math inline">\(\mathcal{X}\)</span>. La divergence de
Jensen est définie comme : <span class="math display">\[D_f(p) =
\mathbb{E}_{X \sim p}[f(X)] - f\left(\mathbb{E}_{X \sim
p}[X]\right)\]</span> où <span class="math inline">\(\mathbb{E}\)</span>
désigne l’espérance.</p>
</div>
<p>Cette divergence peut également être exprimée en termes d’intégrales
: <span class="math display">\[D_f(p) = \int_{\mathcal{X}} f(x) \, dp(x)
- f\left(\int_{\mathcal{X}} x \, dp(x)\right)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Nous présentons ici un théorème fondamental concernant la divergence
de Jensen.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f\)</span> une fonction convexe sur
un ensemble <span class="math inline">\(\mathcal{X}\)</span>. Pour toute
distribution de probabilité <span class="math inline">\(p\)</span> sur
<span class="math inline">\(\mathcal{X}\)</span>, on a : <span
class="math display">\[\mathbb{E}_{X \sim p}[f(X)] \geq
f\left(\mathbb{E}_{X \sim p}[X]\right)\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la définition
même de la convexité. Soit <span class="math inline">\(x_0\)</span> un
point tel que <span class="math inline">\(f(x_0)\)</span> est le minimum
de <span class="math inline">\(f\)</span>. Par convexité, pour tout
<span class="math inline">\(x \in \mathcal{X}\)</span>, on a : <span
class="math display">\[f(x) \geq f(x_0) + \nabla f(x_0)^T (x -
x_0)\]</span> En prenant l’espérance des deux côtés, on obtient : <span
class="math display">\[\mathbb{E}_{X \sim p}[f(X)] \geq f(x_0) + \nabla
f(x_0)^T \left(\mathbb{E}_{X \sim p}[X] - x_0\right)\]</span> En
choisissant <span class="math inline">\(x_0 = \mathbb{E}_{X \sim
p}[X]\)</span>, on obtient : <span class="math display">\[\mathbb{E}_{X
\sim p}[f(X)] \geq f\left(\mathbb{E}_{X \sim p}[X]\right)\]</span> ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Nous énumérons ici quelques propriétés importantes de la divergence
de Jensen.</p>
<ol>
<li><p>Pour une fonction <span class="math inline">\(f\)</span>
strictement convexe, la divergence de Jensen est strictement positive
pour toute distribution non-dégénérée <span
class="math inline">\(p\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> La strict convexité de <span
class="math inline">\(f\)</span> implique que l’inégalité de Jensen est
stricte : <span class="math display">\[\mathbb{E}_{X \sim p}[f(X)] &gt;
f\left(\mathbb{E}_{X \sim p}[X]\right)\]</span> pour toute distribution
<span class="math inline">\(p\)</span> qui n’est pas une masse de
Dirac. ◻</p>
</div></li>
<li><p>La divergence de Jensen est invariante par translation.
C’est-à-dire, pour toute constante <span class="math inline">\(c \in
\mathbb{R}\)</span>, on a : <span class="math display">\[D_{f+c}(p) =
D_f(p)\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle directement de la linéarité de
l’espérance : <span class="math display">\[\mathbb{E}_{X \sim p}[f(X) +
c] = \mathbb{E}_{X \sim p}[f(X)] + c\]</span> et <span
class="math display">\[f\left(\mathbb{E}_{X \sim p}[X]\right) + c =
f\left(\mathbb{E}_{X \sim p}[X]\right) + c\]</span> donc <span
class="math display">\[D_{f+c}(p) = \mathbb{E}_{X \sim p}[f(X) + c] -
\left(f\left(\mathbb{E}_{X \sim p}[X]\right) + c\right) =
D_f(p)\]</span> ◻</p>
</div></li>
<li><p>La divergence de Jensen est homogène de degré 1. C’est-à-dire,
pour tout <span class="math inline">\(\lambda &gt; 0\)</span>, on a :
<span class="math display">\[D_{\lambda f}(p) = \lambda
D_f(p)\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cela découle de l’homogénéité de l’espérance : <span
class="math display">\[\mathbb{E}_{X \sim p}[\lambda f(X)] = \lambda
\mathbb{E}_{X \sim p}[f(X)]\]</span> et <span
class="math display">\[\lambda f\left(\mathbb{E}_{X \sim p}[X]\right) =
\lambda f\left(\mathbb{E}_{X \sim p}[X]\right)\]</span> donc <span
class="math display">\[D_{\lambda f}(p) = \mathbb{E}_{X \sim p}[\lambda
f(X)] - \lambda f\left(\mathbb{E}_{X \sim p}[X]\right) = \lambda
D_f(p)\]</span> ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence de Jensen est un outil puissant pour analyser les
erreurs d’approximation linéaire dans des contextes non-linéaires. Ses
propriétés et ses applications en font un concept central en théorie de
l’information et en apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


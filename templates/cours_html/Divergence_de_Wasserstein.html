{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Wasserstein : Une Mesure de Distance pour les Distributions de Probabilité</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Wasserstein : Une Mesure de Distance
pour les Distributions de Probabilité</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La divergence de Wasserstein, également connue sous le nom de
distance de Wasserstein ou distance d’Emd (Earth Mover’s Distance), est
une mesure de distance entre deux distributions de probabilité. Elle
trouve ses origines dans le domaine des mathématiques appliquées et a
été popularisée par les travaux de Leonid Kantorovich dans les années
1940. Cette notion est particulièrement utile en théorie des
probabilités, en traitement d’images et en apprentissage
automatique.</p>
<p>L’idée sous-jacente à la divergence de Wasserstein est de mesurer le
coût minimal pour transformer une distribution de probabilité en une
autre. Cette approche intuitive et puissante permet de capturer des
informations fines sur les distributions, ce qui la rend indispensable
dans de nombreux domaines scientifiques et techniques.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour définir formellement la divergence de Wasserstein, commençons
par comprendre ce que nous cherchons à mesurer. Imaginons deux
distributions de probabilité sur un espace métrique <span
class="math inline">\((X, d)\)</span>. Nous voulons quantifier la
quantité de travail nécessaire pour transformer une distribution en
l’autre, où le "travail" est défini par le coût de transport des masses
entre les deux distributions.</p>
<div class="definition">
<p>Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux distributions de probabilité sur
un espace métrique <span class="math inline">\((X, d)\)</span>. La
divergence de Wasserstein d’ordre <span class="math inline">\(p\)</span>
entre <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie comme : <span
class="math display">\[W_p(\mu, \nu) = \left( \inf_{\gamma \in
\Gamma(\mu, \nu)} \int_{X \times X} d(x, y)^p \, d\gamma(x, y)
\right)^{1/p}\]</span> où <span class="math inline">\(\Gamma(\mu,
\nu)\)</span> est l’ensemble des mesures de couplage entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, c’est-à-dire l’ensemble des mesures
<span class="math inline">\(\gamma\)</span> sur <span
class="math inline">\(X \times X\)</span> telles que pour tout ensemble
mesurable <span class="math inline">\(A \subset X\)</span>, <span
class="math display">\[\gamma(A \times X) = \mu(A) \quad \text{et} \quad
\gamma(X \times A) = \nu(A).\]</span></p>
</div>
<p>Pour une meilleure compréhension, reformulons cette définition de
manière équivalente. La divergence de Wasserstein peut également être
exprimée comme : <span class="math display">\[W_p(\mu, \nu) =
\inf_{\gamma \in \Gamma(\mu, \nu)} \left( \int_{X \times X} d(x, y)^p \,
d\gamma(x, y) \right)^{1/p}.\]</span> Cette formulation met en évidence
le fait que nous cherchons à minimiser l’intégrale du coût de transport
<span class="math inline">\(d(x, y)^p\)</span> sur toutes les mesures de
couplage possibles entre <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la divergence de Wasserstein est le
théorème de Kantorovich-Rubinstein, qui donne une caractérisation
alternative de la divergence de Wasserstein d’ordre 1.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux distributions de probabilité sur
un espace métrique compact <span class="math inline">\((X, d)\)</span>.
Alors, <span class="math display">\[W_1(\mu, \nu) = \sup_{f \in
\text{Lip}_1(X)} \left( \int_X f \, d\mu - \int_X f \, d\nu
\right),\]</span> où <span
class="math inline">\(\text{Lip}_1(X)\)</span> est l’ensemble des
fonctions Lipschitziennes de constante de Lipschitz inférieure ou égale
à 1.</p>
</div>
<p>Pour démontrer ce théorème, nous utilisons le fait que toute fonction
Lipschitzienne peut être approchée par des fonctions simples et que
l’infimum sur les mesures de couplage peut être transformé en un
supremum sur les fonctions Lipschitziennes.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>La preuve du théorème de Kantorovich-Rubinstein repose sur plusieurs
étapes clés. Tout d’abord, nous montrons que pour toute fonction
Lipschitzienne <span class="math inline">\(f\)</span>, l’intégrale de
<span class="math inline">\(f\)</span> par rapport à une mesure de
couplage <span class="math inline">\(\gamma\)</span> est bornée par le
coût de transport. Ensuite, nous utilisons la dualité entre les mesures
et les fonctions pour transformer l’infimum sur les mesures de couplage
en un supremum sur les fonctions Lipschitziennes.</p>
<div class="proof">
<p><em>Proof.</em> Soient <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span> deux distributions de
probabilité sur un espace métrique compact <span
class="math inline">\((X, d)\)</span>. Pour toute fonction
Lipschitzienne <span class="math inline">\(f \in
\text{Lip}_1(X)\)</span>, nous avons : <span
class="math display">\[\int_X f \, d\mu - \int_X f \, d\nu = \int_{X
\times X} (f(x) - f(y)) \, d\gamma(x, y).\]</span> En utilisant
l’inégalité de Lipschitz, nous obtenons : <span
class="math display">\[\left| \int_X f \, d\mu - \int_X f \, d\nu
\right| \leq \int_{X \times X} d(x, y) \, d\gamma(x, y).\]</span> En
prenant le supremum sur toutes les fonctions Lipschitziennes <span
class="math inline">\(f \in \text{Lip}_1(X)\)</span>, nous obtenons :
<span class="math display">\[\sup_{f \in \text{Lip}_1(X)} \left( \int_X
f \, d\mu - \int_X f \, d\nu \right) \leq W_1(\mu, \nu).\]</span> Pour
montrer l’égalité, nous utilisons le fait que pour toute mesure de
couplage <span class="math inline">\(\gamma\)</span>, il existe une
fonction Lipschitzienne <span class="math inline">\(f\)</span> telle que
: <span class="math display">\[\int_{X \times X} (f(x) - f(y)) \,
d\gamma(x, y) = \int_{X \times X} d(x, y) \, d\gamma(x, y).\]</span>
Cela achève la preuve. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Wasserstein possède plusieurs propriétés
intéressantes qui en font un outil puissant pour comparer des
distributions de probabilité. Voici quelques-unes de ces propriétés
:</p>
<ol>
<li><p>**Non-négativité** : Pour toute paire de distributions <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, nous avons <span
class="math inline">\(W_p(\mu, \nu) \geq 0\)</span>.</p></li>
<li><p>**Symétrie** : Pour toute paire de distributions <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>, nous avons <span
class="math inline">\(W_p(\mu, \nu) = W_p(\nu, \mu)\)</span>.</p></li>
<li><p>**Inégalité triangulaire** : Pour toute triplet de distributions
<span class="math inline">\(\mu, \nu, \rho\)</span>, nous avons <span
class="math inline">\(W_p(\mu, \rho) \leq W_p(\mu, \nu) + W_p(\nu,
\rho)\)</span>.</p></li>
</ol>
<p>Ces propriétés montrent que la divergence de Wasserstein est une
véritable distance sur l’espace des distributions de probabilité. De
plus, elle possède des propriétés métriques importantes qui la rendent
utile dans de nombreuses applications.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Wasserstein est une mesure de distance puissante et
intuitive pour comparer des distributions de probabilité. Ses
applications vont du traitement d’images à l’apprentissage automatique,
en passant par la théorie des probabilités. Grâce à ses propriétés
métriques et à sa capacité à capturer des informations fines sur les
distributions, la divergence de Wasserstein continue d’être un sujet de
recherche actif et prometteur.</p>
</body>
</html>
{% include "footer.html" %}


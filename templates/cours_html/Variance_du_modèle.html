{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Variance du Modèle : Une Analyse Mathématique Approfondie</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Variance du Modèle : Une Analyse Mathématique
Approfondie</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<p>Voici un article scientifique complet en LaTeX sur le sujet de la
variance du modèle :</p>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La variance du modèle est une notion fondamentale en statistique et
en apprentissage automatique. Elle mesure la sensibilité d’un modèle aux
variations des données d’entraînement, c’est-à-dire à quel point le
modèle change lorsqu’on modifie légèrement l’ensemble d’apprentissage.
Cette notion est cruciale pour comprendre la stabilité et la fiabilité
des prédictions d’un modèle.</p>
<p>L’essor des méthodes d’apprentissage automatique a rendu la
compréhension de la variance du modèle indispensable. En effet, un
modèle avec une grande variance peut surapprendre les données
d’entraînement et mal généraliser aux nouvelles observations. À
l’inverse, un modèle avec une faible variance peut sous-apprendre et
manquer de précision.</p>
<p>Dans cet article, nous explorons les définitions formelles de la
variance du modèle, présentons des théorèmes clés et discutons des
propriétés importantes. Nous nous concentrons sur les aspects
mathématiques pour fournir une compréhension approfondie de cette
notion.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la variance du modèle, commençons par comprendre ce
que nous cherchons à mesurer. Supposons que nous avons un ensemble de
données d’entraînement <span class="math inline">\(D\)</span> et un
modèle <span class="math inline">\(f\)</span> entraîné sur ces données.
Si nous modifions légèrement <span class="math inline">\(D\)</span>, par
exemple en ajoutant ou en supprimant quelques observations, comment le
modèle <span class="math inline">\(f\)</span> change-t-il ?</p>
<p>La variance du modèle mesure précisément cette sensibilité.
Formellement, nous définissons la variance du modèle comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\mathcal{D}\)</span> l’ensemble de
tous les ensembles de données possibles, et soit <span
class="math inline">\(f_D\)</span> le modèle entraîné sur un ensemble de
données <span class="math inline">\(D \in \mathcal{D}\)</span>. La
variance du modèle en un point <span class="math inline">\(x\)</span>
est définie comme la variance de <span
class="math inline">\(f_D(x)\)</span> sur tous les ensembles de données
possibles <span class="math inline">\(D\)</span>.</p>
<p>Matérialisons cette idée avec une notation plus formelle. Soit <span
class="math inline">\(P(D)\)</span> la distribution de probabilité sur
les ensembles de données <span class="math inline">\(D\)</span>. La
variance du modèle en un point <span class="math inline">\(x\)</span>
est donnée par :</p>
<p><span class="math display">\[\text{Var}(f)(x) = \mathbb{E}_{D \sim
P(D)} \left[ (f_D(x) - \mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)])^2
\right]\]</span></p>
<p>où <span class="math inline">\(\mathbb{E}\)</span> désigne
l’espérance mathématique.</p>
</div>
<p>Une autre manière de formuler cette définition est d’utiliser la
notion d’espérance conditionnelle. Soit <span
class="math inline">\(y\)</span> la vraie valeur de la variable
dépendante en un point <span class="math inline">\(x\)</span>. La
variance du modèle peut également être exprimée comme :</p>
<p><span class="math display">\[\text{Var}(f)(x) = \mathbb{E}_{D \sim
P(D)} \left[ (f_D(x) - y)^2 \right] - \mathbb{E}_{D \sim P(D)} \left[
(f_D(x) - y)^2 \mid D \right]\]</span></p>
<p>Cette formulation met en évidence la différence entre la variance
totale et la variance conditionnelle.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Pour mieux comprendre la variance du modèle, examinons quelques
théorèmes clés. Le premier théorème que nous présentons est le théorème
de la décomposition de l’erreur, qui relie la variance du modèle à
d’autres notions importantes en apprentissage automatique.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(f_D\)</span> un modèle entraîné sur
un ensemble de données <span class="math inline">\(D\)</span>, et soit
<span class="math inline">\(y\)</span> la vraie valeur de la variable
dépendante en un point <span class="math inline">\(x\)</span>. L’erreur
quadratique moyenne (MSE) peut être décomposée comme suit :</p>
<p><span class="math display">\[\mathbb{E}_{D \sim P(D)} \left[ (f_D(x)
- y)^2 \right] = \text{Bias}(f)(x)^2 + \text{Var}(f)(x) +
\sigma^2\]</span></p>
<p>où <span class="math inline">\(\text{Bias}(f)(x) = \mathbb{E}_{D \sim
P(D)} [f_D(x)] - y\)</span> est le biais du modèle en <span
class="math inline">\(x\)</span>, et <span
class="math inline">\(\sigma^2\)</span> est la variance de l’erreur
irréductible.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la décomposition
de l’erreur en trois composantes : le biais, la variance et l’erreur
irréductible. Commençons par exprimer l’erreur quadratique moyenne :</p>
<p><span class="math display">\[\mathbb{E}_{D \sim P(D)} \left[ (f_D(x)
- y)^2 \right] = \mathbb{E}_{D \sim P(D)} \left[ (f_D(x) -
\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)] + \mathbb{E}_{D&#39; \sim
P(D)} [f_{D&#39;}(x)] - y)^2 \right]\]</span></p>
<p>En développant cette expression, nous obtenons :</p>
<p><span class="math display">\[\mathbb{E}_{D \sim P(D)} \left[ (f_D(x)
- y)^2 \right] = \mathbb{E}_{D \sim P(D)} \left[ (f_D(x) -
\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)])^2 \right] +
(\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)] - y)^2 + 2 \mathbb{E}_{D
\sim P(D)} \left[ (f_D(x) - \mathbb{E}_{D&#39; \sim P(D)}
[f_{D&#39;}(x)])(\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)] - y)
\right]\]</span></p>
<p>Le dernier terme est nul car <span
class="math inline">\(\mathbb{E}_{D \sim P(D)} [f_D(x) -
\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)]] = 0\)</span>. Il reste
donc :</p>
<p><span class="math display">\[\mathbb{E}_{D \sim P(D)} \left[ (f_D(x)
- y)^2 \right] = \text{Var}(f)(x) + \text{Bias}(f)(x)^2\]</span></p>
<p>En ajoutant l’erreur irréductible <span
class="math inline">\(\sigma^2\)</span>, nous obtenons la décomposition
souhaitée. ◻</p>
</div>
<p>Un autre théorème important est le théorème de la loi des grands
nombres pour les modèles. Ce théorème montre que, sous certaines
conditions, la variance du modèle tend vers zéro lorsque la taille de
l’ensemble d’apprentissage augmente.</p>
<div class="proof">
<p><em>Proof.</em> La preuve de ce théorème repose sur la loi des grands
nombres. En effet, lorsque la taille de l’ensemble d’apprentissage
augmente, les fluctuations du modèle dûes aux variations des données
deviennent négligeables. Formellement, nous pouvons écrire :</p>
<p><span class="math display">\[\text{Var}(f_n)(x) = \mathbb{E}_{D \sim
P(D)} \left[ (f_D(x) - \mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)])^2
\right] \leq \mathbb{E}_{D \sim P(D)} \left[ \|f_D - f_{D&#39;}\|^2
\right]\]</span></p>
<p>où <span class="math inline">\(\| \cdot \|\)</span> désigne une norme
appropriée. En utilisant la loi des grands nombres, nous savons que
<span class="math inline">\(\|f_D - f_{D&#39;}\|\)</span> tend vers zéro
en probabilité lorsque <span class="math inline">\(n\)</span> tend vers
l’infini. Par conséquent, la variance du modèle tend également vers
zéro. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Examinons maintenant quelques propriétés importantes de la variance
du modèle. Ces propriétés nous aideront à mieux comprendre le
comportement de la variance dans différents contextes.</p>
<div class="proposition">
<p>La variance du modèle satisfait les propriétés suivantes :</p>
<ol>
<li><p>La variance est toujours non négative, c’est-à-dire <span
class="math inline">\(\text{Var}(f)(x) \geq 0\)</span> pour tout <span
class="math inline">\(x\)</span>.</p></li>
<li><p>La variance est invariante par translation, c’est-à-dire que si
<span class="math inline">\(g(x) = f(x) + c\)</span> pour une constante
<span class="math inline">\(c\)</span>, alors <span
class="math inline">\(\text{Var}(g)(x) =
\text{Var}(f)(x)\)</span>.</p></li>
<li><p>La variance est homogène de degré deux, c’est-à-dire que si <span
class="math inline">\(g(x) = a f(x)\)</span> pour une constante <span
class="math inline">\(a\)</span>, alors <span
class="math inline">\(\text{Var}(g)(x) = a^2
\text{Var}(f)(x)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> Preuve de la propriété (i) : La non-négativité de la
variance découle directement de sa définition. En effet, <span
class="math inline">\(\text{Var}(f)(x)\)</span> est une espérance de
carrés, qui sont toujours non négatifs.</p>
<p>Preuve de la propriété (ii) : Soit <span class="math inline">\(g(x) =
f(x) + c\)</span>. Alors,</p>
<p><span class="math display">\[\text{Var}(g)(x) = \mathbb{E}_{D \sim
P(D)} \left[ (g_D(x) - \mathbb{E}_{D&#39; \sim P(D)} [g_{D&#39;}(x)])^2
\right] = \mathbb{E}_{D \sim P(D)} \left[ (f_D(x) + c -
\mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)] - c)^2 \right] =
\text{Var}(f)(x)\]</span></p>
<p>Preuve de la propriété (iii) : Soit <span class="math inline">\(g(x)
= a f(x)\)</span>. Alors,</p>
<p><span class="math display">\[\text{Var}(g)(x) = \mathbb{E}_{D \sim
P(D)} \left[ (a f_D(x) - a \mathbb{E}_{D&#39; \sim P(D)}
[f_{D&#39;}(x)])^2 \right] = a^2 \mathbb{E}_{D \sim P(D)} \left[ (f_D(x)
- \mathbb{E}_{D&#39; \sim P(D)} [f_{D&#39;}(x)])^2 \right] = a^2
\text{Var}(f)(x)\]</span> ◻</p>
</div>
<p>Un corollaire important de la décomposition de l’erreur est le
suivant :</p>
<div class="corollary">
<p>Soit <span class="math inline">\(f_D\)</span> un modèle entraîné sur
un ensemble de données <span class="math inline">\(D\)</span>. Alors,
pour minimiser l’erreur quadratique moyenne en un point <span
class="math inline">\(x\)</span>, il faut minimiser à la fois le biais
et la variance du modèle.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve découle directement du théorème de la
décomposition de l’erreur. En effet, l’erreur quadratique moyenne est la
somme du carré du biais, de la variance et de l’erreur irréductible. Par
conséquent, pour minimiser l’erreur quadratique moyenne, il faut
minimiser à la fois le biais et la variance du modèle. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré la notion de variance du modèle
en détail. Nous avons présenté des définitions formelles, discuté des
théorèmes clés et examiné les propriétés importantes de la variance. La
compréhension de la variance du modèle est cruciale pour développer des
modèles robustes et fiables en apprentissage automatique.</p>
<p>En conclusion, la variance du modèle est une notion fondamentale qui
mérite une attention particulière. Les résultats présentés dans cet
article fournissent une base solide pour des recherches futures sur ce
sujet fascinant.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de McDiarmid : Un outil fondamental en théorie des probabilités</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de McDiarmid : Un outil fondamental en
théorie des probabilités</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de McDiarmid, nommée en l’honneur du mathématicien
écossais David McDiarmid, est un résultat profond et élégant en théorie
des probabilités. Elle émerge dans le cadre de l’étude des variables
aléatoires dépendantes, et plus précisément des martingales. Cette
inégalité est indispensable pour établir des bornes de concentration
pour des fonctions de variables aléatoires dépendantes, ce qui est
crucial dans de nombreuses applications en statistique, apprentissage
automatique et théorie des graphes.</p>
<p>L’origine historique de cette inégalité remonte aux années 1980,
lorsque McDiarmid a introduit ce résultat pour répondre à des questions
de complexité algorithmique. Depuis, elle est devenue un outil
incontournable pour les mathématiciens et les statisticiens cherchant à
comprendre le comportement des fonctions de variables aléatoires
dépendantes.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant d’énoncer l’inégalité de McDiarmid, il est essentiel de
comprendre les concepts fondamentaux qui la sous-tendent. Nous
commençons par définir ce qu’est une fonction de variables aléatoires
dépendantes et introduisons les notions de martingale et de différence
conditionnelle.</p>
<h2 id="fonction-de-variables-aléatoires-dépendantes">Fonction de
variables aléatoires dépendantes</h2>
<p>Considérons un ensemble de variables aléatoires <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span> définies sur un
espace probabilisé <span class="math inline">\((\Omega, \mathcal{F},
P)\)</span>. Une fonction <span class="math inline">\(f: \mathbb{R}^n
\rightarrow \mathbb{R}\)</span> est dite dépendante des variables
aléatoires <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> si
elle peut être exprimée comme une fonction de ces variables.</p>
<p>Formellement, nous avons : <span class="math display">\[f(X_1, X_2,
\ldots, X_n)\]</span></p>
<h2 id="martingale">Martingale</h2>
<p>Une martingale est une séquence de variables aléatoires <span
class="math inline">\(\{M_n\}_{n \geq 0}\)</span> telle que, pour tout
<span class="math inline">\(n\)</span>, l’espérance conditionnelle de
<span class="math inline">\(M_{n+1}\)</span> sachant <span
class="math inline">\(M_0, M_1, \ldots, M_n\)</span> est égale à <span
class="math inline">\(M_n\)</span>. En d’autres termes : <span
class="math display">\[E[M_{n+1} | M_0, M_1, \ldots, M_n] =
M_n\]</span></p>
<h2 id="différence-conditionnelle">Différence conditionnelle</h2>
<p>La différence conditionnelle d’une fonction <span
class="math inline">\(f\)</span> par rapport à une variable aléatoire
<span class="math inline">\(X_i\)</span> est définie comme la différence
entre la valeur de <span class="math inline">\(f\)</span> lorsque <span
class="math inline">\(X_i\)</span> prend une valeur fixe et sa valeur
attendue. Formellement, pour chaque <span
class="math inline">\(i\)</span>, nous définissons : <span
class="math display">\[D_i f(X_1, X_2, \ldots, X_n) = \sup_{x_i,
x_i&#39;} |f(X_1, \ldots, X_{i-1}, x_i, X_{i+1}, \ldots, X_n) - f(X_1,
\ldots, X_{i-1}, x_i&#39;, X_{i+1}, \ldots, X_n)|\]</span></p>
<h1 id="inégalité-de-mcdiarmid">Inégalité de McDiarmid</h1>
<p>Nous sommes maintenant prêts à énoncer l’inégalité de McDiarmid.
Cette inégalité fournit une borne supérieure pour la probabilité que la
valeur d’une fonction de variables aléatoires dépendantes s’écarte
significativement de son espérance.</p>
<h2 id="énoncé">Énoncé</h2>
<p>Soit <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> des
variables aléatoires indépendantes définies sur un espace probabilisé
<span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, et soit
<span class="math inline">\(f: \mathbb{R}^n \rightarrow
\mathbb{R}\)</span> une fonction mesurable. Supposons que pour chaque
<span class="math inline">\(i\)</span>, il existe des constantes <span
class="math inline">\(c_i \geq 0\)</span> telles que : <span
class="math display">\[|f(X_1, X_2, \ldots, X_n) - f(X_1, \ldots,
X_{i-1}, x_i&#39;, X_{i+1}, \ldots, X_n)| \leq c_i\]</span></p>
<p>Alors, pour tout <span class="math inline">\(t &gt; 0\)</span>, nous
avons : <span class="math display">\[P(|f(X_1, X_2, \ldots, X_n) -
E[f(X_1, X_2, \ldots, X_n)]| \geq t) \leq 2
\exp\left(-\frac{2t^2}{\sum_{i=1}^n c_i^2}\right)\]</span></p>
<h2 id="formulations-équivalentes">Formulations équivalentes</h2>
<p>L’inégalité de McDiarmid peut être formulée de plusieurs manières
équivalentes. Par exemple, en utilisant la notation des différences
conditionnelles, nous avons : <span class="math display">\[P(|f(X_1,
X_2, \ldots, X_n) - E[f]| \geq t) \leq 2
\exp\left(-\frac{2t^2}{\sum_{i=1}^n D_i f^2}\right)\]</span></p>
<p>De plus, en introduisant la variance de <span
class="math inline">\(f\)</span>, nous pouvons écrire : <span
class="math display">\[P(|f - E[f]| \geq t) \leq 2
\exp\left(-\frac{2t^2}{\text{Var}(f) + \sum_{i=1}^n
c_i^2}\right)\]</span></p>
<h1 id="preuves">Preuves</h1>
<p>Nous allons maintenant fournir une preuve détaillée de l’inégalité de
McDiarmid. Cette preuve repose sur des techniques avancées de théorie
des probabilités, notamment l’utilisation des martingales et des
inégalités exponentielles.</p>
<h2 id="preuve-de-linégalité-de-mcdiarmid">Preuve de l’inégalité de
McDiarmid</h2>
<p>Commençons par rappeler que les variables <span
class="math inline">\(X_1, X_2, \ldots, X_n\)</span> sont indépendantes.
Nous définissons une martingale <span class="math inline">\(\{M_k\}_{0
\leq k \leq n}\)</span> par : <span class="math display">\[M_0 =
E[f(X_1, X_2, \ldots, X_n)]\]</span> <span class="math display">\[M_k =
E[f(X_1, X_2, \ldots, X_n) | X_1, X_2, \ldots, X_k]\]</span></p>
<p>Nous avons alors : <span class="math display">\[M_n = f(X_1, X_2,
\ldots, X_n)\]</span></p>
<p>En utilisant l’inégalité de Hoeffding pour les martingales, nous
obtenons : <span class="math display">\[P(|M_n - M_0| \geq t) \leq 2
\exp\left(-\frac{2t^2}{\sum_{k=1}^n c_k^2}\right)\]</span></p>
<p>Cette inégalité est précisément l’inégalité de McDiarmid que nous
cherchions à démontrer.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de McDiarmid possède plusieurs propriétés intéressantes
et corollaires qui en découlent. Nous allons les énumérer et les
démontrer une par une.</p>
<h2 id="propriété-i-bornes-de-concentration">Propriété (i) : Bornes de
concentration</h2>
<p>L’inégalité de McDiarmid fournit une borne de concentration pour la
fonction <span class="math inline">\(f\)</span>. Plus précisément, elle
montre que la probabilité que <span class="math inline">\(f\)</span>
s’écarte de son espérance est exponentiellement petite en fonction de
<span class="math inline">\(t^2\)</span>.</p>
<h2 id="preuve">Preuve</h2>
<p>Cette propriété découle directement de l’énoncé de l’inégalité de
McDiarmid. En effet, pour tout <span class="math inline">\(t &gt;
0\)</span>, nous avons : <span class="math display">\[P(|f - E[f]| \geq
t) \leq 2 \exp\left(-\frac{2t^2}{\sum_{i=1}^n c_i^2}\right)\]</span></p>
<h2 id="propriété-ii-indépendance-des-variables">Propriété (ii) :
Indépendance des variables</h2>
<p>L’inégalité de McDiarmid s’applique à des fonctions de variables
aléatoires indépendantes. Cette hypothèse d’indépendance est cruciale
pour la validité de l’inégalité.</p>
<h2 id="preuve-1">Preuve</h2>
<p>La preuve repose sur le fait que les différences conditionnelles
<span class="math inline">\(D_i f\)</span> sont définies en supposant
que les variables <span class="math inline">\(X_1, X_2, \ldots,
X_n\)</span> sont indépendantes. Sans cette hypothèse, les bornes
fournies par l’inégalité de McDiarmid ne seraient pas valables.</p>
<h2 id="propriété-iii-généralisation">Propriété (iii) :
Généralisation</h2>
<p>L’inégalité de McDiarmid peut être généralisée à des fonctions de
variables aléatoires dépendantes sous certaines conditions. Cette
généralisation est utile dans des contextes où les variables ne sont pas
indépendantes.</p>
<h2 id="preuve-2">Preuve</h2>
<p>La généralisation repose sur l’introduction de nouvelles différences
conditionnelles qui prennent en compte les dépendances entre les
variables. En utilisant des techniques avancées de théorie des
probabilités, il est possible d’étendre l’inégalité de McDiarmid à ce
cadre plus général.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’inégalité de McDiarmid est un résultat fondamental en théorie des
probabilités, avec des applications dans de nombreux domaines. Elle
fournit une borne de concentration pour des fonctions de variables
aléatoires dépendantes, ce qui est crucial pour l’analyse statistique et
algorithmique. Les preuves et propriétés présentées dans cet article
montrent la puissance et la flexibilité de cette inégalité, ainsi que
son importance dans le développement de nouvelles méthodes en
statistique et apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


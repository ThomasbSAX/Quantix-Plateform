{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>L’inégalité de Bhattacharyya : Une mesure de divergence entre distributions</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">L’inégalité de Bhattacharyya : Une mesure de
divergence entre distributions</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’inégalité de Bhattacharyya, nommée en l’honneur du statisticien
indien Anil Kumar Bhattacharyya, émerge dans le cadre de la théorie des
probabilités et de l’information. Elle fournit une mesure de divergence
entre deux distributions de probabilité, quantifiant ainsi à quel point
ces distributions sont distinctes. Cette notion est indispensable dans
de nombreux domaines, notamment en apprentissage automatique, en théorie
de l’information et en statistique, où la comparaison de distributions
est une tâche fondamentale.</p>
<p>L’origine historique de cette inégalité remonte aux travaux de
Bhattacharyya dans les années 1940, où il a introduit une mesure de
divergence qui porte désormais son nom. Cette mesure est
particulièrement utile car elle est symétrique et bornée, ce qui la rend
adaptée à de nombreuses applications pratiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire l’inégalité de Bhattacharyya, commençons par
comprendre ce que nous cherchons à mesurer. Supposons que nous avons
deux distributions de probabilité <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> sur un espace mesurable <span
class="math inline">\(\Omega\)</span>. Nous voulons quantifier à quel
point ces deux distributions sont différentes. Une manière intuitive de
le faire est de considérer la probabilité que les deux distributions
génèrent le même événement.</p>
<p>Formellement, l’inégalité de Bhattacharyya est définie comme suit
:</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. La
divergence de Bhattacharyya entre <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_B(P || Q) = -\ln \left( \int_{\Omega}
\sqrt{dP(\omega) dQ(\omega)} \right)\]</span> où <span
class="math inline">\(dP\)</span> et <span
class="math inline">\(dQ\)</span> sont les éléments de masse ou de
densité associés à <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, respectivement.</p>
</div>
<p>Cette définition peut être réécrite de plusieurs manières. Par
exemple, si <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> sont des distributions discrètes, nous
avons : <span class="math display">\[D_B(P || Q) = -\ln \left(
\sum_{\omega \in \Omega} \sqrt{P(\omega) Q(\omega)} \right)\]</span></p>
<p>Pour des distributions continues, si <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> ont des densités de probabilité <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span>, respectivement, alors : <span
class="math display">\[D_B(P || Q) = -\ln \left( \int_{\Omega}
\sqrt{p(\omega) q(\omega)} d\omega \right)\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à l’inégalité de Bhattacharyya est le
suivant :</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un espace mesurable <span class="math inline">\(\Omega\)</span>. Alors,
pour toute fonction mesurable <span class="math inline">\(f : \Omega \to
\mathbb{R}\)</span>, nous avons : <span
class="math display">\[P(f(\omega) &gt; a) + Q(f(\omega) \leq a) \geq 2
e^{-D_B(P || Q)/8}\]</span> pour tout <span class="math inline">\(a \in
\mathbb{R}\)</span>.</p>
</div>
<p>Pour démontrer ce théorème, commençons par rappeler que la divergence
de Bhattacharyya est une mesure de la distance entre deux distributions.
Nous voulons montrer que cette divergence implique une borne sur la
probabilité que <span class="math inline">\(f\)</span> prenne des
valeurs différentes sous <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<p>Considérons d’abord le cas où <span class="math inline">\(f\)</span>
est une fonction indicatrice. Supposons que <span
class="math inline">\(f(\omega) = 1\)</span> si <span
class="math inline">\(\omega \in A\)</span> et <span
class="math inline">\(f(\omega) = 0\)</span> sinon. Alors, nous avons :
<span class="math display">\[P(f(\omega) &gt; a) = P(A) \quad \text{et}
\quad Q(f(\omega) \leq a) = 1 - Q(A)\]</span> Nous voulons montrer que :
<span class="math display">\[P(A) + (1 - Q(A)) \geq 2 e^{-D_B(P ||
Q)/8}\]</span></p>
<p>En utilisant la définition de la divergence de Bhattacharyya, nous
avons : <span class="math display">\[D_B(P || Q) = -\ln \left(
\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \right)\]</span> Nous pouvons
alors écrire : <span class="math display">\[e^{-D_B(P || Q)/8} = \left(
\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \right)^{1/8}\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons : <span
class="math display">\[\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \leq
\left( \int_{\Omega} dP(\omega) \right)^{1/2} \left( \int_{\Omega}
dQ(\omega) \right)^{1/2} = 1\]</span> Donc : <span
class="math display">\[e^{-D_B(P || Q)/8} \leq 1\]</span></p>
<p>Cependant, nous avons besoin d’une borne inférieure. Pour ce faire,
nous utilisons l’inégalité de Jensen : <span
class="math display">\[\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \geq
\sqrt{\int_{\Omega} dP(\omega) \int_{\Omega} dQ(\omega)} = 1\]</span>
Donc : <span class="math display">\[e^{-D_B(P || Q)/8} \geq
1\]</span></p>
<p>Nous avons donc : <span class="math display">\[P(A) + (1 - Q(A)) \geq
2 e^{-D_B(P || Q)/8}\]</span></p>
<p>Pour une fonction générale <span class="math inline">\(f\)</span>,
nous pouvons utiliser une approximation par des fonctions indicatrices
et appliquer le résultat précédent.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver l’inégalité de Bhattacharyya, nous avons besoin de
plusieurs étapes intermédiaires. Commençons par rappeler que la
divergence de Bhattacharyya est une mesure de la distance entre deux
distributions. Nous voulons montrer que cette divergence implique une
borne sur la probabilité que <span class="math inline">\(f\)</span>
prenne des valeurs différentes sous <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<p>Considérons d’abord le cas où <span class="math inline">\(f\)</span>
est une fonction indicatrice. Supposons que <span
class="math inline">\(f(\omega) = 1\)</span> si <span
class="math inline">\(\omega \in A\)</span> et <span
class="math inline">\(f(\omega) = 0\)</span> sinon. Alors, nous avons :
<span class="math display">\[P(f(\omega) &gt; a) = P(A) \quad \text{et}
\quad Q(f(\omega) \leq a) = 1 - Q(A)\]</span> Nous voulons montrer que :
<span class="math display">\[P(A) + (1 - Q(A)) \geq 2 e^{-D_B(P ||
Q)/8}\]</span></p>
<p>En utilisant la définition de la divergence de Bhattacharyya, nous
avons : <span class="math display">\[D_B(P || Q) = -\ln \left(
\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \right)\]</span> Nous pouvons
alors écrire : <span class="math display">\[e^{-D_B(P || Q)/8} = \left(
\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \right)^{1/8}\]</span></p>
<p>En utilisant l’inégalité de Cauchy-Schwarz, nous avons : <span
class="math display">\[\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \leq
\left( \int_{\Omega} dP(\omega) \right)^{1/2} \left( \int_{\Omega}
dQ(\omega) \right)^{1/2} = 1\]</span> Donc : <span
class="math display">\[e^{-D_B(P || Q)/8} \leq 1\]</span></p>
<p>Cependant, nous avons besoin d’une borne inférieure. Pour ce faire,
nous utilisons l’inégalité de Jensen : <span
class="math display">\[\int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \geq
\sqrt{\int_{\Omega} dP(\omega) \int_{\Omega} dQ(\omega)} = 1\]</span>
Donc : <span class="math display">\[e^{-D_B(P || Q)/8} \geq
1\]</span></p>
<p>Nous avons donc : <span class="math display">\[P(A) + (1 - Q(A)) \geq
2 e^{-D_B(P || Q)/8}\]</span></p>
<p>Pour une fonction générale <span class="math inline">\(f\)</span>,
nous pouvons utiliser une approximation par des fonctions indicatrices
et appliquer le résultat précédent.</p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’inégalité de Bhattacharyya possède plusieurs propriétés
intéressantes. Nous en listons quelques-unes ci-dessous :</p>
<ol>
<li><p>Symétrie : La divergence de Bhattacharyya est symétrique,
c’est-à-dire que <span class="math inline">\(D_B(P || Q) = D_B(Q ||
P)\)</span>.</p></li>
<li><p>Bornes : La divergence de Bhattacharyya est toujours non négative
et bornée supérieurement par <span
class="math inline">\(+\infty\)</span>. Plus précisément, nous avons :
<span class="math display">\[0 \leq D_B(P || Q) \leq
+\infty\]</span></p></li>
<li><p>Continuité : La divergence de Bhattacharyya est continue par
rapport aux distributions <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span>. Cela signifie que si <span
class="math inline">\(P_n \to P\)</span> et <span
class="math inline">\(Q_n \to Q\)</span> faiblement, alors <span
class="math inline">\(D_B(P_n || Q_n) \to D_B(P || Q)\)</span>.</p></li>
</ol>
<p>Pour prouver la symétrie, nous utilisons simplement la définition de
la divergence de Bhattacharyya : <span class="math display">\[D_B(P ||
Q) = -\ln \left( \int_{\Omega} \sqrt{dP(\omega) dQ(\omega)} \right) =
-\ln \left( \int_{\Omega} \sqrt{dQ(\omega) dP(\omega)} \right) = D_B(Q
|| P)\]</span></p>
<p>Pour la continuité, nous utilisons le fait que l’intégrale est une
fonction continue par rapport aux mesures. Donc, si <span
class="math inline">\(P_n \to P\)</span> et <span
class="math inline">\(Q_n \to Q\)</span> faiblement, alors : <span
class="math display">\[\int_{\Omega} \sqrt{dP_n(\omega) dQ_n(\omega)}
\to \int_{\Omega} \sqrt{dP(\omega) dQ(\omega)}\]</span> En prenant le
logarithme, nous obtenons : <span class="math display">\[D_B(P_n || Q_n)
\to D_B(P || Q)\]</span></p>
</body>
</html>
{% include "footer.html" %}


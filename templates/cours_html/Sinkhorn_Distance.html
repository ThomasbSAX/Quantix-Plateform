{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Sinkhorn Distance : Une Métrique pour l’Appariement Optimal</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Sinkhorn Distance : Une Métrique pour l’Appariement
Optimal</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’étude des distances entre distributions de probabilité est un
domaine central en théorie des probabilités, en statistique et en
apprentissage automatique. Parmi les nombreuses métriques proposées, la
distance de Sinkhorn se distingue par son lien étroit avec le problème
d’appariement optimal et sa capacité à capturer des structures
géométriques complexes.</p>
<p>Historiquement, le problème d’appariement optimal remonte aux travaux
de Monge (1781) sur le transport de terre. Plus récemment, les
algorithmes de Sinkhorn, développés par Sinkhorn et Knopp (1967), ont
fourni une méthode efficace pour résoudre ce problème en introduisant
des contraintes de régularisation. La distance de Sinkhorn émerge
naturellement comme une métrique adaptée pour mesurer la dissimilarité
entre distributions de probabilité dans ce cadre.</p>
<p>Dans cet article, nous explorons les fondements mathématiques de la
distance de Sinkhorn, ses propriétés et ses applications. Nous montrons
comment cette métrique peut être utilisée pour résoudre des problèmes
d’appariement optimal de manière efficace et robuste.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la distance de Sinkhorn, commençons par rappeler
quelques notions préliminaires. Considérons deux distributions de
probabilité discrètes <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> définies sur un ensemble fini <span
class="math inline">\(X\)</span>. Nous cherchons à mesurer la
dissimilarité entre ces deux distributions.</p>
<h2 id="problème-dappariement-optimal">Problème d’Appariement
Optimal</h2>
<p>Le problème d’appariement optimal consiste à trouver un couplage
<span class="math inline">\(\pi\)</span> entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> qui minimise une certaine fonction de
coût. Formellement, on cherche à résoudre le problème d’optimisation
suivant :</p>
<p><span class="math display">\[\min_{\pi \in \Pi(\mu, \nu)} \langle
\pi, C \rangle\]</span></p>
<p>où <span class="math inline">\(\Pi(\mu, \nu)\)</span> est l’ensemble
des couplages possibles entre <span class="math inline">\(\mu\)</span>
et <span class="math inline">\(\nu\)</span>, <span
class="math inline">\(C\)</span> est une matrice de coût, et <span
class="math inline">\(\langle \cdot, \cdot \rangle\)</span> désigne le
produit scalaire.</p>
<h2 id="distance-de-sinkhorn">Distance de Sinkhorn</h2>
<p>La distance de Sinkhorn est une métrique qui émerge naturellement
dans le cadre du problème d’appariement optimal. Elle est définie comme
la solution régulière du problème d’optimisation précédent, en
introduisant des contraintes de régularisation.</p>
<div class="definition">
<p>Soient <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> deux distributions de probabilité
discrètes définies sur un ensemble fini <span
class="math inline">\(X\)</span>, et soit <span
class="math inline">\(C\)</span> une matrice de coût symétrique définie
positive. La distance de Sinkhorn entre <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span> est définie par :</p>
<p><span class="math display">\[W_{\varepsilon}(\mu, \nu) = \min_{\pi
\in \Pi(\mu, \nu)} \left( \langle \pi, C \rangle + \varepsilon \cdot
KL(\pi \| \mu \otimes \nu) \right)\]</span></p>
<p>où <span class="math inline">\(KL(\pi \| \mu \otimes \nu)\)</span>
est la divergence de Kullback-Leibler entre <span
class="math inline">\(\pi\)</span> et le produit <span
class="math inline">\(\mu \otimes \nu\)</span>, et <span
class="math inline">\(\varepsilon &gt; 0\)</span> est un paramètre de
régularisation.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons quelques théorèmes clés
concernant la distance de Sinkhorn.</p>
<h2 id="existence-et-unicité">Existence et Unicité</h2>
<div class="theorem">
<p>Pour tout <span class="math inline">\(\varepsilon &gt; 0\)</span>, il
existe une unique solution <span
class="math inline">\(\pi_{\varepsilon}\)</span> au problème
d’optimisation définissant la distance de Sinkhorn.</p>
</div>
<div class="proof">
<p><em>Proof.</em> L’existence et l’unicité de <span
class="math inline">\(\pi_{\varepsilon}\)</span> découlent du fait que
la fonction objectif est strictement convexe et coercive, et que
l’ensemble des couplages <span class="math inline">\(\Pi(\mu,
\nu)\)</span> est compact. ◻</p>
</div>
<h2 id="propriétés-de-la-distance-de-sinkhorn">Propriétés de la Distance
de Sinkhorn</h2>
<div class="theorem">
<p>La distance de Sinkhorn <span
class="math inline">\(W_{\varepsilon}\)</span> satisfait les propriétés
suivantes :</p>
<ol>
<li><p>(Positivité) <span class="math inline">\(W_{\varepsilon}(\mu,
\nu) \geq 0\)</span>, avec égalité si et seulement si <span
class="math inline">\(\mu = \nu\)</span>.</p></li>
<li><p>(Symétrie) <span class="math inline">\(W_{\varepsilon}(\mu, \nu)
= W_{\varepsilon}(\nu, \mu)\)</span>.</p></li>
<li><p>(Inégalité Triangulaire) <span
class="math inline">\(W_{\varepsilon}(\mu, \nu) \leq
W_{\varepsilon}(\mu, \lambda) + W_{\varepsilon}(\lambda, \nu)\)</span>
pour toute distribution <span
class="math inline">\(\lambda\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> La positivité et la symétrie découlent directement de
la définition. L’inégalité triangulaire peut être démontrée en utilisant
les propriétés de la divergence de Kullback-Leibler et les inégalités de
Jensen. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Dans cette section, nous fournissons des preuves détaillées pour les
théorèmes présentés précédemment.</p>
<h2 id="preuve-de-lexistence-et-de-lunicité">Preuve de l’Existence et de
l’Unicité</h2>
<div class="proof">
<p><em>Proof.</em> Considérons la fonction objectif <span
class="math inline">\(F(\pi) = \langle \pi, C \rangle + \varepsilon
\cdot KL(\pi \| \mu \otimes \nu)\)</span>. Cette fonction est
strictement convexe car la divergence de Kullback-Leibler est
strictement convexe. De plus, <span
class="math inline">\(F(\pi)\)</span> est coercive car <span
class="math inline">\(KL(\pi \| \mu \otimes \nu)\)</span> tend vers
l’infini lorsque <span class="math inline">\(\pi\)</span> s’éloigne de
<span class="math inline">\(\mu \otimes \nu\)</span>.</p>
<p>L’ensemble des couplages <span class="math inline">\(\Pi(\mu,
\nu)\)</span> est compact car il s’agit d’un ensemble fermé et borné
dans l’espace des matrices stochastiques. Par le théorème de
Weierstrass, <span class="math inline">\(F\)</span> atteint son minimum
sur <span class="math inline">\(\Pi(\mu, \nu)\)</span>.</p>
<p>Pour montrer l’unicité, supposons qu’il existe deux solutions <span
class="math inline">\(\pi_1\)</span> et <span
class="math inline">\(\pi_2\)</span>. Par convexité stricte, nous avons
:</p>
<p><span class="math display">\[F\left( \frac{\pi_1 + \pi_2}{2} \right)
&lt; \frac{F(\pi_1) + F(\pi_2)}{2}\]</span></p>
<p>Mais <span class="math inline">\(\frac{\pi_1 + \pi_2}{2}\)</span> est
également un couplage entre <span class="math inline">\(\mu\)</span> et
<span class="math inline">\(\nu\)</span>, ce qui contredit le fait que
<span class="math inline">\(\pi_1\)</span> et <span
class="math inline">\(\pi_2\)</span> sont des minima. Donc, la solution
est unique. ◻</p>
</div>
<h2 id="preuve-des-propriétés-métriques">Preuve des Propriétés
Métriques</h2>
<div class="proof">
<p><em>Proof.</em> La positivité découle du fait que <span
class="math inline">\(C\)</span> est symétrique définie positive et que
la divergence de Kullback-Leibler est non négative. L’égalité <span
class="math inline">\(W_{\varepsilon}(\mu, \nu) = 0\)</span> implique
que <span class="math inline">\(KL(\pi \| \mu \otimes \nu) = 0\)</span>,
ce qui signifie que <span class="math inline">\(\pi = \mu \otimes
\nu\)</span>. Par conséquent, <span class="math inline">\(\mu =
\nu\)</span>.</p>
<p>La symétrie est une conséquence directe de la définition et du fait
que <span class="math inline">\(C\)</span> est symétrique.</p>
<p>Pour l’inégalité triangulaire, considérons trois distributions <span
class="math inline">\(\mu, \nu, \lambda\)</span>. Nous avons :</p>
<p><span class="math display">\[W_{\varepsilon}(\mu, \nu) = \min_{\pi
\in \Pi(\mu, \nu)} F(\pi)\]</span></p>
<p>En utilisant les propriétés de la divergence de Kullback-Leibler et
les inégalités de Jensen, on peut montrer que :</p>
<p><span class="math display">\[F(\pi) \leq F(\pi_1) + F(\pi_2) -
F(\pi_3)\]</span></p>
<p>où <span class="math inline">\(\pi_1\)</span>, <span
class="math inline">\(\pi_2\)</span> et <span
class="math inline">\(\pi_3\)</span> sont les couplages optimaux pour
<span class="math inline">\(W_{\varepsilon}(\mu, \lambda)\)</span>,
<span class="math inline">\(W_{\varepsilon}(\lambda, \nu)\)</span> et
<span class="math inline">\(W_{\varepsilon}(\mu, \nu)\)</span>,
respectivement. Cela implique l’inégalité triangulaire. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés
supplémentaires et corollaires de la distance de Sinkhorn.</p>
<h2 id="propriétés">Propriétés</h2>
<ol>
<li><p>(Continuité) La distance de Sinkhorn <span
class="math inline">\(W_{\varepsilon}\)</span> est continue par rapport
aux distributions <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>.</p></li>
<li><p>(Convergence) Lorsque <span class="math inline">\(\varepsilon \to
0\)</span>, la distance de Sinkhorn converge vers la distance de
Wasserstein.</p></li>
<li><p>(Robustesse) La distance de Sinkhorn est robuste aux
perturbations des distributions <span class="math inline">\(\mu\)</span>
et <span class="math inline">\(\nu\)</span>.</p></li>
</ol>
<h2 id="corollaires">Corollaires</h2>
<div class="corollary">
<p>Lorsque <span class="math inline">\(\varepsilon \to 0\)</span>, nous
avons :</p>
<p><span class="math display">\[\lim_{\varepsilon \to 0}
W_{\varepsilon}(\mu, \nu) = W(\mu, \nu)\]</span></p>
<p>où <span class="math inline">\(W(\mu, \nu)\)</span> est la distance
de Wasserstein.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce corollaire découle du fait que la divergence de
Kullback-Leibler tend vers zéro lorsque <span
class="math inline">\(\varepsilon \to 0\)</span>, et que la fonction
objectif converge vers celle définissant la distance de
Wasserstein. ◻</p>
</div>
<div class="corollary">
<p>La distance de Sinkhorn est robuste aux perturbations des
distributions <span class="math inline">\(\mu\)</span> et <span
class="math inline">\(\nu\)</span>. Plus précisément, pour toute
perturbation <span class="math inline">\(\delta \mu\)</span> et <span
class="math inline">\(\delta \nu\)</span>, nous avons :</p>
<p><span class="math display">\[|W_{\varepsilon}(\mu + \delta \mu, \nu +
\delta \nu) - W_{\varepsilon}(\mu, \nu)| \leq L \cdot (\|\delta \mu\| +
\|\delta \nu\|)\]</span></p>
<p>où <span class="math inline">\(L\)</span> est une constante dépendant
de <span class="math inline">\(C\)</span> et <span
class="math inline">\(\varepsilon\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Cette robustesse découle des propriétés de la
divergence de Kullback-Leibler et des inégalités de Lipschitz. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré les fondements mathématiques de
la distance de Sinkhorn, ses propriétés et ses applications. Nous avons
montré comment cette métrique peut être utilisée pour résoudre des
problèmes d’appariement optimal de manière efficace et robuste. Les
théorèmes et les preuves présentés fournissent une base solide pour
l’utilisation de la distance de Sinkhorn dans divers domaines, notamment
en apprentissage automatique et en statistique.</p>
</body>
</html>
{% include "footer.html" %}


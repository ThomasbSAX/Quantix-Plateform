{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de normalisation Une approche avancée pour l’analyse des données</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de
normalisation<br />
Une approche avancée pour l’analyse des données</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’encodage par extraction de caractéristiques de normalisation (NFC)
émerge comme une méthode puissante dans le domaine du traitement des
données. Historiquement, cette approche trouve ses racines dans les
techniques d’extraction de caractéristiques et de normalisation,
essentielles pour préparer les données à l’analyse. La NFC résout le
problème de la variabilité des échelles et des distributions dans les
ensembles de données, ce qui est indispensable pour les algorithmes
sensibles aux différences d’échelle, comme les réseaux de neurones et
les machines à vecteurs de support.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre la NFC, commençons par définir ce que nous cherchons
à atteindre. Nous voulons transformer un ensemble de données brutes en
un format où chaque caractéristique est normalisée, c’est-à-dire mise à
l’échelle de manière à ce que toutes les caractéristiques contribuent de
manière équitable à l’analyse. La NFC est une méthode systématique pour
y parvenir.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données
avec <span class="math inline">\(n\)</span> échantillons et <span
class="math inline">\(m\)</span> caractéristiques. Nous définissons
l’encodage par extraction de caractéristiques de normalisation comme une
fonction <span class="math inline">\(\mathcal{N} : X \rightarrow
Y\)</span>, où <span class="math inline">\(Y\)</span> est l’ensemble de
données normalisé, telle que pour chaque caractéristique <span
class="math inline">\(j\)</span>, nous avons : <span
class="math display">\[\mathcal{N}_j(x_i) = \frac{x_{ij} -
\mu_j}{\sigma_j}\]</span> où <span class="math inline">\(\mu_j\)</span>
est la moyenne de la caractéristique <span
class="math inline">\(j\)</span> et <span
class="math inline">\(\sigma_j\)</span> est son écart-type.</p>
</div>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la NFC est le Théorème de
Normalisation, qui garantit que l’application de la NFC préserve les
relations linéaires entre les caractéristiques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> un ensemble de données et
<span class="math inline">\(Y = \mathcal{N}(X)\)</span> son encodage
NFC. Alors, pour toute paire de caractéristiques <span
class="math inline">\((j, k)\)</span>, le coefficient de corrélation
entre <span class="math inline">\(Y_j\)</span> et <span
class="math inline">\(Y_k\)</span> est égal au coefficient de
corrélation entre <span class="math inline">\(X_j\)</span> et <span
class="math inline">\(X_k\)</span>.</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver le Théorème de Normalisation, nous devons montrer que la
normalisation ne change pas les coefficients de corrélation. Considérons
deux caractéristiques <span class="math inline">\(X_j\)</span> et <span
class="math inline">\(X_k\)</span>. Le coefficient de corrélation <span
class="math inline">\(r_{jk}\)</span> est défini comme : <span
class="math display">\[r_{jk} = \frac{\text{Cov}(X_j, X_k)}{\sigma_j
\sigma_k}\]</span> où <span class="math inline">\(\text{Cov}(X_j,
X_k)\)</span> est la covariance entre <span
class="math inline">\(X_j\)</span> et <span
class="math inline">\(X_k\)</span>. Après normalisation, les nouvelles
caractéristiques sont : <span class="math display">\[Y_j = \frac{X_j -
\mu_j}{\sigma_j} \quad \text{et} \quad Y_k = \frac{X_k -
\mu_k}{\sigma_k}\]</span> La covariance entre <span
class="math inline">\(Y_j\)</span> et <span
class="math inline">\(Y_k\)</span> est : <span
class="math display">\[\text{Cov}(Y_j, Y_k) = \text{Cov}\left(\frac{X_j
- \mu_j}{\sigma_j}, \frac{X_k - \mu_k}{\sigma_k}\right) =
\frac{\text{Cov}(X_j, X_k)}{\sigma_j \sigma_k}\]</span> Les écarts-types
de <span class="math inline">\(Y_j\)</span> et <span
class="math inline">\(Y_k\)</span> sont tous deux égaux à 1. Par
conséquent, le coefficient de corrélation entre <span
class="math inline">\(Y_j\)</span> et <span
class="math inline">\(Y_k\)</span> est : <span
class="math display">\[r_{jk}^Y = \frac{\text{Cov}(Y_j, Y_k)}{1 \cdot 1}
= \frac{\text{Cov}(X_j, X_k)}{\sigma_j \sigma_k} = r_{jk}\]</span> Ce
qui prouve le théorème.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La NFC possède plusieurs propriétés importantes :</p>
<ol>
<li><p>**Conservation des Relations Linéaires** : Comme démontré par le
Théorème de Normalisation, la NFC préserve les coefficients de
corrélation entre les caractéristiques.</p></li>
<li><p>**Réduction de la Variabilité** : La NFC réduit la variabilité
des caractéristiques en les mettant à l’échelle, ce qui facilite
l’analyse et l’interprétation des données.</p></li>
<li><p>**Amélioration des Performances des Modèles** : De nombreux
algorithmes d’apprentissage automatique, comme les réseaux de neurones
et les machines à vecteurs de support, performant mieux lorsque les
caractéristiques sont normalisées.</p></li>
</ol>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>L’encodage par extraction de caractéristiques de normalisation est
une méthode puissante pour préparer les données à l’analyse. En
normalisant les caractéristiques, la NFC préserve les relations
linéaires et améliore les performances des modèles d’apprentissage
automatique. Cette approche est indispensable pour toute analyse de
données sérieuse et mérite une attention particulière dans le domaine du
traitement des données.</p>
</body>
</html>
{% include "footer.html" %}


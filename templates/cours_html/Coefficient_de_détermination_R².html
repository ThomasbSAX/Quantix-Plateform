{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Coefficient de Détermination R^2 : Une Mesure Fondamentale en Statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Coefficient de Détermination <span
class="math inline">\(R^2\)</span> : Une Mesure Fondamentale en
Statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>Le coefficient de détermination, noté <span
class="math inline">\(R^2\)</span>, est une mesure statistique
fondamentale utilisée pour évaluer la qualité d’un modèle de régression.
Il quantifie la proportion de la variance de la variable dépendante qui
est expliquée par les variables indépendantes dans le modèle. L’origine
historique de <span class="math inline">\(R^2\)</span> remonte aux
travaux de Sir Francis Galton sur la régression linéaire au 19ème
siècle. Ce coefficient est indispensable pour comparer différents
modèles de régression et pour évaluer leur performance prédictive.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour comprendre le coefficient de détermination, commençons par
définir les concepts clés. Supposons que nous avons un ensemble de
données avec une variable dépendante <span
class="math inline">\(Y\)</span> et des variables indépendantes <span
class="math inline">\(X_1, X_2, \ldots, X_p\)</span>. Nous ajustons un
modèle de régression linéaire multiple :</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2
+ \ldots + \beta_p X_p + \epsilon\]</span></p>
<p>où <span class="math inline">\(\beta_0, \beta_1, \ldots,
\beta_p\)</span> sont les coefficients de régression et <span
class="math inline">\(\epsilon\)</span> est le terme d’erreur.</p>
<p>La variance totale de <span class="math inline">\(Y\)</span> peut
être décomposée en deux parties : la variance expliquée par le modèle et
la variance non expliquée (ou résiduelle). Le coefficient de
détermination <span class="math inline">\(R^2\)</span> est défini comme
le rapport entre la variance expliquée et la variance totale.</p>
<div class="definition">
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> est donné par :</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n (Y_i -
\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\]</span></p>
<p>où <span class="math inline">\(Y_i\)</span> est la valeur observée de
la variable dépendante, <span class="math inline">\(\hat{Y}_i\)</span>
est la valeur prédite par le modèle, et <span
class="math inline">\(\bar{Y}\)</span> est la moyenne des valeurs
observées.</p>
</div>
<p>Une autre formulation de <span class="math inline">\(R^2\)</span> est
:</p>
<p><span class="math display">\[R^2 = \frac{\text{Variance
Expliquée}}{\text{Variance Totale}}\]</span></p>
<p>où la variance expliquée est <span class="math inline">\(\sum_{i=1}^n
(\hat{Y}_i - \bar{Y})^2\)</span> et la variance totale est <span
class="math inline">\(\sum_{i=1}^n (Y_i - \bar{Y})^2\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème important lié au coefficient de détermination est le
théorème suivant :</p>
<div class="theorem">
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> satisfait les propriétés suivantes
:</p>
<ol>
<li><p><span class="math inline">\(0 \leq R^2 \leq 1\)</span> : Le
coefficient de détermination est toujours compris entre 0 et 1.</p></li>
<li><p><span class="math inline">\(R^2 = 0\)</span> : Le modèle ne
explique aucune variance de la variable dépendante.</p></li>
<li><p><span class="math inline">\(R^2 = 1\)</span> : Le modèle explique
toute la variance de la variable dépendante.</p></li>
</ol>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver que <span class="math inline">\(0 \leq R^2 \leq
1\)</span>, nous utilisons les propriétés des variances et des
carrés.</p>
<div class="proof">
<p><em>Proof.</em> Considérons la décomposition de la variance totale
:</p>
<p><span class="math display">\[\sum_{i=1}^n (Y_i - \bar{Y})^2 =
\sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 + \sum_{i=1}^n (Y_i -
\hat{Y}_i)^2\]</span></p>
<p>En divisant les deux côtés par <span
class="math inline">\(\sum_{i=1}^n (Y_i - \bar{Y})^2\)</span>, nous
obtenons :</p>
<p><span class="math display">\[1 = \frac{\sum_{i=1}^n (\hat{Y}_i -
\bar{Y})^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2} + \frac{\sum_{i=1}^n (Y_i -
\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\]</span></p>
<p>En réarrangeant les termes, nous obtenons :</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n (Y_i -
\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2}\]</span></p>
<p>Puisque <span class="math inline">\(\sum_{i=1}^n (Y_i - \hat{Y}_i)^2
\geq 0\)</span> et <span class="math inline">\(\sum_{i=1}^n (Y_i -
\bar{Y})^2 &gt; 0\)</span>, il s’ensuit que <span
class="math inline">\(0 \leq R^2 \leq 1\)</span>. ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>Le coefficient de détermination possède plusieurs propriétés
importantes :</p>
<ol>
<li><p><strong>Interprétation</strong> : <span
class="math inline">\(R^2\)</span> mesure la proportion de la variance
totale de la variable dépendante qui est expliquée par le modèle. Un
<span class="math inline">\(R^2\)</span> élevé indique un bon ajustement
du modèle aux données.</p></li>
<li><p><strong>Comparaison de Modèles</strong> : <span
class="math inline">\(R^2\)</span> permet de comparer différents modèles
de régression. Un modèle avec un <span
class="math inline">\(R^2\)</span> plus élevé est généralement
préféré.</p></li>
<li><p><strong>Ajustement pour le Nombre de Paramètres</strong> : Pour
éviter le surajustement, on utilise souvent le <span
class="math inline">\(R^2\)</span> ajusté, qui prend en compte le nombre
de paramètres dans le modèle.</p></li>
</ol>
<div class="corollaire">
<p>Le <span class="math inline">\(R^2\)</span> ajusté est donné par
:</p>
<p><span class="math display">\[R^2_{\text{ajusté}} = 1 - \frac{(1 -
R^2)(n - 1)}{n - p - 1}\]</span></p>
<p>où <span class="math inline">\(n\)</span> est le nombre
d’observations et <span class="math inline">\(p\)</span> est le nombre
de variables indépendantes.</p>
</div>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>Le coefficient de détermination <span
class="math inline">\(R^2\)</span> est une mesure fondamentale en
statistique pour évaluer la qualité d’un modèle de régression. Il permet
de quantifier la proportion de la variance expliquée par le modèle et de
comparer différents modèles. Les propriétés et les corollaires associés
à <span class="math inline">\(R^2\)</span> en font un outil
indispensable pour l’analyse statistique.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Tsallis : Une Généralisation de la Divergence de Kullback-Leibler</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Tsallis : Une Généralisation de la
Divergence de Kullback-Leibler</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence de Tsallis émerge comme une généralisation naturelle de
la divergence de Kullback-Leibler (KL), un concept fondamental en
théorie de l’information. Introduite par Constantino Tsallis dans les
années 1980, cette notion a été motivée par le besoin de modéliser des
systèmes complexes présentant des comportements non extensifs. La
divergence de KL, bien que puissante, est limitée par son caractère
extensif, ce qui la rend inadaptée pour décrire des phénomènes où
l’additivité n’est pas une propriété valable.</p>
<p>La divergence de Tsallis, en revanche, introduit un paramètre
d’entropie <span class="math inline">\(q\)</span> qui permet de capturer
des comportements à la fois extensifs (<span
class="math inline">\(q=1\)</span>) et non extensifs (<span
class="math inline">\(q \neq 1\)</span>). Cette flexibilité en fait un
outil précieux dans des domaines variés, allant de la physique
statistique à l’apprentissage automatique. Dans cet article, nous
explorerons les définitions, théorèmes et propriétés associées à cette
divergence, en mettant l’accent sur son importance et ses
applications.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence de Tsallis, commençons par rappeler ce
que nous cherchons à modéliser. Supposons que nous ayons deux
distributions de probabilité <span class="math inline">\(P\)</span> et
<span class="math inline">\(Q\)</span> définies sur un espace <span
class="math inline">\(\Omega\)</span>. Nous voulons mesurer à quel point
<span class="math inline">\(P\)</span> s’écarte de <span
class="math inline">\(Q\)</span>. La divergence de KL répond
partiellement à cette question, mais elle est limitée par son caractère
extensif.</p>
<p>La divergence de Tsallis généralise cette notion en introduisant un
paramètre <span class="math inline">\(q\)</span> qui contrôle le degré
d’extensivité. Avant de la définir formellement, essayons de deviner sa
forme. Nous cherchons une mesure qui, pour <span
class="math inline">\(q=1\)</span>, se réduit à la divergence de KL. De
plus, cette mesure doit être non négative et symétrique.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
définies sur un espace <span class="math inline">\(\Omega\)</span>, et
soit <span class="math inline">\(q \in \mathbb{R}\)</span>. La
divergence de Tsallis d’ordre <span class="math inline">\(q\)</span>
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_q(P \parallel Q) = \frac{1}{q-1} \left( 1 -
\sum_{x \in \Omega} \left( \frac{P(x)}{Q(x)} \right)^q Q(x)
\right)\]</span> pour <span class="math inline">\(q \neq 1\)</span>, et
par : <span class="math display">\[D_1(P \parallel Q) = \lim_{q \to 1}
D_q(P \parallel Q) = \sum_{x \in \Omega} P(x) \log \left(
\frac{P(x)}{Q(x)} \right)\]</span> qui est la divergence de
Kullback-Leibler.</p>
</div>
<p>Remarquons que pour <span class="math inline">\(q=0\)</span>, la
divergence de Tsallis se réduit à la mesure de l’entropie de Shannon.
Pour <span class="math inline">\(q=2\)</span>, elle capture des
comportements quadratiques, utiles dans certains contextes
physiques.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Passons maintenant aux théorèmes fondamentaux associés à la
divergence de Tsallis. Le premier théorème que nous allons explorer est
celui de non négativité, qui montre que la divergence de Tsallis est
toujours positive.</p>
<div class="theoreme">
<p>Pour toute distribution de probabilité <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>, et pour tout <span
class="math inline">\(q \in \mathbb{R}\)</span>, la divergence de
Tsallis vérifie : <span class="math display">\[D_q(P \parallel Q) \geq
0\]</span> avec égalité si et seulement si <span class="math inline">\(P
= Q\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous allons utiliser
l’inégalité de Jensen. Considérons la fonction <span
class="math inline">\(f(x) = x^q\)</span>. Cette fonction est convexe
pour <span class="math inline">\(q \geq 1\)</span> et concave pour <span
class="math inline">\(q \leq 1\)</span>. Par l’inégalité de Jensen, nous
avons : <span class="math display">\[\sum_{x \in \Omega} Q(x) f\left(
\frac{P(x)}{Q(x)} \right) \leq f\left( \sum_{x \in \Omega} Q(x)
\frac{P(x)}{Q(x)} \right) = f(1) = 1\]</span> pour <span
class="math inline">\(q \geq 1\)</span>, et : <span
class="math display">\[\sum_{x \in \Omega} Q(x) f\left(
\frac{P(x)}{Q(x)} \right) \geq f\left( \sum_{x \in \Omega} Q(x)
\frac{P(x)}{Q(x)} \right) = f(1) = 1\]</span> pour <span
class="math inline">\(q \leq 1\)</span>. En utilisant ces inégalités,
nous obtenons : <span class="math display">\[D_q(P \parallel Q) =
\frac{1}{q-1} \left( 1 - \sum_{x \in \Omega} \left( \frac{P(x)}{Q(x)}
\right)^q Q(x) \right) \geq 0\]</span> avec égalité si et seulement si
<span class="math inline">\(P = Q\)</span>. ◻</p>
</div>
<p>Un autre théorème important est celui de la continuité de la
divergence de Tsallis par rapport au paramètre <span
class="math inline">\(q\)</span>.</p>
<div class="theoreme">
<p>La fonction <span class="math inline">\(q \mapsto D_q(P \parallel
Q)\)</span> est continue sur <span class="math inline">\(\mathbb{R}
\setminus \{1\}\)</span> et admet une limite finie en <span
class="math inline">\(q=1\)</span> donnée par la divergence de
Kullback-Leibler.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous allons utiliser le
développement de Taylor de la fonction <span class="math inline">\(x
\mapsto x^q\)</span> autour de <span class="math inline">\(q=1\)</span>.
Pour <span class="math inline">\(q\)</span> proche de 1, nous avons :
<span class="math display">\[x^q \approx x + (q-1) x \log x\]</span> En
substituant ce développement dans la définition de <span
class="math inline">\(D_q(P \parallel Q)\)</span>, nous obtenons : <span
class="math display">\[D_q(P \parallel Q) \approx \frac{1}{q-1} \left( 1
- \sum_{x \in \Omega} \left( 1 + (q-1) \log \left( \frac{P(x)}{Q(x)}
\right) \right) Q(x) \right)\]</span> En simplifiant, nous trouvons :
<span class="math display">\[D_q(P \parallel Q) \approx \sum_{x \in
\Omega} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]</span> qui est la
divergence de Kullback-Leibler. Cela montre que <span
class="math inline">\(D_q(P \parallel Q)\)</span> admet une limite finie
en <span class="math inline">\(q=1\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Passons maintenant aux propriétés fondamentales de la divergence de
Tsallis. Nous allons en énumérer quelques-unes et les démontrer une par
une.</p>
<ol>
<li><p><strong>Invariance par transformation de probabilité</strong> :
La divergence de Tsallis est invariante sous les transformations de
probabilité. Plus précisément, pour toute fonction bijective <span
class="math inline">\(T : \Omega \to \Omega\)</span>, nous avons : <span
class="math display">\[D_q(P \parallel Q) = D_q(P \circ T^{-1} \parallel
Q \circ T^{-1})\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la définition
de la divergence de Tsallis. En effet, en substituant <span
class="math inline">\(x\)</span> par <span
class="math inline">\(T(x)\)</span> dans la somme, nous obtenons : <span
class="math display">\[D_q(P \parallel Q) = \frac{1}{q-1} \left( 1 -
\sum_{x \in \Omega} \left( \frac{P(T(x))}{Q(T(x))} \right)^q Q(T(x))
\right)\]</span> qui est égal à <span class="math inline">\(D_q(P \circ
T^{-1} \parallel Q \circ T^{-1})\)</span>. ◻</p>
</div></li>
<li><p><strong>Limite pour <span class="math inline">\(q \to
0\)</span></strong> : Pour <span class="math inline">\(q \to 0\)</span>,
la divergence de Tsallis se réduit à l’entropie de Shannon. Plus
précisément, nous avons : <span class="math display">\[\lim_{q \to 0}
D_q(P \parallel Q) = H(P)\]</span> où <span
class="math inline">\(H(P)\)</span> est l’entropie de Shannon de la
distribution <span class="math inline">\(P\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, nous allons utiliser
le développement de Taylor de la fonction <span class="math inline">\(x
\mapsto x^q\)</span> autour de <span class="math inline">\(q=0\)</span>.
Pour <span class="math inline">\(q\)</span> proche de 0, nous avons :
<span class="math display">\[x^q \approx 1 + q \log x\]</span> En
substituant ce développement dans la définition de <span
class="math inline">\(D_q(P \parallel Q)\)</span>, nous obtenons : <span
class="math display">\[D_q(P \parallel Q) \approx \frac{1}{q} \left( 1 -
\sum_{x \in \Omega} (1 + q \log P(x)) Q(x) \right)\]</span> En
simplifiant, nous trouvons : <span class="math display">\[D_q(P
\parallel Q) \approx -\sum_{x \in \Omega} P(x) \log P(x) = H(P)\]</span>
ce qui montre que <span class="math inline">\(\lim_{q \to 0} D_q(P
\parallel Q) = H(P)\)</span>. ◻</p>
</div></li>
<li><p><strong>Limite pour <span class="math inline">\(q \to
2\)</span></strong> : Pour <span class="math inline">\(q \to 2\)</span>,
la divergence de Tsallis se réduit à une mesure quadratique. Plus
précisément, nous avons : <span class="math display">\[\lim_{q \to 2}
D_q(P \parallel Q) = \sum_{x \in \Omega} (P(x) - Q(x))^2\]</span></p>
<div class="proof">
<p><em>Proof.</em> Pour démontrer cette propriété, nous allons utiliser
le développement de Taylor de la fonction <span class="math inline">\(x
\mapsto x^q\)</span> autour de <span class="math inline">\(q=2\)</span>.
Pour <span class="math inline">\(q\)</span> proche de 2, nous avons :
<span class="math display">\[x^q \approx x^2 + (q-2) x^2 \log x\]</span>
En substituant ce développement dans la définition de <span
class="math inline">\(D_q(P \parallel Q)\)</span>, nous obtenons : <span
class="math display">\[D_q(P \parallel Q) \approx \frac{1}{q-1} \left( 1
- \sum_{x \in \Omega} (P(x)^2 + (q-2) P(x)^2 \log P(x)) Q(x)
\right)\]</span> En simplifiant, nous trouvons : <span
class="math display">\[D_q(P \parallel Q) \approx \sum_{x \in \Omega}
(P(x)^2 - P(x) Q(x))\]</span> ce qui montre que <span
class="math inline">\(\lim_{q \to 2} D_q(P \parallel Q) = \sum_{x \in
\Omega} (P(x) - Q(x))^2\)</span>. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Dans cet article, nous avons exploré la divergence de Tsallis, une
généralisation puissante de la divergence de Kullback-Leibler. Nous
avons défini cette divergence, démontré ses propriétés fondamentales et
exploré ses limites. La divergence de Tsallis trouve des applications
dans de nombreux domaines, allant de la physique statistique à
l’apprentissage automatique. Son caractère non extensif en fait un outil
précieux pour modéliser des systèmes complexes présentant des
comportements non additifs.</p>
<p>Nous avons vu que la divergence de Tsallis est toujours non négative,
continue par rapport au paramètre <span class="math inline">\(q\)</span>
et invariante sous les transformations de probabilité. De plus, elle se
réduit à des mesures bien connues pour certaines valeurs spécifiques de
<span class="math inline">\(q\)</span>. Ces propriétés en font un outil
flexible et puissant pour l’analyse des distributions de
probabilité.</p>
<p>En conclusion, la divergence de Tsallis est une notion riche et
profonde qui mérite d’être explorée plus en détail. Ses applications
potentielles sont vastes, et de nombreuses questions restent ouvertes.
Nous espérons que cet article a suscité l’intérêt du lecteur et l’a
incité à explorer davantage cette notion fascinante.</p>
</body>
</html>
{% include "footer.html" %}


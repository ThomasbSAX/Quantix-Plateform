{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence de Huber : Un outil robuste pour l’estimation statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence de Huber : Un outil robuste pour
l’estimation statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’estimation statistique est au cœur de nombreuses applications en
sciences des données, ingénierie et économétrie. Cependant, les méthodes
classiques d’estimation, comme la méthode des moindres carrés, sont
sensibles aux valeurs aberrantes ou outliers. Pour pallier ce problème,
Peter J. Huber a introduit une fonction de divergence robuste qui permet
d’estimer les paramètres d’un modèle tout en minimisant l’impact des
outliers.</p>
<p>La divergence de Huber émerge naturellement dans le cadre de la
théorie de la robustesse. Elle combine les avantages des fonctions de
perte quadratiques pour les petites erreurs et des fonctions de perte
linéaires pour les grandes erreurs. Cette propriété rend la divergence
de Huber particulièrement utile dans les contextes où les données sont
susceptibles de contenir des valeurs aberrantes.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la divergence de Huber, considérons d’abord une
situation où nous cherchons à minimiser l’impact des erreurs dans un
modèle. Supposons que nous ayons un ensemble de données <span
class="math inline">\((x_i, y_i)\)</span> et un modèle linéaire <span
class="math inline">\(y_i = \beta x_i + \epsilon_i\)</span>, où <span
class="math inline">\(\epsilon_i\)</span> représente l’erreur. Nous
voulons estimer le paramètre <span
class="math inline">\(\beta\)</span>.</p>
<p>La méthode des moindres carrés minimise la somme des carrés des
erreurs : <span class="math display">\[\sum_{i=1}^n (y_i - \beta
x_i)^2.\]</span></p>
<p>Cependant, cette méthode est sensible aux outliers. Pour rendre
l’estimation plus robuste, nous cherchons une fonction de perte qui soit
moins sensible aux grandes erreurs. La divergence de Huber est définie
comme suit :</p>
<div class="definition">
<p>Soit <span class="math inline">\(\delta &gt; 0\)</span> un paramètre
de robustesse. La divergence de Huber est définie par : <span
class="math display">\[\rho_\delta(u) = \begin{cases}
\frac{1}{2} u^2 &amp; \text{si } |u| \leq \delta, \\
\delta (|u| - \frac{1}{2} \delta) &amp; \text{sinon.}
\end{cases}\]</span> où <span class="math inline">\(u\)</span>
représente l’erreur résiduelle <span class="math inline">\(y_i - \beta
x_i\)</span>.</p>
</div>
<p>Cette définition peut être réécrite en utilisant des quantificateurs
: <span class="math display">\[\rho_\delta(u) = \begin{cases}
\frac{1}{2} u^2 &amp; \text{si } \exists k \in \mathbb{R}, |u| \leq
\delta \land u = k, \\
\delta (|u| - \frac{1}{2} \delta) &amp; \text{sinon.}
\end{cases}\]</span></p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Pour comprendre l’importance de la divergence de Huber, nous devons
examiner ses propriétés. Un théorème clé est le suivant :</p>
<div class="theorem">
<p>Pour tout <span class="math inline">\(\delta &gt; 0\)</span> et pour
tout <span class="math inline">\(u \in \mathbb{R}\)</span>, la
divergence de Huber <span class="math inline">\(\rho_\delta(u)\)</span>
est convexe.</p>
</div>
<p>La preuve de ce théorème repose sur le fait que la divergence de
Huber est une combinaison convexe de deux fonctions convexes : <span
class="math inline">\(\frac{1}{2} u^2\)</span> et <span
class="math inline">\(\delta (|u| - \frac{1}{2} \delta)\)</span>.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour prouver la convexité de <span
class="math inline">\(\rho_\delta(u)\)</span>, nous devons montrer que
pour tout <span class="math inline">\(u_1, u_2 \in \mathbb{R}\)</span>
et pour tout <span class="math inline">\(\lambda \in [0, 1]\)</span>,
nous avons : <span class="math display">\[\rho_\delta(\lambda u_1 + (1 -
\lambda) u_2) \leq \lambda \rho_\delta(u_1) + (1 - \lambda)
\rho_\delta(u_2).\]</span></p>
<p>Considérons d’abord le cas où <span class="math inline">\(|u_1|,
|u_2| \leq \delta\)</span>. Dans ce cas, <span
class="math inline">\(\rho_\delta(u) = \frac{1}{2} u^2\)</span>, qui est
clairement convexe. Ensuite, considérons le cas où <span
class="math inline">\(|u_1| &gt; \delta\)</span> et <span
class="math inline">\(|u_2| &gt; \delta\)</span>. Dans ce cas, <span
class="math inline">\(\rho_\delta(u) = \delta (|u| - \frac{1}{2}
\delta)\)</span>, qui est également convexe. Enfin, considérons le cas
mixte où <span class="math inline">\(|u_1| \leq \delta\)</span> et <span
class="math inline">\(|u_2| &gt; \delta\)</span>. La convexité suit de
la combinaison convexe des deux cas précédents.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La divergence de Huber possède plusieurs propriétés intéressantes
:</p>
<ol>
<li><p>**Robustesse aux outliers** : La divergence de Huber est moins
sensible aux grandes erreurs que la fonction de perte
quadratique.</p></li>
<li><p>**Continuité** : La divergence de Huber est continue en <span
class="math inline">\(u = \delta\)</span>.</p></li>
<li><p>**Différentiabilité** : La divergence de Huber est différentiable
en <span class="math inline">\(u = 0\)</span>.</p></li>
</ol>
<p>Pour prouver la continuité de <span
class="math inline">\(\rho_\delta(u)\)</span> en <span
class="math inline">\(u = \delta\)</span>, nous devons montrer que :
<span class="math display">\[\lim_{u \to \delta} \rho_\delta(u) =
\frac{1}{2} \delta^2.\]</span></p>
<p>Cela suit directement de la définition de <span
class="math inline">\(\rho_\delta(u)\)</span> et de la continuité des
fonctions quadratiques et linéaires.</p>
<p>Pour prouver la différentiabilité de <span
class="math inline">\(\rho_\delta(u)\)</span> en <span
class="math inline">\(u = 0\)</span>, nous devons montrer que la dérivée
de <span class="math inline">\(\rho_\delta(u)\)</span> existe et est
continue en <span class="math inline">\(u = 0\)</span>. La dérivée de
<span class="math inline">\(\rho_\delta(u)\)</span> est donnée par :
<span class="math display">\[\rho&#39;_\delta(u) = \begin{cases}
u &amp; \text{si } |u| &lt; \delta, \\
\delta \cdot \text{sgn}(u) &amp; \text{sinon.}
\end{cases}\]</span></p>
<p>En <span class="math inline">\(u = 0\)</span>, la dérivée est <span
class="math inline">\(\rho&#39;_\delta(0) = 0\)</span>, ce qui montre
que <span class="math inline">\(\rho_\delta(u)\)</span> est
différentiable en <span class="math inline">\(u = 0\)</span>.</p>
<h1 class="unnumbered" id="conclusion">Conclusion</h1>
<p>La divergence de Huber est un outil puissant pour l’estimation
statistique robuste. Elle combine les avantages des fonctions de perte
quadratiques et linéaires, ce qui la rend particulièrement utile dans
les contextes où les données contiennent des outliers. Les propriétés de
convexité, continuité et différentiabilité en font un choix naturel pour
les méthodes d’estimation robuste.</p>
</body>
</html>
{% include "footer.html" %}


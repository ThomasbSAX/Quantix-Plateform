{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence Prédictive : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence Prédictive : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence prédictive émerge comme un concept central en théorie
de l’apprentissage statistique et en modélisation probabiliste. Son
origine remonte aux travaux pionniers sur la complexité algorithmique et
la théorie de l’information, où elle a été identifiée comme un outil
puissant pour évaluer la performance des modèles prédictifs. Cette
notion est indispensable dans le cadre de l’évaluation des modèles, où
elle permet de mesurer la capacité d’un modèle à généraliser à partir
des données observées.</p>
<p>La divergence prédictive résout un problème fondamental : comment
quantifier la différence entre les distributions de probabilité prédites
par un modèle et celles observées dans les données réelles. Elle est
particulièrement utile dans des contextes où les modèles sont complexes
et où les méthodes traditionnelles d’évaluation peuvent être
insuffisantes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la divergence prédictive, commençons par définir ce
que nous cherchons à mesurer. Supposons que nous avons un modèle
prédictif qui, pour une entrée <span class="math inline">\(x\)</span>,
produit une distribution de probabilité <span
class="math inline">\(p(y|x)\)</span>. Nous voulons mesurer à quel point
cette distribution diffère de la distribution réelle <span
class="math inline">\(q(y|x)\)</span>, où <span
class="math inline">\(y\)</span> est la sortie observée.</p>
<p>Formellement, la divergence prédictive entre deux distributions de
probabilité <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> est définie comme suit :</p>
<div class="definition">
<p>La divergence prédictive <span class="math inline">\(D(p \|
q)\)</span> entre deux distributions de probabilité <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> est donnée par : <span
class="math display">\[D(p \| q) = \mathbb{E}_{y \sim p} \left[ \log
\frac{p(y)}{q(y)} \right]\]</span> où <span
class="math inline">\(\mathbb{E}\)</span> désigne l’espérance
mathématique.</p>
</div>
<p>Une autre formulation équivalente est : <span
class="math display">\[D(p \| q) = \int p(y) \log \frac{p(y)}{q(y)} \,
dy\]</span></p>
<p>Cette divergence est toujours non négative, c’est-à-dire <span
class="math inline">\(D(p \| q) \geq 0\)</span>, et elle est nulle si et
seulement si <span class="math inline">\(p = q\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence prédictive est le
théorème de l’information mutuelle, qui relie la divergence à
l’information mutuelle entre deux distributions.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> deux distributions de probabilité. La
divergence prédictive <span class="math inline">\(D(p \| q)\)</span> est
égale à l’information mutuelle entre <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span>, c’est-à-dire : <span
class="math display">\[D(p \| q) = I(p; q)\]</span> où <span
class="math inline">\(I(p; q)\)</span> est l’information mutuelle
définie par : <span class="math display">\[I(p; q) = \int p(y) \log
\frac{p(y)}{q(y)} \, dy\]</span></p>
</div>
<p>La preuve de ce théorème repose sur les propriétés fondamentales de
l’information mutuelle et de la divergence de Kullback-Leibler.</p>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de l’information mutuelle, commençons par
rappeler la définition de l’information mutuelle. L’information mutuelle
entre deux distributions <span class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> est donnée par : <span
class="math display">\[I(p; q) = \int p(y) \log \frac{p(y)}{q(y)} \,
dy\]</span></p>
<p>Nous voulons montrer que <span class="math inline">\(D(p \| q) = I(p;
q)\)</span>. Par définition, nous avons : <span
class="math display">\[D(p \| q) = \mathbb{E}_{y \sim p} \left[ \log
\frac{p(y)}{q(y)} \right]\]</span></p>
<p>En utilisant la linéarité de l’espérance, nous obtenons : <span
class="math display">\[D(p \| q) = \int p(y) \log \frac{p(y)}{q(y)} \,
dy\]</span></p>
<p>Ce qui est exactement la définition de l’information mutuelle <span
class="math inline">\(I(p; q)\)</span>. Donc, nous avons : <span
class="math display">\[D(p \| q) = I(p; q)\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence prédictive possède plusieurs propriétés importantes qui
en font un outil puissant pour l’évaluation des modèles.</p>
<ol>
<li><p><strong>Non négativité</strong> : La divergence prédictive est
toujours non négative, c’est-à-dire <span class="math inline">\(D(p \|
q) \geq 0\)</span>. Cette propriété est une conséquence directe de
l’inégalité de Gibbs.</p></li>
<li><p><strong>Nulle si et seulement si les distributions sont
égales</strong> : La divergence prédictive est nulle si et seulement si
<span class="math inline">\(p = q\)</span>. Cela signifie que la
divergence mesure effectivement la différence entre les deux
distributions.</p></li>
<li><p><strong>Invariance sous transformation</strong> : La divergence
prédictive est invariante sous les transformations bijectives. Cela
signifie que si nous appliquons une transformation bijective <span
class="math inline">\(\phi\)</span> à la variable aléatoire <span
class="math inline">\(y\)</span>, la divergence reste
inchangée.</p></li>
</ol>
<p>Chacune de ces propriétés peut être démontrée en utilisant les
définitions et théorèmes précédents. Par exemple, la non négativité
découle directement de l’inégalité de Gibbs, qui stipule que pour toute
distribution de probabilité <span class="math inline">\(p\)</span> et
toute fonction <span class="math inline">\(f\)</span>, nous avons :
<span class="math display">\[\mathbb{E}_{y \sim p} [f(y)] \geq
f(\mathbb{E}_{y \sim p} [y])\]</span></p>
<p>En appliquant cette inégalité à la fonction <span
class="math inline">\(f(y) = \log \frac{p(y)}{q(y)}\)</span>, nous
obtenons : <span class="math display">\[D(p \| q) = \mathbb{E}_{y \sim
p} \left[ \log \frac{p(y)}{q(y)} \right] \geq \log \mathbb{E}_{y \sim p}
\left[ \frac{p(y)}{q(y)} \right] = 0\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence prédictive est un concept fondamental en théorie de
l’apprentissage statistique et en modélisation probabiliste. Elle permet
de mesurer la différence entre les distributions de probabilité prédites
par un modèle et celles observées dans les données réelles. Les
propriétés et théorèmes associés à la divergence prédictive en font un
outil puissant pour l’évaluation des modèles et la compréhension de leur
performance.</p>
</body>
</html>
{% include "footer.html" %}


{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Encodage par extraction de caractéristiques de binning par tendance</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Encodage par extraction de caractéristiques de binning
par tendance</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’encodage de variables catégorielles est une étape cruciale dans le
processus d’apprentissage automatique. Parmi les nombreuses techniques
disponibles, l’encodage par extraction de caractéristiques de binning
par tendance se distingue par sa capacité à capturer des informations
nuancées tout en réduisant la dimensionalité. Cette méthode combine le
binning, une technique de discrétisation des données, avec l’extraction
de caractéristiques pour produire un encodage riche et informatif.</p>
<p>L’origine de cette technique remonte aux travaux pionniers sur la
discrétisation des données et l’encodage des variables catégorielles. Le
binning, ou discrétisation en intervalles, est une méthode bien établie
pour transformer des variables continues en variables catégorielles.
L’idée d’extraire des caractéristiques à partir de ces intervalles pour
encoder les données est une extension naturelle qui permet de mieux
capturer la structure sous-jacente des données.</p>
<p>Dans ce chapitre, nous explorerons les concepts fondamentaux de
l’encodage par extraction de caractéristiques de binning par tendance,
en fournissant des définitions formelles et des théorèmes clés. Nous
discuterons également des preuves et des propriétés de cette méthode,
ainsi que de ses applications pratiques.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir formellement l’encodage par extraction de
caractéristiques de binning par tendance, il est important de comprendre
les concepts sous-jacents. Nous cherchons à transformer une variable
catégorielle en un ensemble de caractéristiques numériques qui capturent
les tendances et les relations sous-jacentes dans les données.</p>
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
avec <span class="math inline">\(n\)</span> catégories distinctes. Nous
voulons encoder cette variable en un vecteur de caractéristiques
numériques <span class="math inline">\(\mathbf{f}(X) = (f_1(X), f_2(X),
\ldots, f_k(X))\)</span>, où <span class="math inline">\(k\)</span> est
le nombre de caractéristiques extraites.</p>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable cible. Le binning
par tendance consiste à diviser les catégories de <span
class="math inline">\(X\)</span> en <span
class="math inline">\(m\)</span> intervalles (bins) basés sur une mesure
de tendance, telle que la moyenne ou la médiane de <span
class="math inline">\(Y\)</span> dans chaque catégorie.</p>
<p>Formellement, pour chaque catégorie <span
class="math inline">\(c_i\)</span> de <span
class="math inline">\(X\)</span>, nous calculons la moyenne de <span
class="math inline">\(Y\)</span> dans cette catégorie : <span
class="math display">\[\mu_{c_i} = \frac{1}{|Y_{c_i}|}
\sum_{j=1}^{|Y_{c_i}|} Y_j\]</span> où <span
class="math inline">\(Y_{c_i}\)</span> est l’ensemble des valeurs de
<span class="math inline">\(Y\)</span> associées à la catégorie <span
class="math inline">\(c_i\)</span>.</p>
<p>Ensuite, nous divisons les catégories en <span
class="math inline">\(m\)</span> intervalles basés sur les valeurs de
<span class="math inline">\(\mu_{c_i}\)</span>. Les catégories avec des
valeurs de <span class="math inline">\(\mu_{c_i}\)</span> similaires
sont regroupées dans le même intervalle.</p>
</div>
<div class="definition">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable cible.
L’extraction de caractéristiques consiste à calculer un ensemble de
statistiques pour chaque intervalle (bin) obtenu par le binning par
tendance.</p>
<p>Formellement, pour chaque intervalle <span
class="math inline">\(B_j\)</span> obtenu par le binning par tendance,
nous calculons les statistiques suivantes : <span
class="math display">\[\text{Mean}(B_j) = \frac{1}{|Y_{B_j}|}
\sum_{i=1}^{|Y_{B_j}|} Y_i\]</span> <span
class="math display">\[\text{Variance}(B_j) = \frac{1}{|Y_{B_j}|}
\sum_{i=1}^{|Y_{B_j}|} (Y_i - \text{Mean}(B_j))^2\]</span> <span
class="math display">\[\text{Skewness}(B_j) = \frac{1}{|Y_{B_j}|}
\sum_{i=1}^{|Y_{B_j}|} \left( \frac{Y_i -
\text{Mean}(B_j)}{\sqrt{\text{Variance}(B_j)}} \right)^3\]</span> <span
class="math display">\[\text{Kurtosis}(B_j) = \frac{1}{|Y_{B_j}|}
\sum_{i=1}^{|Y_{B_j}|} \left( \frac{Y_i -
\text{Mean}(B_j)}{\sqrt{\text{Variance}(B_j)}} \right)^4 - 3\]</span> où
<span class="math inline">\(Y_{B_j}\)</span> est l’ensemble des valeurs
de <span class="math inline">\(Y\)</span> associées à l’intervalle <span
class="math inline">\(B_j\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Dans cette section, nous présentons un théorème clé concernant
l’encodage par extraction de caractéristiques de binning par
tendance.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable cible. Supposons
que nous appliquions l’encodage par extraction de caractéristiques de
binning par tendance à <span class="math inline">\(X\)</span> avec <span
class="math inline">\(m\)</span> intervalles. Alors, pour tout <span
class="math inline">\(\epsilon &gt; 0\)</span>, il existe un nombre
suffisant d’intervalles <span class="math inline">\(m\)</span> tel que
l’erreur de prédiction de <span class="math inline">\(Y\)</span> basée
sur l’encodage soit inférieure à <span
class="math inline">\(\epsilon\)</span>.</p>
<p>Formellement, pour tout <span class="math inline">\(\epsilon &gt;
0\)</span>, il existe un <span class="math inline">\(m_0\)</span> tel
que pour tout <span class="math inline">\(m \geq m_0\)</span>, nous
avons : <span class="math display">\[\mathbb{E}[(Y - \hat{Y})^2] &lt;
\epsilon\]</span> où <span class="math inline">\(\hat{Y}\)</span> est la
prédiction de <span class="math inline">\(Y\)</span> basée sur
l’encodage.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Nous allons maintenant prouver le théorème de consistance de
l’encodage.</p>
<div class="proof">
<p><em>Proof.</em> Pour prouver ce théorème, nous utilisons le théorème
de la limite centrale et les propriétés des statistiques d’ordre. Le
binning par tendance divise les catégories de <span
class="math inline">\(X\)</span> en intervalles basés sur la moyenne de
<span class="math inline">\(Y\)</span>. En augmentant le nombre
d’intervalles <span class="math inline">\(m\)</span>, nous réduisons la
variance des statistiques calculées dans chaque intervalle.</p>
<p>Par le théorème de la limite centrale, pour un nombre suffisant
d’intervalles <span class="math inline">\(m\)</span>, les statistiques
calculées dans chaque intervalle convergent vers leur espérance. Cela
signifie que l’erreur de prédiction basée sur ces statistiques peut être
rendue arbitrairement petite en augmentant <span
class="math inline">\(m\)</span>.</p>
<p>Plus formellement, pour tout <span class="math inline">\(\epsilon
&gt; 0\)</span>, il existe un <span class="math inline">\(m_0\)</span>
tel que pour tout <span class="math inline">\(m \geq m_0\)</span>, la
variance des statistiques calculées dans chaque intervalle est
inférieure à <span class="math inline">\(\epsilon^2 / 4\)</span>. Par
l’inégalité de Bienaymé-Tchebyshev, nous avons : <span
class="math display">\[\mathbb{P}(|Y - \hat{Y}| \geq \epsilon) \leq
\frac{\text{Var}(Y - \hat{Y})}{\epsilon^2} &lt; \frac{\epsilon^2 /
4}{\epsilon^2} = \frac{1}{4}\]</span> En utilisant l’inégalité de
Markov, nous obtenons : <span class="math display">\[\mathbb{E}[(Y -
\hat{Y})^2] &lt; \epsilon\]</span> Ce qui prouve le théorème. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Dans cette section, nous présentons quelques propriétés et
corollaires de l’encodage par extraction de caractéristiques de binning
par tendance.</p>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable cible. Supposons
que nous appliquions l’encodage par extraction de caractéristiques de
binning par tendance à <span class="math inline">\(X\)</span> avec <span
class="math inline">\(m\)</span> intervalles. Alors, l’encodage est
stable au sens où de petites perturbations dans les données ne
conduisent pas à des changements significatifs dans l’encodage.</p>
<p>Formellement, pour toute perturbation <span
class="math inline">\(\delta Y\)</span> telle que <span
class="math inline">\(\|\delta Y\| &lt; \epsilon\)</span>, nous avons :
<span class="math display">\[\|\mathbf{f}(X) - \mathbf{f}(X + \delta
Y)\| &lt; C \epsilon\]</span> où <span class="math inline">\(C\)</span>
est une constante dépendant de <span
class="math inline">\(m\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La stabilité de l’encodage découle du fait que les
statistiques calculées dans chaque intervalle sont continues par rapport
aux données. Par le théorème des accroissements finis, pour toute
perturbation <span class="math inline">\(\delta Y\)</span> telle que
<span class="math inline">\(\|\delta Y\| &lt; \epsilon\)</span>, nous
avons : <span class="math display">\[\|\mathbf{f}(X) - \mathbf{f}(X +
\delta Y)\| \leq L \|\delta Y\|\]</span> où <span
class="math inline">\(L\)</span> est la constante de Lipschitz des
statistiques calculées. En choisissant <span class="math inline">\(C =
L\)</span>, nous obtenons le résultat souhaité. ◻</p>
</div>
<div class="corollary">
<p>Soit <span class="math inline">\(X\)</span> une variable catégorielle
et <span class="math inline">\(Y\)</span> une variable cible. Supposons
que nous appliquions l’encodage par extraction de caractéristiques de
binning par tendance à <span class="math inline">\(X\)</span> avec <span
class="math inline">\(m\)</span> intervalles. Alors, l’encodage peut
être calculé en temps polynomial par rapport au nombre de catégories et
au nombre d’intervalles.</p>
<p>Formellement, le temps de calcul de l’encodage est <span
class="math inline">\(O(n + m \log m)\)</span>, où <span
class="math inline">\(n\)</span> est le nombre de catégories.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Le temps de calcul du binning par tendance est <span
class="math inline">\(O(n)\)</span>, car il nécessite de calculer la
moyenne de <span class="math inline">\(Y\)</span> pour chaque catégorie.
Le temps de calcul des statistiques pour chaque intervalle est <span
class="math inline">\(O(m \log m)\)</span>, car il nécessite de trier
les catégories par ordre croissant de moyenne et de calculer les
statistiques pour chaque intervalle. Par conséquent, le temps total de
calcul est <span class="math inline">\(O(n + m \log m)\)</span>. ◻</p>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>Dans ce chapitre, nous avons exploré l’encodage par extraction de
caractéristiques de binning par tendance. Nous avons fourni des
définitions formelles, présenté un théorème clé et ses preuves, ainsi
que discuté des propriétés et corollaires de cette méthode. L’encodage
par extraction de caractéristiques de binning par tendance est une
technique puissante pour transformer des variables catégorielles en
caractéristiques numériques riches et informatives, ce qui en fait un
outil précieux dans le domaine de l’apprentissage automatique.</p>
</body>
</html>
{% include "footer.html" %}


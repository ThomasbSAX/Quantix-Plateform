{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Macro-averaged F1: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Macro-averaged F1: A Comprehensive Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-and-motivations">Introduction and Motivations</h1>
<p>In the realm of machine learning and information retrieval,
evaluating the performance of a classifier is paramount. Among the
myriad of metrics available, the F1 score stands out as a harmonic mean
between precision and recall, providing a balanced measure of a
classifier’s accuracy. However, when dealing with multi-class
classification problems, the straightforward application of the F1 score
can be misleading. This is where the macro-averaged F1 score comes into
play.</p>
<p>The macro-averaged F1 score is indispensable in scenarios where class
imbalance is prevalent. It treats all classes equally, regardless of
their size, and provides a more nuanced understanding of the
classifier’s performance across different classes. This metric is
particularly useful in applications such as text classification, where
certain categories may be underrepresented.</p>
<h1 id="definitions">Definitions</h1>
<p>To understand the macro-averaged F1 score, we first need to grasp the
concepts of precision and recall.</p>
<h2 id="precision-and-recall">Precision and Recall</h2>
<p>Consider a binary classification problem where we have a set of true
labels <span class="math inline">\(y\)</span> and predicted labels <span
class="math inline">\(\hat{y}\)</span>. Precision measures the
proportion of true positive predictions among all positive predictions
made by the classifier, while recall measures the proportion of true
positive predictions among all actual positives.</p>
<p>Formally, for a given class <span class="math inline">\(c\)</span>,
precision and recall are defined as:</p>
<p><span class="math display">\[\text{Precision}_c = \frac{TP_c}{TP_c +
FP_c}\]</span></p>
<p><span class="math display">\[\text{Recall}_c = \frac{TP_c}{TP_c +
FN_c}\]</span></p>
<p>where <span class="math inline">\(TP_c\)</span>, <span
class="math inline">\(FP_c\)</span>, and <span
class="math inline">\(FN_c\)</span> represent the true positives, false
positives, and false negatives for class <span
class="math inline">\(c\)</span>, respectively.</p>
<h2 id="f1-score">F1 Score</h2>
<p>The F1 score is the harmonic mean of precision and recall, providing
a single metric that balances both concerns. For a given class <span
class="math inline">\(c\)</span>, the F1 score is defined as:</p>
<p><span class="math display">\[F1_c = 2 \cdot \frac{\text{Precision}_c
\cdot \text{Recall}_c}{\text{Precision}_c +
\text{Recall}_c}\]</span></p>
<h2 id="macro-averaged-f1-score">Macro-averaged F1 Score</h2>
<p>The macro-averaged F1 score extends the concept of the F1 score to
multi-class classification problems. It calculates the F1 score for each
class independently and then takes the average of these scores, treating
all classes equally.</p>
<p>Formally, for a set of <span class="math inline">\(N\)</span>
classes, the macro-averaged F1 score is defined as:</p>
<p><span class="math display">\[\text{Macro-F1} = \frac{1}{N}
\sum_{c=1}^{N} F1_c\]</span></p>
<h1 id="theorems">Theorems</h1>
<h2 id="theorem-1-monotonicity-of-macro-averaged-f1">Theorem 1:
Monotonicity of Macro-averaged F1</h2>
<p>The macro-averaged F1 score is monotonically increasing with respect
to the individual F1 scores of each class. This means that as the F1
score for any class improves, the macro-averaged F1 score will not
decrease.</p>
<p><span class="math display">\[\forall c \in \{1, 2, \dots, N\},
F1_c&#39; \geq F1_c \implies \text{Macro-F1}&#39; \geq
\text{Macro-F1}\]</span></p>
<h2 id="proof-of-theorem-1">Proof of Theorem 1</h2>
<p>To prove this theorem, we consider the definition of the
macro-averaged F1 score:</p>
<p><span class="math display">\[\text{Macro-F1} = \frac{1}{N}
\sum_{c=1}^{N} F1_c\]</span></p>
<p>If <span class="math inline">\(F1_c&#39; \geq F1_c\)</span> for all
<span class="math inline">\(c\)</span>, then:</p>
<p><span class="math display">\[\sum_{c=1}^{N} F1_c&#39; \geq
\sum_{c=1}^{N} F1_c\]</span></p>
<p>Dividing both sides by <span class="math inline">\(N\)</span>, we
obtain:</p>
<p><span class="math display">\[\frac{1}{N} \sum_{c=1}^{N} F1_c&#39;
\geq \frac{1}{N} \sum_{c=1}^{N} F1_c\]</span></p>
<p>Thus, <span class="math inline">\(\text{Macro-F1}&#39; \geq
\text{Macro-F1}\)</span>.</p>
<h1 id="properties-and-corollaries">Properties and Corollaries</h1>
<h2 id="property-1-class-independence">Property 1: Class
Independence</h2>
<p>The macro-averaged F1 score is independent of the class distribution.
This means that it does not take into account the size of each class,
treating all classes equally.</p>
<p><span class="math display">\[\text{Macro-F1} = \frac{1}{N}
\sum_{c=1}^{N} F1_c\]</span></p>
<h2 id="property-2-upper-bound">Property 2: Upper Bound</h2>
<p>The macro-averaged F1 score is bounded above by 1. This is because
the F1 score for each class is bounded above by 1, and the average of
values bounded above by 1 will also be bounded above by 1.</p>
<p><span class="math display">\[\text{Macro-F1} \leq 1\]</span></p>
<h2 id="property-3-lower-bound">Property 3: Lower Bound</h2>
<p>The macro-averaged F1 score is bounded below by 0. This is because
the F1 score for each class is bounded below by 0, and the average of
values bounded below by 0 will also be bounded below by 0.</p>
<p><span class="math display">\[\text{Macro-F1} \geq 0\]</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>The macro-averaged F1 score is a powerful metric for evaluating the
performance of classifiers in multi-class classification problems,
particularly when dealing with class imbalance. Its ability to treat all
classes equally and provide a balanced measure of precision and recall
makes it an indispensable tool in the arsenal of machine learning
practitioners.</p>
</body>
</html>
{% include "footer.html" %}


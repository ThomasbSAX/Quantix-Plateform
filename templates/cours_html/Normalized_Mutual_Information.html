{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Normalized Mutual Information: A Comprehensive Study</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Normalized Mutual Information: A Comprehensive
Study</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>The concept of Normalized Mutual Information (NMI) emerges from the
fundamental need to quantify the similarity between two data partitions
in a normalized manner. Mutual Information (MI), a measure from
information theory, captures the amount of information shared between
two random variables. However, MI is not bounded, making it less
intuitive for comparison purposes. NMI addresses this by normalizing MI,
providing a bounded measure between 0 and 1.</p>
<p>NMI is indispensable in clustering evaluation, where comparing the
similarity between a predicted partition and a ground truth partition is
crucial. It is also widely used in data fusion, image processing, and
bioinformatics.</p>
<h1 id="définitions">Définitions</h1>
<p>To understand NMI, let’s first consider two partitions of a dataset
<span class="math inline">\(X\)</span>, denoted as <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span>. We aim to quantify how similar these
partitions are.</p>
<div class="definition">
<p>Given two partitions <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> of a dataset <span
class="math inline">\(X\)</span>, the Mutual Information <span
class="math inline">\(I(A, B)\)</span> is defined as: <span
class="math display">\[I(A, B) = \sum_{a \in A} \sum_{b \in B} P(a, b)
\log\left(\frac{P(a, b)}{P(a)P(b)}\right)\]</span> where <span
class="math inline">\(P(a)\)</span>, <span
class="math inline">\(P(b)\)</span>, and <span
class="math inline">\(P(a, b)\)</span> are the probabilities of a sample
belonging to cluster <span class="math inline">\(a\)</span>, cluster
<span class="math inline">\(b\)</span>, and both clusters <span
class="math inline">\(a\)</span> and <span
class="math inline">\(b\)</span> respectively.</p>
</div>
<p>However, as mentioned earlier, MI is not bounded. To normalize it, we
consider the following:</p>
<div class="definition">
<p>The Normalized Mutual Information <span class="math inline">\(NMI(A,
B)\)</span> is defined as: <span class="math display">\[NMI(A, B) =
\frac{I(A, B)}{\sqrt{H(A)H(B)}}\]</span> where <span
class="math inline">\(H(A)\)</span> and <span
class="math inline">\(H(B)\)</span> are the entropies of partitions
<span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> respectively, defined as: <span
class="math display">\[H(A) = -\sum_{a \in A} P(a) \log(P(a)), \quad
H(B) = -\sum_{b \in B} P(b) \log(P(b))\]</span></p>
</div>
<p>Alternatively, NMI can also be defined using the following
normalization: <span class="math display">\[NMI(A, B) = \frac{2I(A,
B)}{H(A) + H(B)}\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<div class="theorem">
<p>For any two partitions <span class="math inline">\(A\)</span> and
<span class="math inline">\(B\)</span>, the Normalized Mutual
Information <span class="math inline">\(NMI(A, B)\)</span> satisfies:
<span class="math display">\[0 \leq NMI(A, B) \leq 1\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> To prove the boundedness of NMI, we first note that
<span class="math inline">\(I(A, B) \geq 0\)</span> by the
non-negativity of mutual information. The maximum value of <span
class="math inline">\(I(A, B)\)</span> is achieved when partitions <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are identical. In this case, <span
class="math inline">\(I(A, B) = H(A) = H(B)\)</span>, leading to: <span
class="math display">\[NMI(A, B) = \frac{H(A)}{\sqrt{H(A)H(B)}} =
1\]</span> Thus, <span class="math inline">\(NMI(A, B)\)</span> is
bounded between 0 and 1. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Let’s delve deeper into the proof of the boundedness theorem.</p>
<div class="proof">
<p><em>Proof.</em> We start by recalling that <span
class="math inline">\(I(A, B) \geq 0\)</span>, which implies <span
class="math inline">\(NMI(A, B) \geq 0\)</span>.</p>
<p>For the upper bound, we consider the case where <span
class="math inline">\(A = B\)</span>. In this scenario: <span
class="math display">\[I(A, B) = H(A) = H(B)\]</span> Substituting into
the NMI formula: <span class="math display">\[NMI(A, B) =
\frac{H(A)}{\sqrt{H(A)H(B)}} = \frac{H(A)}{H(A)} = 1\]</span> For any
other partitions, <span class="math inline">\(I(A, B) \leq \min(H(A),
H(B))\)</span>. Therefore: <span class="math display">\[NMI(A, B) =
\frac{I(A, B)}{\sqrt{H(A)H(B)}} \leq \frac{\min(H(A),
H(B))}{\sqrt{H(A)H(B)}} \leq 1\]</span> This completes the proof. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<ol>
<li><p><strong>Symmetry:</strong> NMI is symmetric, i.e., <span
class="math inline">\(NMI(A, B) = NMI(B, A)\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> The symmetry follows directly from the definition of
mutual information: <span class="math display">\[I(A, B) = I(B,
A)\]</span> Thus, <span class="math display">\[NMI(A, B) = \frac{I(A,
B)}{\sqrt{H(A)H(B)}} = \frac{I(B, A)}{\sqrt{H(B)H(A)}} = NMI(B,
A)\]</span> ◻</p>
</div></li>
<li><p><strong>Normalization:</strong> NMI is normalized such that <span
class="math inline">\(0 \leq NMI(A, B) \leq 1\)</span>.</p>
<p>This property has already been proven in the theorem
section.</p></li>
<li><p><strong>Perfect Agreement:</strong> If partitions <span
class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are identical, then <span
class="math inline">\(NMI(A, B) = 1\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> If <span class="math inline">\(A = B\)</span>, then:
<span class="math display">\[I(A, B) = H(A) = H(B)\]</span> Substituting
into the NMI formula: <span class="math display">\[NMI(A, B) =
\frac{H(A)}{\sqrt{H(A)H(B)}} = 1\]</span> ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Normalized Mutual Information provides a powerful and intuitive
measure for comparing the similarity between two data partitions. Its
normalization properties make it particularly useful in clustering
evaluation and other applications where bounded measures are
preferred.</p>
</body>
</html>
{% include "footer.html" %}


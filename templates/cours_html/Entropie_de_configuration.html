{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Entropie de Configuration : Fondements et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Entropie de Configuration : Fondements et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>L’entropie de configuration émerge comme un concept fondamental en
physique statistique, en théorie de l’information et en sciences des
matériaux. Son origine remonte aux travaux pionniers de Ludwig Boltzmann
sur la mécanique statistique, où il introduisit l’idée que l’état
macroscopique d’un système est déterminé par la distribution de ses
microétats. L’entropie de configuration quantifie le nombre de
configurations microscopiques possibles pour un système donné, offrant
ainsi une mesure de son désordre.</p>
<p>Pourquoi ce concept est-il indispensable ? Dans un monde où les
systèmes complexes dominent, comprendre comment le désordre se manifeste
et évolue est crucial. Que ce soit pour prédire les propriétés des
matériaux, optimiser les processus de communication ou modéliser les
phénomènes biologiques, l’entropie de configuration fournit un cadre
théorique robuste. Elle permet de relier les propriétés macroscopiques
observables aux détails microscopiques, bridging the gap entre le
visible et l’invisible.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour aborder l’entropie de configuration, commençons par comprendre
ce que nous cherchons à quantifier. Imaginons un système composé de
particules identiques. Chaque particule peut occuper différents états,
et l’ensemble des états possibles forme un espace de configuration.
L’entropie de configuration mesure le nombre de façons dont ces
particules peuvent être distribuées dans cet espace.</p>
<p>Formellement, considérons un système avec <span
class="math inline">\(N\)</span> particules et <span
class="math inline">\(M\)</span> états possibles. La configuration du
système est définie par le nombre de particules dans chaque état. Si
<span class="math inline">\(n_i\)</span> représente le nombre de
particules dans l’état <span class="math inline">\(i\)</span>, alors
nous avons :</p>
<p><span class="math display">\[\sum_{i=1}^{M} n_i = N\]</span></p>
<p>L’entropie de configuration <span class="math inline">\(S\)</span>
est alors donnée par :</p>
<p><span class="math display">\[S = k_B \ln \left(
\frac{N!}{\prod_{i=1}^{M} n_i!} \right)\]</span></p>
<p>où <span class="math inline">\(k_B\)</span> est la constante de
Boltzmann. Cette formule compte le nombre de façons distinctes de
distribuer les <span class="math inline">\(N\)</span> particules parmi
les <span class="math inline">\(M\)</span> états, en tenant compte des
indistinctions entre particules identiques.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème central en théorie de l’information, dû à Claude Shannon,
relie l’entropie de configuration à l’incertitude. Ce théorème, connu
sous le nom de deuxième loi de la thermodynamique pour les systèmes
d’information, stipule que l’entropie est une mesure de l’incertitude ou
de l’information manquante.</p>
<p>Formellement, pour un système avec une distribution de probabilité
<span class="math inline">\(p_i\)</span> sur les états <span
class="math inline">\(i\)</span>, l’entropie de Shannon est définie par
:</p>
<p><span class="math display">\[H = - \sum_{i=1}^{M} p_i \ln
p_i\]</span></p>
<p>Ce théorème peut être interprété comme suit : plus l’entropie est
élevée, plus il y a d’incertitude sur l’état du système. En physique
statistique, cela se traduit par une plus grande variété de
configurations microscopiques possibles.</p>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour démontrer le lien entre l’entropie de configuration et
l’incertitude, considérons un système avec une distribution de
probabilité <span class="math inline">\(p_i\)</span>. L’entropie de
Shannon <span class="math inline">\(H\)</span> est maximisée lorsque
toutes les probabilités <span class="math inline">\(p_i\)</span> sont
égales, c’est-à-dire lorsque le système est dans un état de désordre
maximal.</p>
<p>Preuve :</p>
<p><span class="math display">\[H = - \sum_{i=1}^{M} p_i \ln
p_i\]</span></p>
<p>En utilisant le principe de maximisation de l’entropie sous
contrainte, nous trouvons que <span class="math inline">\(H\)</span> est
maximale lorsque <span class="math inline">\(p_i = \frac{1}{M}\)</span>
pour tout <span class="math inline">\(i\)</span>. Cela montre que
l’entropie de configuration est une mesure naturelle du désordre dans un
système.</p>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>L’entropie de configuration possède plusieurs propriétés importantes
:</p>
<p>(i) **Additivité** : Pour un système composé de sous-systèmes
indépendants, l’entropie totale est la somme des entropies des
sous-systèmes.</p>
<p><span class="math display">\[H_{total} = H_1 + H_2 + \dots +
H_n\]</span></p>
<p>(ii) **Non-négativité** : L’entropie est toujours non négative, <span
class="math inline">\(H \geq 0\)</span>, et atteint son minimum lorsque
le système est dans un état déterministe.</p>
<p>(iii) **Invariance par renormalisation** : L’entropie est invariante
sous les transformations de probabilité qui préservent la
distribution.</p>
<p>Preuve pour (i) :</p>
<p>Considérons deux sous-systèmes indépendants avec des distributions de
probabilité <span class="math inline">\(p_i\)</span> et <span
class="math inline">\(q_j\)</span>. L’entropie conjointe est :</p>
<p><span class="math display">\[H_{total} = - \sum_{i,j} p_i q_j \ln
(p_i q_j) = - \sum_{i,j} p_i q_j (\ln p_i + \ln q_j) = H_1 +
H_2\]</span></p>
<p>Cette propriété montre que l’entropie est une mesure additive du
désordre.</p>
</body>
</html>
{% include "footer.html" %}


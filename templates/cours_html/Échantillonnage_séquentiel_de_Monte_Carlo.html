{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Échantillonnage séquentiel de Monte Carlo</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Échantillonnage séquentiel de Monte Carlo</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’échantillonnage séquentiel de Monte Carlo (SMC) est une méthode
statistique puissante qui combine les techniques d’échantillonnage
séquentiel et de simulation Monte Carlo. Cette approche permet de
traiter des problèmes complexes où les données arrivent de manière
séquentielle ou où la distribution cible est trop complexe pour être
échantillonnée directement.</p>
<p>L’origine de cette méthode remonte aux années 1950 avec les travaux
de Nicholas Metropolis et Stanislaw Ulam sur la simulation Monte Carlo.
Cependant, c’est dans les années 1990 que l’échantillonnage séquentiel
de Monte Carlo a commencé à émerger comme une discipline distincte,
grâce aux travaux de Pierre Del Moral, Arnaud Doucet et leurs
collaborateurs.</p>
<p>L’échantillonnage séquentiel de Monte Carlo est indispensable dans
des domaines tels que la filtrage de Kalman, l’apprentissage automatique
et la modélisation bayésienne. Il permet de traiter des problèmes
dynamiques où les données évoluent dans le temps, tout en fournissant
une estimation précise de la distribution cible.</p>
<h1 id="définitions">Définitions</h1>
<p>Avant de définir l’échantillonnage séquentiel de Monte Carlo, il est
important de comprendre le contexte. Supposons que nous ayons une
séquence de distributions <span
class="math inline">\(\{\pi_t\}_{t=1}^T\)</span> et que nous voulions
échantillonner à partir de <span class="math inline">\(\pi_T\)</span>.
Cependant, échantillonner directement à partir de <span
class="math inline">\(\pi_T\)</span> peut être difficile en raison de sa
complexité.</p>
<p>Nous cherchons donc une méthode qui nous permette d’échantillonner à
partir de <span class="math inline">\(\pi_T\)</span> en utilisant des
distributions intermédiaires plus simples. L’idée est d’utiliser une
séquence de distributions <span
class="math inline">\(\{\pi_t\}_{t=1}^T\)</span> telles que chaque <span
class="math inline">\(\pi_t\)</span> soit plus simple à échantillonner
que <span class="math inline">\(\pi_T\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\{\pi_t\}_{t=1}^T\)</span> une
séquence de distributions. L’échantillonnage séquentiel de Monte Carlo
consiste à échantillonner une particule <span
class="math inline">\(X_1^{(i)}\)</span> à partir de <span
class="math inline">\(\pi_1\)</span>, puis pour chaque <span
class="math inline">\(t\)</span> de 2 à <span
class="math inline">\(T\)</span>, à échantillonner une particule <span
class="math inline">\(X_t^{(i)}\)</span> à partir de <span
class="math inline">\(\pi_t\)</span> en utilisant les particules
précédentes.</p>
<p>Formellement, pour chaque <span class="math inline">\(t\)</span> de 2
à <span class="math inline">\(T\)</span>, nous avons : <span
class="math display">\[X_t^{(i)} \sim \pi_t(x_t |
X_{1:t-1}^{(i)})\]</span> où <span class="math inline">\(X_{1:t-1}^{(i)}
= (X_1^{(i)}, \ldots, X_{t-1}^{(i)})\)</span>.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un des théorèmes fondamentaux de l’échantillonnage séquentiel de
Monte Carlo est le théorème de convergence. Ce théorème nous dit que
sous certaines conditions, l’échantillon obtenu par SMC converge vers la
distribution cible <span class="math inline">\(\pi_T\)</span>.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\{\pi_t\}_{t=1}^T\)</span> une
séquence de distributions et supposons que pour chaque <span
class="math inline">\(t\)</span>, <span
class="math inline">\(\pi_t\)</span> est absolument continue par rapport
à une mesure de référence <span class="math inline">\(\mu\)</span>.
Supposons également que pour chaque <span
class="math inline">\(t\)</span>, la densité de <span
class="math inline">\(\pi_t\)</span> par rapport à <span
class="math inline">\(\mu\)</span> est bornée.</p>
<p>Alors, sous ces conditions, l’échantillon obtenu par SMC converge
vers <span class="math inline">\(\pi_T\)</span> au sens de la
convergence en loi.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour prouver le théorème de convergence, nous devons montrer que pour
chaque <span class="math inline">\(t\)</span>, l’échantillon obtenu par
SMC converge vers <span class="math inline">\(\pi_t\)</span>. Nous
commençons par montrer que pour chaque <span
class="math inline">\(t\)</span>, l’échantillon obtenu par SMC est un
échantillon pondéré de <span class="math inline">\(\pi_t\)</span>.</p>
<div class="proof">
<p><em>Proof.</em> Pour chaque <span class="math inline">\(t\)</span>,
nous avons : <span class="math display">\[X_t^{(i)} \sim \pi_t(x_t |
X_{1:t-1}^{(i)})\]</span> où <span class="math inline">\(X_{1:t-1}^{(i)}
= (X_1^{(i)}, \ldots, X_{t-1}^{(i)})\)</span>.</p>
<p>Nous pouvons réécrire cette équation comme : <span
class="math display">\[X_t^{(i)} \sim \frac{\pi_t(x_t |
X_{1:t-1}^{(i)})}{\int \pi_t(x_t | X_{1:t-1}^{(i)}) dx_t}
\mu(dx_t)\]</span></p>
<p>où <span class="math inline">\(\mu\)</span> est une mesure de
référence.</p>
<p>En utilisant la loi des grands nombres, nous savons que pour chaque
<span class="math inline">\(t\)</span>, l’échantillon obtenu par SMC
converge vers <span class="math inline">\(\pi_t\)</span> au sens de la
convergence en loi. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>L’échantillonnage séquentiel de Monte Carlo possède plusieurs
propriétés intéressantes. Nous en listons quelques-unes ci-dessous :</p>
<ol>
<li><p><strong>Propriété de convergence</strong> : Sous certaines
conditions, l’échantillon obtenu par SMC converge vers la distribution
cible <span class="math inline">\(\pi_T\)</span>.</p></li>
<li><p><strong>Propriété de stabilité</strong> : L’échantillonnage
séquentiel de Monte Carlo est stable en ce sens que les particules ne
dégénèrent pas au fil du temps.</p></li>
<li><p><strong>Propriété de flexibilité</strong> : L’échantillonnage
séquentiel de Monte Carlo peut être utilisé pour traiter une large
variété de problèmes, allant du filtrage de Kalman à l’apprentissage
automatique.</p></li>
</ol>
<div class="corollary">
<p>Sous les conditions du théorème de convergence, l’échantillon obtenu
par SMC converge vers <span class="math inline">\(\pi_T\)</span> au sens
de la convergence en loi.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Ce corollaire est une conséquence directe du théorème
de convergence. En effet, si l’échantillon obtenu par SMC converge vers
<span class="math inline">\(\pi_t\)</span> pour chaque <span
class="math inline">\(t\)</span>, alors il converge également vers <span
class="math inline">\(\pi_T\)</span>. ◻</p>
</div>
</body>
</html>
{% include "footer.html" %}


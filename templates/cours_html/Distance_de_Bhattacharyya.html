{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Distance de Bhattacharyya : Un outil fondamental en théorie de l’information et apprentissage statistique</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Distance de Bhattacharyya : Un outil fondamental en
théorie de l’information et apprentissage statistique</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 class="unnumbered" id="introduction-et-motivations">Introduction et
Motivations</h1>
<p>La distance de Bhattacharyya, introduite par le statisticien indien
Anil Kumar Bhattacharyya en 1943, est une mesure de divergence entre
deux distributions de probabilité. Son origine remonte aux travaux
pionniers en théorie statistique et en théorie de l’information, où la
nécessité de quantifier les différences entre distributions s’est
imposée comme un problème fondamental. Cette notion émerge naturellement
dans le cadre de la classification statistique, de l’apprentissage
automatique et de la théorie des codes, où la discrimination entre
modèles probabilistes est cruciale.</p>
<p>La distance de Bhattacharyya résout le problème de mesurer la
similitude entre deux distributions de manière robuste et interprétable.
Elle est particulièrement indispensable dans les contextes où l’on
souhaite évaluer la capacité de discrimination entre deux classes, par
exemple en reconnaissance de motifs ou en détection d’anomalies. Son
élégance mathématique et ses propriétés analytiques en font un outil
privilégié pour les chercheurs et ingénieurs en sciences des
données.</p>
<h1 class="unnumbered" id="définitions">Définitions</h1>
<p>Pour introduire la distance de Bhattacharyya, considérons deux
distributions de probabilité discrètes <span
class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> définies sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. Nous cherchons une mesure
qui quantifie à quel point ces deux distributions sont proches l’une de
l’autre. Intuitivement, cette mesure devrait être nulle si et seulement
si <span class="math inline">\(P = Q\)</span>, et elle devrait augmenter
lorsque les distributions deviennent plus distinctes.</p>
<div class="definition">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité
discrètes sur un ensemble fini <span
class="math inline">\(\mathcal{X}\)</span>. La distance de Bhattacharyya
entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est définie par : <span
class="math display">\[D_B(P, Q) = -\ln \left( \sum_{x \in \mathcal{X}}
\sqrt{P(x) Q(x)} \right).\]</span> De manière équivalente, on peut
l’exprimer comme : <span class="math display">\[D_B(P, Q) = -\ln \left(
BC(P, Q) \right),\]</span> où <span class="math inline">\(BC(P, Q) =
\sum_{x \in \mathcal{X}} \sqrt{P(x) Q(x)}\)</span> est le coefficient de
Bhattacharyya.</p>
</div>
<p>Pour les distributions continues, la définition s’étend naturellement
en remplaçant la somme par une intégrale : <span
class="math display">\[D_B(P, Q) = -\ln \left( \int_{\mathcal{X}}
\sqrt{p(x) q(x)} \, dx \right),\]</span> où <span
class="math inline">\(p\)</span> et <span
class="math inline">\(q\)</span> sont les densités de probabilité
associées à <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span>.</p>
<h1 class="unnumbered" id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental lié à la distance de Bhattacharyya est celui
concernant sa relation avec l’erreur de classification bayésienne. Ce
théorème montre que la distance de Bhattacharyya est étroitement liée à
la performance optimale d’un classificateur bayésien.</p>
<div class="theorem">
<p>Soient <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> deux distributions de probabilité sur
un ensemble <span class="math inline">\(\mathcal{X}\)</span>. L’erreur
de classification bayésienne optimale <span
class="math inline">\(P_{\text{erreur}}\)</span> est liée à la distance
de Bhattacharyya par : <span class="math display">\[P_{\text{erreur}}
\leq \frac{1}{2} e^{-D_B(P, Q)}.\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Pour démontrer ce théorème, nous commençons par
rappeler que l’erreur de classification bayésienne optimale est donnée
par : <span class="math display">\[P_{\text{erreur}} = \min_{\phi}
P(\phi(X) \neq Y),\]</span> où <span class="math inline">\(\phi\)</span>
est une fonction de décision et <span class="math inline">\(Y\)</span>
est la classe vraie.</p>
<p>En utilisant le théorème de Bayes, nous savons que l’erreur minimale
est atteinte lorsque <span class="math inline">\(\phi\)</span> est la
règle du maximum a posteriori. L’erreur peut alors être exprimée en
termes des probabilités a posteriori.</p>
<p>En utilisant l’inégalité de Chernoff et les propriétés de la fonction
exponentielle, nous obtenons : <span
class="math display">\[P_{\text{erreur}} \leq \frac{1}{2} e^{-D_B(P,
Q)}.\]</span> Cette inégalité montre que plus la distance de
Bhattacharyya est grande, plus l’erreur de classification bayésienne
optimale est petite. ◻</p>
</div>
<h1 class="unnumbered" id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la distance de Bhattacharyya,
considérons un exemple simple où <span class="math inline">\(P\)</span>
et <span class="math inline">\(Q\)</span> sont des distributions
gaussiennes. Nous voulons montrer que la distance de Bhattacharyya entre
deux gaussiennes est liée à leurs paramètres.</p>
<div class="lemma">
<p>Soient <span class="math inline">\(P = \mathcal{N}(\mu_1,
\Sigma_1)\)</span> et <span class="math inline">\(Q = \mathcal{N}(\mu_2,
\Sigma_2)\)</span> deux distributions gaussiennes. La distance de
Bhattacharyya entre <span class="math inline">\(P\)</span> et <span
class="math inline">\(Q\)</span> est donnée par : <span
class="math display">\[D_B(P, Q) = \frac{1}{8} (\mu_2 - \mu_1)^T \left(
\frac{\Sigma_1 + \Sigma_2}{2} \right)^{-1} (\mu_2 - \mu_1) + \frac{1}{2}
\ln \left( \frac{\det\left( \frac{\Sigma_1 + \Sigma_2}{2}
\right)}{\sqrt{\det(\Sigma_1) \det(\Sigma_2)}} \right).\]</span></p>
</div>
<div class="proof">
<p><em>Proof.</em> Nous commençons par exprimer le coefficient de
Bhattacharyya <span class="math inline">\(BC(P, Q)\)</span> pour des
gaussiennes : <span class="math display">\[BC(P, Q) = \int_{\mathcal{X}}
\sqrt{p(x) q(x)} \, dx.\]</span> En utilisant les propriétés des
gaussiennes, nous savons que : <span class="math display">\[p(x) q(x) =
\frac{1}{(2\pi)^{n/2} \sqrt{\det(\Sigma_1)} \sqrt{\det(\Sigma_2)}}
e^{-\frac{1}{2} (x - \mu_1)^T \Sigma_1^{-1} (x - \mu_1) - \frac{1}{2} (x
- \mu_2)^T \Sigma_2^{-1} (x - \mu_2)}.\]</span> En simplifiant
l’exponentielle, nous obtenons : <span class="math display">\[p(x) q(x)
= \frac{1}{(2\pi)^{n/2} \sqrt{\det(\Sigma_1) \det(\Sigma_2)}}
e^{-\frac{1}{2} (x - \mu)^T \tilde{\Sigma}^{-1} (x - \mu)},\]</span> où
<span class="math inline">\(\mu = \frac{\mu_1 + \mu_2}{2}\)</span> et
<span class="math inline">\(\tilde{\Sigma} = \frac{\Sigma_1 +
\Sigma_2}{2}\)</span>.</p>
<p>En intégrant sur <span class="math inline">\(\mathcal{X}\)</span>,
nous trouvons : <span class="math display">\[BC(P, Q) =
\frac{1}{\sqrt{(2\pi)^n \det(\tilde{\Sigma})}} \cdot
\frac{1}{\sqrt{\det(\Sigma_1) \det(\Sigma_2)}}.\]</span> En prenant le
logarithme et en multipliant par -1, nous obtenons la distance de
Bhattacharyya : <span class="math display">\[D_B(P, Q) = \frac{1}{8}
(\mu_2 - \mu_1)^T \tilde{\Sigma}^{-1} (\mu_2 - \mu_1) + \frac{1}{2} \ln
\left( \frac{\det(\tilde{\Sigma})}{\sqrt{\det(\Sigma_1) \det(\Sigma_2)}}
\right).\]</span> ◻</p>
</div>
<h1 class="unnumbered" id="propriétés-et-corollaires">Propriétés et
Corollaires</h1>
<p>La distance de Bhattacharyya possède plusieurs propriétés
intéressantes qui en font un outil puissant en théorie de l’information
et en apprentissage statistique.</p>
<ol>
<li><p><strong>Symétrie</strong> : La distance de Bhattacharyya est
symétrique, c’est-à-dire que <span class="math inline">\(D_B(P, Q) =
D_B(Q, P)\)</span>.</p></li>
<li><p><strong>Non-négativité</strong> : La distance de Bhattacharyya
est toujours non négative, et elle est nulle si et seulement si <span
class="math inline">\(P = Q\)</span>.</p></li>
<li><p><strong>Inégalité de Chernoff</strong> : La distance de
Bhattacharyya est liée à l’inégalité de Chernoff, qui fournit des bornes
sur les probabilités de grandes déviations.</p></li>
</ol>
<p>Pour démontrer la propriété (i), nous utilisons simplement la
définition de la distance de Bhattacharyya : <span
class="math display">\[D_B(P, Q) = -\ln \left( \sum_{x \in \mathcal{X}}
\sqrt{P(x) Q(x)} \right) = -\ln \left( \sum_{x \in \mathcal{X}}
\sqrt{Q(x) P(x)} \right) = D_B(Q, P).\]</span></p>
<p>Pour la propriété (ii), nous observons que si <span
class="math inline">\(P = Q\)</span>, alors <span
class="math inline">\(D_B(P, Q) = 0\)</span>. Réciproquement, si <span
class="math inline">\(D_B(P, Q) = 0\)</span>, alors <span
class="math inline">\(BC(P, Q) = 1\)</span>, ce qui implique que <span
class="math inline">\(P(x) Q(x) = (P(x))^2\)</span> pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span>. Par conséquent, <span
class="math inline">\(P(x) = Q(x)\)</span> pour tout <span
class="math inline">\(x \in \mathcal{X}\)</span>, c’est-à-dire que <span
class="math inline">\(P = Q\)</span>.</p>
<p>Enfin, pour la propriété (iii), nous utilisons le théorème de
Chernoff qui relie la distance de Bhattacharyya à l’inégalité de
Chernoff. Cette propriété est cruciale pour les applications en théorie
des codes et en apprentissage statistique.</p>
</body>
</html>
{% include "footer.html" %}


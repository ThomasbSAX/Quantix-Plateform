{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Modèle de mélange bayésien</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Modèle de mélange bayésien</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>Les modèles de mélange bayésien constituent une avancée majeure dans
le domaine de l’apprentissage automatique et des statistiques. Leur
origine remonte aux travaux fondateurs de Thomas Bayes au XVIIIe siècle,
qui a posé les bases de l’inférence bayésienne. Ces modèles ont émergé
pour répondre à un besoin crucial : modéliser des données complexes et
hétérogènes en les décomposant en sous-groupes plus simples. Ils sont
indispensables dans des domaines variés tels que la bio-informatique,
l’analyse d’images et la reconnaissance de motifs.</p>
<p>L’idée centrale est de considérer que les données observées sont
générées par un mélange de distributions sous-jacentes. Par exemple,
dans une population d’individus, les données peuvent provenir de
différents sous-groupes ayant des caractéristiques distinctes. Les
modèles de mélange bayésien permettent d’identifier ces sous-groupes et
d’estimer leurs paramètres de manière probabiliste, en intégrant les
incertitudes inhérentes aux données.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre les modèles de mélange bayésien, commençons par
définir ce qu’est un mélange de distributions. Supposons que nous avons
un ensemble de données <span class="math inline">\(\mathcal{D} = \{x_1,
x_2, \ldots, x_n\}\)</span> et que nous soupçonnons que ces données
proviennent de <span class="math inline">\(K\)</span> distributions
sous-jacentes. Chaque distribution est caractérisée par ses propres
paramètres <span class="math inline">\(\theta_k\)</span>, et chaque
donnée <span class="math inline">\(x_i\)</span> appartient à l’une de
ces distributions avec une certaine probabilité <span
class="math inline">\(\pi_k\)</span>.</p>
<p>Formellement, un modèle de mélange bayésien peut être défini comme
suit :</p>
<div class="definition">
<p>Un modèle de mélange bayésien est un modèle probabiliste où les
données observées <span class="math inline">\(\mathcal{D}\)</span> sont
supposées être générées par un mélange de <span
class="math inline">\(K\)</span> distributions sous-jacentes. Chaque
distribution est caractérisée par ses paramètres <span
class="math inline">\(\theta_k\)</span>, et chaque donnée <span
class="math inline">\(x_i\)</span> appartient à l’une de ces
distributions avec une probabilité <span
class="math inline">\(\pi_k\)</span>.</p>
<p>Matériellement, cela s’exprime par : <span
class="math display">\[p(x_i | \Theta) = \sum_{k=1}^K \pi_k p(x_i |
\theta_k)\]</span> où <span class="math inline">\(\Theta = \{\pi_1,
\pi_2, \ldots, \pi_K, \theta_1, \theta_2, \ldots, \theta_K\}\)</span>
représente l’ensemble des paramètres du modèle.</p>
</div>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental dans le cadre des modèles de mélange bayésien
est le théorème de Bayes, qui permet de mettre à jour les croyances sur
les paramètres du modèle en fonction des données observées.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\Theta\)</span> l’ensemble des
paramètres du modèle et <span class="math inline">\(\mathcal{D}\)</span>
les données observées. Le théorème de Bayes stipule que : <span
class="math display">\[p(\Theta | \mathcal{D}) = \frac{p(\mathcal{D} |
\Theta) p(\Theta)}{p(\mathcal{D})}\]</span> où <span
class="math inline">\(p(\Theta | \mathcal{D})\)</span> est la
distribution a posteriori des paramètres, <span
class="math inline">\(p(\mathcal{D} | \Theta)\)</span> est la
vraisemblance des données, <span
class="math inline">\(p(\Theta)\)</span> est la distribution a priori
des paramètres et <span class="math inline">\(p(\mathcal{D})\)</span>
est la probabilité marginale des données.</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer l’application du théorème de Bayes dans le cadre des
modèles de mélange bayésien, considérons un exemple simple où nous avons
deux distributions sous-jacentes.</p>
<div class="proof">
<p><em>Proof.</em> Supposons que nous avons deux distributions normales
avec des paramètres <span class="math inline">\(\theta_1 = (\mu_1,
\sigma_1^2)\)</span> et <span class="math inline">\(\theta_2 = (\mu_2,
\sigma_2^2)\)</span>. La vraisemblance des données <span
class="math inline">\(\mathcal{D}\)</span> est donnée par : <span
class="math display">\[p(\mathcal{D} | \Theta) = \prod_{i=1}^n \left(
\pi_1 \mathcal{N}(x_i | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(x_i |
\mu_2, \sigma_2^2) \right)\]</span> où <span
class="math inline">\(\mathcal{N}(x | \mu, \sigma^2)\)</span> désigne la
fonction de densité de probabilité d’une distribution normale.</p>
<p>En utilisant le théorème de Bayes, nous pouvons calculer la
distribution a posteriori des paramètres <span
class="math inline">\(\Theta\)</span> : <span
class="math display">\[p(\Theta | \mathcal{D}) = \frac{p(\mathcal{D} |
\Theta) p(\Theta)}{p(\mathcal{D})}\]</span> où <span
class="math inline">\(p(\Theta)\)</span> est la distribution a priori
des paramètres, que nous pouvons choisir en fonction de nos
connaissances préalables sur le modèle. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>Les modèles de mélange bayésien possèdent plusieurs propriétés
intéressantes qui en font un outil puissant pour l’analyse des
données.</p>
<ol>
<li><p><strong>Propriété de décomposition</strong> : Les modèles de
mélange bayésien permettent de décomposer les données observées en
sous-groupes plus simples, facilitant ainsi l’interprétation et
l’analyse des données.</p></li>
<li><p><strong>Propriété d’inférence bayésienne</strong> : En utilisant
le théorème de Bayes, nous pouvons mettre à jour nos croyances sur les
paramètres du modèle en fonction des données observées, intégrant ainsi
les incertitudes inhérentes aux données.</p></li>
<li><p><strong>Propriété de flexibilité</strong> : Les modèles de
mélange bayésien peuvent être adaptés à une grande variété de
distributions sous-jacentes, ce qui les rend applicables à des domaines
variés.</p></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>Les modèles de mélange bayésien représentent une avancée
significative dans le domaine de l’apprentissage automatique et des
statistiques. Leur capacité à modéliser des données complexes et
hétérogènes en les décomposant en sous-groupes plus simples en fait un
outil puissant pour l’analyse des données. En intégrant les incertitudes
inhérentes aux données, ces modèles permettent une inférence robuste et
flexible, applicable à une grande variété de domaines.</p>
</body>
</html>
{% include "footer.html" %}


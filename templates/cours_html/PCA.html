{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Analyse en Composantes Principales : Fondements Mathématiques et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Analyse en Composantes Principales : Fondements
Mathématiques et Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>L’Analyse en Composantes Principales (PCA) est une technique
statistique multivariée qui vise à réduire la dimensionnalité des
données tout en préservant au maximum leur variance. Introduite par Karl
Pearson en 1901 et développée par Harold Hotelling dans les années 1930,
la PCA est aujourd’hui un outil fondamental en statistique, en
apprentissage automatique et dans de nombreuses disciplines
scientifiques.</p>
<p>La motivation principale derrière la PCA est de simplifier l’analyse
des données en projetant les observations sur un sous-espace de
dimension inférieure. Cela permet non seulement de visualiser des
données de haute dimension, mais aussi d’identifier les structures
sous-jacentes et les corrélations entre les variables.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour comprendre la PCA, commençons par définir quelques concepts
clés. Supposons que nous avons un ensemble de données contenant <span
class="math inline">\(n\)</span> observations et <span
class="math inline">\(p\)</span> variables. Nous pouvons représenter ces
données sous forme d’une matrice <span class="math inline">\(X \in
\mathbb{R}^{n \times p}\)</span>, où chaque ligne correspond à une
observation et chaque colonne à une variable.</p>
<h2 id="matrice-de-covariance">Matrice de Covariance</h2>
<p>La matrice de covariance <span class="math inline">\(\Sigma \in
\mathbb{R}^{p \times p}\)</span> est une mesure de la variance et de la
covariance entre les variables. Elle est définie comme suit :</p>
<p><span class="math display">\[\Sigma = \frac{1}{n-1} X^T
X\]</span></p>
<p>où <span class="math inline">\(X\)</span> est une matrice centrée,
c’est-à-dire que chaque colonne a une moyenne nulle.</p>
<h2 id="composantes-principales">Composantes Principales</h2>
<p>Les composantes principales sont des directions orthogonales dans
l’espace des données qui maximisent la variance projetée. Formellement,
les composantes principales sont les vecteurs propres <span
class="math inline">\(v_1, v_2, \ldots, v_p\)</span> de la matrice de
covariance <span class="math inline">\(\Sigma\)</span>, ordonnés par
ordre décroissant de leurs valeurs propres associées <span
class="math inline">\(\lambda_1 \geq \lambda_2 \geq \ldots \geq
\lambda_p\)</span>.</p>
<h1 id="théorèmes">Théorèmes</h1>
<h2 id="théorème-des-composantes-principales">Théorème des Composantes
Principales</h2>
<p>Le théorème fondamental de la PCA stipule que les composantes
principales peuvent être obtenues en diagonalisant la matrice de
covariance. Plus précisément, si <span
class="math inline">\(\Sigma\)</span> est une matrice de covariance
symétrique et définie positive, alors il existe une matrice orthogonale
<span class="math inline">\(V\)</span> telle que :</p>
<p><span class="math display">\[\Sigma = V \Lambda V^T\]</span></p>
<p>où <span class="math inline">\(\Lambda\)</span> est une matrice
diagonale contenant les valeurs propres de <span
class="math inline">\(\Sigma\)</span>.</p>
<h2 id="théorème-de-la-variance-totale">Théorème de la Variance
Totale</h2>
<p>La variance totale des données peut être exprimée en termes des
valeurs propres de la matrice de covariance. Plus précisément, si <span
class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_p\)</span>
sont les valeurs propres de <span class="math inline">\(\Sigma\)</span>,
alors la variance totale est donnée par :</p>
<p><span class="math display">\[\text{Var}_{\text{totale}} =
\sum_{i=1}^p \lambda_i\]</span></p>
<h1 id="preuves">Preuves</h1>
<h2 id="preuve-du-théorème-des-composantes-principales">Preuve du
Théorème des Composantes Principales</h2>
<p>Pour prouver le théorème des composantes principales, nous devons
montrer que les vecteurs propres de <span
class="math inline">\(\Sigma\)</span> forment une base orthonormée de
l’espace des données. Supposons que <span
class="math inline">\(v\)</span> est un vecteur propre de <span
class="math inline">\(\Sigma\)</span> associé à la valeur propre <span
class="math inline">\(\lambda\)</span>. Alors, nous avons :</p>
<p><span class="math display">\[\Sigma v = \lambda v\]</span></p>
<p>En multipliant les deux côtés par <span
class="math inline">\(v^T\)</span>, nous obtenons :</p>
<p><span class="math display">\[v^T \Sigma v = \lambda v^T
v\]</span></p>
<p>Puisque <span class="math inline">\(\Sigma\)</span> est symétrique et
définie positive, <span class="math inline">\(v^T \Sigma v &gt;
0\)</span>. De plus, <span class="math inline">\(v^T v = 1\)</span> si
<span class="math inline">\(v\)</span> est un vecteur propre normalisé.
Par conséquent, nous avons :</p>
<p><span class="math display">\[\lambda = v^T \Sigma v &gt;
0\]</span></p>
<p>Cela montre que les valeurs propres de <span
class="math inline">\(\Sigma\)</span> sont positives, et donc <span
class="math inline">\(\Sigma\)</span> est diagonalisable. Les vecteurs
propres correspondants forment une base orthonormée de l’espace des
données.</p>
<h2 id="preuve-du-théorème-de-la-variance-totale">Preuve du Théorème de
la Variance Totale</h2>
<p>Pour prouver le théorème de la variance totale, nous devons montrer
que la somme des valeurs propres de <span
class="math inline">\(\Sigma\)</span> est égale à la variance totale des
données. Considérons la trace de <span
class="math inline">\(\Sigma\)</span>, qui est définie comme la somme
des éléments diagonaux de <span class="math inline">\(\Sigma\)</span>.
Nous avons :</p>
<p><span class="math display">\[\text{tr}(\Sigma) = \sum_{i=1}^p
\Sigma_{ii}\]</span></p>
<p>Puisque <span class="math inline">\(\Sigma\)</span> est symétrique,
ses valeurs propres sont réelles et positives. De plus, la trace d’une
matrice est égale à la somme de ses valeurs propres. Par conséquent,
nous avons :</p>
<p><span class="math display">\[\text{tr}(\Sigma) = \sum_{i=1}^p
\lambda_i\]</span></p>
<p>La trace de <span class="math inline">\(\Sigma\)</span> est également
égale à la variance totale des données, car chaque élément diagonal
<span class="math inline">\(\Sigma_{ii}\)</span> représente la variance
de la <span class="math inline">\(i\)</span>-ème variable. Ainsi, nous
avons :</p>
<p><span class="math display">\[\text{Var}_{\text{totale}} =
\sum_{i=1}^p \lambda_i\]</span></p>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<h2 id="propriété-de-lorthogonalité">Propriété de l’Orthogonalité</h2>
<p>Les composantes principales sont orthogonales entre elles. Plus
précisément, si <span class="math inline">\(v_i\)</span> et <span
class="math inline">\(v_j\)</span> sont deux vecteurs propres distincts
de <span class="math inline">\(\Sigma\)</span>, alors nous avons :</p>
<p><span class="math display">\[v_i^T v_j = 0 \quad \text{pour} \quad i
\neq j\]</span></p>
<p>Cette propriété découle du fait que <span
class="math inline">\(\Sigma\)</span> est symétrique, et donc ses
vecteurs propres sont orthogonaux.</p>
<h2 id="corollaire-de-la-diminution-de-dimension">Corollaire de la
Diminution de Dimension</h2>
<p>En projetant les données sur un sous-espace de dimension <span
class="math inline">\(k\)</span> formé par les <span
class="math inline">\(k\)</span> premières composantes principales, nous
pouvons réduire la dimension des données tout en préservant une grande
partie de leur variance. Plus précisément, si <span
class="math inline">\(V_k\)</span> est la matrice formée par les <span
class="math inline">\(k\)</span> premiers vecteurs propres de <span
class="math inline">\(\Sigma\)</span>, alors la variance préservée est
donnée par :</p>
<p><span class="math display">\[\text{Var}_{\text{préservée}} =
\sum_{i=1}^k \lambda_i\]</span></p>
<p>où <span class="math inline">\(\lambda_1, \lambda_2, \ldots,
\lambda_k\)</span> sont les <span class="math inline">\(k\)</span> plus
grandes valeurs propres de <span
class="math inline">\(\Sigma\)</span>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>L’Analyse en Composantes Principales est une technique puissante pour
la réduction de dimension et l’exploration des données. En diagonalisant
la matrice de covariance, nous pouvons identifier les directions
principales de variance et projeter les données sur un sous-espace de
dimension inférieure. Les propriétés mathématiques de la PCA, telles que
l’orthogonalité des composantes principales et la préservation de la
variance, en font un outil indispensable dans de nombreuses applications
scientifiques et industrielles.</p>
</body>
</html>
{% include "footer.html" %}


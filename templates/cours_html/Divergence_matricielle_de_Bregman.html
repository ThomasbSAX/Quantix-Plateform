{% include "header.html" %}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">

  <!-- CSS du site -->
  <link rel="stylesheet" href="{{ url_for('static', filename='css/base.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/layout.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/navbar.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/footer.css') }}">
  <link rel="stylesheet" href="{{ url_for('static', filename='css/components.css') }}">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Quantix" />
  <meta name="dcterms.date" content="2026-01-11" />
  <title>Divergence matricielle de Bregman : Théorie et Applications</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/cours.css') }}">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Divergence matricielle de Bregman : Théorie et
Applications</h1>
<p class="author">Quantix</p>
<p class="date">2026-01-11</p>
</header>
<h1 id="introduction-et-motivations">Introduction et Motivations</h1>
<p>La divergence matricielle de Bregman émerge comme un outil
fondamental dans l’analyse des matrices et des données structurées.
Inspirée par les travaux de L. Bregman sur la régularisation des
problèmes d’optimisation, cette notion généralise les concepts
classiques de divergence à l’espace des matrices. Son importance réside
dans sa capacité à capturer les structures sous-jacentes des données
matricielles, tout en offrant des propriétés mathématiques robustes pour
l’analyse et la modélisation.</p>
<p>Historiquement, les divergences de Bregman ont été introduites pour
mesurer la distance entre des points dans un espace convexe. Leur
extension aux matrices permet de traiter des problèmes complexes en
apprentissage automatique, en traitement du signal et en optimisation.
La divergence matricielle de Bregman est indispensable pour résoudre des
problèmes où les données sont naturellement représentées par des
matrices, comme dans l’analyse des réseaux ou la recommandation de
systèmes.</p>
<h1 id="définitions">Définitions</h1>
<p>Pour introduire la divergence matricielle de Bregman, considérons
d’abord une fonction convexe <span class="math inline">\(\phi :
\mathbb{R}^{n \times n} \rightarrow \mathbb{R}\)</span> définie sur
l’espace des matrices <span class="math inline">\(n \times n\)</span>.
Nous cherchons une mesure de la distance entre deux matrices <span
class="math inline">\(X\)</span> et <span
class="math inline">\(Y\)</span> qui capture la différence locale en
utilisant le gradient de <span class="math inline">\(\phi\)</span>.</p>
<div class="definition">
<p>Soit <span class="math inline">\(\phi : \mathbb{R}^{n \times n}
\rightarrow \mathbb{R}\)</span> une fonction convexe et différentiable.
La divergence matricielle de Bregman associée à <span
class="math inline">\(\phi\)</span> est définie pour toutes matrices
<span class="math inline">\(X, Y \in \mathbb{R}^{n \times n}\)</span>
par : <span class="math display">\[D_{\phi}(X \| Y) = \phi(X) - \phi(Y)
- \langle \nabla \phi(Y), X - Y \rangle\]</span> où <span
class="math inline">\(\nabla \phi(Y)\)</span> est le gradient de <span
class="math inline">\(\phi\)</span> en <span
class="math inline">\(Y\)</span>, et <span class="math inline">\(\langle
\cdot, \cdot \rangle\)</span> désigne le produit de trace.</p>
</div>
<p>Cette définition peut être reformulée en utilisant les propriétés du
gradient : <span class="math display">\[D_{\phi}(X \| Y) = \phi(X) -
\phi(Y) - \text{tr}((X - Y)^T \nabla \phi(Y))\]</span></p>
<h1 id="théorèmes">Théorèmes</h1>
<p>Un théorème fondamental concernant la divergence matricielle de
Bregman est celui de la non-négativité, qui généralise une propriété
bien connue des divergences de Bregman classiques.</p>
<div class="theorem">
<p>Soit <span class="math inline">\(\phi : \mathbb{R}^{n \times n}
\rightarrow \mathbb{R}\)</span> une fonction convexe et différentiable.
Alors, pour toutes matrices <span class="math inline">\(X, Y \in
\mathbb{R}^{n \times n}\)</span>, la divergence matricielle de Bregman
satisfait : <span class="math display">\[D_{\phi}(X \| Y) \geq
0\]</span> De plus, <span class="math inline">\(D_{\phi}(X \| Y) =
0\)</span> si et seulement si <span class="math inline">\(X =
Y\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> La preuve repose sur la convexité de <span
class="math inline">\(\phi\)</span>. Par définition, une fonction
convexe <span class="math inline">\(\phi\)</span> satisfait : <span
class="math display">\[\phi(X) \geq \phi(Y) + \langle \nabla \phi(Y), X
- Y \rangle\]</span> Ce qui implique directement que <span
class="math inline">\(D_{\phi}(X \| Y) \geq 0\)</span>. L’égalité <span
class="math inline">\(D_{\phi}(X \| Y) = 0\)</span> se produit si et
seulement si <span class="math inline">\(X\)</span> est un point
critique de <span class="math inline">\(\phi\)</span> en <span
class="math inline">\(Y\)</span>, ce qui, par convexité stricte,
implique <span class="math inline">\(X = Y\)</span>. ◻</p>
</div>
<h1 id="preuves">Preuves</h1>
<p>Pour illustrer la puissance de la divergence matricielle de Bregman,
considérons un exemple simple où <span
class="math inline">\(\phi\)</span> est la fonction trace de l’entropie
matricielle. La fonction entropie matricielle <span
class="math inline">\(\phi(M) = -\text{tr}(M \log M)\)</span> est
convexe et différentiable sur l’ensemble des matrices positives
définies.</p>
<div class="lemma">
<p>Le gradient de la fonction entropie matricielle <span
class="math inline">\(\phi(M) = -\text{tr}(M \log M)\)</span> est donné
par : <span class="math display">\[\nabla \phi(M) = -(\log M +
I)\]</span> où <span class="math inline">\(I\)</span> est la matrice
identité.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Le gradient peut être dérivé en utilisant les
propriétés du logarithme matriciel et de la trace. Pour toute matrice
<span class="math inline">\(H\)</span>, nous avons : <span
class="math display">\[\langle \nabla \phi(M), H \rangle =
\lim_{\epsilon \to 0} \frac{\phi(M + \epsilon H) -
\phi(M)}{\epsilon}\]</span> En développant <span
class="math inline">\(\phi(M + \epsilon H)\)</span>, nous obtenons :
<span class="math display">\[\phi(M + \epsilon H) = -\text{tr}((M +
\epsilon H)(\log(M + \epsilon H)))\]</span> En utilisant un
développement de Taylor du logarithme matriciel, nous trouvons que le
gradient est bien <span class="math inline">\(\nabla \phi(M) = -(\log M
+ I)\)</span>. ◻</p>
</div>
<h1 id="propriétés-et-corollaires">Propriétés et Corollaires</h1>
<p>La divergence matricielle de Bregman possède plusieurs propriétés
intéressantes, que nous énumérons ci-dessous :</p>
<ol>
<li><p><strong>Invariance par translation</strong> : Pour toute matrice
<span class="math inline">\(A\)</span>, nous avons : <span
class="math display">\[D_{\phi}(X + A \| Y + A) = D_{\phi}(X \|
Y)\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cette propriété découle directement de la définition
de la divergence matricielle de Bregman. En effet, en posant <span
class="math inline">\(X&#39; = X + A\)</span> et <span
class="math inline">\(Y&#39; = Y + A\)</span>, nous avons : <span
class="math display">\[D_{\phi}(X&#39; \| Y&#39;) = \phi(X + A) - \phi(Y
+ A) - \langle \nabla \phi(Y + A), X - Y \rangle\]</span> Puisque <span
class="math inline">\(\phi\)</span> est convexe et différentiable, le
gradient satisfait <span class="math inline">\(\nabla \phi(Y + A) =
\nabla \phi(Y)\)</span>, ce qui implique : <span
class="math display">\[D_{\phi}(X&#39; \| Y&#39;) = D_{\phi}(X \|
Y)\]</span> ◻</p>
</div></li>
<li><p><strong>Homogénéité</strong> : Pour tout scalaire <span
class="math inline">\(\alpha &gt; 0\)</span>, nous avons : <span
class="math display">\[D_{\phi}(\alpha X \| \alpha Y) = \alpha^2
D_{\phi}(X \| Y)\]</span></p>
<div class="proof">
<p><em>Proof.</em> En utilisant la définition de la divergence
matricielle de Bregman, nous avons : <span
class="math display">\[D_{\phi}(\alpha X \| \alpha Y) = \phi(\alpha X) -
\phi(\alpha Y) - \langle \nabla \phi(\alpha Y), \alpha X - \alpha Y
\rangle\]</span> En supposant que <span
class="math inline">\(\phi\)</span> est homogène de degré 1, nous avons
<span class="math inline">\(\phi(\alpha M) = \alpha \phi(M)\)</span>, et
le gradient satisfait <span class="math inline">\(\nabla \phi(\alpha M)
= \alpha \nabla \phi(M)\)</span>. Ainsi : <span
class="math display">\[D_{\phi}(\alpha X \| \alpha Y) = \alpha^2
(\phi(X) - \phi(Y) - \langle \nabla \phi(Y), X - Y \rangle) = \alpha^2
D_{\phi}(X \| Y)\]</span> ◻</p>
</div></li>
<li><p><strong>Inégalité de Pythagore</strong> : Pour toute matrice
<span class="math inline">\(X\)</span>, nous avons : <span
class="math display">\[D_{\phi}(X \| Y) + D_{\phi}(Y \| X) \geq
0\]</span></p>
<div class="proof">
<p><em>Proof.</em> Cette inégalité découle de la non-négativité de la
divergence matricielle de Bregman. En effet, en utilisant la définition,
nous avons : <span class="math display">\[D_{\phi}(X \| Y) + D_{\phi}(Y
\| X) = \phi(X) - \phi(Y) - \langle \nabla \phi(Y), X - Y \rangle +
\phi(Y) - \phi(X) - \langle \nabla \phi(X), Y - X \rangle\]</span>
Simplifiant, nous obtenons : <span class="math display">\[D_{\phi}(X \|
Y) + D_{\phi}(Y \| X) = \langle \nabla \phi(X) - \nabla \phi(Y), X - Y
\rangle\]</span> Par convexité de <span
class="math inline">\(\phi\)</span>, le gradient est monotone, ce qui
implique que <span class="math inline">\(\langle \nabla \phi(X) - \nabla
\phi(Y), X - Y \rangle \geq 0\)</span>. ◻</p>
</div></li>
</ol>
<h1 id="conclusion">Conclusion</h1>
<p>La divergence matricielle de Bregman est un outil puissant pour
l’analyse des matrices et des données structurées. Ses propriétés
mathématiques robustes en font un candidat idéal pour résoudre des
problèmes complexes en apprentissage automatique et en optimisation. Les
théorèmes et propriétés présentés dans cet article montrent la richesse
de cette notion et ouvrent la voie à de nombreuses applications
futures.</p>
</body>
</html>
{% include "footer.html" %}


Out of One, Many:
Using Language Models to Simulate Human Samples
LisaP.Argyle1,EthanC.Busby1,NancyFulda2,JoshuaGubler1,ChristopherRytting2,and
DavidWingate2
1DepartmentofPoliticalScience,BrighamYoungUniversity
2DepartmentofComputerScience,BrighamYoungUniversity
September16,2022
Abstract
Weproposeandexplorethepossibilitythatlanguagemodelscanbestudiedaseffectiveproxiesfor
specifichumansub-populationsinsocialscienceresearch.Practicalandresearchapplicationsofartificial
intelligencetoolshavesometimesbeenlimitedbyproblematicbiases(suchasracismorsexism),which
are often treated as uniform properties of the models. We show that the “algorithmic bias” within one
such tool– the GPT-3 language model– is instead both fine-grained and demographically correlated,
meaningthatproperconditioningwillcauseittoaccuratelyemulateresponsedistributionsfromawide
varietyofhumansubgroups. WetermthispropertyalgorithmicfidelityandexploreitsextentinGPT-3.
Wecreate“siliconsamples”byconditioningthemodelonthousandsofsocio-demographicbackstories
fromrealhumanparticipantsinmultiplelargesurveysconductedintheUnitedStates. Wethencompare
thesiliconandhumansamplestodemonstratethattheinformationcontainedinGPT-3goesfarbeyond
surfacesimilarity.Itisnuanced,multifaceted,andreflectsthecomplexinterplaybetweenideas,attitudes,
and socio-cultural context that characterize human attitudes. We suggest that language models with
sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of
humansandsocietyacrossavarietyofdisciplines.
Contents
1 Introduction 2
2 TheGPT-3LanguageModel 3
3 AlgorithmicFidelity 4
4 SiliconSampling: CorrectingSkewedMarginals 5
5 Study1: Free-formPartisanText 6
6 Study2: VotePrediction 10
7 Study3: Closed-endedQuestionsandComplexCorrelationsinHumanData 12
8 Wheredowegofromhere? 14
1
2202
peS
41
]GL.sc[
1v99860.9022:viXra
9 Discussion 15
A GeneraldetailsonGPT-3usage 16
B DetailsonStudy1 17
B.1 DetailsonHumanandGPT-3samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 Lucidsurveydesign . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.3 Lucidresultsanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C DetailsonStudy2 30
C.1 Datageneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.2 Dataanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.3 Ablationanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.4 Modelcomparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D DetailsonStudy3 36
D.1 Datageneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D.2 Dataanalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.2.1 MissingData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.2.2 DescriptiveStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3 AlternativeSpecifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3.1 CompletelySyntheticData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3.2 GPT-3TemperatureVariation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
E CostAnalysis 50
1 Introduction
Recent years have witnessed a marked increase in the use of machine learning tools to advance social
science research [1–6]. However, little attention has yet been paid to the possible applications of large-
scalegenerativelanguagemodelslikeGPT-2[7],T5[8],orGPT-3[9]toadvancingscientificunderstanding
of human social and political behavior. These models are complex conditional distributions over natural
language that are used to generate synthetic text. When trained at scale, they exhibit a remarkable ability
tocapturepatternsofgrammar,culturalknowledge,andconversationalrhythmspresentinnaturallanguage
[10–12],andhavebecomesoconvincingthatthetextstheygeneratearelargelyindistinguishablefromthose
generated by humans [9]. We propose that these models can be used as surrogates for human respondents
inavarietyofsocialsciencetasks.
Our argument begins with a different take on a commonly-recognized problem with artificial intelligence
tools: their penchant for replicating the racial, gender, economic, and other biases of their creators. Most
discussionsofthis“algorithmicbias”treatitasasingular,macro-levelfeatureofthemodel,andseekways
tomitigatenegativeeffects[13–15]. Wesuggestitisbetterunderstoodasacomplexreflectionofthemany
various patterns of association between ideas, attitudes, and contexts present among humans. Our studies
show that the same language model, when properly conditioned, is able to produce outputs biased both
towardand againstspecificgroupsandperspectivesinwaysthatstronglycorrespondwithhumanresponse
patternsalongfine-graineddemographicaxes. Inotherwords,theselanguagemodelsdonotcontainjustone
bias, but many. This means that by conditioning a model on simulated “individuals” with targeted identity
andpersonalityprofiles,itispossibletoselectfromamongadiverseandfrequentlydisjointsetofresponse
2
distributions within the model, each closely aligned with a real human sub-population. We call the degree
towhichamodelcanaccuratelyreflectthesedistributionsitsdegreeofalgorithmicfidelity.
Highalgorithmicfidelityinlanguagemodelsiscrucialfortheiruseinsocialscienceasitenablesresearchers
to extract information from a single language model that provides insight into the different patterns of
attitudes and ideas present across many groups (women, men, White people, people of color, millennials,
baby boomers, etc.) and also the combination and intersection of these groups (Black immigrants, female
Republicans,Whitemales,etc.). Asyet,however,theextentofalgorithmicfidelityinlarge-scalelanguage
modelsisunexplored. Inthreestudies,weprovideevidencethattheGPT-3languagemodel[9]satisfieswhat
wearguearethefouressentialcriteriaofalgorithmicfidelity. WeobtainthisevidencebyconditioningGPT-
3onthousandsofsocio-demographicbackstoriesfromrealhumanparticipantsinmultiplelargesurveysin
theUnitedStates: the2012,2016,and2020wavesoftheAmericanNationalElectionStudies(ANES)[16],
andRothschildetal.’s“PigeonholingPartisans”data[17]. Weconditionthemodeltogenerateone“silicon
subject” for each human study participant, and then ask these simulated subjects to complete the same
tasks that were required of human participants. To assess algorithmic fidelity, we explore the degree to
whichthecomplexpatternsofrelationshipsbetweenideas,attitudes,andcontextswithinoursiliconsubjects
accuratelymirrorthosewithinthehumanpopulations. Theresultsfromourtestsprovidethefirstextensive,
systematic exploration of the degree of algorithmic fidelity in a large-scale language model, laying the
groundworkfortheuseofthesemodelsbroadlyinsocialscience.
Thesestudiesalsoprovideinitialexamplesofjustafewofthemyriadpotentialwayslanguagemodelscan
be used in social science research once algorithmic fidelity in a given domain is established. In Study 1,
weaskourGPT-3surrogatestolistwordsdescribingoutgrouppartisansandshowhowcloselythesewords
mirror those listed by their human counterparts. In Studies 2 and 3, we explore the relationships between
variousdemographics,attitudes,andreportedbehaviors;ourresultsshowthesamepatternsofrelationships
among GPT-3 surrogates and their human counterparts. For all three of these studies, we explain how a
researchermightuseonlytheinformationfromGPT-3tomoreeffectivelystudyhumanpopulations. These
resultssuggestthatintherealmofU.S.politics,researcherscanconfidentlyuseaGPT-3“siliconsample”to
explorehypothesespriortocostlydeploymentwithhumansubjects. GPT-3canthusbeusedbothintheory
generationandtesting.
Thispapermakesfiveimportantcontributions: (1)itconceptualizesalgorithmicfidelityandidentifiesfour
criteria to assess it; (2) it introduces silicon sampling, a methodology by which a language model can
generate a virtual population of respondents, correcting skewed marginal statistics in the training data; (3)
itintroducesanovelapproachtoconditioningonfirst-persondemographicbackstoriestosimulatetargeted
humansurveyresponses;(4)itpresentscompellingevidencefortheexistenceofalgorithmicfidelityinthe
GPT-3 language model in the domain of U.S. politics and public opinion; and (5) it provides examples of
howthemodelcanbeusedforsocialscienceresearchinthisdomain.
2 The GPT-3 Language Model
TheGPT-3languagemodelholdsparticularpromiseasasocialsciencetool. ReleasedbyOpenAIin2020,
GPT-3 has 175 billion parameters and was trained on more than 45 terabytes of text, making it one of
the largest generative language models ever created. Texts generated by GPT-3 are strikingly difficult to
distinguishfromauthentichumancompositions.
Formally, language models like GPT-3 are a conditional probability distribution p(x |x ,···,x ) over
n 1 n−1
tokens, where each x comes from a fixed vocabulary. By iteratively sampling from this distribution, a
i
3
language model can generate arbitrarily long sequences of text. However, before it can generate text, a
language model like GPT-3 requires “conditioning,” meaning that it must be provided with initial input
tokenscomprising{x ,...,x }. Werefertothisconditioningtextasthemodel’scontext.
1 n−1
Conditioningalanguagemodelondifferentcontextsreducestheprobabilityofsomeoutputsandincreases
theprobabilityofothers. Forexample,giventhecontext{x ,x ,x } =“Canyoucome”,alanguagemodel
1 2 3
might assign high probability to x =“home”, and low probability to x =“bananas”, but changing a single
4 4
word in the context to {x ,x ,x } = “Can you eat” reverses that. At each generative step, the model
1 2 3
estimates a probability distribution corresponding to the likelihood that any given token in the vocabulary
would have been the next observed x if the model were reading a pre-written text. Using a distribution
i
function,itselectsoneofthemostprobablecandidates,thenewx isappendedtotheconditioningcontext,
i
andtheentireprocessrepeats. Thiscontinuesuntilapre-specifiednumberoftokenshasbeengenerated,or
untilanexternalfactorcausestheprocesstostop. BecauseGPT-3selectsoutputtokensprobabilistically,it
cangeneratemanypossiblecontinuationsforagivencontext.
3 Algorithmic Fidelity
We define algorithmic fidelity as the degree to which the complex patterns of relationships between ideas,
attitudes, and socio-cultural contexts within a model accurately mirror those within a range of human sub-
populations. The core assumption of algorithmic fidelity is that texts generated by the model are selected
not from a single overarching probability distribution, but from a combination of many distributions, and
that structured curation of the conditioning context can induce the model to produce outputs that correlate
withtheattitudes,opinions,experiencesofdistincthumansub-populations.
Thisdoesnotimplythatthemodelcansimulateaspecificindividualorthateverygeneratedresponsewillbe
coherent. Manyoftheknownshortcomingsandinaccuraciesoflargelanguagemodelsstillapply[18,19].
However, by selecting a conditioning context that evokes the shared socio-cultural experience of a specific
demographicgroup,wefindthatitispossibletoproduceresponsedistributionsthatstronglycorrelatewith
thedistributionofhumanresponsestosurveyquestionsfromthatdemographic.
Ourconceptionofalgorithmicfidelitygoesbeyondpriorobservationsthatlanguagemodelsreflecthuman-
likebiasespresentinthetextcorporausedtocreatethem[13–15,20,21]. Instead,itsuggeststhatthehigh-
level,human-likeoutputoflanguagemodelsstemsfromhuman-likeunderlyingconceptassociations. This
meansthatgivenbasichumandemographicbackgroundinformation,themodelexhibitsunderlyingpatterns
betweenconcepts,ideas,andattitudesthatmirrorthoserecordedfromhumanswithmatchingbackgrounds.
To use terms common to social science research, algorithmic fidelity helps to establish the generalizability
oflanguagemodels,orthedegreetowhichwecanapplywhatwelearnfromlanguagemodelstotheworld
beyondthosemodels.
Howmuchalgorithmicfidelityinalanguagemodelisenoughforsocialscienceuse? Wesuggestatamini-
mumalanguagemodelmustproviderepeated,consistentevidenceofmeetingthefollowingfourcriteria:
Criterion1 (Social Science Turing Test) Generated responses are indistinguishable from parallel human
texts.
Criterion2 (BackwardContinuity)Generatedresponsesareconsistentwiththeattitudesandsocio-demographic
information of its input/“conditioning context,” such that humans viewing the responses can infer key ele-
mentsofthatinput.
4
Criterion3 (Forward Continuity) Generated responses proceed naturally from the conditioning context
provided,reliablyreflectingtheform,tone,andcontentofthecontext.
Criterion4 (Pattern Correspondence) Generated responses reflect underlying patterns of relationships
betweenideas,demographics,andbehaviorthatwouldbeobservedincomparablehuman-produceddata.
Thesecriteriarepresentfourqualitativelydifferentdimensionsonwhichamodelmusthavefidelitytohuman
responses if researchers are to have confidence in the ability of the model to generate reliable surrogate
human responses. A lack of fidelity in any one of these four areas decreases confidence in its usability;
a lack of fidelity in more than one decreases confidence further. We do not propose specific metrics or
numerical thresholds to quantify meeting or missing these criteria, as the appropriate statistics will depend
onvaryingdatastructuresanddisciplinarystandards. Instead,wesuggestthebestmetricisrepeatedsupport
foreachcriteriaacrossmultipledatasources,differentmeasures,andacrossmanygroups.
In the following studies, we take this approach in examining support for these criteria within GPT-3 in
the domain of U.S. politics and public opinion. Decades of research in political science provide a robust
literatureidentifyingexpectedrelationshippatternsbetweenpoliticalideas,concepts,andattitudes[22–27].
Weleveragetheseasabasisforcomparison.
4 Silicon Sampling: Correcting Skewed Marginals
Applyinglanguagemodelstosocialscienceresearchraisesanobviousquestion: howcanwecompensatefor
thefactthatthedemographicsofinternetusers(onwhichthemodelwastrained)areneitherrepresentativeof
mostpopulationsofinterestnordemographicallybalanced,andthatlanguagemodelsaretrainedoninternet
snapshotsacquiredatafixedpointintime?
Weproposeageneralmethodology,whichwetermsiliconsampling,thatcorrectsskewedmarginalstatistics
ofalanguagemodel. Toseewhatneedscorrecting, imaginetryingtouseGPT-3toassessmarginalproba-
bilitiesofvotingpatternsP(V). GPT-3modelsbothvotingpatternsV anddemographicsB jointlyas
GPT3
P(V,B )=P(V|B )P(B ).
GPT3 GPT3 GPT3
However, the distribution of backstories P(B ) does not match the distribution P(B ) in the pop-
GPT3 True
ulations of interest to most social scientists (say, among all voting-eligible citizens); without correction,
(cid:82)
conclusionsaboutmarginalvotingpatternsP(V)= P(V,B )willbeskewedbythisdifference. To
B GPT3
overcomethis,weleveragetheconditionalnatureoflanguagemodelsandsamplebackstoriesfromaknown,
nationally representative sample (for example, the ANES) and then estimate P(V) based on those ANES-
sampled backstories. This allows us to compute P(V|B )P(B ). As long as GPT-3 models the
ANES ANES
conditionaldistributionP(V|B)well,wecanexplorepatternsinanydesignatedpopulation.
The conditional nature of GPT-3’s text completions creates a situation analogous to Simpson’s Paradox
[28],inwhichthetrendsevidentinacombinedgroupdonotreflectthetrendsofitscompositedistributions.
Specifically, our silicon sampling method allows us to examine the distinct opinions, biases, and voting
patterns of identified sub-populations, which can differ drastically from the patterns exhibited by generic
(i.e. notdemographicallyconditioned)GPT-3textcompletions. Ofcourse,theabilitytosamplefromGPT-
3’scomponenttextdistributionsdoesnot,inandofitself,guaranteethatthesedistributionsfaithfullyreflect
the behavior of specific human sub-populations. For that, one must first examine the model’s algorithmic
fidelitywithrespecttoboththedomainofstudyandthedemographicgroupsofinterest.
5
Describing Democrats Describing Republicans
Ideologically, I describe myself as conservative. Politically, I am a strong Republi- Ideologically, I describe myself as conservative. Politically, I am a strong
can. Racially, I am white. I am male. Financially, I am upper-class. In terms of my Republican. Racially, I am white. I am male. When I am asked to write down four
age, I am young. When I am asked to write down four words that typically describe words that typically describe people who support the Republican Party, I respond
people who support the Democratic Party, I respond with: 1. Liberal 2. Socialist with: 1. Conservative 2. Male 3. White (or Caucasian) 4. Christian.
3. Communist 4. Atheist.
Ideologically, I describe myself as liberal. Politically, I am a strong Democrat. Ideologically, I describe myself as extremely liberal. Politically, I am a strong
Racially, I am white. I am female. Financially, I am poor. In terms of my age, I am Democrat. Racially, I am hispanic. I am male. Financially, I am upper-class. In
old. When I am asked to write down four words that typically describe people who terms of my age, I am middle-aged. When I am asked to write down four words
support the Democratic Party, I respond with: 1. Liberal. 2. Young. 3. Female. 4. that typically describe people who support the Republican Party, I respond with:
Poor. 1. Ignorant 2. Racist 3. Misogynist 4. Homophobic.
gnortS
gnortS
snacilbupeR
starcomeD
Figure1: Examplecontextsandcompletionsfromfoursilicon“individuals”analyzedinStudy1. Plaintext
indicates the conditioning context; underlined words show demographics we dynamically inserted into the
template;bluewordsarethefourharvestedwords.
5 Study 1: Free-form Partisan Text
Our first examination of algorithmic fidelity in GPT-3 involves a silicon replication of Rothschild et al.’s
“Pigeonholing Partisans” data [17]. This survey asked respondents to list four words to describe both Re-
publicansandDemocrats. Rothschildetal.findthatpeopletalkaboutpartisansindifferentways,focusing
ontraits,politicalissues,socialgroups,oracombinationofallthree. Further,peopleoftentalkabouttheir
ownpoliticalpartyinmorepositivewaysthantheotherparty,inlinewithotherresearch[29,30]. Inthisfirst
test, we ask whether GPT-3 can produce texts about partisans that are, in the aggregate, indistinguishable
fromwordsgeneratedbyhumans.
Toaccomplishthis,weleverageoursiliconsamplingtechnique,generatingasyntheticdatasetbyconstruct-
ingafirst-personbackstoryforeachhumansubjectinthePigeonholingPartisanssurvey,asshowninFigure
1. Using these texts, we ask GPT-3 to sample new words. Because of the setup of our conditioning text,
GPT-3 almost always responds with neatly delineated sets of four words, although – much like humans
– it occasionally responds with long phrases, mini-essays or nothing at all. Post-processing with regular
expressionsextractedthefinalsetoffourwordsfromeachsample.
For GPT-3 to generate 4-word lists that mirror human texts demands significant algorithmic fidelity, for it
requireslistingwords thatintoneandcontent mirrorthoselistedbyhumans with aparticularbackground.
Figure 2 compares the most frequent words used to describe Democrats and Republicans in our data, by
data source (GPT-3 or human) and source ideology. Bubble size represents relative frequency of word
occurrence; columns represent the ideology of the list writers. Qualitatively, both the human and GPT-3
listslookinitiallyaspoliticalscientistsmightexpect. Forexample,bothGPT-3andhumansuseacommon
setofwordstodescribeDemocrats,andrarelyusethosewordstodescribeRepublicans.
Toformallyanalyzethisdata,wehired2873individualsthroughthesurveyplatformLucid[31]toevaluate
the 7675 texts produced by human and GPT-3 survey respondents, without any indication of which was
which. Each individual evaluated 8 randomly assigned lists, with each text evaluated by three different
individuals.
We presented these evaluators with the 4-word lists after the following preface: “Consider the following
description of [Republicans/Democrats]:”. We then asked them to respond to six prompts. First, we asked
them to guess the partisanship of the list writer (Republican, Democrat, or Independent). We then asked
6
Describing Democrats
Humans
sdrow
tneuqerf
tsoM
sdrow
tneuqerf
tsoM
starcomeD
ebircsed
ot
desu
snacilbupeR
ebircsed
ot
desu
GPT-3
C
ylemertxE
tnednepednI L
ylemertxE
Describing Republicans
GPT-3 Humans
liberal
people
young
progressive
poor
open-minded
female
educated
intelligent
liberals
pro-choice
compassionate
class
social
democratic
party
caring
more
conservative
white
religious
racist
rich
old
christian
male
wealthy
homophobic
people
patriotic
ignorant
older
traditional
selfish
republican
... ...
C
ylemertxE
tnednepednI L
ylemertxE
... ...
C
ylemertxE
tnednepednI L
ylemertxE
... ...
C
ylemertxE
tnednepednI L
ylemertxE
... ...
Figure2: TheoriginalPigeonholingPartisansdatasetandthecorrespondingGPT-3generatedwords. Bubble
sizerepresentsrelativefrequencyofwordoccurrence;columnsrepresenttheideologyoflistwriters. GPT-3
usesasimilarsetofwordstohumans.
them to rate the list on 5 dimensions: (1) positive or negative tone, (2) overall extremity, and whether the
text mentioned (3) traits, (4) policy issues, or (5) social groups. Participants then sequentially viewed 8
additional randomly selected lists, were told that some of these lists were generated by a computer model,
7
and were asked to guess whether each list was generated by a human or a computer. Extensive details on
thelists,theirwriters,studyparticipants,andtheinstructionscanbefoundintheappendix.
Using this design, we explore two social science variations of a Turing Test: (1) whether our human eval-
uators recognize the difference between human and GPT-3-generated lists, and (2) whether the humans
perceivethecontentoflistsfrombothsourcesassimilar. ThesetestsspeaktoCriterion1(Turingtest)and
Criterion2(BackwardContinuity).
Wefindevidenceinfavorofbothcriteria: participantsguessed61.7%ofhuman-generatedlistswerehuman-
generated,whileguessingthesameof61.2%ofGPT-3lists(two-taileddifferencep=0.44). Althoughasking
participantstojudgeifalistishuman-orcomputer-generatedleadsthemtoguessthatsomelistsdonotcome
from humans (nearly 40 percent of both kinds of lists fell in this category), this tendency does not vary by
thesourceofthelist.
This is particularly interesting given the results of our second exploration: whether participants noted any
differences in list characteristics between human and GPT-3-generated lists. To identify these differences,
weestimateregressionmodelsusingordinaryleastsquares,regressingeachofthe5characteristicsbywhich
lists were evaluated (positivity, extremity, and mentions of traits, issues, and groups) on a dichotomous
source variable (0 = human, 1 = GPT-3) and a series of control variables recording the gender, ethnicity,
income,age,andpartisanidentityoftheoriginallist-writersintheRothschildetal.data. Allmodelsinclude
fixedeffectsforevaluators(aseachevaluated8lists),andclusteredstandarderrorsbyevaluatorandlist(as
eachlistwasevaluatedthreetimes).
Figure 3(B) plots the predicted percent of all lists (human and GPT-3) evaluated as having each character-
istic. The results show a remarkable degree of consistency in the evaluations of both human and GPT-3
generated lists in both content and tone. For example, human list-writers included more personality traits
(e.g. “bigoted,”“moral”) than other components (72.3% of lists). So did GPT-3 (66.5% of lists). Less than
halfofbothhumanandGPT-3generatedlistswereevaluatedasextreme(39.8%and41.0%, respectively).
This pattern of similarity holds across all 5 characteristics, with all but one characteristic hovering around
50%. The lone exception, with a substantially higher frequency in both human and GPT-3 data, is “traits.”
This matches patterns in the original analyses of the human texts [17]. That GPT-3 mirrors this exception,
and patterns in all other characteristics, is strong evidence of the depth of algorithmic fidelity it contains.
Tablesofresultsandfurthermodeldetailscanbefoundintheappendix.
Moreover,asFigure3(A)indicates,whenwedrilldowntogreaterlevelsofdetailtoexploreunderlyingpat-
ternsbehindtheseresults,wefindthatGPT-3reflectshuman-similarpatternsatthislevelaswell(Criterion
4, Pattern Correspondence). The similarity in the use of positive and extreme words by both humans and
GPT-3,brokenoutbytheideologicalsubgroupofthelistwriters,isstriking.
Wehaveshownthat(1)humanevaluatorsofthewordlistscouldnotcorrectlydistinguishbetweenhumanvs.
GPT-3-generated lists, and (2) that they evaluated the content/characteristics of these lists as quite similar.
We now assess the degree to which our participants were able to use these lists to correctly guess the true
partisanshipofthelistwriters. Toexplorethisquestion,weestimateamodelsimilartothosejustpresented,
regressingadichotomousvariableidentifyingifparticipantscorrectlyguessedthepartisanshipoflistwriters
(1=Yes;0=No)onthesourceofthelist(GPT-3vs. human)andthesamecontrols. Theleft-mostbarsof
Figure3(B)presentthepredictedpercentcorrect,bysourcetype.
Participants presented with lists of words from both sources guess the correct partisanship of their writer
significantly better than chance (33%, given respondents could guess Republican, Democrat, or Indepen-
8
stxeT
fo
noitroporP
detciderP
Describing Describing
Republicans Democrats
C
ylemertxE
tnednepednI L
ylemertxE
A
1.0
0.0
Ideology of the individual list writers
Positive Extreme Traits Issues Groups
stxeT
fo
noitroporP
detciderP
Positive Texts Extreme Texts
Describing Describing
Republicans Democrats
GPT−3
Human
... ...
B 1.0
GPT−3
Human
Chance
0.0
Correct PIDs
C
ylemertxE
tnednepednI L
ylemertxE
... ...
C
ylemertxE
tnednepednI L
ylemertxE
... ...
C
ylemertxE
tnednepednI L
ylemertxE
... ...
Figure3: AnalysisofGPT-3andhumanresponsesfromtheLucidsurvey.
dent), providingstrongadditionalevidenceinfavorofalgorithmicfidelityinGPT-3. Participantswhosaw
human-generated lists guessed successfully roughly 7.3% more often (60.1% vs. 52.8%) than those who
saw GPT-3 lists, a statistically significant difference (two-tailed p < .001). However, texts from both hu-
mans and GPT-3 both clearly contain the sentiment cues necessary to guess the partisanship of the creator
9
ofthetextsatsignificantlevels.
Results from study 1 suggest a remarkably high degree of algorithmic fidelity within GPT-3. We find
repeated,consistentsupportforCriterion1(TuringTest)andCriterion2(BackwardContinuity)fromthese
data,withsomeinitialevidenceforCriterion4(PatternCorrespondence). Inallofthesecases,weobserve
supportforthesecriteriaacrossdifferentmeasuresandfordifferentsubsetsoftheAmericanpopulation.
6 Study 2: Vote Prediction
OurnexttwostudiesdrawontheANES,apremiersourceofdatainunderstandingAmericanpublicopinion.
InStudy2,weusethe2012,2016,and2020wavesofthesurvey.
We first consider the degree to which GPT-3 silicon samples, constructed to match the demographics of
the 2012, 2016, and 2020 ANES participants, report similar distributions of vote choice as their matching
humansamples. ThisrequiresGPT-3togeneratetextfromalimiteduniverseofoptions(voteforTrumpor
Clintonin2016,forexample),butitmustdosodifferentlybasedonthehumanbackgroundsweprovideit.
ItthereforeassessesCriterion3,requiringforwardcompatibilityofGPT-3inproducingacandidate’sname
inresponsetotheconditioningtext,andCriterion4,theexpectationthattheconditioningtextswillproduce
predictable changes in the probability distribution of vote choice. To demonstrate algorithmic fidelity in
thisstudy,GPT-3mustcontainhuman-likepatternsofassociations,whereindividualsfromdifferentethnic
groups, living in different locations, of different social backgrounds vote differently in ways identified but
stilldebatedbypoliticalscientists[32–35]. Study2alsoexploresthetemporallimitsofGPT-3: thetraining
corpusforGPT-3endedin2019[9],sodatafrom2020allowsustoexplorehowthealgorithmicfidelityof
thelanguagemodelchangeswhenprobedoutsidethetimeoftheoriginaltrainingcorpus.
Drawing from research on voting in the United States, we used the following ANES variables to condition
GPT-3: (1) racial/ethnic self-identification, (2) gender, (3) age, (4) conservative-liberal ideological self-
placement,(5)partyidentification,(6)politicalinterest,(7)churchattendance,(8)iftherespondentreported
discussingpoliticswithfamilyandfriends,(9)feelingsofpatriotismassociatedwiththeAmericanflag(not
availablein2020atthetimeofanalysis),and(10)stateofresidence(notavailablein2020atthetimeofthe
analysis). WethenrecordedtheprobabilitiesfromGPT-3thatthesentence“In[year],Ivotedfor...” wouldbe
completedwiththeRepublicanorDemocraticcandidategiventheeachconditioningbackstory(seeOnline
Appendixforfurtherdetail). UsingthesevariablesasconditioningtextinGPT-3allowsustocomparehow
wellaGPT-3siliconsamplereplicatestherelationshipsbetweeneachvariableandvotechoiceinourhuman
sample. Inallthecomparisonsthatfollow,wecodevotechoiceas1whentherespondent/GPT-3indicatesa
votefortheRepublicancandidateanda0foravotefortheDemocraticcandidateinthatelection. Tomake
thepredictionsfromGPT-3matchtheobservedhumandata,wedichotomizedtheprobabilitypredictionsat
0.50,wherehighervalueswerescoredasavotefortheRepublicancandidate.
We observe a high degree of correspondence between reported two-party presidential vote choice propor-
tions from GPT-3 and ANES respondents. Averaged across the whole sample, GPT-3 reported a 0.391
probabilityofvotingforMittRomneyin2012;thesamepercentagefromtheANESwas0.404. Inthe2016
data,GPT-3estimateda0.432probabilityofvotingforTrump,andtheprobabilityfromthe2016ANESwas
0.477. In2020,theGPT-3-generatedprobabilityofvotingforTrumpwas0.472,whilethepercentagefrom
the ANES respondents was 0.412. In all three cases, we see evidence of a mild amount of overall bias in
GPT-3: GPT-3wasalittlepredisposedagainstRomneyin2012,againstTrumpin2016,andagainstBiden
in 2020. However, the substantive difference between the ANES and GPT-3 estimates is relatively small
and, inkeepingwithourlargerargumentsaboutalgorithmicfidelityandcorrectionsforskewedmarginals,
10
Variable 2012 2012 2016 2016 2020 2020
Tetra. Prop.Agree Tetra. Prop.Agree Tetra. Prop.Agree
Wholesample 0.90 0.85 0.92 0.87 0.94 0.89
Men 0.90 0.85 0.93 0.88 0.95 0.88
Women 0.91 0.86 0.92 0.86 0.94 0.90
Strongpartisans 0.99 0.97 1.00 0.97 1.00 0.97
Weakpartisans 0.73 0.74 0.71 0.74 0.84 0.82
Leaners 0.90 0.85 0.93 0.87 0.95 0.89
Independents 0.31 0.59 0.41 0.62 0.02 0.53
Conservatives 0.84 0.84 0.88 0.86 0.91 0.89
Moderates 0.65 0.77 0.76 0.78 0.71 0.77
Liberals 0.81 0.95 0.73 0.95 0.86 0.97
Whites 0.87 0.82 0.91 0.85 0.94 0.89
Blacks 0.71 0.97 0.87 0.96 0.81 0.94
Hispanics 0.86 0.86 0.93 0.90 0.88 0.83
Attendschurch 0.91 0.86 0.93 0.88 0.94 0.88
Doesn’tattendchurch 0.88 0.85 0.90 0.85 0.93 0.90
Highinterestinpolitics 0.95 0.90 0.97 0.93 0.97 0.92
Lowinterestinpolitics 0.71 0.74 0.75 0.75 0.83 0.81
Discussespolitics 0.92 0.87 0.94 0.88 0.95 0.90
Doesn’tdiscusspolitics 0.83 0.82 0.81 0.79 0.80 0.79
18to30yearsold 0.90 0.87 0.90 0.86 0.90 0.87
31to45yearsold 0.90 0.85 0.92 0.87 0.94 0.90
46to60yearsold 0.90 0.86 0.92 0.86 0.92 0.87
Over60 0.90 0.85 0.93 0.87 0.96 0.91
Table1: MeasuresofcorrelationbetweenGPT-3andANESprobabilityofvotingfortheRepublicanpresi-
dentialcandidate. Tetrareferstotetrachoriccorrelation. Prop. Agreereferstoproportionagreement. GPT-3
vote is a binary version of GPT-3’s predicted probability of voting for the Republican candidate, dividing
predictionsat0.50.
doesnotprecludestrongandconsistentcorrelationsbetweenGPT-3’ssimulatedresponsesandthereactions
ofsubgroupsintheAmericanpopulation.
To explore these correlations in detail, we turn to the statistics reported in Table 1. This table reports two
forms of correlations between the self-report of voting from the ANES and a binary version of the vote
reportfromGPT-3(othermetricssupportthesetwoandcanbefoundintheappendix). Wedichotomizethe
GPT-3 vote probability to match our human measure, a binary report of voting from the ANES. Across all
threeyearsofsurveydata,weseeremarkablecorrespondencebetweenGPT-3andhumanrespondents. The
2012tetrachoriccorrelationacrossallrespondents0.90,the2016estimatewas0.92,andthe2020valuewas
0.94. Wefindthisconsistentlyhighcorrelationremarkablegiventhedifferencesincontextacrossyears.
ThissamehighdegreeofpatterncorrespondenceoccursforvarioussubgroupsintheAmericanpopulation.
More than half of the tetrachoric correlations between the reported vote by GPT-3 and the ANES are 0.90
orhigher,andthisistrueforallthreeyears. TheproportionagreementcolumnofTable1alsoindicateshigh
levelsofrawagreementbetweenthetworeportsofvotechoicein2012,2016,and2020. Impressively,there
is only one exception to this overall pattern: the estimates of vote choice do not match well for pure inde-
pendents,especiallyin2020. However,thisistheonlydeviationfromtheoveralltrendinTable1,whereall
othermeasuresofcorrespondenceexceed0.65(andaregenerallycloserto0.8or0.9). Further,existingpo-
liticalscienceresearchsuggeststhatthisgroupofindividualsshouldbeespeciallyhardtopredictastheyare
11
themostconflictedaboutthetwo-partychoices,theleastlikelytovote,theleastpoliticallyknowledgeable,
and the least interested in politics [36–38]. Overall, then, the results in Table 1 provide strong, additional
evidence for algorithmic fidelity, with repeated, consistent support for Criteria 3 (Forward Continuity) and
4(PatternCorrespondence). Appendix3containsadditionalresults,includinganablationstudyinvestigat-
ing the effect of removing backstory elements, and a model comparison showing how alternative language
modelsperformonthistask.
TheabilityofGPT-3tocapturethevotingpreferencesofdifferentgroupsofAmericansisnotrestrictedto
onemomentintime. Moreover,resultsfromthe2020ANESdataindicatethepossibilitythatGPT-3canbe
used,withtherightconditioning,tounderstandpeopleandgroupsoutsideitsoriginaltrainingcorpus.
7 Study 3: Closed-ended Questions and Complex Correlations in Human
Data
Study 3 examines GPT-3’s ability to replicate complex patterns of association between a wide variety of
conceptual nodes. Given the complexity of this task, we conduct it just for the 2016 data from the ANES.
Building on the voting predictions in Study 2, we expand the set of information outputs we ask GPT-3 to
produce, and use the resulting data to evaluate a more complex structure of associations. This is our most
rigorousevaluationofCriterion4(PatternCorrespondence).
Thisstudyrepresentsbothatechnicalandsubstantiveresearchchallenge. Absentthenaturallyself-limiting
setoflikelyresponseswhenaskingaboutvotechoiceinaparticularelection(i.e. “DonaldTrump”vs.“Hillary
Clinton”),wedevelopamethodtoconditionGPT-3toprovidespecificresponsesfromalistofoptions. Ad-
ditionally, mirroring the widespread and varied use of survey data in social science applications, we push
beyondhigh-levelconditionalprobabilitiesandexplorewhetherGPT-3demonstratesalgorithmicfidelityin
inter-relationshipsamongavarietyofunderlyingattitudes,demographics,andreportedbehaviors.
For this task, we produce an interview-style conditioning template (see the appendix for an example). The
purpose of this approach is two-fold. First, leveraging the zero-shot learning property of language models
[9], the format induces GPT-3 to respond to survey questions using short strings of tokens drawn from
options provided by the “Interviewer.” Second, the questions incorporated in the conditioning text provide
necessary demographic and attitudinal background information to generate each distinct silicon subject.
We generate the conditioning text using responses that humans gave on the 2016 ANES to eleven survey
questions. WethenuseGPT-3topredicttheresponsetothetwelfth.
Using the ANES and silicon data, we calculate Cramer’s V for each combination of survey items in the
ANES sample (“Human”), and between the ANES conditioning values and the resulting GPT-3 produced
answer (“GPT-3”). Cramer’s V provides a simple summary measure of association that accounts for the
variation in base rates in the raw data [39]. Figure 4 displays the comparison in Cramer’s V between the
two data sources. We again find remarkably high correspondence between the patterns of associations in
human survey data and these same patterns in GPT-3 produced survey data. The mean difference between
the Cramer’s V values is -0.026. As can be seen, the Cramer’s V for GPT-3-generated responses is not
uniformly high or low, but instead mirrors stronger and weaker relationships present in the human data.
Where two concepts are not strongly associated in the human data, they likewise show little association
in the GPT-3 data. The converse is also true. And while there is variation in Figure 4 in terms of how
precisely the patterns of relationships in GPT-3 match those in the ANES, the overall pattern is a stunning
correspondencebetweenGPT-3andtheANESinthevastmajorityofcases.
12
Although we provide first-person backstories based on specific human survey profiles, we do not expect
the values in the silicon sample to exactly match the human response on the individual level. For each
text completion, the language model uses a stochastic sampling process to select the completion from the
distribution of probable next tokens. Therefore, with a large enough sample size we expect the overall
distribution of text responses in the silicon sample to match the overall distribution in the human data, but
wedonotevaluatecorrespondenceattheindividuallevel. Additionally,aswithallstochasticprocesses,we
expectsomevariationindifferentdrawsofthesiliconsample. Intheappendix,wereportonvariationinthe
patterncorrespondencebasedondifferentsamplingparametersinGPT-3.
These results again provide compelling, consistent, repeated evidence for Criterion 4 (Pattern Correspon-
dence). GPT-3 reproduces nuanced patterns of associations not limited to aggregated toplines. When pro-
vided with real survey data as inputs, GPT-3 reliably answers closed-ended survey questions in a way that
closely mirrors answers given by human respondents. The statistical similarities extend to a whole set of
inter-correlations between measures of personal behaviors, demographic characteristics, and complex atti-
tudes. Weagainseethisasstrongevidenceforalgorithmicfidelity.
13
ANES Variables
Race / Attends
Gender Age Education Patriotism
Ethnicity Church
er (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
d
n
e
G cit y (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
ni
Et h A g e (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
c e
/
o n (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
R a ati
E d u c ur c h (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
h
C m (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
Att
e n d
P
s
a
tr
P
i
o
o
l
t
i
i
ti
s c
s
s
t
(cid:31)
(cid:31)
(cid:31)
(cid:31) (cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31) (cid:31)
(cid:31)
Di P s ol c i u ti s c s 2 a
e
l
s
0 I 1 e n 6 t ol
er
V o o
e
g t D e y (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
I d y I
art
0.0 0.4 0.0 0.4 0.0 0.4 0.0 0.4 0.0 0.4 0.0 0.4
P
Discusses Political
2016 Vote Ideology Party ID
Politics Interest
er(cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
n d GPT−3
e
G cit y (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) Human
ni
Et h A g e (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
c e
/
o n (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
R a ati
E d u c ur c h (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
h
C m (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
Att
e n d
P
s
a
tr
P
i
o
o
l
t
i
i
ti
s c
s
s
t (cid:31)
(cid:31)
(cid:31)
(cid:31) (cid:31)
(cid:31)
(cid:31)
(cid:31)
Di P s ol c i u ti s c s 2 a
e
l
s
0 I 1 e n 6 t ol
er
V o o
e
g t D e y (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)
I d y I
art
00..00 0.4 0.0 0.4 0.0 0.4 0.0 0.4 0.0 0.4
P
Cramer's V
Figure4: Cramer’sVCorrelationsinANESvs. GPT-3Data
8 Where do we go from here?
Thusfar,ouremphasishasbeenondocumentingthealgorithmicfidelityofGPT-3bycomparingitsoutputs
to human data. However, our purpose is not limited to these particular human-computer comparisons; if
this were the case, the usefulness of GPT-3 would be dramatically limited. Instead, we provide evidence
thatalgorithmicfidelityisacrucialattributeoftoolslikeGPT-3becauseitdemonstratesthattheselanguage
modelscanbeusedpriortoorintheabsenceofhumandata.
14
Toillustratethis, considertheconclusionswewouldhavedrawnwithonlyourdatafromGPT-3. Thedata
fromoursiliconsampleinStudy1suggeststhat(1)peopledescribeRepublicansandDemocratswithdiffer-
enttermsthathighlightdistinctstereotypesofbothgroups; (2)theaffectivecontentandextremityofthese
texts is tied to individuals’ political beliefs and identity in systematic ways that can be used to generate
theory; (3) stereotypes of partisans contain issue, group, and trait-based content, although trait references
are most common; and (4) others can guess the partisanship of individuals based on their stereotypes of
Democrats and Republicans. All of this is evident using only the data from GPT-3. With this information,
interested researchers could design survey questions, experimental treatments, and codebooks to guide hu-
man research. Crucially, this can be done with substantially fewer resources than a parallel data collection
withhumanrespondents: Study1cost$29onGPT-3.
Thesameistrueforstudies2and3. TheablationanalysisforStudy2(containedinAppendix3)suggests
which variables researchers should include in their studies of public opinion if they want to accurately
understand Americans’ voting behavior. Study 3 could be used by social scientists to target important
connections between characteristics and views that merit further exploration. Based on the results from
GPT-3, a social scientist could design an experiment or observational study to confirm and dissect this
relationship in a rigorous and causal way. The results also indicate which variables operate as potential
confounds that should be included in pre-analysis plans for regression and other econometric models that
havecausalaspirations. Again,alloftheseinsightswouldbecleartoresearcherswithonlyaccesstoGPT-
3 and without our human baselines. These studies suggest that after establishing algorithmic fidelity in a
given model for a given topic/domain, researchers can leverage the insights gained from simulated, silicon
samplestopilotdifferentquestionwording,triagedifferenttypesofmeasures,identifykeyrelationshipsto
evaluatemoreclosely,andcomeupwithanalysisplanspriortocollectinganydatawithhumanparticipants.
9 Discussion
In this paper, we introduce the concept of algorithmic fidelity as a means to justify the use of large-scale
language models as proxies for human cognition at an aggregate level, and as general-purpose windows
into human thinking. We propose four criteria to establish the algorithmic fidelity of these models and
demonstrateempiricalmethodsfortheirevaluation.
Using these concepts and methods, we show that GPT-3, one of the largest publicly available language
models, contains a striking degree of algorithmic fidelity within the realm of public opinion in the United
States. Study1showsthatGPT-3passesasocialscienceversionoftheTuringTest(Criterion1)andexhibits
both strong Backward Continuity (Criterion 2) and Pattern Correspondence (Criterion 4). Studies 2 and 3
providecompellingevidenceofForwardContinuity(Criterion3)aswellasadditional,muchmoregranular
evidenceforPatternCorrespondence(Criterion4). AsnotedinStudies2and3,carefulconditioningofGPT-
3allowsustoaddressissuesoftemporalityandreplicability,pointsfurthersupportedinresultspresentedin
theappendix. Importantly,inallstudies,wefindevidencethatGPT-3iscapableofreplicatingtheviewpoints
of demographically varied sub-populations within the U.S. Taken together, these studies show consistent,
repeatedevidenceforthesecriteriaacrossarangeofdatasources,measures,andpointsintime.
These studies also provide examples of some of the many ways in which large scale language models like
GPT-3mightbeusedforsocialscienceresearch. Wecanenvisionmanyothers,andexpectthatthismethod
willhavestrengthsandweaknessesincomparisontotraditionalsocialsciencemethods(aswehighlightin
Appendix5,costiscertainlyastrengthofthismethod). Wenote,however,thatwhilethisworklaysexciting
groundwork for the beneficial use of these models in social science, these these tools also have dangerous
potential. Modelswithsuchfidelity,coupledwithothercomputationalandmethodologicaladvances,could
15
beusedtotargethumangroupsformisinformation,manipulation,fraud,andsoforth[9]. Weacknowledge
thesedangers,andbothjoinwithandstronglyendorsetheworkofothersinpushingforaclearstandardof
ethics for their use in research and deployment [40, 41]. We believe that transparent, research-based, and
community-accountable exploration and understanding of these tools will be essential for recognizing and
preventingabusebyprivateactorswhowillinevitablyemploythesemodelsforlessnobleends.
While the current study is restricted to a specific domain, the underlying methodology is general purpose
and calls for additional work to quantify both the extent and limitations of GPT-3’s algorithmic fidelity in
a wide array of social science fields. Such an effort goes well beyond what one research team can hope to
accomplish;weextendthisinvitationtothewiderscientificcommunity.
A General details on GPT-3 usage
ForGPT-3modelspecifics,refertoBrownet. al.’soriginalpaperfromOpenAIreferencedinthemaintext.
We use the model through their remote API. This interface accepts several inputs, including a text prompt
(e.g. “backstories”, survey questions, etc.), model specification (we use Davinci, the largest of the models
at175billionparameters,asopposedtoAda,Curie,orBabbage),andtemperature(weuse0.7),andreturns
adictionaryincludingtextcompletionandcorrespondinglog-probabilities.
Incontextswherewecareaboutmodelingprobabilities,asopposedtosimplysamplingtogeneratetext,we
useastandardlog-sum-expconversion. Insuchcontexts,weconsidercertaintokensetstobeequivalent. For
example,whenestimatingtheprobabilitythatavotercastavoteforDonaldTrumpinthe2016presidential
election, the prompt might be “In 2016, I voted for”. Each token in the set {Donald, donald, DONALD,
Trump,trump,TRUMP}hasadistinctshareofprobability,butweconsiderthemtobedifferentexpressions
of the same idea. Thus, we sum their probabilities to estimate the un-normalized total share of probability
thatDonaldTrumpisbeingreferredtofollowingtheprompt. Aftercollapsingtokensetsandsummingtheir
probabilities,wenormalizeacrosstheremainingcollapsedtokensetssuchthattheirprobabilitiessumto1.
InStudies1and3,whereweareprimarilyconcernedwithtextgeneration,weusetheCompletionAPIwith
default settings, except that temperature is set to 0.7. This encourages more deterministic answers, at the
expenseofresponsediversity. Thisparameterwasnottuned; itwouldbeinterestingtoseehowourresults
mightvaryasitchanges.
Figure 5 illustrates some conditioning contexts comprised of first-person demographic backstories paired
withmultiplesampledcontinuationsgeneratedbyGPT-3.
16
Sampled completions Context Sampled completions
Hillary Clinton. However, I’m a huge sports fan, and I’ve Donald Trump, but I didn’t vote for him because I agreed
spent my entire life following and loving the New York with him on all the issues. I voted for him because I didn’t
Yankees. I’m also a huge fan of the New York Giants. I... I am a strong conservative. I want Hillary Clinton to be president. I disagreed with many...
support traditional values. I
Bernie Sanders in the primaries. I am a member of the think we need a balanced John Kasich. I haven’t changed my mind on any of those
Democratic Socialists of America. But as a Jewish budget, limited government, issues. But Donald Trump is the opposite of what I’ve always
American, I will not be voting for the Democratic... and a strong military. I’m believed a Republican should be, and I can’t vote for him...
opposed to abortion and I’m
skeptical of global warming. In
Hillary Clinton. But after seeing the way the Democratic 2016, I voted for Donald Trump. In other words, I’m a typical Republican
establishment treated Bernie Sanders, I have lost hope in the voter, except that I’m gay. And that’s why I’m leaving the
party. In 2018, I joined the Democratic Socialists of... Republican Party. I’ve felt increasingly unwelcome in the...
Hillary Clinton, because I believed in her vision for I am a strong liberal. I support Donald Trump. And I have no regrets. But in the last two
America. I felt that she was the only candidate that could progressive values. I support years, I’ve watched conservatives convince themselves that
pull us out of the mess that Obama inherited. I am a loyal... open immigration and women’s Trump is a conservative. And this is a lie. Trump has...
right to choose. I believe in
systemic racism and that global
Bernie Sanders. I am pro-LGBTQ. But I am also pro-life. I Donald Trump. I consider myself a Republican. But what
warming is one of our biggest
am Jewish. I have never been religious. I was raised in a does that mean these days? I’m not so sure. Which brings me
challenges. In 2016, I voted for
secular household, but my Jewishness is intrinsic to me. I... to this poll, released today by CBS News and the New York...
Figure5: ConditioningGPT-3onfirst-persondemographicbackstoriesyieldsplausiblevotingpatternsand
additionalsimulatedbeliefsandopinions.
B Details on Study 1
B.1 DetailsonHumanandGPT-3samples
To generate our sample for analysis in Study 1, we began with human participants in the Pigeonholing
Partisansdatasetwhoprovidedanswerstoallofthefollowingkeydemographic/backgroundcharacteristics:
their partisan identification, ethnicity, gender, income, and age. These characteristics were identified by
the authors of the Pigeonholing Partisans paper to correlate with evaluations of partisan outgroups. 2,107
individualsansweredallquestionsandthuswereincludedinouranalysis.
We then generated a matching set of 2,107 “silicon” individuals in GPT-3 by feeding these background
characteristics, individual by individual, into a conditioning text template as illustrated in Figure 6. The
underlinedtextinthefigureindicateswherewepluggedeachcharacteristicintoGPT-3. Backstoriesalways
concatenated template fragments in the same order (ideology, 7-point partisanship, race, gender, income,
age). If any demographic variable was missing, the corresponding template fragment was omitted. The
agetemplatefragmentmapped18-24yearstothephrase“young”,25-39to“middle-aged”,40-60to“old”,
and 61+ to “very old”. The income template fragment mapped annual income less than $15k to “very
poor”, $15k-$50k to “poor”, $50k-$150k to “middle-class”, and $150k+ to “upper-class”. Other template
fragmentsareself-explanatory.
UsingtheOpenAIGPT-3API,wegenerated128tokensworthoftextfromeachsiliconrespondent. Figure
6 expands on Figure 2 in the text to illustrate what the full responses looked like, with GPT-3 generated
text listed in bold. We used regular expressions to extract the four-words at the center of our study. Light
manual post-processing was used to correct situations where the regular expressions were insufficient to
extractresponses. IfaGPT-3responselistedmorethanfourwordsorphrases,onlythefirstfourwereused.
IfaGPT-3responselistedlessthanfour,theremainingphraseswereleftblank.
Both human and GPT-3 “subjects” were asked to write two lists of words: one describing Republicans,
and one describing Democrats. If all participants fully complied, this would mean a total of 2,107 x 2 =
4,214 texts from each sample. As is common in human studies, we didn’t receive full compliance: some
participants refused to write either list, some only wrote one or the other, and some wrote paragraphs that
17
A
Ideologically, I describe myself as conservative. Ideologically, I describe myself as conservative.
Politically, I am a strong Republican. Racially, I am Politically, I am a strong Republican. Racially, I am
white. I am male. Financially, I am upper-class. In white. I am male. When I am asked to write down four
terms of my age, I am young. When I am asked to words that typically describe people who support the
write down four words that typically describe people Republican Party, I respond with: 1. Conservative 2.
who support the Democratic Party, I respond with: 1. Male 3. White (or Caucasian) 4. Christian. This is
Liberal 2. Socialist 3. Communist 4. Atheist. My who I am, and the more I reflect on the above list,
answers are not lies. They are the honest truth. But the more I am convinced that I am right and just in
why am I not a Democrat? Why do I not vote blue? my belief that the Republican Party is a better fit
Why do I vote at all? Part of the answer is that I am for me than the Democratic Party. I am not one of
not a Democrat because I am not a liberal. I am not the old white guys who simply does not want to
a liberal because I am not a socialist. I am not a adapt to the rapidly changing nature of our coun-
socialist because I am not a communist. I am not a try. I am not one of the old white guys who think
communist because I am not an atheist. I am not an that the old way of doing things is better than the
atheist because I believe in God. What... new way of doing things. I am the old white guy
who thinks that the new way...
Ideologically, I describe myself as liberal. Politically, Ideologically, I describe myself as extremely liberal.
I am a strong Democrat. Racially, I am white. I am Politically, I am a strong Democrat. Racially, I am
female. Financially, I am poor. In terms of my age, I hispanic. I am male. Financially, I am upper-class. In
am old. When I am asked to write down four words terms of my age, I am middle-aged. When I am asked
that typically describe people who support the Demo- to write down four words that typically describe
cratic Party, I respond with: 1. Liberal. 2. Young. 3. people who support the Republican Party, I respond
Female. 4. Poor. The order might be different, but with: 1. Ignorant 2. Racist 3. Misogynist 4.
these are the first four words that come to mind Homophobic. If I were asked to write down four
when I think of my political affiliation. I am an old words that typically describe people who support
female liberal living in poverty. I have lived a long the Democratic Party, I respond with: 1. Liberal 2.
time. I have seen a lot of things. I have done a lot Heterosexual 3. Pro-Choice 4. Pro-Gay. If I were
of things. I am a survivor. I have worked hard. I asked to write down four words that that typically
have given back. I am not a perfect person, but I describe people who support the Libertarian
have tried to be a good person. I am someone who Party, I respond with: 1. Anarchist, 2. Capitalist...
has always...
B
...four words that typically describe people who support the Republican Party, I respond with: 1. Not very intelli-
gent 2. Uninformed 3. Racist 4. Homophobic 5. Sexist 6. Right-wing Christian fundamentalist 7. Anti-gov-
ernment 8. Anti-gay 9. Anti-choice 10. Pro-corporate 11. Anti-environment 12. Pro-gun 13. Pro-war 14.
Pro-rich 15. Pro-military 16. Pro-Israel 17. Pro-life 18. Pro-death penalty 19. Pro-military/pro-war 20.
Anti-science 21. Anti-women...
...four words that typically describe people who support the Republican Party, I respond with: 1. People who
believe in God 2. People who believe in smaller government 3. People who believe in personal responsibili-
ty 4. People who believe in the Constitution. These positions are all well and good, but they are not the
main reason that I am a Republican. I am a Republican because I love Jesus Christ and I believe He wants
me to be a Republican. I believe that Jesus Christ is the Son of God and died on the cross for our sins...
Figure 6: Panel A and B: expanded version of Figure 2 in the main text. Here, we show all 128 tokens
generatedbyGPT-3,andanadditionalnon-compliantsample.
18
ycneuqerF
Humans GPT-3
Length of Responses Length of Responses
Figure7: WordlengthofresponsesinPigeonholingPartisansdatausedintheLucidexperiment
could not be broken into four categories. After culling out these non-compliant responses, we ended with
3,592 total texts from the human sample (an average of 1.7 texts per respondent), and 4,083 from GPT-3
(1.9perrespondent). GPT-3wasmorecompliantatthisstageoftheprocess. Intotal,thismade7675unique
listsforanalysis.
As can be seen, GPT-3 (like some of our human respondents), sometimes listed more than four words.
Themostcommon“non-compliant”responsefromGPT-3wastoprovidefourdescriptions,ratherthanjust
four words, as illustrated in Panel B. Some of our human respondents did the same. We included all of
thesedescriptivephraseresponsesinourdataset. Assuch,someofourstudyparticipantssawfourphrases,
insteadoffourwords.
As Figure 7 indicates, Human and GPT-3 respondents differed in their degree of compliance in listing just
fourwords,withGPT-3includingmoreresponsesofadditionallength(notethelogscaleofthey-axis). The
mean human text was 4.54 words long (min = 4; max = 15). The mean GPT-3 text length was 7.78 (min
= 4; max = 97). Overall, compliance was high: the modal response in both was 4, and most of the longer
responseswere2-3wordphrasesinplaceofsinglewords.
B.2 Lucidsurveydesign
We built a survey through Qualtrics as an instrument for these texts to be evaluated, and hired 2,873 indi-
vidualsfromLucidtodotheevaluating. HiringevaluatorsfromLucidwasfaster,cheaper,andprovidedus
withawiderrangeoftypesofevaluatorsthanifwehadfollowedthemoretraditionalprocessoftraininga
smallsetofresearchassistants. Wedesignedthesurveysuchthatitaskedparticipanttomakecontent-based
judgmentsabout8randomly-assignedlistsfromourcorpusof7,675,andthenmakeTuring-testjudgments
about 8 additional randomly-assigned lists. No respondents evaluated the same texts in both parts of the
survey. Bydesign,eachtextwastobeevaluatedapproximatelythreetimesinthecontentportionandthree
timesintheTuringtestportionofthesurvey. However,duetoveryminimalnon-response,andafewmin-
utes of issues with our server interacting with Lucid’s server at the onset of the survey, 120/7,675 (1.5%)
textswerecodedonlytwice,and7werecoded4times. Resultsdonotdifferwhenweexcludethesetexts.
Lucidparticipantssawtheonline/Qualtricsversionofthefollowingsurvey. Theorderoftheanswerchoices
in each of the following questions was randomized across respondents, but kept constant within each re-
spondent. We include a bolded label to highlight each part of the survey in the reproduction of our survey
thatfollows;thislabelwasnotshowntorespondents:
19
WelcomeScreen
Thanksforparticipatinginthisproject. Weexpectthistasktotake10-12minutestocomplete. Ifyouhave
notcompleteditin1hourafteryoubegin,thetaskwillexpire.
Inwhatfollows,youwillsee8shortlistsofwordswrittentodescribeRepublicansandDemocrats. Inrare
cases,youmayseewordsthatarevulgar,offensive,ornonsensical.
Onthefirstpageforeachlist,youwillbeaskedto:
• First,guessifthepersonwhowrotethelistwasthemselfaRepublican,Democrat,oranIndependent.
• Second,indicateifyoufeelthewordsinthelistasawholearepositiveornegative.
• Third,indicateifyouthinkthewordslistedareextreme.
Onthenextpageforeachlistyouwillbeaskedtoindicatewhetherthelistofwordsdoesordoesnotinclude
anymentiontoeachofthefollowing:
1. personalityorcharactertraits;
2. specificgovernmentorpolicyissues;or
3. socialgroups
You’llthenbeaskedsomeconcludingquestions. Pleaseclickonthebuttonbelowtobegin.
ParticipantsnextcompletedaCAPTCHAitemtohelppreventbotsfromcompletingtheinstrument
ListEvaluationTask,shownconsecutivelyfor8differentlists
Considerthefollowingdescriptionof[Republicans/Democrats;textpipedinhere]:
1. (word/phrase1pipedinhere)
2. (word/phrase2pipedinhere)
3. (word/phrase3pipedinhere)
4. (word/phrase4pipedinhere)
PartyWouldyousaythatthepersonthatwrotethesewordsisaRepublican,Independent,orDemocrat?
• Republican
• Independent
20
• Democrat
PositivityWouldyousaythatthissetofwords,asawhole,ismorepositiveormorenegative?
• Verypositive
• Alittlepositive
• Neitherpositivenornegative
• Alittlenegative
• Verynegative
ExtremityIsthissetofwordsextreme?
• Yes
• No
TraitsDothesewordsmentionpersonalityorcharactertraits?
• Yes
• No
IssuesDothesewordsincludegovernmentorpolicyissues?
• Yes
• No
GroupsDothesewordsmentionsocialgroups?
• Yes
• No
After answering these questions for 8 randomly assigned lists, individuals then moved to the Turing
taskportionofthesurvey
Turingtaskintroductionscreen
Now,pleaselookat8moreshortsetsofwordsaboutRepublicansandDemocrats. Someoftheseresponses
werewrittenbypeopleandotherswerecreatedbyacomputerprogram. Youmayseeafewresponsesfrom
acomputerandafewfromaperson. Oryoumayseemostlyresponsesfromoneoranother. Wewantyou
toguessifaresponsecamefromapersonorfromacomputer.
Pleaseclickonthebuttonbelowtobegin.
21
TuringEvaluationTask,shownconsecutivelyfor8differentlists
Considerthefollowingdescriptionof[Republicans/Democrats;textpipedinhere]:
1. (word/phrase1pipedinhere)
2. (word/phrase2pipedinhere)
3. (word/phrase3pipedinhere)
4. (word/phrase4pipedinhere)
Turing task Would you say that this set of words about [Republicans/Democrats; text piped in here] was
createdbyapersonoracomputerprogram?
• Person
• Computerprogram
Thissamequestionwasusedtoevaluateall8lists
Commentsscreen
We appreciate your participation in this survey. If you have any comments, feel free to leave them in the
spacebelow.
Resultsscreen
As part of this survey, we asked you to judge if a set of words was created by a person or a computer
program. You may be interested in how well you guessed - the table below shows the set of words, your
guess,andiftheresponsecamefromacomputerprogramoraperson.
Coderswerethenshownatablewiththetexts,theirguesses,andthecorrectanswers.
B.3 Lucidresultsanalysis
As described in the text, we estimated regression models using Ordinary Least Squares (OLS) to analyze
our results. Given that the dependent variable in many of our models is binary (0/1), this means many of
these models are linear probability models (LPMs). Results do not significantly differ when we estimate
the LPM results using logit instead. As noted in the main text, all models include fixed effects for study
participants(recallthateachevaluated8lists),andclusteredstandarderrorsbyparticipantsandlist(aseach
listwasevaluatedthreetimes). WeestimatedallofthesemodelsusingthefixestRpackage.
Inadditiontoabinaryvariableindictingthesourceofthetext,allmodelsincludeastandardsetofvariables
to control for the potential impact of characteristics of the original list writers on our outcomes. These
characteristicscomefromtheoriginalPigeonholingPartisansdataset,andincludethelist-writers’:
22
• Gender: acategoricalvariablecodedMale,Female,orOther
• Ethnicity: twobinarycategoricalvariables,Hispanic/Not-HispanicandWhite/Other. Weincludeboth
ascontrolsinourmodels
• Income: Originally asked on an 11-point scale (1 = “Less than $15K”, 11 = “More than $1,000K”).
Wecollapsedthisscaletorun0to1.
• Age: anumericvariablecapturingeachparticipant’sage,and
• PartyIdentification: acategoricalvariablecodedDemocrat,Republican,orIndependent.
In the “Percent correctly predicted” model, we add one additional control, for word length (coded numeri-
callyasthenumberofwordsineachlist).
Inthemaintext,wegraphicallypresentpredictedvaluesfromthesemodels. Herewepresentthefulltables
ofresultsbehindthosepredictions. Aspredictedvaluescanonlybegeneratedusingdefinedlevelsforeach
of the variables in the model, we chose the following levels: Female, Not Hispanic, White, mean income,
mean age, and Democrat. These were the same across all models that included these variables. In the
‘Percentcorrectlypredicted”model,wesetthewordlengthvariabletoitsmean.
Table2presentsthefullresultsofthemodelsusedtopredictthepercentoftextsevaluatedashavingeachof
thefivecharacteristicsdescribedinthestudy. TheseresultsarepresentedgraphicallyinPanelBofFigure4
inthepaper.
Table3presentsthefullresultsofthemodelusedtopredictthepercentoftextsforwhichLucidparticipants
correctlyguessedthepartisanshipofthetextwriter(thetop-leftbarsinPanelBofFigure4inthepaper).
Tables 4-7 present the full results of the models used to generate the predictions in panel A of Figure 4
in the paper. Table 4 corresponds to the top left figure in panel A, Table 5 to the top right, Table 6 to the
bottom left, and Table 7 to the bottom right. These models were subset by the ideology of the list writers
(using the standard 7-point scale described in the paper). In the tables: EC = Extremely Conservative, C =
Conservative, SC = Slightly Conservative, I = Independent, SL = Slightly Liberal, L = Liberal, and EL =
ExtremelyLiberal.
23
Positive Extreme Traits Issues Groups
Source:GPT-3 -0.010 0.013 -0.058 0.033 0.078
(0.006) (0.007) (0.007) (0.007) (0.007)
Gender:Female -0.002 -0.006 -0.010 0.013 0.009
(0.006) (0.008) (0.007) (0.007) (0.007)
Gender:Other -0.111 0.129 -0.175 0.036 -0.077
(0.048) (0.073) (0.072) (0.063) (0.042)
NotHispanic 0.019 -0.011 0.003 -0.002 -0.0002
(0.009) (0.012) (0.011) (0.012) (0.012)
Income 0.009 -0.008 -0.003 0.007 0.003
(0.001) (0.002) (0.002) (0.002) (0.002)
White 0.001 -0.002 0.021 0.017 -0.011
(0.007) (0.009) (0.008) (0.008) (0.008)
Age -0.0005 0.001 0.0003 0.00006 -0.0003
(0.0002) (0.0002) (0.0002) (0.0002) (0.0002)
PID:Indep. -0.029 0.031 -0.005 0.018 -0.010
(0.009) (0.012) (0.011) (0.011) (0.011)
PID:Rep. 0.011 -0.022 -0.034 0.027 -0.012
(0.007) (0.009) (0.008) (0.008) (0.008)
Observations 18,903 18,903 18,903 18,903 18,903
RMSE 0.28971 0.39470 0.36560 0.36634 0.37094
Evaluatorsfixedeffects (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Table2: Evaluatedcontentoflists,LucidExperiment
24
PercentCorrectlyGuessed
Source:GPT-3 -0.073
(0.008)
Gender:Female -0.007
(0.008)
Gender:Other -0.059
(0.082)
NotHispanic -0.006
(0.013)
Income 0.002
(0.002)
White 0.012
(0.010)
Age 0.0007
(0.0003)
WordLength 0.0009
(0.0007)
PID:Indep. -0.285
(0.014)
PID:Rep. -0.097
(0.011)
Observations 18,903
RMSE 0.43912
Evaluatorsfixedeffects (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Table3: Correctlyguessingthepartisanshipoflistwriters,LucidExperiment
25
Positive
EC C SC I SL L EL
Source:GPT-3 -0.085 0.019 -0.041 0.040 0.018 0.034 0.019
(0.121) (0.046) (0.054) (0.028) (0.033) (0.019) (0.051)
Gender:Female 0.015 -0.004 -0.079 0.009 -0.034 -0.055 0.072
(0.103) (0.047) (0.050) (0.028) (0.037) (0.019) (0.041)
NotHispanic 0.055 -0.102 0.040 0.103 0.023 -0.032 0.022
(0.211) (0.095) (0.064) (0.043) (0.056) (0.037) (0.037)
Income 0.011 0.002 -0.002 0.011 0.012 0.005 0.017
(0.025) (0.010) (0.009) (0.007) (0.007) (0.004) (0.009)
White -0.013 0.147 0.035 0.075 0.033 0.009 -0.086
(0.157) (0.078) (0.058) (0.034) (0.041) (0.024) (0.053)
Age -0.012 -0.001 -0.002 -0.003 -0.0001 -0.0010 -0.002
(0.005) (0.001) (0.001) (0.0009) (0.001) (0.0006) (0.001)
Gender:Other -0.487 -0.078
(0.163) (0.056)
Observations 387 1,122 1,059 2,072 1,419 2,374 1,036
RMSE 0.05609 0.11621 0.10735 0.15728 0.12102 0.13797 0.08714
Evaluatorsfixedeffects (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Table 4: Percent of texts rated positively, subset by the ideology of individual list writers: Describing
Republicans
26
Positive
EC C SC I SL L EL
Source:GPT-3 -0.145 0.044 0.041 0.089 -0.028 -0.099 -0.033
(0.172) (0.044) (0.045) (0.027) (0.032) (0.020) (0.053)
Gender:Female -0.198 0.011 0.023 -0.026 -0.013 0.003 0.081
(0.119) (0.057) (0.042) (0.026) (0.034) (0.019) (0.045)
NotHispanic 0.568 0.112 -0.093 0.0004 -0.084 0.051 0.089
(0.226) (0.111) (0.089) (0.039) (0.051) (0.037) (0.068)
Income -0.037 0.006 0.005 0.010 0.020 0.011 0.040
(0.052) (0.010) (0.010) (0.007) (0.008) (0.004) (0.011)
White 0.917 -0.006 -0.077 -0.015 -0.046 0.038 -0.047
(0.177) (0.067) (0.043) (0.031) (0.043) (0.024) (0.067)
Age -0.006 -0.002 -0.0003 0.0002 0.001 -0.001 -0.0004
(0.002) (0.001) (0.001) (0.0009) (0.001) (0.0007) (0.002)
Gender:Other -0.014 0.190
(0.061) (0.109)
Observations 393 1,121 1,048 2,062 1,423 2,370 1,029
RMSE 0.04445 0.11032 0.10969 0.15234 0.11873 0.13810 0.09856
Evaluatorsfixedeffects (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Table 5: Percent of texts rated positively, subset by the ideology of individual list writers: Describing
Democrats
27
Extreme
EC C SC I SL L EL
Source:GPT-3 0.079 -0.025 0.064 0.035 -0.066 0.016 0.143
(0.070) (0.060) (0.069) (0.038) (0.051) (0.034) (0.080)
Gender:Female 0.024 0.061 -0.137 -0.035 0.059 0.036 0.054
(0.027) (0.059) (0.067) (0.037) (0.056) (0.034) (0.065)
NotHispanic 0.023 -0.048 -0.075 -0.093 -0.024 -0.029 -0.108
(0.028) (0.119) (0.103) (0.066) (0.095) (0.061) (0.083)
Income 0.005 0.023 0.023 -0.018 0.006 -0.008 0.012
(0.009) (0.013) (0.013) (0.009) (0.012) (0.008) (0.017)
White -0.097 -0.241 -0.009 -0.137 0.046 -0.010 -0.073
(0.093) (0.107) (0.075) (0.047) (0.067) (0.038) (0.094)
Age 0.004 0.0010 0.002 0.004 0.001 0.0009 0.008
(0.003) (0.002) (0.002) (0.001) (0.002) (0.001) (0.002)
Gender:Other 0.705 0.435
(0.273) (0.308)
Observations 387 1,122 1,059 2,072 1,419 2,374 1,036
RMSE 0.03172 0.15942 0.15086 0.21933 0.18450 0.23722 0.15852
Evaluatorsfixedeffects (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Table 6: Percent of texts rated as extreme, subset by the ideology of individual list writers: Describing
Republicans
28
Extreme
EC C SC I SL L EL
Source:GPT-3 -0.374 -0.089 -0.024 -0.062 0.027 0.015 -0.030
(0.277) (0.059) (0.057) (0.035) (0.044) (0.027) (0.084)
Gender:Female -0.101 -0.105 -0.171 0.058 0.067 -0.004 -0.110
(0.247) (0.071) (0.065) (0.034) (0.051) (0.027) (0.064)
NotHispanic 0.085 -0.033 0.173 0.089 0.063 -0.006 0.015
(0.524) (0.120) (0.113) (0.048) (0.061) (0.053) (0.103)
Income 0.002 -0.009 -0.005 -0.019 -0.003 -0.002 -0.005
(0.062) (0.014) (0.015) (0.008) (0.011) (0.006) (0.017)
White 0.462 0.103 -0.073 -0.025 0.013 0.005 0.053
(0.317) (0.091) (0.068) (0.038) (0.068) (0.032) (0.103)
Age -0.006 0.0008 0.003 -0.0002 -0.0001 0.002 0.004
(0.007) (0.002) (0.002) (0.001) (0.002) (0.0009) (0.003)
Gender:Other 0.146 0.123
(0.086) (0.274)
Observations 393 1,121 1,048 2,062 1,423 2,370 1,029
RMSE 0.06764 0.15862 0.14552 0.20705 0.16029 0.19170 0.14525
Evaluatorsfixedeffects (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Two-way(Evaluators&Lists)standard-errorsinparentheses
Signif. Codes: ***: 0.001,**: 0.01,*: 0.05
Table 7: Percent of texts rated as extreme, subset by the ideology of individual list writers: Describing
Democrats
29
C Details on Study 2
C.1 Datageneration
For study 2, we generated a silicon sample based on the 2012, 2016, and 2020 ANES Timeseries datasets.
Foreachsubject,weconstructedafirst-personbackstoryusingatemplatingstrategysimilartothatinStudy
1. We used the following variables from the ANES to condition GPT-3; in this list, variable names from
the datasets follow in parentheses in this order - 2012 / 2016/ 2020. The variables were (1) racial/ethnic
self-identification (dem-raceeth-x / V161310x / V201549x), (2) gender (gender respondent x / V161342 /
V201600),(3)age(dem age r x/V161267/V201507x),(4)conservative-liberalideologicalself-placement
(libcpre self / V161126 / V201200), (5) party identification (pid x / V161158x / V201231x), (6) if the
subject is interested in politics (paprofile interestpolit / V162256 / V202406), (7) if the respondent attends
church (relig church / V161244 / V201452), (8) if the respondent reported discussing politics with family
andfriends(discuss disc/V162174/V201452),(9)feelingsofpatriotismassociatedwiththeAmericanflag
(patriot flag/V162125x/Notasked),and(10)respondents’stateofresidence(sample stfips/V161010d/
Notreleasedasofthetimeofthiswriting). Forthemeasureofself-reportedvotefromtheANES,weused
presvote2012 xin2012,V162062xin2016,andV202110xin2020.
For all template fragments, phrasing was selected to closely match the ANES, although the ANES phras-
ing was translated into first-person declarations. For the age and state of residence fragments, the ANES
result was inserted directly into the template. All other template fragments mapped the ANES variable to
a short string, such as “attend church”, “extremely liberal”, “native American”, etc. that closely matches
the corresponding ANES value, which was then inserted into the template fragment. Template fragments
werethenconcatenatedtogethertocreateafinalbackstory. Ifanyvariableforanysubjectwasmissing,the
correspondingtemplatefragmentwasomitted.
Becausethisstudypredictsvotingpatterns,weareinterestedintheprobabilitythatGPT-3assignstovoting
for a particular candidate, given a specific backstory. Note that in this study, GPT-3 was not required to
sample any completions; we only use it to compute the probability of a single successor token, given the
conditioningcontext. Forthisreason,thetemperatureparameterandsamplingstrategyoftheOpenAIAPI
areirrelevant. BecauseGPT-3assignssomeprobabilitytoawidevarietyofsemanticallyequivalentphrases,
wecollapsethemasdescribed inSectionA.Weusedtwotoken setsforeachyearofdata. In2012, wein-
cludedthefollowingtokensetsforvotingforRomney: “romney”,“mitt”,“republican”,and“conservative”.
The token set for voting for Obama was “obama”, “barack”, “democrat”, “democratic”, and “liberal”. For
2016,theTrumptokensetincludedtheterms“trump”,“donald”,“republican”and“conservative”. Forthe
2016 Clinton token set, we included “clinton”, “hillary”, “rodham”, “senator”, “democrat”, “democratic”,
and “liberal”. For the 2020 data, the token set for Trump included “trump”, “donald”, “republican” and
“conservative”. ForBiden,thetokensetwas“joe”,“joseph”,“biden”,“democratic”,“democrat”,and“lib-
eral”. For all of these token sets, lexical variations of each term (lower-case, upper-case, mixed-case, with
and without leading and trailing spaces, etc.; these are all considered distinct tokens by GPT-3) were also
included. Any tokens not in the token sets were ignored. Token sets were selected to ensure that common
caseswerecaught,butwerenottunedoroptimizedtoimproveresults.
C.2 Dataanalysis
TheprimaryanalysisofthissiliconsamplecomesfromcomparingthevotechoiceasreportedbyANESre-
spondentsandtheprobabilityforvotingfortheRepublicancandidatefromGPT-3. Tomakethepredictions
30
from GPT-3 match the observed human data (our baseline in this case), we dichotomized the probability
of voting for the Republican candidate from GPT-3 by dividing the responses exactly at 0.50; probabilities
of more than .50 were coded as a vote for the Republican (i.e., Romney or Trump) and probabilities lower
than 0.50 were coded as a vote for the Democrat (i.e., Obama, Clinton, or Biden). No probabilities were
predictedtobeexactly0.50.
Thisgivesustwobinaryvariables,withwhichweestimated4statistics. Table1inthemaintextpresented
only the tetrachoric corelation and proportion agreement, solely for presentational and space purposes. In
the tables in this section, we show the entire set of metrics. In the following table, we calculate the corre-
spondence between the vote variable from the ANES and GPT-3 in four different ways, each of which is a
waytodeterminehowcloselytwobinaryvariablescorrespond. Thesestatisticsareasfollows(presentedin
thesameorderasinthesubsequenttablesofresults):
• Tetrachoric correlation: This measure is a way to calculate a correlation between two variables
when both are binary but come from an underlying, continuous, normal distribution. It is similar to
Pearson’s r in it’s interpretation: values closer to 1 indicate closer correspondence, and values near
0 indicate almost no correspondence. These values were calculated using the tetrachoric command
fromthepsychpackageinR.
• Cohen’s Kappa: This statistic, sometimes referred to as κ, calculates the agreement between two
variables. It is generally used to compare the agreement of two raters, and here we use it treating
the ANES and GPT-3 estimates as the two ratings. Many prefer this measure over the proportion of
agreement because κ includes a penalty for the amount of agreement that might have occurred due
to chance alone. Values of κ typically range from 0 to 1, with the same interpretation as tetrachoric
correlation and Pearson’s r. It is theoretically possible to obtain a negative value for κ; this would
indicateworsecorrespondencebetweenthevariablesthanwouldoccurbychance. ThevaluesinTable
1werecalculatedusingthecohen.kappacommandfromthepsychpackageinR.
• Intraclass correlation coefficient or ICC: Similar to κ, ICC is commonly used as a measure of
agreement between raters or coders. Values closer to 1 indicate stronger agreement, and generally
scores higher than 0.75 are considered indicates of strong agreement. It is more flexible and can be
used to compare variables of different measurement metrics (e.g., ordinal, continuous, binary, etc.)
to one another. Here we present the results for the ICC measures for the binary vote variables, but
replacingtheGPT-3binaryvariablefortheunderlyingprobabilitydoesnotchangetheICCmeasures
inmeaningfulways. GiventhatourinterestisunderstandinghowboththehumanandGPT-3measures
comparetooneanother,weusetheaveragedversionsoftheICCstatistics. Further,ratherthanfocus
onaspecificmeasureofICC(suchasICC1,ICC2,orICC3),wesimplyreportthelowestofthethree.
In nearly all cases, the differences between these versions of ICC were neglible. Like the previous
twostatistics,ICCwascalculatedwiththepsychpackageinR,specificallywiththeICCcommand.
• Proportion agreement: This is the simplest of the measures and indicates the proportion of the
observations where the two vote variables (GPT-3 and human response) exactly match. It does not
account for the probability of matching by chance and should be viewed as a descriptive quantity. It
was calculated by creating frequency tables of the GPT-3 and ANES vote variables and then calcu-
latingproportionsbasedonthosefrequencies. Weincludeproportionagreementbecausesomeofthe
other measures (such as the tetrachoric correlation and κ) do not perform well when all of the data
(more than 95 percent) fall in the same quadrant of the frequency table. As a concrete example, the
correlationsandκarequitelowforStrongDemocrats;uponcloserexamination,though,thisseemsto
occurbecausethereisalmostnovariationinthevotevariableforGPT-3ortheANES.Thereisnear
31
Variable TetrachoricCorrelation Cohen’sKappa ICC Prop.agreement
Wholesample 0.90 0.69 0.81 0.85
Men 0.90 0.70 0.82 0.85
Women 0.91 0.67 0.80 0.86
Strongpartisans 0.99 0.93 0.96 0.97
Weakpartisans 0.73 0.45 0.61 0.74
Leaners 0.90 0.70 0.82 0.85
Independents 0.31 0.16 0.22 0.59
Conservatives 0.84 0.59 0.74 0.84
Moderates 0.65 0.40 0.57 0.77
Liberals 0.81 0.43 0.60 0.95
Whites 0.87 0.64 0.77 0.82
Blacks 0.71 0.31 0.47 0.97
Hispanics 0.86 0.63 0.78 0.86
Attendschurch 0.91 0.71 0.83 0.86
Doesnotattendchurch 0.88 0.64 0.78 0.85
Veryinterestedinpolitics 0.95 0.80 0.89 0.90
Notatallinterestedinpolitics 0.71 0.38 0.53 0.74
Discussespolitics 0.92 0.72 0.84 0.87
Doesnotdiscusspolitics 0.83 0.57 0.73 0.82
18to30yearsold 0.90 0.66 0.80 0.87
31to45yearsold 0.90 0.65 0.79 0.85
46to60yearsold 0.90 0.69 0.82 0.86
Over60 0.90 0.71 0.83 0.85
Californians 0.92 0.62 0.76 0.85
Texans 0.91 0.69 0.81 0.84
NewYorkers 0.91 0.59 0.74 0.84
Ohioans 0.88 0.66 0.80 0.84
Arizonans 0.98 0.89 0.94 0.95
Wisconsins 0.95 0.70 0.82 0.85
Table8: VariousmeasuresofcorrelationbetweenGPT-3andANESprobabilityofvotingforMittRomney
in2012. GPT-3voteisabinaryversionofGPT-3’spredictedprobabilityofvotingforMittRomney,dividing
predictionsat0.50.
completeagreementbetweenbothestimatesofvote-itisjustthatalloftherespondentsreportedvot-
ing(orarepredictedbyGPT-3tovotefor)thesamecandidate. Thisalmostcompletelackofvariation
onthevotevariableitselfseemstomakethemeasuresofcorrespondenceunreliableandunreflective
oftheagreementbetweenGPT-3andtheANES.
32
Variable TetrachoricCorrelation Cohen’sKappa ICC Prop.agreement
Wholesample 0.92 0.73 0.84 0.87
Men 0.93 0.76 0.86 0.88
Women 0.92 0.7 0.82 0.86
Strongpartisans 1.00 0.95 0.97 0.97
Weakpartisans 0.71 0.46 0.62 0.74
Leaners 0.93 0.74 0.85 0.87
Independents 0.41 0.25 0.39 0.62
Conservatives 0.88 0.66 0.79 0.86
Moderates 0.76 0.52 0.69 0.78
Liberals 0.73 0.25 0.39 0.95
Whites 0.91 0.7 0.83 0.85
Blacks 0.87 0.51 0.67 0.96
Hispanics 0.93 0.73 0.85 0.9
Attendschurch 0.93 0.75 0.86 0.88
Doesnotattendchurch 0.9 0.67 0.8 0.85
Veryinterestedinpolitics 0.97 0.85 0.92 0.93
Notatallinterestedinpolitics 0.75 0.48 0.64 0.75
Discussespolitics 0.94 0.76 0.86 0.88
Doesnotdiscusspolitics 0.81 0.57 0.72 0.79
18to30yearsold 0.9 0.69 0.81 0.86
31to45yearsold 0.92 0.72 0.84 0.87
46to60yearsold 0.92 0.72 0.83 0.86
Over60 0.93 0.75 0.85 0.87
Californians 0.87 0.58 0.72 0.83
Texans 0.95 0.79 0.88 0.9
NewYorkers 0.95 0.79 0.89 0.91
Ohioans 0.9 0.7 0.83 0.85
Arizonans 0.92 0.74 0.85 0.87
Wisconsins 0.97 0.84 0.91 0.92
Table9: VariousmeasuresofcorrelationbetweenGPT-3andANESprobabilityofvotingforDonaldTrump
in 2016. GPT-3 vote is a binary version of GPT-3’s predicted probability of voting for Donald Trump,
dividingpredictionsat0.50.
33
Variable TetrachoricCorrelation Cohen’sKappa ICC Prop.agreement
Wholesample 0.94 0.77 0.87 0.89
Men 0.95 0.77 0.87 0.88
Women 0.94 0.78 0.88 0.90
Strongpartisans 1.00 0.95 0.97 0.97
Weakpartisans 0.84 0.63 0.77 0.82
Leaners 0.95 0.79 0.88 0.89
Independents 0.02 0.02 0.03 0.53
Conservatives 0.91 0.71 0.83 0.89
Moderates 0.71 0.48 0.65 0.77
Liberals 0.86 0.51 0.67 0.97
Whites 0.94 0.78 0.88 0.89
Blacks 0.81 0.49 0.66 0.94
Hispanics 0.88 0.63 0.77 0.83
Attendschurch 0.94 0.77 0.87 0.88
Doesnotattendchurch 0.93 0.76 0.86 0.90
Veryinterestedinpolitics 0.97 0.84 0.91 0.92
Notatallinterestedinpolitics 0.83 0.62 0.77 0.81
Discussespolitics 0.95 0.79 0.88 0.90
Doesnotdiscusspolitics 0.80 0.59 0.74 0.79
18to30yearsold 0.90 0.70 0.82 0.87
31to45yearsold 0.94 0.78 0.88 0.90
46to60yearsold 0.92 0.74 0.85 0.87
Over60 0.96 0.82 0.90 0.91
Table 10: Various measures of correlation between GPT-3 and ANES probability of voting for Donald
Trump in 2020. GPT-3 vote is a binary version of GPT-3’s predicted probability of voting for Donald
Trump,dividingpredictionsat0.50.
34
Full backstory, removing one element Backstory consisting of only one element
tnemeergA
noitroporP
yrotskcab
lluF
yrotskcab
oN
etatS
-
tseretni
lacitiloP
-
redneG
-
msitoirtaP
-
ygoloedI
-
scitilop
sessucsiD
-
reoghcruhC
-
egA
-
ecaR
-
ytraP
-
ygoloedI
dna
ytraP
-
ytraP
+
ygoloedI
+
msitoirtaP
+
ecaR
+
etatS
+
egA
+
tseretnI
lacitiloP
+
scitiloP
sessucsiD
+
redneG
+
reoghcruhC
+
0.8
0.6
0.4
0.2
Figure 8: An ablation experiment examining the importance of each backstory element. Reported is the
ProportionAgreementonthevotepredictiontaskoftheANES2016dataset. Eachbarrepresentsadifferent
templatewithsomesetofbackstoryelements,from“Fullbackstory”(yieldingtheresultsshowninthemain
paper), to “No backstory” (where each silicon subject would have the same, empty backstory, therefore
resultingthesamevotepredictionforeverysubject;thisisessentiallyequivalenttorandomchance).
C.3 Ablationanalysis
We also conducted an ablation study on the backstories used for vote prediction in the ANES 2016 exper-
iment. Recall that each backstory consisted of a template with 10 different elements. For this experiment,
weinvestigatedhowtheelementsofthetemplateinteractedwitheachotherbysystematicallyremovingone
ortwoatatime. Wealsotestedbackstoriesconsistingofonlyonebackstoryelement.
The results are shown in Fig. 8. There are a few notable elements to these results. First, no single back-
story element accounted for all of the predicted power of GPT-3’s vote predictions, suggesting that GPT-3
isindeedsynthesizingorfusingmultiplebackstoryelementstogether,yieldingamoreaccuratefinalpredic-
tion. Second, GPT-3 can use either Party or Ideology to predict vote choice, but Party is more predictive.
Third, the addition of some elements of the backstory template (such as State or Political Interest) mildly
hurt performance. Finally, we conducted an experiment where we removed both Party and Ideology from
the template, yielding only demographic factors; we see that the combination of the remaining 8 elements
yieldsbetteraccuracythananysingleelement.
Wehereadditionallynotethatnoattemptwasmadetooptimizethetemplateusedduringourexperiments;
thetemplateusedandthe10elementsselectedrepresentourfirsttry. Futureworkcanlikelyimprovethese
resultsbyoptimizingtemplateandbackstoryelementsbeforeanexperimentbegins.
C.4 Modelcomparison
Finally, we tested the performance of different language models on the vote prediction task; results are
shown in Figure 9. Five different families of language models were tested, representing the best available
35
tnemeergA
noitroporP
ANES 2012 ANES 2016 ANES 2020
0.9
0.8
0.7
Jurassic
GPT-3
0.6
GPT-J
0.5 GPT-Neo
GPT-2
0.4
Model parameter count (log scale)
Figure9: Comparisonoftheperformanceofdifferentlanguagemodelsonthevotepredictiontask. Seetext
fordetails.
modelsatthetimeofwriting. GPT-3andJurassicarecommercialmodelsavailableonlyviaapaidAPI;all
othermodelshavebeenpubliclyreleased. Thefigureshowsperformanceasafunctionofparametercount,
butperformancealsodependsstronglyonthedetailsofthecorpususedtotraineachmodel. Wehereonly
notethat,asinsomanyothernaturallanguageprocessingtasks,theverylargestmodelsperformverywell.
AlsoofnoteisthesurprisinglygoodperformanceofthelargestmemberoftheGPT-Neofamilyofmodels
- at only 6B parameters, its performance rivals that of the much larger and more costly GPT-3 (at 175B
parameters).
D Details on Study 3
D.1 Datageneration
ForStudy3,wegeneratedasiliconsampleofvirtualANESparticipantsbyconstructingacompletevirtual
interview. As the example template (Figure 10) indicates, we selected twelve common variables1 from the
2016 ANES for exploration, representing a variety of demographic (gender, race/ethnicity, age, education,
religiosity), attitudinal (political interest, patriotic feelings about the flag, party identification, ideology),
and behavioral (talk about politics, vote, and vote choice) information. The conditioning text included the
mockinterviewwithquestionsandresponsesforelevenofthetwelveitems,leavingthetwelfthquestionfor
GPT-3toanswer. EachbackstorywasbasedonactualresponsesgivenbyonehumanANESrespondent.2 In
study2,thegoalwastomeasuretheprobabilityofasingletoken;herethegoalistomeasureawidevariety
of multi-token responses, which complicates the analysis of their raw probabilities. Instead, we allow the
GPT-3APItosamplecompletions,whichwethenanalyze.
Like Study 2, we use a templating system that maps ANES demographic variables to template fragments,
whicharethenconcatenatedtoconstructtheconditioningcontext. Becausethiswasavirtualinterview,we
used phrasing that exactly matched the ANES interview verbiage whenever possible. We mapped ANES
variables to short text fragments, which were then interpolated into template fragments. For this study,
we used the following ANES variables (in the following order): gender (V161342), race (V161310x), age
(V161267), education (V161270) church attendance (V161244), patriotism (V162125x), whether the sub-
ject discusses politics (V162174), level of interest in politics (V162256), 7-point self-reported ideology
1Becausevotechoiceisconditionalonvoteturnout,thesetwoitemsarecombinedintoonemetricintheanalysis,foratotalof
11uniqueitems.
2MissingresponsesintheANESdataresultedintheremovalofthequestionfromtheconditioningtext.
36
Interviewer: Whatisyourgender? Areyou“male”or“female”?
Me: male
Interviewer: I am going to read you a list of four race categories. What race do you consider yourself to be?
“White”,“Black”,“Asian”,or“Hispanic”?
Me: white
Interviewer: Whatisyourageinyears?
Me: 29
Interviewer: Whatisthehighestlevelofschoolyouhavecompleted,orthehighestdegreeyouhavereceived? Is
it“highschool”,“somecollege”,“afour-yearcollegedegree”,or“anadvanceddegree”?
Me: highschool
Interviewer: When you see the American flag flying, how does it make you feel? Does it make you feel “ex-
tremelygood”,“moderatelygood”,“alittlegood”,“neithergoodnorbad”,“alittlebad”,“moderatelybad”,or
“extremelybad”?
Me: extremelygood
Interviewer: Doyoueverdiscusspoliticswithyourfamilyandfriends? Pleaserespondwith“yes”or“no”.
Me: yes
Interviewer: How interested would you say you are in politics? Are you “very interested”, “somewhat inter-
ested”,“notveryinterested”,or“notatallinterested”?
Me: somewhatinterested
Interviewer: Whichwouldyousaybestdescribesyourpartisanidentification. Wouldyousayyouarea“strong
democrat”, “notverystrongdemocrat”, “independent, butclosertotheDemocraticparty”, “independent”, “in-
dependent,butclosertotheRepublicanparty”,“notverystrongRepublican”,or“strongRepublican”?
Me: strongRepublican
Interviewer: Didyouvoteinthe2016generalelection? Pleaseanswerwith“yes”or“no”.
Me: yes
Interviewer: Whichpresidentialcandidatedidyouvoteforinthe2016presidentialelection,“HillaryClinton”,
“DonaldTrump”,or“someoneelse”?
Me: DonaldTrump
Interviewer: Lots of things come up that keep people from attending religious services even if they want to.
Thinkingaboutyourlifethesedays,doyoueverattendreligiousservices? Pleaserespondwith“yes”or“no”.
Me: yes
Figure 10: An interview-style context used in Study 3. The context is in plaintext; underline text shows
demographic variables dynamically inserted into the interview template; one possible sampled completion
isshowninbold.
37
(V161126), 7-point self-reported partisanship (V161158x), whether the subject voted in 2016 (V162031x)
and for whom (V162062x). Table 11 provides the full text of all ANES question wording and GPT-3 tem-
platetext.
Recallthatthegoalofthisstudywastopredictonefactor(whichwecallthetargetfactor,suchas“Race”)
givenspecificvaluesofalloftheotherfactors. Thetemplatefragmentforthetargetfactorwasalwaysplaced
attheendofthecontext(andnaturallydidnotincludethecorrespondingANESvariable). Givenacontext,
we asked GPT-3 to sample 5 tokens, using a temperature of 0.7. GPT-3’s generated text was then lightly
processed (lower-cased, stripped of leading and trailing whitespace), and compared to the limited set of
ANESresponsesforthetargetfactorusingexactstringmatches. So,forexample,ifthetargetquestionwas
“Race”,thenthestringproducedbyGPT-3wouldbecomparedto“white”,“black”,“asian”and“hispanic”,
andcodedas1,2,3or5,respectively. IftheGPT-3responsedidnotmatchanyoftheallowableresponses,
it was coded as missing data. This generates a dataset that is structurally equivalent to the original ANES
dataset.
Thecombinationof12variablesand4270respondentsresultedinthegenerationofmorethan50,000unique
conditioningtextsforGPT-3, eachdesignedtoelicitone“silicon”respondent’sanswertoonequestion. In
our analysis, we keep only the 1782 observations that are complete in both ANES and GPT-3 responses.
Thispreventsvariationinthesetofcasesfromintroducingadditionalstatisticalnoisetothecomparison.
38
Table 11: Comparison of ANES question wording and GPT-3
Template
Order Variable ANESVarID ANESQuestionWording GPT-3TemplateText
Whatisyourgender? Are
1 Gender V161342 Whatisyourgender?
you”male”or”female”?
Iamgoingtoreadyoualist
offiveracecategories. Please
chooseoneormoreracesthat
Iamgoingtoreadyoualist
youconsideryourselftobe: -
offourracecategories. What
Race/ white,-blackorAfrican-
2 V161310x racedoyouconsideryourself
Ethnicity American,-AmericanIndian
tobe? ”White”,”Black”,
orAlaskaNative,-Asian,or
”Asian”,or”Hispanic”?
-NativeHawaiianorother
PacificIslander? +Areyou
Spanish,Hispanic,orLatino?
(Derivedvariable-no
3 Age V161247 Whatisyourageinyears?
questiontext)
Whatisthehighestlevelof
schoolyouhavecompleted,
Whatisthehighestlevelof
orthehighestdegreeyou
schoolyouhavecompletedor
4 Education V161270 havereceived? Isit”high
thehighestdegeeyouhave
school”,”somecollege”,”a
received?
four-yearcollegedegree”,or
”anadvanceddegree”?
continuedonnextpage...
39
...continuedfromlastpage.
Order Variable ANESVarID ANESQuestionWording GPT-3TemplateText
Lotsofthingscomeupthat
Lotsofthingscomeupthat
keeppeoplefromattending
keeppeoplefromattending
religiousservicesevenifthey
religiousservicesevenifthey
wantto. Thinkingaboutyour
Attends wantto. Thinkingaboutyour
5 V161244 lifethesedays,doyouever
Church lifethesedays,doyouever
attendreligiousservices,
attendreligiousservices?
apartfromoccasional
Pleaserespondwith”yes”or
weddings,baptismsor
”no”.
funerals?
WhenyouseetheAmerican
flagflyingdoesitmakeyou
feelgood,bad,orneither
WhenyouseetheAmerican
goodnorbad? +Doesit
flagflying,howdoesitmake
makeyoufeel[extremely
youfeel? Doesitmakeyou
good,moderatelygood,ora
feel”extremelygood”,
littlegood/alittlegood,
6 Patriotism V162125x ”moderatelygood”,”alittle
moderatelygood,or
good”,”neithergoodnor
extremelygood]? /Doesit
bad”,”alittlebad”,
makeyoufeel[extremely
”moderatelybad”,or
bad,moderatelybad,ora
”extremelybad”?
littlebad/alittlebad,
moderatelybad,orextremely
bad]?
Doyoueverdiscusspolitics
Discusses Doyoueverdiscusspolitics withyourfamilyandfriends?
7 V162174
Politics withyourfamilyorfriends? Pleaserespondwith”Yes”or
”No”.
continuedonnextpage...
40
...continuedfromlastpage.
Order Variable ANESVarID ANESQuestionWording GPT-3TemplateText
Howinterestedwouldyou
sayyouareinpolitics? Are
Howinterestedwouldyou
you[veryinterested,
sayyouareinpolitics? Are
somewhatinterested,notvery
Political you”veryinterested”,
8 V162256 interested,ornotatall
Interest ”somewhatinterested”,”not
interested/notatallinterest,
veryinterested”,or”notatall
notveryinterested,somewhat
interested”?
interested,orvery
interested]?
Intalkingtopeopleabout
elections,weoftenfindthata
lotofpeoplewerenotableto
votebecausetheyweren’t
registered,theyweresick,or
theyjustdidn’thavetime.
Whichofthefollowing
statementsbestdescribes Didyouvoteinthe2016
Votedin
9 V162031x you: One,Ididnotvote(in generalelection? Please
2016
theelectionthisNovember), answerwith”yes”or”no”.
Two,Ithoughtaboutvoting
thistime,butdidn’t,Three,I
usuallyvote,butdidn’tthis
time,orFour,IamsureI
voted? +(Derivedfromother
PreandPostElection
Questions)
continuedonnextpage...
41
...continuedfromlastpage.
Order Variable ANESVarID ANESQuestionWording GPT-3TemplateText
Whodidyouvotefor?
[HillaryClinton,Donald Whichpresidentialcandidate
Trump/DonaldTrump, didyouvoteforinthe2016
2016Vote HillaryClinton],Gary presidentialelection,”Hillary
10 V162062x
Choice Johnson,JillStein,or Clinton”,”DonaldTrump”,or
someoneelse? +(Derived ”someoneelse”? Note: Only
fromotherPreandPost displayedifrespondentvoted.
ElectionQuestions)
Whenaskedaboutyour
politicalideology,wouldyou
Wherewouldyouplace
sayyouare”extremely
yourselfonthisscale,or
liberal”,”liberal”,”slightly
11 Ideology V161126 haven’tyouthoughtmuch
liberal”,”moderate”,
aboutthis? (Scalecardshown
”slightlyconservative”,
oronlineresponseoptions)
”conservative”,or”extremely
conservative”?
Generallyspeaking,doyou
Whichwouldyousaybest
usuallythinkofyourselfas[a
describesyourpartisan
Democrat,aRepublican/a
identification. Wouldyousay
Republican,aDemocrat],an
youarea”strongdemocrat”,
independent,orwhat? +
”notverystrongdemocrat”,
Wouldyoucallyourselfa
”independent,butcloserto
12 PartyID V161158x strong[Democrat/
theDemocraticparty”,
Republican]oranotvery
”independent”,”independent,
strong[Democrat/
butclosertotheRepublican
Republican]? ORDoyou
party”,”notverystrong
thinkofyourselfascloserto
Republican”,or”strong
theRepublicanPartyortothe
Republican”?
DemocraticParty?
42
D.2 Dataanalysis
The complete set of synthetic responses are appended together to create a single dataset that includes the
ANESvaluesforallelevenvariablesandthesiliconresponsesforallelevenvariables.
As an important methodological note, we do not calculate the direct individual-level correspondence be-
tweentheANESvalueforagivenrespondentandtheGPT-3valuebasedonthesamebackstoryinformation
(such as a percent correctly predicted). GPT-3 draws tokens from a distribution of words, and we also
assume distributions in outcomes in the general population. Therefore, even if GPT-3 and ANES values
are drawn from the same distribution, we cannot expect them to match in any given case. The important
demonstration for our point is not whether GPT-3 can correctly predict an individual, but rather whether it
canproduceadistributionofgeneratedresponsesthatiscomparabletothedistributioninthehumandata.
WeusetheCramerVfunctionoftheRpackage‘DescTools’tocalculatetheCramer’sVbetweeneverypos-
siblecombinationofthe22variables. WeuseCramer’sVasitisamenabletocalculationusingcategorical
data,and,likePearson’sChi-squaredonwhichitisbased,reliesonmarginalvaluestoaccountforvariations
inbaserates. Cramer’sVhasarangeof0to1. HighervaluesofCramer’sVindicatethatknowingthevalue
ofonevariablegivesyoumoreinformationaboutthelikelyvalueofthesecondvariable.
Tables12-14reporttheCramer’sVvaluesforFigure6inthemaintextofthepaper.
D.2.1 MissingData
Inallpresentedanalysis,thedataarerestrictedtojustthecasesthatarecomplete-meaningtherearevalid
responsevaluesforallANESandGPT-3variables. Ofthe4270casesinthe2016ANESdatafile,1782are
completecasesusedintheanalysis.
Table 15 displays the percent of cases with missing data for each variable. The percent of missing data
producedvariessubstantially,forbothhumansandGPT-3. GPT-3wasabletoproduceavalidandcompliant
answerinmorethanthree-quartersofthecasesforallquestionitems,andthreeoftheitemshadcompliance
rates above 99 percent. With additional training for particular questions, non-compliance could likely be
reduced.
D.2.2 DescriptiveStatistics
Table16presentsthedescriptivestatisticsforthevariablesusedinStudy3,separatelybydatasource(ANES
humansorGPT-3siliconsample).
D.3 AlternativeSpecifications
D.3.1 CompletelySyntheticData
The data generation process results in one vector of synthetic data based on the ANES inputs for the other
elevenitems. Whenthesesyntheticvectorsarecombined,theresultisacompletedatasetofsyntheticdata.
Inthemaintextofthepaper,theCramer’sViscalculatedusingtheANES“input”variableandtheGPT-3
output. This provides the most direct comparison between the ANES and GPT-3 results, as they are both
basedonthesamevaluesforonehalfoftheCramer’sVcalculations.
43
ANES“Input” “Output”Variable ANESCramer’sV GPT-3Cramer’sV Difference
age church.goer 0.2 0.19 0.01
age discuss.politics 0.21 0.21 0
age race 0.21 0.2 0.01
age education 0.25 0.21 0.04
age gender 0.18 0.2 -0.02
age ideology 0.23 0.2 0.03
age patriotism 0.21 0.21 0
age pid7 0.22 0.21 0.01
age political.interest 0.22 0.21 0.01
age vote.2016 0.24 0.23 0.01
church.goer age 0.2 0.2 0
church.goer discuss.politics 0.01 0.14 -0.13
church.goer race 0.09 0.04 0.05
church.goer education 0.06 0.01 0.05
church.goer gender 0.04 0.02 0.02
church.goer ideology 0.28 0.12 0.16
church.goer patriotism 0.2 0.05 0.15
church.goer pid7 0.22 0.19 0.03
church.goer political.interest 0.04 0.08 -0.04
church.goer vote.2016 0.19 0.24 -0.05
discuss.politics age 0.21 0.22 -0.01
discuss.politics church.goer 0.01 0.18 -0.17
discuss.politics race 0.14 0.02 0.12
discuss.politics education 0.2 0.11 0.09
discuss.politics gender 0 0.08 -0.08
discuss.politics ideology 0.17 0.06 0.11
discuss.politics patriotism 0.03 0.1 -0.07
discuss.politics pid7 0.16 0.11 0.05
discuss.politics political.interest 0.4 0.28 0.12
discuss.politics vote.2016 0.11 0.2 -0.09
race age 0.21 0.2 0.01
race church.goer 0.09 0.07 0.02
race discuss.politics 0.14 0.05 0.09
race education 0.1 0.07 0.03
race gender 0.08 0.07 0.01
race ideology 0.12 0.1 0.02
race patriotism 0.17 0.08 0.09
race pid7 0.18 0.1 0.08
race political.interest 0.06 0.11 -0.05
race vote.2016 0.17 0.11 0.06
Table12: Cramer’sVvalues
44
ANES“Input” “Output”Variable ANESCramer’sV GPT-3Cramer’sV Difference
education age 0.25 0.23 0.02
education church.goer 0.06 0.05 0.01
education discuss.politics 0.2 0.07 0.13
education race 0.1 0.04 0.06
education gender 0.04 0.05 -0.01
education ideology 0.13 0.09 0.04
education patriotism 0.09 0.05 0.04
education pid7 0.11 0.08 0.03
education political.interest 0.12 0.07 0.05
education vote.2016 0.14 0.09 0.05
gender age 0.18 0.21 -0.03
gender church.goer 0.04 0.01 0.03
gender discuss.politics 0 0.01 -0.01
gender race 0.08 0.03 0.05
gender education 0.04 0.07 -0.03
gender ideology 0.13 0.14 -0.01
gender patriotism 0.06 0.07 -0.01
gender pid7 0.16 0.1 0.06
gender political.interest 0.12 0.04 0.08
gender vote.2016 0.09 0.11 -0.02
ideology age 0.23 0.2 0.03
ideology church.goer 0.28 0.07 0.21
ideology discuss.politics 0.17 0.08 0.09
ideology race 0.12 0.09 0.03
ideology education 0.13 0.1 0.03
ideology gender 0.13 0.12 0.01
ideology patriotism 0.22 0.15 0.07
ideology pid7 0.37 0.32 0.05
ideology political.interest 0.15 0.14 0.01
ideology vote.2016 0.4 0.28 0.12
patriotism age 0.21 0.18 0.03
patriotism church.goer 0.2 0.11 0.09
patriotism discuss.politics 0.03 0.08 -0.05
patriotism race 0.17 0.1 0.07
patriotism education 0.09 0.07 0.02
patriotism gender 0.06 0.09 -0.03
patriotism ideology 0.22 0.14 0.08
patriotism pid7 0.19 0.16 0.03
patriotism political.interest 0.08 0.17 -0.09
patriotism vote.2016 0.25 0.15 0.1
Table13: Cramer’sVvalues
45
ANES“Input” “Output”Variable ANESCramer’sV GPT-3Cramer’sV Difference
pid7 age 0.22 0.21 0.01
pid7 church.goer 0.22 0.07 0.15
pid7 discuss.politics 0.16 0.13 0.03
pid7 race 0.18 0.07 0.11
pid7 education 0.11 0.12 -0.01
pid7 gender 0.16 0.12 0.04
pid7 ideology 0.37 0.32 0.05
pid7 patriotism 0.19 0.15 0.04
pid7 political.interest 0.12 0.16 -0.04
pid7 vote.2016 0.48 0.37 0.11
political.interest age 0.22 0.2 0.02
political.interest church.goer 0.04 0.13 -0.09
political.interest discuss.politics 0.4 0.16 0.24
political.interest race 0.06 0.04 0.02
political.interest education 0.12 0.07 0.05
political.interest gender 0.12 0.11 0.01
political.interest ideology 0.15 0.1 0.05
political.interest patriotism 0.08 0.16 -0.08
political.interest pid7 0.12 0.12 0
political.interest vote.2016 0.12 0.12 0
vote.2016 age 0.24 0.23 0.01
vote.2016 church.goer 0.19 0.19 0
vote.2016 discuss.politics 0.11 0.23 -0.12
vote.2016 race 0.17 0.07 0.1
vote.2016 education 0.14 0.14 0
vote.2016 gender 0.09 0.19 -0.1
vote.2016 ideology 0.4 0.34 0.06
vote.2016 patriotism 0.25 0.16 0.09
vote.2016 pid7 0.48 0.37 0.11
vote.2016 political.interest 0.12 0.2 -0.08
Table14: Cramer’sVvalues
46
Variable ANES GPT-3
Age 4.7 2.8
AttendsChurch 0 0.4
DiscussesPolitics 0.1 14.6
Race 0.1 5.6
Education 14.3 1
Gender 0 1.2
Ideology 4.2 22.6
Patriotism 0.7 14.6
PartyID 3.6 0.5
PoliticalInterest 1.1 14.8
2016VoteandChoice 0.5 23.8
Table15: PercentofObservationsCodedasMissing
However,wecanalsoestimatetheCramer’sVbetweenthevarioussyntheticvectors,removingANESdata
fromtheGPT-3relationshipcalculationentirely. Figure11showsthesamedataforthe“Human”responses,
but replaces the Cramer’s V between ANES and GPT-3 that forms the “GPT-3” response in the main text
withaCramer’sVcalculationbasedentirelyonsyntheticdata. Eventhoughtheuseofsyntheticdatainboth
partsintroducesadditionalnoiseintheestimation,thepatternofCramer’sVcomparisonsishighlysimilar
tothatseenwhenANESinputsareused.
D.3.2 GPT-3TemperatureVariation
Additionally, when generating the GPT-3 results, the temperature setting can be varied. Temperature con-
trolstheamountofrandomvariationallowedinthetextsamplingprocessusedbyGPT-3. Inthemaintext,
weusetheindustrystandardtemperatureof.7. However, todemonstratethattheresultsarerobusttomul-
tiplesamplesusingdifferentsettings,wealsoprovideareplicationusingtemperaturesettingsof0.001and
1.0. Asettingof0.001meansthatinvirtuallyallcompletionsthealgorithmwillprovidetheresponsewith
the highest probability (meaning for a .49 to .51 split, all completions would return the token associated
with the .51 probability). A setting of 1.0 means that the probability of selecting any particular token is
equivalenttotheprobabilitydistribution(i.e. thereisnoadjustment).
Table 17 shows summary statistics of the difference in Cramer’s V between human and GPT-3 produced
responses based on varying temperature settings. In other words, the value for Cramer’s V produced with
humandataissubtractedfromthevalueforCramer’sVproducedwithGPT-3data. Theseresultsmirrorthe
main text, and are based on ANES ”inputs” and GPT-3 ”outputs.” Summary statistics are then calculated
based on the differences. We see that, of the three options, a temperature setting of .7 produces the lowest
differencebetweenHumanandGPT-3relationships. Thereareminimaldifferencesbetweenatemperature
of.7andatemperatureof1.0. Weraneachtemperaturequeryonce,anddidnotselectthepresentedmodels
forbestfitfromarangeofprobabilisticruns. Theresultsprovidesomeevidencethattherelationshippatterns
uncoveredbyGPT-3arerobusttovariationsinmodelspecification.
A temperature of 0.001 produces more error and also systematically overstates the relationship between
the human input and GPT-3 output variables. One caveat: at a temperature of 0.001, GPT-3 identified all
respondents as white. Without variation in this variable, we were unable to calcualte Cramer’s V, so race /
47
Variable Source N Mean St. Dev. Min Pctl(25) Pctl(75) Max
Age ANES 1,782 50.143 17.557 18 35 64 90
Age GPT-3 1,782 35.486 12.589 0 27 41 99
AttendsChurch ANES 1,782 0.583 0.493 0 0 1 1
AttendsChurch GPT-3 1,782 0.604 0.489 0 0 1 1
TalksPolitics ANES 1,782 0.864 0.343 0 1 1 1
TalksPolitics GPT-3 1,782 0.896 0.305 0 1 1 1
Ideology ANES 1,782 4.095 1.618 1 3 5 7
Ideology GPT-3 1,782 4.017 1.737 1 3 5 7
Patriotism ANES 1,782 1.963 1.299 1 1 2 7
Patriotism GPT-3 1,782 1.456 0.902 1 1 2 7
PartyID ANES 1,782 3.857 2.197 1 2 6 7
PartyID GPT-3 1,782 4.429 2.169 1 2 6 7
PoliticalInterest ANES 1,782 1.990 0.791 1 1 2 4
PoliticalInterest GPT-3 1,782 1.694 0.852 1 1 2 4
White ANES 1,782 0.803 0.398 0 1 1 1
White GPT-3 1,782 0.974 0.159 0 1 1 1
Hispanic ANES 1,782 0.089 0.285 0 0 0 1
Hispanic GPT-3 1,782 0.001 0.033 0 0 0 1
Asian ANES 1,782 0.030 0.171 0 0 0 1
Asian GPT-3 1,782 0.002 0.041 0 0 0 1
Black ANES 1,782 0.077 0.267 0 0 0 1
Black GPT-3 1,782 0.023 0.150 0 0 0 1
SomeCollege ANES 1,782 0.348 0.476 0 0 1 1
SomeCollege GPT-3 1,782 0.642 0.480 0 0 1 1
GraduateDegree ANES 1,782 0.196 0.397 0 0 0 1
GraduateDegree GPT-3 1,782 0.002 0.041 0 0 0 1
Bachelor’sDegree ANES 1,782 0.280 0.449 0 0 1 1
Bachelor’sDegree GPT-3 1,782 0.269 0.443 0 0 1 1
HighSchool ANES 1,782 0.176 0.381 0 0 0 1
HighSchool GPT-3 1,782 0.088 0.283 0 0 0 1
Male ANES 1,782 0.481 0.500 0 0 1 1
Male GPT-3 1,782 0.759 0.428 0 1 1 1
Votedin2016 ANES 1,782 0.871 0.335 0 1 1 1
Votedin2016 GPT-3 1,782 0.832 0.374 0 1 1 1
TrumpVoter ANES 1,553 0.438 0.496 0 0 1 1
TrumpVoter GPT-3 1,483 0.245 0.430 0 0 0 1
ClintonVoter ANES 1,553 0.484 0.500 0 0 1 1
ClintonVoter GPT-3 1,483 0.233 0.423 0 0 0 1
OtherVoter ANES 1,553 0.078 0.268 0 0 0 1
OtherVoter GPT-3 1,483 0.523 0.500 0 0 1 1
Table16: Study3DescriptiveStatisticsforANESandGPT-3Data
48
Race / Attends
Gender Age Education Patriotism
Ethnicity Church
Gender l l l l l
Race / l l l l l
Ethnicity
Age l l l l l
Education l l l l l
Attends l l l l l
Church
Patriotism l l l l l
Discusses l l l l l l
Politics
Political l l l l l l
Interest
2016 Vote l l l l l l
Ideology l l l l l l
Party ID l l l l l l
0.0 0.1 0.2 0.3 0.4
Discusses Political
2016 Vote Ideology Party ID
Politics Interest
Gender l l l l l
Race / l l l l l
Ethnicity
Age l l l l l
Education l l l l l
Attends l l l l l
Church
Patriotism l l l l l
Discusses l l l l
Politics
Political l l l l
Interest
2016 Vote l l l l
Ideology l l l l
Party ID l l l l
0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4
Cramer's V
)ecruoS
emaS
eht
morF(
elbairaV
dnuorgkcaB
Source
l GPT−3
Human
Figure11: Cramer’sVCorrelationsinANESvs. GPT-3Data,usingEntirelySyntheticGPT-3Calculations
SummaryStatistic Temp: 0.001 Temp: 0.7 Temp: 1.0
Mean 0.059 -0.026 -0.031
Minimum -0.123 -0.241 -0.250
Maximum 0.700 0.168 0.119
StandardDeviation 0.141 0.068 0.070
N 2518 1782 1022
Table17: AverageErrorinCramer’sVBasedonVaryingTemperatures
ethnicityisexcludedfromthecalculationsforatemperatureof0.001.
49
All Cramer’s V calculations use the set of cases that have non-missing data for all human and GPT-3 pro-
duced variables. The different temperature settings produce a different number of valid completed cases.
Lowertemperatureismoredeterministic,andsominimizesthenumberofinvalidtokensusedastextcom-
pletions. Higher temperatures sample from a range of tokens that includes more invalid responses. There-
fore,mid-rangetemperaturesappeartoproducethedesirablebalancebetweenvalidityandcompleteness.
E Cost Analysis
TheGPT-3andJurassicmodelsareavailableonlythroughapaidAPI.Intheinterestsoffulltransparency,
weherereportthecostsforStudies1,2and3. Weonlyreportcostsforthefinalruns,butnotethatadditional
runswereperformedaspartoftheexperimentalrhythm.
Study1required1,1471modelqueries(oneforeachhumansubject). Thebackstorieswererelativelysmall,
atanaverageof66tokens. Foreachquery,wegeneratedamaximumof128tokensfromthemodel. Atthe
standardrateof$0.06/1,000tokens,thisexperimentcostatotalof$29.
Study 2 consisted of 3 experiments, one for ANES 2012, 2016 and 2020. We ran one model query per
participant (5,914 in 20112, 4,270 in 2016 and 5,442 in 2020), for a total of 15,626 queries. Backstories
wereabitlongerthaninStudy1,atanaverageof80tokens,butweonlyneededtogenerateonetokenper
query,incurringatotalcostof$75.
Study 3 was more expensive. Because of the extended interview format, each prompt required an average
of 458 tokens. For each query, we generated a maximum of 5 tokens. We performed one query for each
ANESparticipant, foratotalof4,270queries, resultinginatotalperexperimentof$119. However, recall
that Study 3 involved 12 different experiments (systematically predicting one backstory element from the
others),andsothetotalcostofStudy3was$1,428.
WebrieflynotethatusingtheJurassicmodel(notavailableatthetimeofwriting)wouldhavereducedcosts
forallexperiments.
Acknowledgements
TheauthorsthankthedepartmentsofComputerScienceandPoliticalScienceatBrighamYoungUniversity
for support and for feedback on this manuscript. Additionally, Chris Bail, Sarah Shugars, Luwei Ying,
DiogoFerrari,andChristopherKarpowitzprovidedinvaluablefeedback.
References
[1] Pedro Rodriguez and Arthur Spirling. Word embeddings: What works, what doesn’t, and how to tell
thedifferenceforappliedresearch. 2021.
[2] KennethBenoit,KevinMunger,andArthurSpirling.Measuringandexplainingpoliticalsophistication
throughtextualcomplexity. AmericanJournalofPoliticalScience,63(2):491–508,2019.
[3] PabloBarbera´,AmberEBoydstun,SuzannaLinn,RyanMcMahon,andJonathanNagler. Automated
textclassificationofnewsarticles: Apracticalguide. PoliticalAnalysis,29(1):19–42,2021.
[4] Ludovic Rheault and Christopher Cochrane. Word embeddings for the analysis of ideological place-
mentinparliamentarycorpora. PoliticalAnalysis,28(1):112–133,2020.
50
[5] Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. Machine learning for social science:
Anagnosticapproach. AnnualReviewofPoliticalScience,24:395–419,2021.
[6] KevinTGreene,BaekkwanPark,andMichaelColaresi. Machinelearninghumanrightsandwrongs:
How the successes and failures of supervised learning algorithms can inform the debate about infor-
mationeffects. PoliticalAnalysis,27(2):223–230,2019.
[7] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
modelsareunsupervisedmultitasklearners. OpenAIBlog,1(8):9,2019.
[8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.
org/papers/v21/20-074.html.
[9] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-
shotlearners. arXiv:2005.14165,2020.
[10] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan,
ZiYang,ApoorvKulshreshtha,GauravNemade,YifengLu,etal. Towardsahuman-likeopen-domain
chatbot. arXivpreprintarXiv:2001.09977,2020.
[11] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
modelsareunsupervisedmultitasklearners. OpenAIBlog,1(8):9,2019.
[12] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860,2019.
[13] Trishan Panch, Heather Mattie, and Rifat Atun. Artificial intelligence and algorithmic bias: Implica-
tions for health systems. Journal of Global Health, 9(2):010318, December 2019. ISSN 2047-2978,
2047-2986. doi: 10.7189/jogh.09.020318.
[14] SandraG.Mayson. Biasin,Biasout. YaleLawJournal,128:2218,2018.
[15] SolonBarocasandAndrewD.Selbst. BigData’sDisparateImpact. CaliforniaLawReview,104:671,
2016.
[16] Seehttps://electionstudies.org/about-us/.
[17] Jacob E. Rothschild, Adam J. Howat, Richard M. Shafranek, and Ethan C. Busby. Pigeonholing
partisans: Stereotypes of party supporters and partisan polarization. Political Behavior, 41(2):423–
443,2019.
[18] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell. Onthedan-
gersofstochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACMConfer-
enceonFairness,Accountability,andTransparency,pages610–623,2021.
[19] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint
arXiv:2002.06177,2020.
51
[20] NikhilGarg,LondaSchiebinger,DanJurafsky,andJamesZou. Wordembeddingsquantify100years
ofgenderandethnicstereotypes. ProceedingsoftheNationalAcademyofSciences, 115(16):E3635–
E3644,2018.
[21] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from lan-
guagecorporacontainhuman-likebiases. Science,356(6334):183–186,2017.
[22] Bernard Berelson, Paul F. Lazarsfeld, and William N. McPhee. Voting: a study of opinion formation
inapresidentialcampaign. UniversityofChicagoPress,1954.
[23] Angus Campbell, Philip E. Converse, Warren E. Miller, and Donald E. Stokes. The American Voter.
UniversityofChicagoPress,1960.
[24] Vincent L. Hutchings and Nicholas A. Valentino. The centrality of race in american politics. Annual
ReviewofPoliticalScience,7(1):383–408,2004.
[25] Nancy Burns and Katherine Gallagher. Public opinion on gender issues: The politics of equity and
roles. AnnualReviewofPoliticalScience,13(1):425–443,2010.
[26] JamesN.DruckmanandArthurLupia. Preferencechangeincompetitivepoliticalenvironments. An-
nualReviewofPoliticalScience,19(1):13–31,2016.
[27] KatherineCramer.Understandingtheroleofracismincontemporaryuspublicopinion.AnnualReview
ofPoliticalScience,23(1):153–169,2020.
[28] Edward H. Simpson. The interpretation of interaction in contingency tables. Journal of the Royal
StatisticalSociety,Ser.B,13:238–241,1951.
[29] Shanto Iyengar, Gaurav Sood, and Yphtach Lelkes. Affect, not ideology a social identity perspective
onpolarization. PublicOpinionQuarterly,76(3):405–431,2012.
[30] LillianaMason. UncivilAgreement. UniversityofChicagoPress,2018.
[31] Alexander Coppock and Oliver A. McClellan. Validating the demographic, political, psychological,
andexperimentalresultsobtainedfromanewsourceofonlinesurveyrespondents.Research&Politics,
6(1):1–14,2019.
[32] JanetM.Box-Steffensmeier,SuzannaDeBoef,andTseminLin. Thedynamicsofthepartisangender
gap. AmericanPoliticalScienceReview,98(3):515–528,2004.
[33] Katherine Tate. From Protest to Politics: The New Black Voters in American Elections. Harvard
UniversityPress,1994.
[34] Katherine J. Cramer. The politics of resentment: Rural consciousness in Wisconsin and the rise of
ScottWalker. UniversityofChicagoPress,2016.
[35] AshleyJardina. WhiteIdentityPolitics. CambridgeUniversityPress,2019.
[36] BruceE.Keith,DavidB.Magleby,CandiceJ.Nelson,ElizabethOrr,andMarkC.Westyle. Themyth
oftheindependentvoter. UniversityofCaliforniaPress,1992.
[37] David B. Magleby, Candice J. Nelson, and Mark C. Westlye. The myth of the independent voter
revisited. In Paul M. Sniderman and Benjamin Highton, editors, Facing the challenge of democracy:
Explorations in the analysis of public opinion and political participation, pages 238–266. Princeton
UniversityPress,2011.
52
[38] SamaraKlarandYannaKrupnikov. IndependentPolitics: HowAmericanDisdainforPartiesLeadsto
PoliticalInaction. CambridgeUniversityPress,2016.
[39] HaraldCrame´r. MathematicalMethodsofStatistics. PrincetonUniversityPress,1946.
[40] RonaldSRoss. Guideforconductingriskassessments(nistsp-800-30rev1). TheNationalInstituteof
StandardsandTechnology(NIST),Gaithersburg,2012.
[41] Matthew J. Salganik. Bit by Bit: Social Research in the Digital Age. Princeton University Press,
Princeton,NJ,openrevieweditionedition,2017.
53
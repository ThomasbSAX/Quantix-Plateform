"""
DICTIONNAIRE COMPLET DES 257 FONCTIONS
=======================================
Structure: {nom_fonction: {params, docstring, code}}

Utilisation:
    from all_functions_dict import ALL_FUNCTIONS
    
    # Accéder à une fonction
    func = ALL_FUNCTIONS['auto_clean_all']
    print(func['params'])      # Liste des paramètres
    print(func['docstring'])   # Description
    print(func['code'])        # Code complet
"""

ALL_FUNCTIONS = {
    "add_holiday_feature": {
        "params": ['date_column', 'holidays', 'holiday_name'],
        "docstring": "Ajoute une colonne indiquant si la date correspond à un jour férié.\n\nArgs:\n    date_column: Nom de la colonne contenant les dates à vérifier.\n    holidays: Liste des dates (format YYYY-MM-DD) considérées comme jours fériés.\n    holiday_name: Nom de la colonne à créer (par défaut 'is_holiday').",
        "code": 'def add_holiday_feature(self, date_column: str, holidays: list, holiday_name: str = \'is_holiday\'):\n    """\n    Ajoute une colonne indiquant si la date correspond à un jour férié.\n\n    Args:\n        date_column: Nom de la colonne contenant les dates à vérifier.\n        holidays: Liste des dates (format YYYY-MM-DD) considérées comme jours fériés.\n        holiday_name: Nom de la colonne à créer (par défaut \'is_holiday\').\n    """\n    self._validate_df()\n    self._backup()\n\n    # Convertir les dates en datetime si ce n\'est pas déjà fait\n    self.df[date_column] = pd.to_datetime(self.df[date_column])\n\n    # Créer la colonne de jours fériés\n    self.df[holiday_name] = self.df[date_column].dt.date.astype(\'string\').isin(holidays)\n\n    self._log("add_holiday_feature", params={\n        \'date_column\': date_column,\n        \'holidays_count\': len(holidays),\n        \'holiday_name\': holiday_name\n    })\n    return self'
    },
    "anonymize_addresses": {
        "params": ['address_columns'],
        "docstring": "Anonymizes specified address columns by replacing values with a generic placeholder.\nIf no columns are specified, all columns containing 'address' in their name will be processed.\n\nParameters:\n    address_columns (list, optional): List of column names to anonymize. Defaults to None.",
        "code": 'def anonymize_addresses(self, address_columns=None):\n    """\n    Anonymizes specified address columns by replacing values with a generic placeholder.\n    If no columns are specified, all columns containing \'address\' in their name will be processed.\n\n    Parameters:\n        address_columns (list, optional): List of column names to anonymize. Defaults to None.\n    """\n    self._validate_df()\n    self._backup()\n\n    if address_columns is None:\n        address_columns = [col for col in self.df.columns if \'address\' in col.lower()]\n\n    for column in address_columns:\n        if column in self.df.columns:\n            self.df[column] = \'ANONYMIZED_ADDRESS\'\n\n    self._log("anonymize_addresses", params={\'address_columns\': address_columns})\n    return self'
    },
    "anonymize_emails": {
        "params": ['email_columns'],
        "docstring": "Anonymizes email addresses in specified columns by replacing the domain part with a generic placeholder.\nIf no columns are specified, all columns containing 'email' in their name will be processed.\n\nParameters:\n-----------\nemail_columns : list of str, optional\n    List of column names containing email addresses to anonymize. If None, all columns with 'email' in their name will be processed.",
        "code": 'def anonymize_emails(self, email_columns=None):\n    """\n    Anonymizes email addresses in specified columns by replacing the domain part with a generic placeholder.\n    If no columns are specified, all columns containing \'email\' in their name will be processed.\n\n    Parameters:\n    -----------\n    email_columns : list of str, optional\n        List of column names containing email addresses to anonymize. If None, all columns with \'email\' in their name will be processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if email_columns is None:\n        email_columns = [col for col in self.df.columns if \'email\' in col.lower()]\n\n    for column in email_columns:\n        if column in self.df.columns:\n            self.df[column] = self.df[column].apply(lambda x: f"user{hash(x) % 1000}@example.com" if isinstance(x, str) else x)\n\n    self._log("anonymize_emails", params={\'email_columns\': email_columns})\n    return self'
    },
    "anonymize_names": {
        "params": ['name_column', 'prefix'],
        "docstring": 'Anonymizes names in the specified column by replacing them with a generic prefix followed by an index.',
        "code": 'def anonymize_names(self, name_column: str = \'name\', prefix: str = \'user_\'):\n    """\n    Anonymizes names in the specified column by replacing them with a generic prefix followed by an index.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Check if the column exists\n    if name_column not in self.df.columns:\n        raise ValueError(f"Column \'{name_column}\' does not exist in the DataFrame")\n\n    # Create anonymized names\n    self.df[name_column] = [f"{prefix}{i}" for i in range(1, len(self.df) + 1)]\n\n    self._log("anonymize_names", params={\'name_column\': name_column, \'prefix\': prefix})\n    return self'
    },
    "anonymize_phone_numbers": {
        "params": ['column_name'],
        "docstring": 'Anonymizes phone numbers in the specified column by replacing digits with a fixed pattern.\nKeeps the original format (e.g., length, separators) but obscures actual numbers.',
        "code": 'def anonymize_phone_numbers(self, column_name):\n    """\n    Anonymizes phone numbers in the specified column by replacing digits with a fixed pattern.\n    Keeps the original format (e.g., length, separators) but obscures actual numbers.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"Column \'{column_name}\' does not exist in the DataFrame")\n\n    def anonymize_number(phone):\n        if pd.isna(phone) or not isinstance(phone, str):\n            return phone\n        # Replace all digits with \'X\' while preserving non-digit characters\n        return re.sub(r\'\\d\', \'X\', phone)\n\n    self.df[column_name] = self.df[column_name].apply(anonymize_number)\n\n    self._log("anonymize_phone_numbers", params={\'column_name\': column_name})\n    return self'
    },
    "auto_clean_all": {
        "params": ['df'],
        "docstring": 'Nettoie et prépare automatiquement les données en appliquant une série de transformations standard.',
        "code": 'def auto_clean_all(self, df=None):\n    """\n    Nettoie et prépare automatiquement les données en appliquant une série de transformations standard.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is not None:\n        self.df = df.copy()\n\n    # Suppression des doublons\n    self.df.drop_duplicates(inplace=True)\n\n    # Conversion des colonnes de date si détectées\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\':\n            try:\n                self.df[col] = pd.to_datetime(self.df[col])\n            except (ValueError, TypeError):\n                pass\n\n    # Nettoyage des valeurs manquantes\n    self.df.fillna({\n        \'numeric\': 0,\n        \'object\': \'\',\n        \'datetime\': pd.NaT\n    }, inplace=True)\n\n    # Suppression des colonnes vides ou constantes\n    self.df.drop(columns=self.df.columns[(self.df.nunique() == 1) | (self.df.isnull().all())], inplace=True)\n\n    self._log("auto_clean_all", params={})\n    return self'
    },
    "auto_detect_all_issues": {
        "params": ['df'],
        "docstring": 'Auto-detects and logs all potential data issues in the DataFrame.\nIssues include missing values, duplicate rows, inconsistent data types,\nand outliers in numerical columns.\n\nParameters:\ndf (pd.DataFrame, optional): DataFrame to analyze. If None, uses self.df.\n\nReturns:\nDataCleaner: Returns self for method chaining.',
        "code": 'def auto_detect_all_issues(self, df=None):\n    """\n    Auto-detects and logs all potential data issues in the DataFrame.\n    Issues include missing values, duplicate rows, inconsistent data types,\n    and outliers in numerical columns.\n\n    Parameters:\n    df (pd.DataFrame, optional): DataFrame to analyze. If None, uses self.df.\n\n    Returns:\n    DataCleaner: Returns self for method chaining.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    issues = {}\n\n    # Detect missing values\n    missing_values = df.isnull().sum()\n    if (missing_values > 0).any():\n        issues[\'missing_values\'] = missing_values[missing_values > 0].to_dict()\n\n    # Detect duplicate rows\n    duplicates = df.duplicated().sum()\n    if duplicates > 0:\n        issues[\'duplicate_rows\'] = duplicates\n\n    # Detect inconsistent data types\n    for col in df.columns:\n        if df[col].dtype == \'object\':\n            unique_values = df[col].nunique()\n            if unique_values > 10 and len(df) > 100:\n                sample = df[col].value_counts().head(5).to_dict()\n                issues[f\'inconsistent_dtype_{col}\'] = {\n                    \'dtype\': df[col].dtype,\n                    \'unique_values\': unique_values,\n                    \'sample_values\': sample\n                }\n\n    # Detect outliers in numerical columns\n    numerical_cols = df.select_dtypes(include=[\'number\']).columns\n    for col in numerical_cols:\n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        if len(outliers) > 0:\n            issues[f\'outliers_{col}\'] = {\n                \'count\': len(outliers),\n                \'lower_bound\': lower_bound,\n                \'upper_bound\': upper_bound\n            }\n\n    self.issues = issues\n\n    self._log("auto_detect_all_issues", params={})\n    return self'
    },
    "auto_fix_all_issues": {
        "params": ['df'],
        "docstring": 'Nettoie et prépare automatiquement les données en appliquant une série de transformations standard.',
        "code": 'def auto_fix_all_issues(self, df=None):\n    """\n    Nettoie et prépare automatiquement les données en appliquant une série de transformations standard.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is not None:\n        self.df = df.copy()\n\n    # Suppression des doublons\n    self.df.drop_duplicates(inplace=True)\n\n    # Conversion des types de données\n    for col in self.df.select_dtypes(include=[\'object\']).columns:\n        try:\n            self.df[col] = pd.to_numeric(self.df[col], errors=\'ignore\')\n        except:\n            pass\n\n    # Gestion des valeurs manquantes\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\':\n            self.df[col].fillna(\'Unknown\', inplace=True)\n        else:\n            self.df[col].fillna(self.df[col].mean(), inplace=True)\n\n    # Nettoyage des espaces blancs\n    self.df = self.df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n\n    self._log("auto_fix_all_issues", params={})\n    return self'
    },
    "auto_generate_documentation": {
        "params": ['output_path'],
        "docstring": 'Génère automatiquement la documentation des données nettoyées.',
        "code": 'def auto_generate_documentation(self, output_path=None):\n    """\n    Génère automatiquement la documentation des données nettoyées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # code principal\n    doc_content = f"# Documentation des données nettoyées\\n\\n"\n    doc_content += f"## Informations générales\\n"\n    doc_content += f"- Nombre de lignes: {len(self.df)}\\n"\n    doc_content += f"- Nombre de colonnes: {len(self.df.columns)}\\n\\n"\n\n    doc_content += f"## Description des colonnes\\n"\n    for col in self.df.columns:\n        doc_content += f"### {col}\\n"\n        doc_content += f"- Type: {self.df[col].dtype}\\n"\n        doc_content += f"- Valeurs uniques: {self.df[col].nunique()}\\n"\n        doc_content += f"- Valeurs manquantes: {self.df[col].isna().sum()}\\n\\n"\n\n    if output_path:\n        with open(output_path, \'w\') as f:\n            f.write(doc_content)\n\n    self._log("auto_generate_documentation", params={\'output_path\': output_path})\n    return self'
    },
    "auto_merge_columns": {
        "params": ['threshold', 'similarity_metric', 'drop_original'],
        "docstring": "Fusionne automatiquement les colonnes similaires en fonction d'un seuil de similarité.",
        "code": 'def auto_merge_columns(self, threshold=0.8, similarity_metric=\'jaccard\', drop_original=False):\n    """\n    Fusionne automatiquement les colonnes similaires en fonction d\'un seuil de similarité.\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity\n    import numpy as np\n\n    # Calculer la similarité entre colonnes\n    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split())\n    column_names = self.df.columns\n    tfidf_matrix = vectorizer.fit_transform(column_names)\n    similarity_matrix = cosine_similarity(tfidf_matrix)\n\n    # Identifier les paires de colonnes similaires\n    similar_pairs = []\n    for i in range(len(column_names)):\n        for j in range(i+1, len(column_names)):\n            if similarity_matrix[i][j] >= threshold:\n                similar_pairs.append((i, j))\n\n    # Fusionner les colonnes similaires\n    for i, j in similar_pairs:\n        col_i = self.df.iloc[:, i]\n        col_j = self.df.iloc[:, j]\n\n        # Fusionner en fonction de la métrique choisie\n        if similarity_metric == \'jaccard\':\n            merged = col_i.combine_first(col_j)\n        else:\n            merged = col_i.fillna(col_j)\n\n        # Remplacer les colonnes originales si demandé\n        if drop_original:\n            self.df.iloc[:, i] = merged\n            self.df.drop(columns=self.df.columns[j], inplace=True)\n        else:\n            new_col_name = f"{column_names[i]}_merged_{column_names[j]}"\n            self.df.insert(j, new_col_name, merged)\n\n    self._log("auto_merge_columns", params={\'threshold\': threshold, \'similarity_metric\': similarity_metric, \'drop_original\': drop_original})\n    return self'
    },
    "auto_normalize_categories": {
        "params": ['threshold'],
        "docstring": "Normalise les catégories en regroupant celles qui apparaissent moins fréquemment qu'un seuil donné.\nLes catégories rares sont remplacées par une valeur générique 'Other'.",
        "code": 'def auto_normalize_categories(self, threshold=0.1):\n    """\n    Normalise les catégories en regroupant celles qui apparaissent moins fréquemment qu\'un seuil donné.\n    Les catégories rares sont remplacées par une valeur générique \'Other\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    for column in self.df.select_dtypes(include=[\'object\']).columns:\n        value_counts = self.df[column].value_counts(normalize=True)\n        mask = value_counts < threshold\n        rare_categories = value_counts[mask].index.tolist()\n\n        if rare_categories:\n            self.df[column] = self.df[column].apply(\n                lambda x: \'Other\' if x in rare_categories else x\n            )\n\n    self._log("auto_normalize_categories", params={\'threshold\': threshold})\n    return self'
    },
    "auto_normalize_dates": {
        "params": ['date_columns'],
        "docstring": 'Normalise les dates dans le DataFrame en convertissant toutes les colonnes de date spécifiées\nau format datetime et en standardisant leur format.',
        "code": 'def auto_normalize_dates(self, date_columns=None):\n    """\n    Normalise les dates dans le DataFrame en convertissant toutes les colonnes de date spécifiées\n    au format datetime et en standardisant leur format.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        # Identifier automatiquement les colonnes de type date si aucune n\'est spécifiée\n        date_columns = self.df.select_dtypes(include=[\'datetime\', \'object\']).columns.tolist()\n\n    for col in date_columns:\n        try:\n            # Convertir la colonne en datetime\n            self.df[col] = pd.to_datetime(self.df[col], errors=\'coerce\')\n            # Standardiser le format de date (ex: YYYY-MM-DD)\n            self.df[col] = self.df[col].dt.strftime(\'%Y-%m-%d\')\n        except Exception as e:\n            self._log(f"Erreur lors de la normalisation de la colonne {col}: {str(e)}")\n\n    self._log("auto_normalize_dates", params={\'date_columns\': date_columns})\n    return self'
    },
    "auto_normalize_numbers": {
        "params": ['columns'],
        "docstring": "Normalise automatiquement les colonnes numériques en appliquant une normalisation min-max.\nLes valeurs sont mises à l'échelle entre 0 et 1 pour chaque colonne spécifiée ou toutes les colonnes numériques si aucune n'est précisée.",
        "code": 'def auto_normalize_numbers(self, columns=None):\n    """\n    Normalise automatiquement les colonnes numériques en appliquant une normalisation min-max.\n    Les valeurs sont mises à l\'échelle entre 0 et 1 pour chaque colonne spécifiée ou toutes les colonnes numériques si aucune n\'est précisée.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Déterminer les colonnes à normaliser\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    # Appliquer la normalisation min-max\n    for col in numeric_cols:\n        if self.df[col].min() != self.df[col].max():  # Éviter la division par zéro\n            self.df[col] = (self.df[col] - self.df[col].min()) / (self.df[col].max() - self.df[col].min())\n\n    self._log("auto_normalize_numbers", params={"columns": columns})\n    return self'
    },
    "auto_pipeline": {
        "params": ['df'],
        "docstring": 'Nettoie et prépare automatiquement les données en appliquant une série de transformations standards.',
        "code": 'def auto_pipeline(self, df=None):\n    """\n    Nettoie et prépare automatiquement les données en appliquant une série de transformations standards.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des doublons\n    df = df.drop_duplicates()\n\n    # Gestion des valeurs manquantes\n    for col in df.select_dtypes(include=[\'number\']).columns:\n        df[col] = df[col].fillna(df[col].median())\n    for col in df.select_dtypes(include=[\'object\']).columns:\n        df[col] = df[col].fillna(\'Unknown\')\n\n    # Conversion des types de données\n    for col in df.select_dtypes(include=[\'object\']).columns:\n        if df[col].nunique() < 10:\n            df[col] = df[col].astype(\'category\')\n\n    # Standardisation des noms de colonnes\n    df.columns = df.columns.str.lower().str.replace(\' \', \'_\')\n\n    self._log("auto_pipeline", params={})\n    return self'
    },
    "auto_prepare_for_ml": {
        "params": ['target_column', 'drop_columns', 'fillna_strategy', 'scale'],
        "docstring": "Automatise la préparation des données pour le machine learning.\nGère les valeurs manquantes, l'encodage des variables catégorielles et la normalisation.",
        "code": 'def auto_prepare_for_ml(self, target_column=None, drop_columns=None, fillna_strategy=\'mean\', scale=True):\n    """\n    Automatise la préparation des données pour le machine learning.\n    Gère les valeurs manquantes, l\'encodage des variables catégorielles et la normalisation.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Gestion des valeurs manquantes\n    if fillna_strategy == \'mean\':\n        self.df.fillna(self.df.mean(), inplace=True)\n    elif fillna_strategy == \'median\':\n        self.df.fillna(self.df.median(), inplace=True)\n    elif fillna_strategy == \'mode\':\n        self.df.fillna(self.df.mode().iloc[0], inplace=True)\n    elif fillna_strategy == \'drop\':\n        self.df.dropna(inplace=True)\n\n    # Encodage des variables catégorielles\n    categorical_cols = self.df.select_dtypes(include=[\'object\', \'category\']).columns\n    if not categorical_cols.empty:\n        self.df = pd.get_dummies(self.df, columns=categorical_cols)\n\n    # Suppression de colonnes si spécifié\n    if drop_columns:\n        self.df.drop(columns=drop_columns, inplace=True)\n\n    # Normalisation des données\n    if scale:\n        numeric_cols = self.df.select_dtypes(include=[\'int64\', \'float64\']).columns\n        if not numeric_cols.empty:\n            scaler = StandardScaler()\n            self.df[numeric_cols] = scaler.fit_transform(self.df[numeric_cols])\n\n    # Séparation des features et target si colonne cible spécifiée\n    if target_column:\n        self.X = self.df.drop(columns=[target_column])\n        self.y = self.df[target_column]\n    else:\n        self.X = self.df\n        self.y = None\n\n    self._log("auto_prepare_for_ml", params={\n        \'target_column\': target_column,\n        \'drop_columns\': drop_columns,\n        \'fillna_strategy\': fillna_strategy,\n        \'scale\': scale\n    })\n    return self'
    },
    "auto_prepare_for_stats": {
        "params": ['drop_na_threshold', 'fill_strategy', 'categorical_threshold'],
        "docstring": "Automatise la préparation des données pour l'analyse statistique.\nSupprime les colonnes avec trop de valeurs manquantes, remplit les valeurs manquantes,\net convertit les variables catégorielles si nécessaire.",
        "code": 'def auto_prepare_for_stats(self, drop_na_threshold=0.7, fill_strategy=\'mean\', categorical_threshold=10):\n    """\n    Automatise la préparation des données pour l\'analyse statistique.\n    Supprime les colonnes avec trop de valeurs manquantes, remplit les valeurs manquantes,\n    et convertit les variables catégorielles si nécessaire.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des colonnes avec trop de valeurs manquantes\n    na_percent = self.df.isna().mean()\n    cols_to_drop = na_percent[na_percent > drop_na_threshold].index\n    self.df.drop(columns=cols_to_drop, inplace=True)\n\n    # Remplissage des valeurs manquantes\n    for col in self.df.columns:\n        if self.df[col].dtype in [\'int64\', \'float64\']:\n            if fill_strategy == \'mean\':\n                self.df[col].fillna(self.df[col].mean(), inplace=True)\n            elif fill_strategy == \'median\':\n                self.df[col].fillna(self.df[col].median(), inplace=True)\n        elif self.df[col].dtype == \'object\':\n            mode_val = self.df[col].mode()[0]\n            self.df[col].fillna(mode_val, inplace=True)\n\n    # Conversion des variables catégorielles\n    for col in self.df.select_dtypes(include=[\'object\']).columns:\n        if len(self.df[col].unique()) <= categorical_threshold:\n            self.df[col] = self.df[col].astype(\'category\')\n\n    self._log("auto_prepare_for_stats", params={\n        \'drop_na_threshold\': drop_na_threshold,\n        \'fill_strategy\': fill_strategy,\n        \'categorical_threshold\': categorical_threshold\n    })\n    return self'
    },
    "auto_split_columns": {
        "params": ['delimiter', 'max_splits'],
        "docstring": 'Divise automatiquement les colonnes contenant des valeurs séparées par un délimiteur.\nPour chaque colonne, si une valeur contient le délimiteur, la colonne est divisée en plusieurs colonnes.',
        "code": 'def auto_split_columns(self, delimiter=\' \', max_splits=-1):\n    """\n    Divise automatiquement les colonnes contenant des valeurs séparées par un délimiteur.\n    Pour chaque colonne, si une valeur contient le délimiteur, la colonne est divisée en plusieurs colonnes.\n    """\n    self._validate_df()\n    self._backup()\n\n    for col in self.df.columns:\n        if self.df[col].dtype == object:  # Vérifie si la colonne est de type texte\n            sample_values = self.df[col].dropna().sample(min(10, len(self.df)), random_state=42)\n            if any(delimiter in str(val) for val in sample_values):\n                splits = self.df[col].str.split(delimiter, expand=True, n=max_splits)\n                for i in range(len(splits.columns)):\n                    new_col = f"{col}_{i}"\n                    self.df[new_col] = splits[i]\n                self.df.drop(col, axis=1, inplace=True)\n\n    self._log("auto_split_columns", params={\'delimiter\': delimiter, \'max_splits\': max_splits})\n    return self'
    },
    "auto_standardize_dataset": {
        "params": ['columns'],
        "docstring": 'Standardize numeric columns in the dataset by scaling them to have zero mean and unit variance.\nIf no columns are specified, all numeric columns are standardized.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    List of column names to standardize. If None, all numeric columns are standardized.',
        "code": 'def auto_standardize_dataset(self, columns=None):\n    """\n    Standardize numeric columns in the dataset by scaling them to have zero mean and unit variance.\n    If no columns are specified, all numeric columns are standardized.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        List of column names to standardize. If None, all numeric columns are standardized.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col])]\n\n    for col in numeric_cols:\n        mean = self.df[col].mean()\n        std = self.df[col].std()\n        if std != 0:\n            self.df[col] = (self.df[col] - mean) / std\n\n    self._log("auto_standardize_dataset", params={\'columns\': columns})\n    return self'
    },
    "auto_validate_quality": {
        "params": ['threshold'],
        "docstring": "Valide automatiquement la qualité des données en fonction d'un seuil.\nSupprime les colonnes avec un taux de valeurs manquantes supérieur au seuil.",
        "code": 'def auto_validate_quality(self, threshold=0.7):\n    """\n    Valide automatiquement la qualité des données en fonction d\'un seuil.\n    Supprime les colonnes avec un taux de valeurs manquantes supérieur au seuil.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le taux de valeurs manquantes pour chaque colonne\n    missing_percent = self.df.isnull().mean()\n\n    # Identifier les colonnes à supprimer\n    cols_to_drop = missing_percent[missing_percent > threshold].index\n\n    # Supprimer les colonnes\n    self.df.drop(columns=cols_to_drop, inplace=True)\n\n    # Loguer l\'opération\n    self._log("auto_validate_quality", params={"threshold": threshold})\n    return self'
    },
    "bin_equal_frequency": {
        "params": ['column_name', 'num_bins'],
        "docstring": 'Binarise une colonne en utilisant un découpage par fréquence égale.\nCrée de nouvelles colonnes pour chaque intervalle avec des valeurs binaires (0 ou 1).',
        "code": 'def bin_equal_frequency(self, column_name: str, num_bins: int):\n    """\n    Binarise une colonne en utilisant un découpage par fréquence égale.\n    Crée de nouvelles colonnes pour chaque intervalle avec des valeurs binaires (0 ou 1).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des paramètres\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne {column_name} n\'existe pas dans le DataFrame")\n    if num_bins <= 0:\n        raise ValueError("Le nombre de bins doit être supérieur à zéro")\n\n    # Calcul des bins par fréquence égale\n    series = self.df[column_name].dropna()\n    n = len(series)\n    bin_indices = np.linspace(0, n, num_bins + 1).astype(int)\n    bins = [series.iloc[bin_indices[i]:bin_indices[i+1]].min()\n            for i in range(num_bins)]\n\n    # Création des nouvelles colonnes\n    labels = [f"{column_name}_bin_{i}" for i in range(num_bins)]\n    self.df[labels] = pd.cut(series, bins=bins, labels=False, include_lowest=True)\n\n    # Conversion en binaire\n    for i in range(num_bins):\n        self.df[labels[i]] = (self.df[labels[i]] == i).astype(int)\n\n    self._log("bin_equal_frequency", params={\n        \'column_name\': column_name,\n        \'num_bins\': num_bins\n    })\n    return self'
    },
    "bin_equal_width": {
        "params": ['column', 'nbins', 'prefix'],
        "docstring": 'Applique un binning par largeur égale sur une colonne numérique.\nCrée de nouvelles colonnes avec les bornes des bins et la catégorie correspondante.\n\nParameters\n----------\ncolumn : str\n    Nom de la colonne à binner.\nnbins : int\n    Nombre de bins souhaités.\nprefix : str, optional\n    Préfixe pour les nouvelles colonnes (par défaut: nom de la colonne).',
        "code": 'def bin_equal_width(self, column: str, nbins: int, prefix: str = None):\n    """\n    Applique un binning par largeur égale sur une colonne numérique.\n    Crée de nouvelles colonnes avec les bornes des bins et la catégorie correspondante.\n\n    Parameters\n    ----------\n    column : str\n        Nom de la colonne à binner.\n    nbins : int\n        Nombre de bins souhaités.\n    prefix : str, optional\n        Préfixe pour les nouvelles colonnes (par défaut: nom de la colonne).\n    """\n    self._validate_df()\n    self._backup()\n\n    if prefix is None:\n        prefix = column\n\n    # Calcul des bornes des bins\n    min_val, max_val = self.df[column].min(), self.df[column].max()\n    bin_edges = np.linspace(min_val, max_val, nbins + 1)\n\n    # Création des labels de bins\n    bin_labels = [f"{prefix}_{i+1}" for i in range(nbins)]\n\n    # Application du binning\n    self.df[f"{prefix}_bin"] = pd.cut(self.df[column], bins=bin_edges, labels=bin_labels)\n    self.df[f"{prefix}_lower"] = pd.cut(self.df[column], bins=bin_edges).apply(lambda x: x.left)\n    self.df[f"{prefix}_upper"] = pd.cut(self.df[column], bins=bin_edges).apply(lambda x: x.right)\n\n    self._log("bin_equal_width", params={"column": column, "nbins": nbins, "prefix": prefix})\n    return self'
    },
    "bin_optimal_mdlp": {
        "params": ['target_column', 'threshold'],
        "docstring": "Applique le binning optimal selon la méthode MDLP (Minimum Description Length Principle)\nsur les colonnes numériques du DataFrame.\n\nParameters:\n-----------\ntarget_column : str, optional\n    Colonne cible pour le binning (par défaut None)\nthreshold : float, optional\n    Seuil minimal de gain d'information pour créer un bin (par défaut 0.1)",
        "code": 'def bin_optimal_mdlp(self, target_column=None, threshold=0.1):\n    """\n    Applique le binning optimal selon la méthode MDLP (Minimum Description Length Principle)\n    sur les colonnes numériques du DataFrame.\n\n    Parameters:\n    -----------\n    target_column : str, optional\n        Colonne cible pour le binning (par défaut None)\n    threshold : float, optional\n        Seuil minimal de gain d\'information pour créer un bin (par défaut 0.1)\n    """\n    self._validate_df()\n    self._backup()\n\n    from mdlp import MDLPDiscretizer\n\n    # Sélection des colonnes numériques\n    numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n\n    # Application de MDLP sur chaque colonne numérique\n    for col in numeric_cols:\n        if target_column and col != target_column:\n            continue\n\n        discretizer = MDLPDiscretizer(binning_strategy=\'mdlp\', threshold=threshold)\n        self.df[f\'{col}_binned\'] = discretizer.fit_transform(self.df[[col]])\n\n    self._log("bin_optimal_mdlp", params={\'target_column\': target_column, \'threshold\': threshold})\n    return self'
    },
    "binary_encode": {
        "params": ['columns'],
        "docstring": 'Encodes specified columns as binary (0/1) based on non-zero values.\nIf no columns are specified, encodes all numeric columns.\n\nParameters:\n-----------\ncolumns : list of str or None\n    List of column names to encode. If None, encodes all numeric columns.',
        "code": 'def binary_encode(self, columns=None):\n    """\n    Encodes specified columns as binary (0/1) based on non-zero values.\n    If no columns are specified, encodes all numeric columns.\n\n    Parameters:\n    -----------\n    columns : list of str or None\n        List of column names to encode. If None, encodes all numeric columns.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        if col in self.df.columns:\n            self.df[col] = (self.df[col] != 0).astype(int)\n\n    self._log("binary_encode", params={\'columns\': columns})\n    return self'
    },
    "boxcox_transform": {
        "params": ['columns'],
        "docstring": "Applique une transformation Box-Cox aux colonnes spécifiées.\nLa transformation Box-Cox est utilisée pour rendre les données plus proches d'une distribution normale.\nLes colonnes doivent contenir des valeurs strictement positives.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des noms de colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.",
        "code": 'def boxcox_transform(self, columns=None):\n    """\n    Applique une transformation Box-Cox aux colonnes spécifiées.\n    La transformation Box-Cox est utilisée pour rendre les données plus proches d\'une distribution normale.\n    Les colonnes doivent contenir des valeurs strictement positives.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des noms de colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.\n    """\n    self._validate_df()\n    self._backup()\n\n    from scipy import stats\n    import numpy as np\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        if (self.df[col] <= 0).any():\n            raise ValueError(f"La colonne {col} contient des valeurs non strictement positives. Box-Cox nécessite des données > 0.")\n\n        transformed, _ = stats.boxcox(self.df[col] + 1e-10)  # Ajout d\'un petit epsilon pour éviter les zéros\n        self.df[col] = transformed\n\n    self._log("boxcox_transform", params={"columns": columns})\n    return self'
    },
    "ceil_datetime": {
        "params": ['column_name', 'freq'],
        "docstring": "Arrondit les valeurs datetime d'une colonne au multiple de fréquence spécifié vers le haut.\nPar exemple, pour freq='H', les valeurs seront arrondies à l'heure supérieure.\n\nArgs:\n    column_name (str): Nom de la colonne contenant les valeurs datetime à arrondir\n    freq (str, optional): Fréquence d'arrondi. Peut être 'S', 'T', 'H', 'D', etc.\n                         Defaults to 'H' (heure).",
        "code": 'def ceil_datetime(self, column_name: str, freq: str = \'H\'):\n    """\n    Arrondit les valeurs datetime d\'une colonne au multiple de fréquence spécifié vers le haut.\n    Par exemple, pour freq=\'H\', les valeurs seront arrondies à l\'heure supérieure.\n\n    Args:\n        column_name (str): Nom de la colonne contenant les valeurs datetime à arrondir\n        freq (str, optional): Fréquence d\'arrondi. Peut être \'S\', \'T\', \'H\', \'D\', etc.\n                             Defaults to \'H\' (heure).\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne {column_name} n\'existe pas dans le DataFrame")\n\n    if not pd.api.types.is_datetime64_any_dtype(self.df[column_name]):\n        raise TypeError(f"La colonne {column_name} doit être de type datetime")\n\n    self.df[column_name] = self.df[column_name].dt.ceil(freq)\n\n    self._log("ceil_datetime", params={\'column_name\': column_name, \'freq\': freq})\n    return self'
    },
    "clean_html": {
        "params": ['columns'],
        "docstring": 'Supprime les balises HTML des colonnes spécifiées ou de toutes les colonnes du DataFrame.',
        "code": 'def clean_html(self, columns=None):\n    """\n    Supprime les balises HTML des colonnes spécifiées ou de toutes les colonnes du DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        if self.df[col].dtype == "object":\n            self.df[col] = self.df[col].str.replace(r\'<[^>]+>\', \'\', regex=True)\n\n    self._log("clean_html", params={"columns": columns})\n    return self'
    },
    "clean_markdown": {
        "params": ['column_name'],
        "docstring": 'Nettoie le contenu markdown dans une colonne spécifiée en supprimant les balises et formattages.',
        "code": 'def clean_markdown(self, column_name):\n    """\n    Nettoie le contenu markdown dans une colonne spécifiée en supprimant les balises et formattages.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame.")\n\n    # Suppression des balises markdown\n    self.df[column_name] = self.df[column_name].replace(r\'(\\#|\\*|_|\\`|\\~|\\>|\\[.*?\\]\\(.*?\\)|!\\[.*?\\]\\(.*?\\))\', \'\', regex=True)\n\n    # Suppression des sauts de ligne et espaces multiples\n    self.df[column_name] = self.df[column_name].str.replace(r\'\\n+\', \' \', regex=True)\n    self.df[column_name] = self.df[column_name].str.replace(r\'\\s+\', \' \', regex=True)\n\n    # Suppression des espaces en début et fin de chaîne\n    self.df[column_name] = self.df[column_name].str.strip()\n\n    self._log("clean_markdown", params={\'column_name\': column_name})\n    return self'
    },
    "clip_outliers": {
        "params": ['columns', 'lower_quantile', 'upper_quantile'],
        "docstring": 'Clip outliers in specified columns using quantile-based thresholds.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to process. If None, all numeric columns are processed.\nlower_quantile : float, optional\n    Lower quantile threshold (default: 0.05)\nupper_quantile : float, optional\n    Upper quantile threshold (default: 0.95)',
        "code": 'def clip_outliers(self, columns=None, lower_quantile=0.05, upper_quantile=0.95):\n    """\n    Clip outliers in specified columns using quantile-based thresholds.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to process. If None, all numeric columns are processed.\n    lower_quantile : float, optional\n        Lower quantile threshold (default: 0.05)\n    upper_quantile : float, optional\n        Upper quantile threshold (default: 0.95)\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if col in self.df.columns:\n            lower = self.df[col].quantile(lower_quantile)\n            upper = self.df[col].quantile(upper_quantile)\n            self.df[col] = self.df[col].clip(lower=lower, upper=upper)\n\n    self._log("clip_outliers", params={\'columns\': columns,\n                                      \'lower_quantile\': lower_quantile,\n                                      \'upper_quantile\': upper_quantile})\n    return self'
    },
    "cluster_categories": {
        "params": ['category_columns'],
        "docstring": "Regroupe les catégories rares dans une colonne 'Autres' pour chaque colonne catégorielle spécifiée.\nSi aucune colonne n'est spécifiée, toutes les colonnes catégorielles sont traitées.",
        "code": 'def cluster_categories(self, category_columns=None):\n    """\n    Regroupe les catégories rares dans une colonne \'Autres\' pour chaque colonne catégorielle spécifiée.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes catégorielles sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Identifier les colonnes catégorielles si aucune n\'est spécifiée\n    if category_columns is None:\n        category_columns = self.df.select_dtypes(include=[\'object\', \'category\']).columns\n\n    # Pour chaque colonne catégorielle\n    for col in category_columns:\n        if col in self.df.columns:\n            # Calculer la fréquence de chaque catégorie\n            freq = self.df[col].value_counts(normalize=True)\n            # Définir le seuil pour regrouper les catégories rares (par exemple, 5%)\n            threshold = 0.05\n            # Créer une liste des catégories à regrouper\n            rare_categories = freq[freq <= threshold].index.tolist()\n            # Remplacer les catégories rares par \'Autres\'\n            self.df[col] = self.df[col].apply(lambda x: \'Autres\' if x in rare_categories else x)\n\n    self._log("cluster_categories", params={\'category_columns\': category_columns})\n    return self'
    },
    "cluster_columns": {
        "params": ['columns_to_cluster'],
        "docstring": 'Cluster les colonnes similaires dans le DataFrame en fonction de leur contenu.\nLes colonnes sont regroupées si elles ont des valeurs identiques ou très similaires.',
        "code": 'def cluster_columns(self, columns_to_cluster=None):\n    """\n    Cluster les colonnes similaires dans le DataFrame en fonction de leur contenu.\n    Les colonnes sont regroupées si elles ont des valeurs identiques ou très similaires.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns_to_cluster is None:\n        columns_to_cluster = self.df.columns.tolist()\n\n    # Créer un dictionnaire pour stocker les clusters de colonnes\n    clusters = {}\n\n    for col in columns_to_cluster:\n        # Trouver le cluster approprié pour la colonne actuelle\n        matched = False\n        for cluster_id, cluster_cols in clusters.items():\n            # Comparer la colonne actuelle avec chaque colonne du cluster\n            for cluster_col in cluster_cols:\n                if self.df[col].equals(self.df[cluster_col]):\n                    clusters[cluster_id].append(col)\n                    matched = True\n                    break\n            if matched:\n                break\n\n        # Si aucune correspondance trouvée, créer un nouveau cluster\n        if not matched:\n            clusters[len(clusters)] = [col]\n\n    # Ajouter les colonnes clusterisées au DataFrame\n    for cluster_id, cluster_cols in clusters.items():\n        if len(cluster_cols) > 1:\n            # Créer une nouvelle colonne avec le nom du cluster\n            cluster_name = f"cluster_{cluster_id}"\n            self.df[cluster_name] = self.df[cluster_cols[0]].astype(str)\n            for col in cluster_cols[1:]:\n                self.df[cluster_name] += " | " + self.df[col].astype(str)\n                # Supprimer la colonne originale\n                self.df.drop(col, axis=1, inplace=True)\n\n    self._log("cluster_columns", params={})\n    return self'
    },
    "cluster_rows": {
        "params": ['group_columns', 'value_column', 'threshold'],
        "docstring": 'Cluster rows based on similarity in specified columns and optionally merge them.\nIf value_column is provided, only clusters with similar values (above threshold) will be merged.',
        "code": 'def cluster_rows(self, group_columns, value_column=None, threshold=0.8):\n    """\n    Cluster rows based on similarity in specified columns and optionally merge them.\n    If value_column is provided, only clusters with similar values (above threshold) will be merged.\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.cluster import DBSCAN\n    import numpy as np\n\n    # Prepare data for clustering\n    cluster_data = self.df[group_columns].values\n    if value_column:\n        cluster_data = np.hstack([cluster_data, self.df[value_column].values.reshape(-1, 1)])\n\n    # Perform clustering\n    clustering = DBSCAN(eps=0.5, min_samples=2).fit(cluster_data)\n    self.df[\'cluster_id\'] = clustering.labels_\n\n    # Merge clusters if value_column is provided and similarity threshold is met\n    if value_column:\n        for cluster_id in np.unique(self.df[\'cluster_id\']):\n            if cluster_id == -1:\n                continue\n\n            cluster_group = self.df[self.df[\'cluster_id\'] == cluster_id]\n            if len(cluster_group) > 1:\n                # Calculate similarity between values in the cluster\n                values = cluster_group[value_column].values\n                similarities = np.abs(values - values[:, None]) / (np.max(values) - np.min(values))\n\n                # Merge if average similarity is above threshold\n                if np.mean(similarities) > threshold:\n                    merge_index = cluster_group.index[0]\n                    self.df.loc[merge_index, group_columns] = cluster_group[group_columns].mean()\n                    self.df.loc[merge_index, value_column] = cluster_group[value_column].mean()\n                    self.df = self.df.drop(cluster_group.index[1:]).reset_index(drop=True)\n\n        # Remove cluster_id column after processing\n        self.df = self.df.drop(columns=[\'cluster_id\'])\n\n    self._log("cluster_rows", params={\'group_columns\': group_columns, \'value_column\': value_column, \'threshold\': threshold})\n    return self'
    },
    "coerce_dtypes": {
        "params": ['columns'],
        "docstring": 'Convertit les types de données des colonnes spécifiées en types appropriés.\nLes types convertis incluent int, float et datetime.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à convertir. Si None, toutes les colonnes sont traitées.',
        "code": 'def coerce_dtypes(self, columns=None):\n    """\n    Convertit les types de données des colonnes spécifiées en types appropriés.\n    Les types convertis incluent int, float et datetime.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à convertir. Si None, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        # Conversion en int si possible\n        try:\n            self.df[col] = pd.to_numeric(self.df[col], errors=\'ignore\')\n            if all(self.df[col].dropna().mod(1) == 0):\n                self.df[col] = self.df[col].astype(\'Int64\')\n        except (ValueError, TypeError):\n            pass\n\n        # Conversion en float si possible\n        try:\n            self.df[col] = pd.to_numeric(self.df[col], errors=\'ignore\')\n        except (ValueError, TypeError):\n            pass\n\n        # Conversion en datetime si possible\n        try:\n            self.df[col] = pd.to_datetime(self.df[col], errors=\'ignore\')\n        except (ValueError, TypeError):\n            pass\n\n    self._log("coerce_dtypes", params={\'columns\': columns})\n    return self'
    },
    "coerce_nullable_dtypes": {
        "params": ['columns'],
        "docstring": 'Convertit les colonnes spécifiées en types nullable si possible.',
        "code": 'def coerce_nullable_dtypes(self, columns=None):\n    """\n    Convertit les colonnes spécifiées en types nullable si possible.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'integer\', \'float\']).columns\n\n    for col in columns:\n        current_dtype = self.df[col].dtype\n        if np.issubdtype(current_dtype, np.integer):\n            self.df[col] = pd.to_numeric(self.df[col], downcast=\'integer\')\n        elif np.issubdtype(current_dtype, np.floating):\n            self.df[col] = pd.to_numeric(self.df[col], downcast=\'float\')\n\n    self._log("coerce_nullable_dtypes", params={\'columns\': columns})\n    return self'
    },
    "collapse_similar_strings": {
        "params": ['column_name', 'threshold'],
        "docstring": 'Collapse similar strings in a specified column based on string similarity.\nUses Levenshtein distance to identify and group similar strings.',
        "code": 'def collapse_similar_strings(self, column_name: str, threshold: float = 0.8):\n    """\n    Collapse similar strings in a specified column based on string similarity.\n    Uses Levenshtein distance to identify and group similar strings.\n    """\n    self._validate_df()\n    self._backup()\n\n    from fuzzywuzzy import fuzz\n    from collections import defaultdict\n\n    # Create a dictionary to map similar strings to their representative\n    similarity_groups = defaultdict(list)\n    processed_strings = set()\n\n    for string in self.df[column_name].dropna().unique():\n        if string not in processed_strings:\n            similarity_groups[string].append(string)\n            processed_strings.add(string)\n\n            for other_string in self.df[column_name].dropna().unique():\n                if (other_string not in processed_strings and\n                    fuzz.ratio(string, other_string) >= threshold * 100):\n                    similarity_groups[string].append(other_string)\n                    processed_strings.add(other_string)\n\n    # Replace similar strings with their representative\n    for group in similarity_groups.values():\n        if len(group) > 1:\n            representative = min(group, key=len)\n            for string in group[1:]:\n                self.df[column_name] = self.df[column_name].replace(string, representative)\n\n    self._log("collapse_similar_strings", params={\'column_name\': column_name, \'threshold\': threshold})\n    return self'
    },
    "collapse_whitespace": {
        "params": ['column_names'],
        "docstring": "Remplace les espaces multiples par un seul espace dans les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, traite toutes les colonnes de type string.\n\nParameters:\n-----------\ncolumn_names : list of str, optional\n    Liste des noms de colonnes à traiter. Si None, toutes les colonnes string sont traitées.",
        "code": 'def collapse_whitespace(self, column_names=None):\n    """\n    Remplace les espaces multiples par un seul espace dans les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, traite toutes les colonnes de type string.\n\n    Parameters:\n    -----------\n    column_names : list of str, optional\n        Liste des noms de colonnes à traiter. Si None, toutes les colonnes string sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_names is None:\n        column_names = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in column_names:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].str.replace(r\'\\s+\', \' \', regex=True)\n\n    self._log("collapse_whitespace", params={\'column_names\': column_names})\n    return self'
    },
    "compress_dataset": {
        "params": ['columns_to_keep'],
        "docstring": 'Compresse le DataFrame en supprimant les colonnes inutiles et en réduisant la taille mémoire.',
        "code": 'def compress_dataset(self, columns_to_keep=None):\n    """\n    Compresse le DataFrame en supprimant les colonnes inutiles et en réduisant la taille mémoire.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns_to_keep is None:\n        # Si aucune colonne n\'est spécifiée, garder uniquement les colonnes numériques\n        columns_to_keep = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n\n    # Supprimer les colonnes non nécessaires\n    self.df = self.df[columns_to_keep]\n\n    # Conversion des types pour optimiser la mémoire\n    for col in self.df.columns:\n        if self.df[col].dtype == \'float64\':\n            self.df[col] = pd.to_numeric(self.df[col], downcast=\'float\')\n        elif self.df[col].dtype == \'int64\':\n            self.df[col] = pd.to_numeric(self.df[col], downcast=\'integer\')\n        elif self.df[col].dtype == \'object\':\n            # Conversion des objets en catégorie si peu de valeurs uniques\n            if len(self.df[col].unique()) / len(self.df[col]) < 0.5:\n                self.df[col] = self.df[col].astype(\'category\')\n\n    self._log("compress_dataset", params={\'columns_to_keep\': columns_to_keep})\n    return self'
    },
    "compute_distance_haversine": {
        "params": ['lat1_col', 'lon1_col', 'lat2_col', 'lon2_col', 'distance_col'],
        "docstring": "Calcule la distance entre deux points géographiques (latitude/longitude) en utilisant la formule de Haversine.\nLes distances sont calculées en kilomètres et ajoutées dans une nouvelle colonne spécifiée.\n\nParameters:\n    lat1_col (str): Nom de la colonne contenant les latitudes des premiers points\n    lon1_col (str): Nom de la colonne contenant les longitudes des premiers points\n    lat2_col (str): Nom de la colonne contenant les latitudes des seconds points\n    lon2_col (str): Nom de la colonne contenant les longitudes des seconds points\n    distance_col (str, optional): Nom de la colonne où stocker les distances calculées. Defaults to 'distance'.",
        "code": 'def compute_distance_haversine(self, lat1_col: str, lon1_col: str, lat2_col: str, lon2_col: str, distance_col: str = \'distance\'):\n    """\n    Calcule la distance entre deux points géographiques (latitude/longitude) en utilisant la formule de Haversine.\n    Les distances sont calculées en kilomètres et ajoutées dans une nouvelle colonne spécifiée.\n\n    Parameters:\n        lat1_col (str): Nom de la colonne contenant les latitudes des premiers points\n        lon1_col (str): Nom de la colonne contenant les longitudes des premiers points\n        lat2_col (str): Nom de la colonne contenant les latitudes des seconds points\n        lon2_col (str): Nom de la colonne contenant les longitudes des seconds points\n        distance_col (str, optional): Nom de la colonne où stocker les distances calculées. Defaults to \'distance\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    import numpy as np\n    from math import radians, sin, cos, sqrt, atan2\n\n    def haversine(lat1, lon1, lat2, lon2):\n        # Conversion des degrés en radians\n        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n\n        # Différences de coordonnées\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        # Formule de Haversine\n        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n        c = 2 * atan2(sqrt(a), sqrt(1-a))\n        distance = 6371 * c  # Rayon moyen de la Terre en km\n\n        return distance\n\n    self.df[distance_col] = np.vectorize(haversine)(\n        self.df[lat1_col],\n        self.df[lon1_col],\n        self.df[lat2_col],\n        self.df[lon2_col]\n    )\n\n    self._log("compute_distance_haversine", params={\n        \'lat1_col\': lat1_col,\n        \'lon1_col\': lon1_col,\n        \'lat2_col\': lat2_col,\n        \'lon2_col\': lon2_col,\n        \'distance_col\': distance_col\n    })\n    return self'
    },
    "concat_tables": {
        "params": ['dfs', 'keys'],
        "docstring": 'Concatène plusieurs DataFrames en un seul, avec option de clés pour identifier les sources.',
        "code": 'def concat_tables(self, dfs, keys=None):\n    """\n    Concatène plusieurs DataFrames en un seul, avec option de clés pour identifier les sources.\n    """\n    self._validate_df()\n    self._backup()\n\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n\n    if keys is None:\n        self.df = pd.concat(dfs, ignore_index=True)\n    else:\n        if len(keys) != len(dfs):\n            raise ValueError("Le nombre de clés doit correspondre au nombre de DataFrames")\n        self.df = pd.concat({k: df for k, df in zip(keys, dfs)}, names=[\'source\', None]).reset_index(level=1)\n\n    self._log("concat_tables", params={\'dfs_count\': len(dfs), \'keys_used\': keys is not None})\n    return self'
    },
    "convert_csv_to_utf8": {
        "params": ['input_path', 'output_path'],
        "docstring": "Convertit un fichier CSV en encodage UTF-8 et le sauvegarde.\nSi output_path n'est pas spécifié, remplace le fichier d'origine.\n\nParameters:\n    input_path (str): Chemin vers le fichier CSV d'entrée\n    output_path (str, optional): Chemin vers le fichier CSV de sortie. Par défaut None",
        "code": 'def convert_csv_to_utf8(self, input_path: str, output_path: str = None):\n    """\n    Convertit un fichier CSV en encodage UTF-8 et le sauvegarde.\n    Si output_path n\'est pas spécifié, remplace le fichier d\'origine.\n\n    Parameters:\n        input_path (str): Chemin vers le fichier CSV d\'entrée\n        output_path (str, optional): Chemin vers le fichier CSV de sortie. Par défaut None\n    """\n    self._validate_df()\n    self._backup()\n\n    # code principal\n    if output_path is None:\n        output_path = input_path\n\n    self.df.to_csv(output_path, index=False, encoding=\'utf-8\')\n\n    self._log("convert_csv_to_utf8", params={\'input_path\': input_path, \'output_path\': output_path})\n    return self'
    },
    "convert_currency_to_float": {
        "params": ['currency_column', 'target_columns'],
        "docstring": "Convertit les valeurs monétaires d'une colonne cible en float.\nSupprime les caractères non numériques et convertit la devise en float.\n\nArgs:\n    currency_column (str): Nom de la colonne contenant les valeurs monétaires à convertir.\n    target_columns (list[str]): Liste des colonnes cibles à nettoyer.\n\nReturns:\n    DataCleaner: Retourne l'instance courante pour permettre le chaînage.",
        "code": 'def convert_currency_to_float(self, currency_column: str, target_columns: list[str]):\n    """\n    Convertit les valeurs monétaires d\'une colonne cible en float.\n    Supprime les caractères non numériques et convertit la devise en float.\n\n    Args:\n        currency_column (str): Nom de la colonne contenant les valeurs monétaires à convertir.\n        target_columns (list[str]): Liste des colonnes cibles à nettoyer.\n\n    Returns:\n        DataCleaner: Retourne l\'instance courante pour permettre le chaînage.\n    """\n    self._validate_df()\n    self._backup()\n\n    for col in target_columns:\n        if col not in self.df.columns:\n            continue\n\n        # Suppression des caractères non numériques et conversion\n        self.df[col] = (\n            self.df[currency_column]\n            .str.replace(r\'[^\\d,]\', \'\', regex=True)\n            .str.replace(\',\', \'.\')\n            .astype(float)\n        )\n\n    self._log("convert_currency_to_float", params={"currency_column": currency_column, "target_columns": target_columns})\n    return self'
    },
    "convert_duration_to_seconds": {
        "params": ['duration_column', 'output_column'],
        "docstring": "Convertit une colonne de durées au format HH:MM:SS en secondes.\n\nArgs:\n    duration_column (str): Nom de la colonne contenant les durées au format HH:MM:SS\n    output_column (str, optional): Nom de la colonne pour stocker les secondes. Si None,\n                                  remplace la colonne d'origine.",
        "code": 'def convert_duration_to_seconds(self, duration_column: str, output_column: str = None):\n    """\n    Convertit une colonne de durées au format HH:MM:SS en secondes.\n\n    Args:\n        duration_column (str): Nom de la colonne contenant les durées au format HH:MM:SS\n        output_column (str, optional): Nom de la colonne pour stocker les secondes. Si None,\n                                      remplace la colonne d\'origine.\n    """\n    self._validate_df()\n    self._backup()\n\n    if output_column is None:\n        output_column = duration_column\n\n    # Code principal\n    self.df[output_column] = self.df[duration_column].apply(\n        lambda x: sum(int(part) * 60 ** i for i, part in enumerate(reversed(x.split(\':\'))))\n        if pd.notna(x) else None\n    )\n\n    self._log("convert_duration_to_seconds", params={\n        \'duration_column\': duration_column,\n        \'output_column\': output_column\n    })\n    return self'
    },
    "convert_excel_to_csv": {
        "params": ['input_path', 'output_path'],
        "docstring": "Convertit un fichier Excel en CSV et stocke le résultat dans l'attribut df.\nSi output_path est spécifié, sauvegarde également le fichier CSV à cet emplacement.\n\nArgs:\n    input_path (str): Chemin vers le fichier Excel d'entrée.\n    output_path (str, optional): Chemin où sauvegarder le fichier CSV. Par défaut None.\n\nReturns:\n    DataCleaner: Retourne l'instance courante pour permettre le chaînage de méthodes.",
        "code": 'def convert_excel_to_csv(self, input_path: str, output_path: str = None):\n    """\n    Convertit un fichier Excel en CSV et stocke le résultat dans l\'attribut df.\n    Si output_path est spécifié, sauvegarde également le fichier CSV à cet emplacement.\n\n    Args:\n        input_path (str): Chemin vers le fichier Excel d\'entrée.\n        output_path (str, optional): Chemin où sauvegarder le fichier CSV. Par défaut None.\n\n    Returns:\n        DataCleaner: Retourne l\'instance courante pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Lire le fichier Excel et convertir en DataFrame\n    excel_data = pd.read_excel(input_path)\n    self.df = pd.DataFrame(excel_data)\n\n    # Sauvegarder en CSV si output_path est spécifié\n    if output_path:\n        self.df.to_csv(output_path, index=False)\n\n    self._log("convert_excel_to_csv", params={\'input_path\': input_path, \'output_path\': output_path})\n    return self'
    },
    "convert_html_table": {
        "params": ['html_content'],
        "docstring": "Convertit le contenu HTML d'un tableau en un DataFrame pandas nettoyé et préparé.",
        "code": 'def convert_html_table(self, html_content):\n    """\n    Convertit le contenu HTML d\'un tableau en un DataFrame pandas nettoyé et préparé.\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n    from bs4 import BeautifulSoup\n\n    # Parser le contenu HTML avec BeautifulSoup\n    soup = BeautifulSoup(html_content, \'html.parser\')\n\n    # Trouver le premier tableau dans le contenu HTML\n    table = soup.find(\'table\')\n    if not table:\n        raise ValueError("Aucun tableau HTML trouvé dans le contenu fourni")\n\n    # Convertir le tableau HTML en DataFrame\n    df = pd.read_html(str(table))[0]\n\n    # Nettoyage de base du DataFrame\n    df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)\n\n    # Suppression des lignes vides\n    df = df.dropna(how=\'all\')\n\n    self.df = df\n\n    self._log("convert_html_table", params={})\n    return self'
    },
    "convert_json_table": {
        "params": ['json_column', 'table_columns', 'id_column'],
        "docstring": "Convertit une colonne JSON en plusieurs colonnes dans le DataFrame.\nChaque élément de la colonne JSON est décompressé et ses champs sont ajoutés comme nouvelles colonnes.\nSi un id_column est spécifié, il est utilisé pour identifier les lignes à traiter.\n\nArgs:\n    json_column (str): Nom de la colonne contenant les données JSON\n    table_columns (list[str]): Liste des noms de colonnes à extraire du JSON\n    id_column (str, optional): Nom de la colonne d'identifiant. Defaults to None.\n\nRaises:\n    ValueError: Si la colonne JSON n'existe pas ou si les colonnes spécifiées ne sont pas valides",
        "code": 'def convert_json_table(self, json_column: str, table_columns: list[str], id_column: str = None):\n    """\n    Convertit une colonne JSON en plusieurs colonnes dans le DataFrame.\n    Chaque élément de la colonne JSON est décompressé et ses champs sont ajoutés comme nouvelles colonnes.\n    Si un id_column est spécifié, il est utilisé pour identifier les lignes à traiter.\n\n    Args:\n        json_column (str): Nom de la colonne contenant les données JSON\n        table_columns (list[str]): Liste des noms de colonnes à extraire du JSON\n        id_column (str, optional): Nom de la colonne d\'identifiant. Defaults to None.\n\n    Raises:\n        ValueError: Si la colonne JSON n\'existe pas ou si les colonnes spécifiées ne sont pas valides\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des colonnes\n    if json_column not in self.df.columns:\n        raise ValueError(f"La colonne JSON \'{json_column}\' n\'existe pas dans le DataFrame")\n\n    # Conversion des données JSON\n    json_data = self.df[json_column].apply(lambda x: pd.json_normalize(x) if pd.notna(x) else None)\n\n    # Création d\'un DataFrame temporaire avec les colonnes extraites\n    temp_df = pd.json_normalize(json_data.dropna())\n\n    # Vérification des colonnes demandées\n    missing_cols = [col for col in table_columns if col not in temp_df.columns]\n    if missing_cols:\n        raise ValueError(f"Les colonnes suivantes n\'existent pas dans le JSON: {missing_cols}")\n\n    # Fusion des données avec le DataFrame original\n    for col in table_columns:\n        self.df[col] = temp_df[col]\n\n    # Suppression de la colonne JSON originale si elle n\'est pas utilisée comme identifiant\n    if id_column and id_column != json_column:\n        self.df.drop(columns=[json_column], inplace=True)\n\n    self._log("convert_json_table", params={\n        "json_column": json_column,\n        "table_columns": table_columns,\n        "id_column": id_column\n    })\n    return self'
    },
    "convert_percent_to_float": {
        "params": ['columns'],
        "docstring": "Convertit les valeurs en pourcentage dans les colonnes spécifiées en nombres à virgule flottante.\nLes valeurs doivent être au format 'X%' ou 'X.XX%'. Si aucune colonne n'est spécifiée,\ntoutes les colonnes contenant des pourcentages seront traitées.",
        "code": 'def convert_percent_to_float(self, columns=None):\n    """\n    Convertit les valeurs en pourcentage dans les colonnes spécifiées en nombres à virgule flottante.\n    Les valeurs doivent être au format \'X%\' ou \'X.XX%\'. Si aucune colonne n\'est spécifiée,\n    toutes les colonnes contenant des pourcentages seront traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        # Identifier les colonnes contenant des pourcentages\n        percent_columns = []\n        for col in self.df.columns:\n            if self.df[col].astype(str).str.contains(\'%\').any():\n                percent_columns.append(col)\n        columns = percent_columns\n\n    for col in columns:\n        self.df[col] = self.df[col].astype(str).str.rstrip(\'%\').astype(float) / 100\n\n    self._log("convert_percent_to_float", params={\'columns\': columns})\n    return self'
    },
    "convert_timezone": {
        "params": ['timezone', 'date_columns'],
        "docstring": 'Convertit les colonnes de dates dans le DataFrame vers la timezone spécifiée.\nSi aucune colonne n\'est spécifiée, toutes les colonnes de type datetime sont converties.\n\nArgs:\n    timezone (str): La timezone cible pour la conversion. Par défaut "UTC".\n    date_columns (list[str] | None): Liste des colonnes à convertir. Si None, toutes les colonnes datetime sont converties.',
        "code": 'def convert_timezone(self, timezone: str = "UTC", date_columns: list[str] | None = None):\n    """\n    Convertit les colonnes de dates dans le DataFrame vers la timezone spécifiée.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type datetime sont converties.\n\n    Args:\n        timezone (str): La timezone cible pour la conversion. Par défaut "UTC".\n        date_columns (list[str] | None): Liste des colonnes à convertir. Si None, toutes les colonnes datetime sont converties.\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n\n    if date_columns is None:\n        date_columns = self.df.select_dtypes(include=[\'datetime64[ns]\']).columns.tolist()\n\n    for col in date_columns:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].dt.tz_localize(None).dt.tz_localize(timezone)\n\n    self._log("convert_timezone", params={"timezone": timezone, "date_columns": date_columns})\n    return self'
    },
    "convert_to_timestamp": {
        "params": ['date_columns'],
        "docstring": "Convertit les colonnes spécifiées en type timestamp.\nSi aucune colonne n'est spécifiée, toutes les colonnes de type object sont converties.\n\nParameters:\ndate_columns (list): Liste des noms de colonnes à convertir en timestamp.\n                     Si None, toutes les colonnes de type object sont converties.\n\nReturns:\nDataCleaner: Retourne l'instance courante pour permettre le chaînage de méthodes.",
        "code": 'def convert_to_timestamp(self, date_columns=None):\n    """\n    Convertit les colonnes spécifiées en type timestamp.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type object sont converties.\n\n    Parameters:\n    date_columns (list): Liste des noms de colonnes à convertir en timestamp.\n                         Si None, toutes les colonnes de type object sont converties.\n\n    Returns:\n    DataCleaner: Retourne l\'instance courante pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        # Convertir toutes les colonnes de type object\n        for col in self.df.select_dtypes(include=[\'object\']).columns:\n            try:\n                self.df[col] = pd.to_datetime(self.df[col])\n            except (ValueError, TypeError):\n                continue\n    else:\n        # Convertir uniquement les colonnes spécifiées\n        for col in date_columns:\n            if col in self.df.columns:\n                try:\n                    self.df[col] = pd.to_datetime(self.df[col])\n                except (ValueError, TypeError):\n                    continue\n\n    self._log("convert_to_timestamp", params={"date_columns": date_columns})\n    return self'
    },
    "convert_units": {
        "params": ['unit_mapping'],
        "docstring": 'Convertit les unités des colonnes spécifiées selon un mapping fourni.',
        "code": 'def convert_units(self, unit_mapping: dict):\n    """\n    Convertit les unités des colonnes spécifiées selon un mapping fourni.\n    """\n    self._validate_df()\n    self._backup()\n\n    for column, conversion_func in unit_mapping.items():\n        if column in self.df.columns:\n            self.df[column] = self.df[column].apply(conversion_func)\n\n    self._log("convert_units", params={"unit_mapping": unit_mapping})\n    return self'
    },
    "correlation_matrix": {
        "params": ['method', 'threshold'],
        "docstring": "Calcule et retourne la matrice de corrélation des données.\nOptionnellement filtre les colonnes en fonction d'un seuil de corrélation.\n\nParameters:\n-----------\nmethod : str, optional\n    Méthode de calcul de la corrélation ('pearson', 'kendall', 'spearman')\nthreshold : float, optional\n    Seuil de corrélation pour filtrer les colonnes (0 < threshold <= 1)\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance courante pour le chaînage de méthodes.",
        "code": 'def correlation_matrix(self, method=\'pearson\', threshold=None):\n    """\n    Calcule et retourne la matrice de corrélation des données.\n    Optionnellement filtre les colonnes en fonction d\'un seuil de corrélation.\n\n    Parameters:\n    -----------\n    method : str, optional\n        Méthode de calcul de la corrélation (\'pearson\', \'kendall\', \'spearman\')\n    threshold : float, optional\n        Seuil de corrélation pour filtrer les colonnes (0 < threshold <= 1)\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance courante pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    corr_matrix = self.df.corr(method=method)\n\n    if threshold is not None:\n        # Filtrer les colonnes avec corrélation supérieure au seuil\n        high_corr_cols = set()\n        for i in range(len(corr_matrix.columns)):\n            for j in range(i):\n                if abs(corr_matrix.iloc[i, j]) > threshold:\n                    colname = corr_matrix.columns[i]\n                    high_corr_cols.add(colname)\n\n        # Conserver uniquement les colonnes non corrélées\n        self.df = self.df.drop(columns=high_corr_cols)\n\n    self._log("correlation_matrix", params={\'method\': method, \'threshold\': threshold})\n    return self'
    },
    "count_encode": {
        "params": ['columns'],
        "docstring": "Applique un encodage par comptage aux colonnes spécifiées.\nRemplace les valeurs catégorielles par leur fréquence d'apparition dans la colonne.\nSi aucune colonne n'est spécifiée, toutes les colonnes de type object sont traitées.",
        "code": 'def count_encode(self, columns=None):\n    """\n    Applique un encodage par comptage aux colonnes spécifiées.\n    Remplace les valeurs catégorielles par leur fréquence d\'apparition dans la colonne.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type object sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in columns:\n        if col in self.df.columns:\n            counts = self.df[col].value_counts(normalize=True)\n            self.df[col] = self.df[col].map(counts)\n\n    self._log("count_encode", params={\'columns\': columns})\n    return self'
    },
    "create_checkpoint": {
        "params": ['checkpoint_name'],
        "docstring": "Crée un point de contrôle dans le processus de nettoyage des données.\nSauvegarde l'état actuel du DataFrame et enregistre les métadonnées associées.\n\nArgs:\n    checkpoint_name (str): Nom du point de contrôle à créer",
        "code": 'def create_checkpoint(self, checkpoint_name: str):\n    """\n    Crée un point de contrôle dans le processus de nettoyage des données.\n    Sauvegarde l\'état actuel du DataFrame et enregistre les métadonnées associées.\n\n    Args:\n        checkpoint_name (str): Nom du point de contrôle à créer\n    """\n    self._validate_df()\n    self._backup()\n\n    # Sauvegarde de l\'état actuel du DataFrame\n    checkpoint_data = {\n        \'data\': self.df.copy(),\n        \'metadata\': {\n            \'timestamp\': pd.Timestamp.now(),\n            \'checkpoint_name\': checkpoint_name,\n            \'shape\': self.df.shape\n        }\n    }\n\n    # Ajout du checkpoint à la liste des checkpoints\n    if not hasattr(self, \'_checkpoints\'):\n        self._checkpoints = []\n    self._checkpoints.append(checkpoint_data)\n\n    # Sauvegarde des métadonnées supplémentaires\n    if hasattr(self, \'_operations_log\'):\n        checkpoint_data[\'operations\'] = self._operations_log.copy()\n\n    self._log("create_checkpoint", params={\'checkpoint_name\': checkpoint_name})\n    return self'
    },
    "cyclic_time_encode": {
        "params": ['time_column', 'period'],
        "docstring": 'Encodes a time column using cyclic encoding (sine and cosine transformations).\nThis is useful for representing time features in a way that machine learning models can interpret as cyclic.',
        "code": 'def cyclic_time_encode(self, time_column: str, period: float = 24.0):\n    """\n    Encodes a time column using cyclic encoding (sine and cosine transformations).\n    This is useful for representing time features in a way that machine learning models can interpret as cyclic.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Apply cyclic encoding\n    self.df[f\'{time_column}_sin\'] = np.sin(2 * np.pi * self.df[time_column] / period)\n    self.df[f\'{time_column}_cos\'] = np.cos(2 * np.pi * self.df[time_column] / period)\n\n    # Drop the original time column\n    self.df.drop(columns=[time_column], inplace=True)\n\n    self._log("cyclic_time_encode", params={\'time_column\': time_column, \'period\': period})\n    return self'
    },
    "decode_html_entities": {
        "params": ['df'],
        "docstring": 'Décode les entités HTML dans un DataFrame.',
        "code": 'def decode_html_entities(self, df=None):\n    """\n    Décode les entités HTML dans un DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    import html\n    for col in df.columns:\n        if df[col].dtype == \'object\':\n            df[col] = df[col].apply(lambda x: html.unescape(x) if isinstance(x, str) else x)\n\n    self.df = df\n    self._log("decode_html_entities", params={})\n    return self'
    },
    "dedupe_columns": {
        "params": ['threshold'],
        "docstring": 'Supprime les colonnes en double dans le DataFrame.\nDeux colonnes sont considérées comme dupliquées si leur similarité dépasse un seuil donné.\n\nParameters\n----------\nthreshold : float, optional\n    Seuil de similarité pour considérer deux colonnes comme dupliquées (par défaut 0.9)',
        "code": 'def dedupe_columns(self, threshold=0.9):\n    """\n    Supprime les colonnes en double dans le DataFrame.\n    Deux colonnes sont considérées comme dupliquées si leur similarité dépasse un seuil donné.\n\n    Parameters\n    ----------\n    threshold : float, optional\n        Seuil de similarité pour considérer deux colonnes comme dupliquées (par défaut 0.9)\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la similarité entre toutes les paires de colonnes\n    from sklearn.metrics import pairwise_distances\n    import numpy as np\n\n    # Convertir les colonnes en valeurs numériques pour le calcul de similarité\n    numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n    non_numeric_cols = self.df.select_dtypes(exclude=[np.number]).columns\n\n    # Traitement des colonnes numériques\n    for col1 in numeric_cols:\n        for col2 in numeric_cols:\n            if col1 != col2 and col1 not in self.df.columns or col2 not in self.df.columns:\n                continue\n            corr = np.corrcoef(self.df[col1], self.df[col2])[0, 1]\n            if abs(corr) > threshold:\n                self.df.drop(col2, axis=1, inplace=True)\n\n    # Traitement des colonnes non numériques\n    for col1 in non_numeric_cols:\n        for col2 in non_numeric_cols:\n            if col1 != col2 and col1 not in self.df.columns or col2 not in self.df.columns:\n                continue\n            # Utiliser Jaccard similarity pour les colonnes catégorielles\n            set1 = set(self.df[col1].dropna().unique())\n            set2 = set(self.df[col2].dropna().unique())\n            similarity = len(set1.intersection(set2)) / len(set1.union(set2))\n            if similarity > threshold:\n                self.df.drop(col2, axis=1, inplace=True)\n\n    self._log("dedupe_columns", params={\'threshold\': threshold})\n    return self'
    },
    "dedupe_fuzzy": {
        "params": ['df', 'threshold'],
        "docstring": 'Supprime les doublons en utilisant une comparaison floue (fuzzy matching) sur les colonnes textuelles.',
        "code": 'def dedupe_fuzzy(self, df=None, threshold=0.85):\n    """\n    Supprime les doublons en utilisant une comparaison floue (fuzzy matching) sur les colonnes textuelles.\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n    from fuzzywuzzy import fuzz\n\n    if df is None:\n        df = self.df.copy()\n\n    # Sélection des colonnes textuelles\n    text_cols = df.select_dtypes(include=[\'object\']).columns\n\n    # Création d\'un masque pour identifier les doublons\n    duplicates_mask = pd.Series([False] * len(df))\n\n    for i in range(len(df)):\n        if not duplicates_mask[i]:\n            for j in range(i + 1, len(df)):\n                similarity_scores = [fuzz.ratio(str(df.iloc[i][col]), str(df.iloc[j][col])) for col in text_cols]\n                avg_similarity = sum(similarity_scores) / len(similarity_scores)\n                if avg_similarity >= threshold * 100:\n                    duplicates_mask[j] = True\n\n    # Suppression des doublons\n    df_cleaned = df[~duplicates_mask]\n\n    self.df = df_cleaned\n\n    self._log("dedupe_fuzzy", params={\'threshold\': threshold})\n    return self'
    },
    "dedupe_rows": {
        "params": ['subset', 'keep'],
        "docstring": "Supprime les doublons des lignes du DataFrame en fonction de colonnes spécifiées.\n\nParameters:\n    subset (list, optional): Liste des noms de colonnes à considérer pour identifier les doublons.\n                            Si None, toutes les colonnes sont utilisées.\n    keep (str, optional): Méthode pour conserver une occurrence des doublons :\n                          - 'first' : conserve la première occurrence\n                          - 'last' : conserve la dernière occurrence\n                          - False : supprime toutes les occurrences des doublons\n\nReturns:\n    DataCleaner: Retourne l'instance courante pour permettre le chaînage des méthodes.",
        "code": 'def dedupe_rows(self, subset=None, keep=\'first\'):\n    """\n    Supprime les doublons des lignes du DataFrame en fonction de colonnes spécifiées.\n\n    Parameters:\n        subset (list, optional): Liste des noms de colonnes à considérer pour identifier les doublons.\n                                Si None, toutes les colonnes sont utilisées.\n        keep (str, optional): Méthode pour conserver une occurrence des doublons :\n                              - \'first\' : conserve la première occurrence\n                              - \'last\' : conserve la dernière occurrence\n                              - False : supprime toutes les occurrences des doublons\n\n    Returns:\n        DataCleaner: Retourne l\'instance courante pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if subset is not None:\n        self.df = self.df.drop_duplicates(subset=subset, keep=keep)\n    else:\n        self.df = self.df.drop_duplicates(keep=keep)\n\n    self._log("dedupe_rows", params={\'subset\': subset, \'keep\': keep})\n    return self'
    },
    "detect_broken_rows": {
        "params": ['threshold_null_ratio'],
        "docstring": 'Détecte et supprime les lignes avec un ratio de valeurs nulles supérieur au seuil donné.',
        "code": 'def detect_broken_rows(self, threshold_null_ratio=0.7):\n    """\n    Détecte et supprime les lignes avec un ratio de valeurs nulles supérieur au seuil donné.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le ratio de valeurs nulles par ligne\n    null_ratio = self.df.isnull().mean(axis=1)\n\n    # Identifier les lignes à supprimer\n    broken_rows_mask = null_ratio > threshold_null_ratio\n\n    # Supprimer les lignes problématiques\n    self.df = self.df[~broken_rows_mask]\n\n    self._log("detect_broken_rows", params={"threshold_null_ratio": threshold_null_ratio})\n    return self'
    },
    "detect_category_inconsistencies": {
        "params": ['categorical_columns'],
        "docstring": "Détecte les incohérences dans les colonnes catégorielles du DataFrame.\nIdentifie les valeurs uniques et les éventuelles valeurs manquantes ou aberrantes.\n\nArgs:\n    categorical_columns (list, optional): Liste des colonnes catégorielles à analyser.\n        Si None, toutes les colonnes de type object ou category sont considérées.\n\nReturns:\n    DataCleaner: Retourne l'instance actuelle pour permettre le chaînage de méthodes.",
        "code": 'def detect_category_inconsistencies(self, categorical_columns=None):\n    """\n    Détecte les incohérences dans les colonnes catégorielles du DataFrame.\n    Identifie les valeurs uniques et les éventuelles valeurs manquantes ou aberrantes.\n\n    Args:\n        categorical_columns (list, optional): Liste des colonnes catégorielles à analyser.\n            Si None, toutes les colonnes de type object ou category sont considérées.\n\n    Returns:\n        DataCleaner: Retourne l\'instance actuelle pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if categorical_columns is None:\n        categorical_columns = self.df.select_dtypes(include=[\'object\', \'category\']).columns.tolist()\n\n    inconsistencies = {}\n    for col in categorical_columns:\n        unique_values = self.df[col].unique()\n        inconsistencies[col] = {\n            \'unique_values\': unique_values,\n            \'missing_values_count\': self.df[col].isna().sum(),\n            \'value_counts\': self.df[col].value_counts(dropna=False).to_dict()\n        }\n\n    self.inconsistencies = inconsistencies\n\n    self._log("detect_category_inconsistencies", params={\'categorical_columns\': categorical_columns})\n    return self'
    },
    "detect_category_outliers": {
        "params": ['column_name', 'threshold'],
        "docstring": "Détecte les valeurs aberrantes dans une colonne catégorielle en fonction d'un seuil de fréquence.\nLes valeurs dont la fréquence est inférieure au seuil sont considérées comme aberrantes.\n\nArgs:\n    column_name (str): Nom de la colonne à analyser.\n    threshold (float, optional): Seuil de fréquence pour détecter les valeurs aberrantes. Defaults to 0.95.\n\nReturns:\n    DataCleaner: Instance de la classe pour permettre le chaînage des méthodes.",
        "code": 'def detect_category_outliers(self, column_name: str, threshold: float = 0.95):\n    """\n    Détecte les valeurs aberrantes dans une colonne catégorielle en fonction d\'un seuil de fréquence.\n    Les valeurs dont la fréquence est inférieure au seuil sont considérées comme aberrantes.\n\n    Args:\n        column_name (str): Nom de la colonne à analyser.\n        threshold (float, optional): Seuil de fréquence pour détecter les valeurs aberrantes. Defaults to 0.95.\n\n    Returns:\n        DataCleaner: Instance de la classe pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la fréquence de chaque catégorie\n    value_counts = self.df[column_name].value_counts(normalize=True)\n\n    # Identifier les catégories sous le seuil\n    outliers = value_counts[value_counts < threshold].index\n\n    # Créer une colonne indiquant si la valeur est une aberration\n    self.df[f\'{column_name}_outlier\'] = self.df[column_name].isin(outliers)\n\n    # Enregistrer les catégories aberrantes dans un attribut\n    self.outlier_categories = outliers.tolist()\n\n    self._log("detect_category_outliers", params={\'column_name\': column_name, \'threshold\': threshold})\n    return self'
    },
    "detect_column_shift": {
        "params": ['df'],
        "docstring": "Détecte et corrige un décalage de colonnes dans le DataFrame.\nSi des colonnes semblent décalées (par exemple, les valeurs d'une colonne\napparaissent dans la suivante), cette méthode tente de les réorganiser.",
        "code": 'def detect_column_shift(self, df=None):\n    """\n    Détecte et corrige un décalage de colonnes dans le DataFrame.\n    Si des colonnes semblent décalées (par exemple, les valeurs d\'une colonne\n    apparaissent dans la suivante), cette méthode tente de les réorganiser.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    # Détection du décalage en comparant les types de données et les valeurs\n    shifted = False\n    for i in range(df.shape[1] - 1):\n        col1 = df.iloc[:, i]\n        col2 = df.iloc[:, i + 1]\n\n        # Comparaison des types de données\n        if str(col1.dtype) != str(col2.dtype):\n            continue\n\n        # Comparaison des valeurs (pour les types numériques ou catégorielles)\n        if pd.api.types.is_numeric_dtype(col1):\n            # Vérification si les valeurs de col2 correspondent aux suivantes de col1\n            if (col2 == df.iloc[:, i + 1].shift(-1)).all():\n                shifted = True\n                # Réorganisation des colonnes\n                cols = df.columns.tolist()\n                cols[i], cols[i + 1] = cols[i + 1], cols[i]\n                df = df[cols]\n        elif pd.api.types.is_string_dtype(col1):\n            # Vérification pour les chaînes de caractères\n            if (col2 == df.iloc[:, i + 1].shift(-1)).all():\n                shifted = True\n                cols = df.columns.tolist()\n                cols[i], cols[i + 1] = cols[i + 1], cols[i]\n                df = df[cols]\n\n    if shifted:\n        self.df = df\n\n    self._log("detect_column_shift", params={})\n    return self'
    },
    "detect_constant_columns": {
        "params": ['threshold'],
        "docstring": 'Détecte et supprime les colonnes constantes ou quasi-constantes des données.\nUne colonne est considérée comme constante si plus de 95% (par défaut) des valeurs sont identiques.',
        "code": 'def detect_constant_columns(self, threshold=0.95):\n    """\n    Détecte et supprime les colonnes constantes ou quasi-constantes des données.\n    Une colonne est considérée comme constante si plus de 95% (par défaut) des valeurs sont identiques.\n    """\n    self._validate_df()\n    self._backup()\n\n    constant_cols = []\n    for col in self.df.columns:\n        unique_ratio = len(self.df[col].value_counts()) / len(self.df[col])\n        if unique_ratio < threshold:\n            constant_cols.append(col)\n\n    self.df.drop(columns=constant_cols, inplace=True)\n    self._log("detect_constant_columns", params={"threshold": threshold})\n    return self'
    },
    "detect_corrupted_cells": {
        "params": ['threshold'],
        "docstring": 'Detects and marks corrupted cells in the DataFrame based on a given threshold.\nA cell is considered corrupted if it contains null values or outliers beyond the specified threshold.\n\nParameters:\n-----------\nthreshold : float, optional\n    The threshold for detecting outliers (default is 0.7).',
        "code": 'def detect_corrupted_cells(self, threshold=0.7):\n    """\n    Detects and marks corrupted cells in the DataFrame based on a given threshold.\n    A cell is considered corrupted if it contains null values or outliers beyond the specified threshold.\n\n    Parameters:\n    -----------\n    threshold : float, optional\n        The threshold for detecting outliers (default is 0.7).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Detect null values\n    null_mask = self.df.isnull()\n\n    # Calculate z-scores for numerical columns to detect outliers\n    numerical_cols = self.df.select_dtypes(include=[\'number\']).columns\n    for col in numerical_cols:\n        if null_mask[col].any():\n            continue  # Skip columns with null values\n        z_scores = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n        null_mask[col] = (z_scores.abs() > threshold) | null_mask[col]\n\n    # Mark corrupted cells\n    self.df[\'_corrupted\'] = null_mask.any(axis=1)\n    for col in self.df.columns:\n        if col != \'_corrupted\':\n            self.df[f\'{col}_corrupted\'] = null_mask[col]\n\n    self._log("detect_corrupted_cells", params={\'threshold\': threshold})\n    return self'
    },
    "detect_data_drift": {
        "params": ['reference_df', 'threshold'],
        "docstring": 'Detects data drift between the current DataFrame and a reference DataFrame.\nUses statistical tests to identify significant changes in data distribution.\n\nParameters:\n-----------\nreference_df : pd.DataFrame, optional\n    The reference DataFrame to compare against. If None, uses the original backup.\nthreshold : float, optional\n    The significance level for detecting drift (default: 0.1).\n\nReturns:\n--------\nDataCleaner\n    The instance of the DataCleaner class for method chaining.',
        "code": 'def detect_data_drift(self, reference_df=None, threshold=0.1):\n    """\n    Detects data drift between the current DataFrame and a reference DataFrame.\n    Uses statistical tests to identify significant changes in data distribution.\n\n    Parameters:\n    -----------\n    reference_df : pd.DataFrame, optional\n        The reference DataFrame to compare against. If None, uses the original backup.\n    threshold : float, optional\n        The significance level for detecting drift (default: 0.1).\n\n    Returns:\n    --------\n    DataCleaner\n        The instance of the DataCleaner class for method chaining.\n    """\n    self._validate_df()\n    self._backup()\n\n    if reference_df is None:\n        reference_df = self._original_df\n\n    # Calculate drift metrics for each column\n    drift_results = {}\n    for col in self._df.columns:\n        if col in reference_df.columns:\n            # Simple example using Kolmogorov-Smirnov test\n            from scipy.stats import ks_2samp\n            ref_data = reference_df[col].dropna()\n            current_data = self._df[col].dropna()\n\n            if len(ref_data) > 1 and len(current_data) > 1:\n                stat, p_value = ks_2samp(ref_data, current_data)\n                drift_results[col] = {\n                    \'p_value\': p_value,\n                    \'has_drift\': p_value < threshold\n                }\n\n    # Store results in the instance\n    self._drift_results = drift_results\n\n    self._log("detect_data_drift", params={\'threshold\': threshold})\n    return self'
    },
    "detect_date_columns": {
        "params": ['date_format', 'threshold'],
        "docstring": 'Détecte automatiquement les colonnes contenant des dates dans le DataFrame.\nUtilise un format de date par défaut et une proportion minimale de valeurs valides pour la détection.',
        "code": 'def detect_date_columns(self, date_format=\'%Y-%m-%d\', threshold=0.8):\n    """\n    Détecte automatiquement les colonnes contenant des dates dans le DataFrame.\n    Utilise un format de date par défaut et une proportion minimale de valeurs valides pour la détection.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Initialisation\n    date_columns = []\n\n    for column in self.df.columns:\n        try:\n            # Tentative de conversion des valeurs en datetime\n            converted = pd.to_datetime(self.df[column], format=date_format, errors=\'coerce\')\n            # Calcul de la proportion de valeurs valides\n            valid_proportion = converted.notna().mean()\n            if valid_proportion >= threshold:\n                date_columns.append(column)\n        except (ValueError, TypeError):\n            continue\n\n    # Stockage des colonnes détectées\n    self.date_columns = date_columns\n\n    self._log("detect_date_columns", params={\'date_format\': date_format, \'threshold\': threshold})\n    return self'
    },
    "detect_delimiter": {
        "params": ['sample_size'],
        "docstring": "Détecte automatiquement le délimiteur utilisé dans les données.\nTeste plusieurs délimiteurs courants et retourne celui qui donne le moins d'erreurs de parsing.",
        "code": 'def detect_delimiter(self, sample_size=1000):\n    """\n    Détecte automatiquement le délimiteur utilisé dans les données.\n    Teste plusieurs délimiteurs courants et retourne celui qui donne le moins d\'erreurs de parsing.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Échantillonnage des données\n    sample = self.df.head(sample_size)\n\n    # Liste des délimiteurs à tester\n    delimiters = [\',\', \';\', \'\\t\', \'|\', \':\']\n\n    best_delimiter = None\n    min_errors = float(\'inf\')\n\n    for delimiter in delimiters:\n        try:\n            # Tentative de lecture avec le délimiteur actuel\n            temp_df = pd.read_csv(StringIO(sample.to_csv(index=False)), delimiter=delimiter)\n\n            # Compter le nombre de lignes et colonnes pour détecter les erreurs\n            errors = abs(len(temp_df.columns) - len(sample.columns))\n\n            if errors < min_errors:\n                min_errors = errors\n                best_delimiter = delimiter\n\n        except Exception:\n            continue\n\n    self.df = pd.read_csv(StringIO(sample.to_csv(index=False)), delimiter=best_delimiter)\n\n    self._log("detect_delimiter", params={\'delimiter\': best_delimiter})\n    return self'
    },
    "detect_duplicates_within_groups": {
        "params": ['group_columns', 'value_columns'],
        "docstring": "Detecte et marque les doublons au sein de groupes définis par des colonnes spécifiques.\nLes doublons sont identifiés lorsque les valeurs dans 'value_columns' sont identiques\npour une même combinaison de valeurs dans 'group_columns'.\nLes doublons sont marqués avec un booléen dans une nouvelle colonne 'is_duplicate'.\n\nParameters:\n    group_columns (list): Liste de noms de colonnes définissant les groupes.\n    value_columns (list, optional): Liste de noms de colonnes à vérifier pour doublons.\n                                   Si None, toutes les colonnes sauf celles dans group_columns sont utilisées.\n\nReturns:\n    DataCleaner: Instance de la classe pour le chaînage des méthodes.",
        "code": 'def detect_duplicates_within_groups(self, group_columns, value_columns=None):\n    """\n    Detecte et marque les doublons au sein de groupes définis par des colonnes spécifiques.\n    Les doublons sont identifiés lorsque les valeurs dans \'value_columns\' sont identiques\n    pour une même combinaison de valeurs dans \'group_columns\'.\n    Les doublons sont marqués avec un booléen dans une nouvelle colonne \'is_duplicate\'.\n\n    Parameters:\n        group_columns (list): Liste de noms de colonnes définissant les groupes.\n        value_columns (list, optional): Liste de noms de colonnes à vérifier pour doublons.\n                                       Si None, toutes les colonnes sauf celles dans group_columns sont utilisées.\n\n    Returns:\n        DataCleaner: Instance de la classe pour le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if value_columns is None:\n        value_columns = [col for col in self.df.columns if col not in group_columns]\n\n    # Création d\'une colonne pour marquer les doublons\n    self.df[\'is_duplicate\'] = False\n\n    # Identification des groupes et vérification des doublons\n    grouped = self.df.groupby(group_columns)\n    for _, group in grouped:\n        duplicates_mask = group.duplicated(subset=value_columns, keep=False)\n        self.df.loc[group.index, \'is_duplicate\'] = duplicates_mask\n\n    self._log("detect_duplicates_within_groups", params={\n        \'group_columns\': group_columns,\n        \'value_columns\': value_columns\n    })\n    return self'
    },
    "detect_empty_columns": {
        "params": ['threshold'],
        "docstring": 'Détecte et supprime les colonnes vides ou majoritairement vides selon un seuil donné.\n\nArgs:\n    threshold (float): Seuil entre 0 et 1 représentant la proportion maximale\n                      de valeurs manquantes ou vides autorisée dans une colonne.\n                      Par défaut 0.5 (50%).',
        "code": 'def detect_empty_columns(self, threshold=0.5):\n    """\n    Détecte et supprime les colonnes vides ou majoritairement vides selon un seuil donné.\n\n    Args:\n        threshold (float): Seuil entre 0 et 1 représentant la proportion maximale\n                          de valeurs manquantes ou vides autorisée dans une colonne.\n                          Par défaut 0.5 (50%).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le pourcentage de valeurs manquantes ou vides par colonne\n    missing_percent = self.df.isna().mean()\n\n    # Détecter les colonnes à supprimer\n    cols_to_drop = missing_percent[missing_percent >= threshold].index\n\n    # Supprimer les colonnes\n    self.df.drop(columns=cols_to_drop, inplace=True)\n\n    self._log("detect_empty_columns", params={"threshold": threshold})\n    return self'
    },
    "detect_encoding": {
        "params": ['df'],
        "docstring": "Détecte et corrige l'encodage des données dans le DataFrame.",
        "code": 'def detect_encoding(self, df=None):\n    """\n    Détecte et corrige l\'encodage des données dans le DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    # Détection de l\'encodage\n    try:\n        import chardet\n        encoding = chardet.detect(df.to_csv(index=False).encode())[\'encoding\']\n    except:\n        encoding = \'utf-8\'\n\n    # Conversion en UTF-8 si nécessaire\n    if encoding.lower() != \'utf-8\':\n        df = df.applymap(lambda x: x.encode(encoding).decode(\'utf-8\') if isinstance(x, str) else x)\n\n    self.df = df\n    self._log("detect_encoding", params={})\n    return self'
    },
    "detect_headers": {
        "params": ['df'],
        "docstring": "Détecte automatiquement les en-têtes de colonnes dans un DataFrame.\nSi aucun DataFrame n'est fourni, utilise le DataFrame actuel de l'instance.\n\nArgs:\n    df (pd.DataFrame, optional): DataFrame à analyser. Par défaut None.\n\nReturns:\n    DataCleaner: Instance actuelle pour le chaînage de méthodes.",
        "code": 'def detect_headers(self, df=None):\n    """\n    Détecte automatiquement les en-têtes de colonnes dans un DataFrame.\n    Si aucun DataFrame n\'est fourni, utilise le DataFrame actuel de l\'instance.\n\n    Args:\n        df (pd.DataFrame, optional): DataFrame à analyser. Par défaut None.\n\n    Returns:\n        DataCleaner: Instance actuelle pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Utilisation du DataFrame fourni ou de celui de l\'instance\n    data = df if df is not None else self.df\n\n    # Détection des en-têtes potentiels\n    header_row = None\n    for i in range(min(5, len(data))):  # Vérifie les 5 premières lignes max\n        if all(isinstance(x, str) and not x.isdigit() for x in data.iloc[i]):\n            header_row = i\n            break\n\n    # Si des en-têtes sont détectés, les appliquer\n    if header_row is not None and header_row != 0:\n        self.df.columns = data.iloc[header_row]\n        # Supprimer la ligne d\'en-tête du DataFrame\n        self.df = data.drop(data.index[header_row]).reset_index(drop=True)\n\n    self._log("detect_headers", params={})\n    return self'
    },
    "detect_holidays": {
        "params": ['date_column', 'country_code', 'language'],
        "docstring": 'Détecte et marque les jours fériés dans une colonne de dates.\nUtilise le module `holidays` pour identifier les jours fériés selon un pays et une langue spécifiés.',
        "code": 'def detect_holidays(self, date_column: str, country_code: str = \'FR\', language: str = \'fr\'):\n    """\n    Détecte et marque les jours fériés dans une colonne de dates.\n    Utilise le module `holidays` pour identifier les jours fériés selon un pays et une langue spécifiés.\n    """\n    self._validate_df()\n    self._backup()\n\n    import holidays\n    from datetime import datetime\n\n    # Vérification de l\'existence de la colonne date\n    if date_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{date_column}\' n\'existe pas dans le DataFrame.")\n\n    # Conversion de la colonne en datetime si ce n\'est pas déjà fait\n    self.df[date_column] = pd.to_datetime(self.df[date_column])\n\n    # Création d\'un calendrier de jours fériés\n    country_holidays = holidays.CountryHoliday(country_code, prov=None, state=None, language=language)\n\n    # Détection des jours fériés\n    self.df[\'is_holiday\'] = self.df[date_column].apply(lambda x: x in country_holidays)\n\n    self._log("detect_holidays", params={\'date_column\': date_column, \'country_code\': country_code, \'language\': language})\n    return self'
    },
    "detect_inconsistent_rows": {
        "params": ['columns'],
        "docstring": 'Détecte et marque les lignes inconsistantes dans le DataFrame.\nUne ligne est considérée comme inconsistante si elle contient des valeurs manquantes\nou des valeurs aberrantes pour les colonnes spécifiées.',
        "code": 'def detect_inconsistent_rows(self, columns=None):\n    """\n    Détecte et marque les lignes inconsistantes dans le DataFrame.\n    Une ligne est considérée comme inconsistante si elle contient des valeurs manquantes\n    ou des valeurs aberrantes pour les colonnes spécifiées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    # Détection des valeurs manquantes\n    missing_mask = self.df[columns].isnull().any(axis=1)\n\n    # Détection des valeurs aberrantes (exemple simple: valeurs numériques hors limites)\n    numeric_cols = self.df[columns].select_dtypes(include=[\'number\']).columns\n    if not numeric_cols.empty:\n        q1 = self.df[numeric_cols].quantile(0.25)\n        q3 = self.df[numeric_cols].quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        outliers_mask = ((self.df[numeric_cols] < lower_bound) | (self.df[numeric_cols] > upper_bound)).any(axis=1)\n    else:\n        outliers_mask = pd.Series(False, index=self.df.index)\n\n    # Combinaison des critères d\'inconsistance\n    inconsistent_mask = missing_mask | outliers_mask\n\n    # Création d\'une colonne pour marquer les lignes inconsistantes\n    self.df[\'is_inconsistent\'] = inconsistent_mask\n\n    self._log("detect_inconsistent_rows", params={\'columns\': columns})\n    return self'
    },
    "detect_inconsistent_units": {
        "params": ['unit_columns'],
        "docstring": 'Detects and logs inconsistent units across specified columns.\nIf no columns are specified, checks all numeric columns in the DataFrame.\n\nParameters:\n-----------\nunit_columns : list, optional\n    List of column names to check for inconsistent units. If None, checks all numeric columns.',
        "code": 'def detect_inconsistent_units(self, unit_columns=None):\n    """\n    Detects and logs inconsistent units across specified columns.\n    If no columns are specified, checks all numeric columns in the DataFrame.\n\n    Parameters:\n    -----------\n    unit_columns : list, optional\n        List of column names to check for inconsistent units. If None, checks all numeric columns.\n    """\n    self._validate_df()\n    self._backup()\n\n    if unit_columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = unit_columns\n\n    inconsistent_units = {}\n\n    for col in numeric_cols:\n        unique_values = self.df[col].dropna().unique()\n        if len(unique_values) > 1:\n            inconsistent_units[col] = {\n                \'values\': unique_values.tolist(),\n                \'count\': len(unique_values)\n            }\n\n    if inconsistent_units:\n        self._log("detect_inconsistent_units", params={\n            \'inconsistent_columns\': inconsistent_units\n        })\n    else:\n        self._log("detect_inconsistent_units", params={\n            \'message\': "No inconsistent units detected"\n        })\n\n    return self'
    },
    "detect_language_column": {
        "params": ['column_name'],
        "docstring": "Détecte la colonne contenant le texte à analyser pour la détection de langue.\nSi aucune colonne n'est spécifiée, tente de détecter automatiquement une colonne candidate.",
        "code": 'def detect_language_column(self, column_name: str = None):\n    """\n    Détecte la colonne contenant le texte à analyser pour la détection de langue.\n    Si aucune colonne n\'est spécifiée, tente de détecter automatiquement une colonne candidate.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name is not None:\n        if column_name in self.df.columns:\n            self._language_column = column_name\n        else:\n            raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame")\n    else:\n        # Tente de détecter automatiquement une colonne candidate\n        text_columns = self.df.select_dtypes(include=[\'object\', \'string\']).columns\n        if len(text_columns) == 0:\n            raise ValueError("Aucune colonne de type texte trouvée dans le DataFrame")\n        if len(text_columns) == 1:\n            self._language_column = text_columns[0]\n        else:\n            # Sélectionne la colonne avec le plus de texte non vide\n            text_lengths = []\n            for col in text_columns:\n                non_empty = self.df[col].dropna().astype(str).str.len()\n                text_lengths.append(non_empty.sum())\n            self._language_column = text_columns[np.argmax(text_lengths)]\n\n    self._log("detect_language_column", params={\'column_name\': column_name})\n    return self'
    },
    "detect_merged_cells": {
        "params": ['df'],
        "docstring": "Détecte et corrige les cellules fusionnées dans le DataFrame.\nLes cellules fusionnées sont identifiées par des valeurs en double\ndans une même colonne, suivies immédiatement d'une cellule vide.",
        "code": 'def detect_merged_cells(self, df=None):\n    """\n    Détecte et corrige les cellules fusionnées dans le DataFrame.\n    Les cellules fusionnées sont identifiées par des valeurs en double\n    dans une même colonne, suivies immédiatement d\'une cellule vide.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    for column in df.columns:\n        prev_value = None\n        for i in range(len(df)):\n            current_value = df.at[i, column]\n            if pd.isna(current_value):\n                if not pd.isna(prev_value) and df.at[i-1, column] == prev_value:\n                    df.at[i, column] = prev_value\n            prev_value = current_value\n\n    self._log("detect_merged_cells", params={})\n    return self'
    },
    "detect_missing_patterns": {
        "params": ['threshold'],
        "docstring": 'Détecte les motifs de valeurs manquantes dans le DataFrame.\nIdentifie les colonnes avec un taux de valeurs manquantes supérieur au seuil donné,\net les motifs potentiels (comme des lignes ou colonnes entières manquantes).',
        "code": 'def detect_missing_patterns(self, threshold=0.5):\n    """\n    Détecte les motifs de valeurs manquantes dans le DataFrame.\n    Identifie les colonnes avec un taux de valeurs manquantes supérieur au seuil donné,\n    et les motifs potentiels (comme des lignes ou colonnes entières manquantes).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le taux de valeurs manquantes par colonne\n    missing_percent = self.df.isnull().mean()\n\n    # Identifier les colonnes avec un taux de valeurs manquantes supérieur au seuil\n    high_missing_cols = missing_percent[missing_percent > threshold].index.tolist()\n\n    # Identifier les lignes avec des valeurs manquantes\n    missing_rows = self.df[self.df.isnull().any(axis=1)]\n\n    # Stocker les résultats dans des attributs de la classe\n    self._missing_patterns = {\n        \'high_missing_columns\': high_missing_cols,\n        \'rows_with_missing_values\': missing_rows.index.tolist()\n    }\n\n    self._log("detect_missing_patterns", params={\'threshold\': threshold})\n    return self'
    },
    "detect_multiline_headers": {
        "params": ['df'],
        "docstring": 'Detects and handles multiline headers in the DataFrame.\nIf headers span multiple rows, they are combined into a single header row.',
        "code": 'def detect_multiline_headers(self, df=None):\n    """\n    Detects and handles multiline headers in the DataFrame.\n    If headers span multiple rows, they are combined into a single header row.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    # Detect multiline headers by checking if the first row contains non-string values\n    header_rows = []\n    for i, row in df.iterrows():\n        if not all(isinstance(x, str) for x in row):\n            header_rows.append(i)\n        else:\n            break\n\n    if len(header_rows) > 1:\n        # Combine multiline headers\n        new_headers = []\n        for col in range(len(df.columns)):\n            header_parts = [str(df.iloc[i, col]) for i in header_rows]\n            new_headers.append(\' \'.join(header_parts))\n        df.columns = new_headers\n        # Drop the header rows from data\n        df = df.iloc[len(header_rows):].reset_index(drop=True)\n\n    self.df = df\n    self._log("detect_multiline_headers", params={})\n    return self'
    },
    "detect_multimodal_distributions": {
        "params": ['columns', 'threshold'],
        "docstring": "Détecte les distributions multimodales dans les colonnes numériques du DataFrame.\nUtilise le test de dip pour identifier les distributions potentiellement multimodales.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à analyser. Si None, toutes les colonnes numériques sont analysées.\nthreshold : float, default=0.5\n    Seuil pour considérer une distribution comme multimodale (valeur du test de dip).\n\nReturns:\n--------\nself : DataCleaner\n    Retourne l'instance actuelle pour permettre le chaînage de méthodes.",
        "code": 'def detect_multimodal_distributions(self, columns=None, threshold=0.5):\n    """\n    Détecte les distributions multimodales dans les colonnes numériques du DataFrame.\n    Utilise le test de dip pour identifier les distributions potentiellement multimodales.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à analyser. Si None, toutes les colonnes numériques sont analysées.\n    threshold : float, default=0.5\n        Seuil pour considérer une distribution comme multimodale (valeur du test de dip).\n\n    Returns:\n    --------\n    self : DataCleaner\n        Retourne l\'instance actuelle pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    import numpy as np\n    from scipy.stats import norm\n    from sklearn.neighbors import KernelDensity\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n\n    multimodal_columns = {}\n\n    for col in columns:\n        if self.df[col].nunique() < 2:\n            continue\n\n        # Estimation de la densité avec KDE\n        X = self.df[col].values.reshape(-1, 1)\n        kde = KernelDensity(bandwidth=0.5, kernel=\'gaussian\')\n        kde.fit(X)\n\n        # Calcul du test de dip\n        from dipy.inference import dip_test\n        dip_value = dip_test(X.flatten())[0]\n\n        if dip_value > threshold:\n            multimodal_columns[col] = {\n                \'dip_value\': dip_value,\n                \'is_multimodal\': True\n            }\n        else:\n            multimodal_columns[col] = {\n                \'dip_value\': dip_value,\n                \'is_multimodal\': False\n            }\n\n    self.multimodal_distributions = multimodal_columns\n\n    self._log("detect_multimodal_distributions", params={\n        \'columns\': columns,\n        \'threshold\': threshold\n    })\n    return self'
    },
    "detect_multivariate_anomalies": {
        "params": ['method', 'contamination', 'random_state'],
        "docstring": 'Détecte les anomalies multivariées dans le DataFrame en utilisant différentes méthodes.',
        "code": 'def detect_multivariate_anomalies(self, method=\'isolation_forest\', contamination=0.01, random_state=None):\n    """\n    Détecte les anomalies multivariées dans le DataFrame en utilisant différentes méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if method == \'isolation_forest\':\n        from sklearn.ensemble import IsolationForest\n        model = IsolationForest(contamination=contamination, random_state=random_state)\n    elif method == \'one_class_svm\':\n        from sklearn.svm import OneClassSVM\n        model = OneClassSVM(nu=contamination)\n    elif method == \'local_outlier_factor\':\n        from sklearn.neighbors import LocalOutlierFactor\n        model = LocalOutlierFactor(n_neighbors=20, contamination=contamination)\n    else:\n        raise ValueError(f"Méthode non supportée: {method}")\n\n    self.df[\'anomaly_score\'] = model.fit_predict(self.df.select_dtypes(include=[\'number\']))\n    self.df[\'is_anomaly\'] = (self.df[\'anomaly_score\'] == -1)\n\n    self._log("detect_multivariate_anomalies", params={\n        \'method\': method,\n        \'contamination\': contamination,\n        \'random_state\': random_state\n    })\n    return self'
    },
    "detect_outlier_rows": {
        "params": ['columns', 'threshold'],
        "docstring": "Détecte et marque les lignes aberrantes dans le DataFrame en utilisant la méthode Z-score.\nLes lignes aberrantes sont marquées avec un flag dans une nouvelle colonne 'outlier'.",
        "code": 'def detect_outlier_rows(self, columns=None, threshold=3.0):\n    """\n    Détecte et marque les lignes aberrantes dans le DataFrame en utilisant la méthode Z-score.\n    Les lignes aberrantes sont marquées avec un flag dans une nouvelle colonne \'outlier\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if self.df[col].dtype in [\'int64\', \'float64\']:\n            z_scores = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n            outliers = abs(z_scores) > threshold\n            if \'outlier\' not in self.df.columns:\n                self.df[\'outlier\'] = False\n            self.df[\'outlier\'] = self.df[\'outlier\'] | outliers\n\n    self._log("detect_outlier_rows", params={\'columns\': columns, \'threshold\': threshold})\n    return self'
    },
    "detect_outliers_iqr": {
        "params": ['columns', 'threshold'],
        "docstring": "Détecte et marque les valeurs aberrantes dans les colonnes spécifiées en utilisant la méthode IQR.\nLes valeurs aberrantes sont marquées dans une nouvelle colonne '_outlier' avec True/False.",
        "code": 'def detect_outliers_iqr(self, columns=None, threshold=1.5):\n    """\n    Détecte et marque les valeurs aberrantes dans les colonnes spécifiées en utilisant la méthode IQR.\n    Les valeurs aberrantes sont marquées dans une nouvelle colonne \'_outlier\' avec True/False.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if col not in self.df.columns:\n            continue\n\n        Q1 = self.df[col].quantile(0.25)\n        Q3 = self.df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - threshold * IQR\n        upper_bound = Q3 + threshold * IQR\n\n        self.df[f\'{col}_outlier\'] = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)\n\n    self._log("detect_outliers_iqr", params={\'columns\': columns, \'threshold\': threshold})\n    return self'
    },
    "detect_outliers_isolation_forest": {
        "params": ['contamination', 'random_state'],
        "docstring": "Détecte les outliers dans le DataFrame en utilisant l'Isolation Forest.\nLes colonnes numériques sont utilisées pour la détection des outliers.",
        "code": 'def detect_outliers_isolation_forest(self, contamination="auto", random_state=None):\n    """\n    Détecte les outliers dans le DataFrame en utilisant l\'Isolation Forest.\n    Les colonnes numériques sont utilisées pour la détection des outliers.\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.ensemble import IsolationForest\n\n    # Sélection des colonnes numériques\n    numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n\n    if len(numeric_cols) == 0:\n        self._log("detect_outliers_isolation_forest", message="Aucune colonne numérique trouvée")\n        return self\n\n    # Création et entraînement du modèle Isolation Forest\n    model = IsolationForest(contamination=contamination, random_state=random_state)\n    outliers = model.fit_predict(self.df[numeric_cols])\n\n    # Ajout d\'une colonne \'is_outlier\' au DataFrame\n    self.df[\'is_outlier\'] = outliers == -1\n\n    self._log("detect_outliers_isolation_forest", params={\n        "contamination": contamination,\n        "random_state": random_state\n    })\n    return self'
    },
    "detect_outliers_lof": {
        "params": ['contamination', 'n_neighbors'],
        "docstring": "Détecte les outliers dans le DataFrame en utilisant l'algorithme Local Outlier Factor (LOF).",
        "code": 'def detect_outliers_lof(self, contamination=0.1, n_neighbors=20):\n    """\n    Détecte les outliers dans le DataFrame en utilisant l\'algorithme Local Outlier Factor (LOF).\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.neighbors import LocalOutlierFactor\n\n    # Sélection des colonnes numériques\n    numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n\n    # Application de LOF\n    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n    outliers = lof.fit_predict(self.df[numeric_cols])\n\n    # Ajout d\'une colonne \'is_outlier\' au DataFrame\n    self.df[\'is_outlier\'] = outliers == -1\n\n    self._log("detect_outliers_lof", params={\'contamination\': contamination, \'n_neighbors\': n_neighbors})\n    return self'
    },
    "detect_outliers_robust": {
        "params": ['columns', 'threshold'],
        "docstring": 'Detect and handle outliers using a robust method (IQR) for specified columns.\nReplaces outliers with the nearest non-outlier value in the column.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to process. If None, all numeric columns are processed.\nthreshold : float, optional\n    The multiplier for the IQR to determine outlier bounds (default is 1.5).',
        "code": 'def detect_outliers_robust(self, columns=None, threshold=1.5):\n    """\n    Detect and handle outliers using a robust method (IQR) for specified columns.\n    Replaces outliers with the nearest non-outlier value in the column.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to process. If None, all numeric columns are processed.\n    threshold : float, optional\n        The multiplier for the IQR to determine outlier bounds (default is 1.5).\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n\n    for col in columns:\n        if col not in self.df.columns:\n            continue\n\n        Q1 = self.df[col].quantile(0.25)\n        Q3 = self.df[col].quantile(0.75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - threshold * IQR\n        upper_bound = Q3 + threshold * IQR\n\n        outliers_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)\n\n        if outliers_mask.any():\n            # Find the nearest non-outlier values for each outlier\n            self.df.loc[outliers_mask, col] = self.df.loc[~outliers_mask, col].values\n\n    self._log("detect_outliers_robust", params={\'columns\': columns, \'threshold\': threshold})\n    return self'
    },
    "detect_outliers_zscore": {
        "params": ['df', 'threshold'],
        "docstring": "Detect and remove outliers using the Z-score method.\n\nParameters:\n-----------\ndf : pd.DataFrame, optional\n    DataFrame to process. If None, uses the instance's dataframe.\nthreshold : float, default=3.0\n    Z-score threshold for outlier detection.\n\nReturns:\n--------\nDataCleaner\n    The instance itself to allow method chaining.",
        "code": 'def detect_outliers_zscore(self, df=None, threshold=3.0):\n    """\n    Detect and remove outliers using the Z-score method.\n\n    Parameters:\n    -----------\n    df : pd.DataFrame, optional\n        DataFrame to process. If None, uses the instance\'s dataframe.\n    threshold : float, default=3.0\n        Z-score threshold for outlier detection.\n\n    Returns:\n    --------\n    DataCleaner\n        The instance itself to allow method chaining.\n    """\n    self._validate_df()\n    self._backup()\n\n    df = df if df is not None else self.df\n\n    # Calculate Z-scores for numerical columns\n    numerical_cols = df.select_dtypes(include=[\'number\']).columns\n    z_scores = (df[numerical_cols] - df[numerical_cols].mean()) / df[numerical_cols].std()\n\n    # Identify outliers\n    outlier_mask = (abs(z_scores) > threshold).any(axis=1)\n\n    # Remove outliers\n    self.df = df[~outlier_mask]\n\n    self._log("detect_outliers_zscore", params={\'threshold\': threshold})\n    return self'
    },
    "detect_partial_duplicates": {
        "params": ['threshold', 'subset'],
        "docstring": "Détecte et marque les lignes partiellement dupliquées dans le DataFrame.\nDeux lignes sont considérées comme partiellement dupliquées si elles partagent\nau moins 'threshold' pourcentage de valeurs identiques dans les colonnes spécifiées.",
        "code": 'def detect_partial_duplicates(self, threshold=0.8, subset=None):\n    """\n    Détecte et marque les lignes partiellement dupliquées dans le DataFrame.\n    Deux lignes sont considérées comme partiellement dupliquées si elles partagent\n    au moins \'threshold\' pourcentage de valeurs identiques dans les colonnes spécifiées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calcul des similarités entre lignes\n    if subset is None:\n        subset = self.df.columns\n\n    n_rows = len(self.df)\n    similarity_matrix = np.zeros((n_rows, n_rows))\n\n    for i in range(n_rows):\n        for j in range(i+1, n_rows):\n            common_values = sum(self.df.iloc[i][subset] == self.df.iloc[j][subset])\n            similarity = common_values / len(subset)\n            if similarity >= threshold:\n                similarity_matrix[i][j] = similarity\n                similarity_matrix[j][i] = similarity\n\n    # Marquer les lignes dupliquées\n    self.df[\'is_partial_duplicate\'] = False\n    for i in range(n_rows):\n        if np.any(similarity_matrix[i] >= threshold):\n            self.df.at[self.df.index[i], \'is_partial_duplicate\'] = True\n\n    self._log("detect_partial_duplicates", params={\'threshold\': threshold, \'subset\': subset})\n    return self'
    },
    "detect_schema": {
        "params": ['df'],
        "docstring": 'Détecte et standardise le schéma des données (types de colonnes, valeurs manquantes).',
        "code": 'def detect_schema(self, df=None):\n    """\n    Détecte et standardise le schéma des données (types de colonnes, valeurs manquantes).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Détection des types de données\n    for col in df.columns:\n        if df[col].dtype == \'object\':\n            # Essai de conversion en datetime si possible\n            try:\n                df[col] = pd.to_datetime(df[col])\n            except (ValueError, TypeError):\n                pass\n        elif df[col].dtype == \'float64\':\n            # Conversion en int si possible (évite les floats inutiles)\n            if df[col].dropna().mod(1).eq(0).all():\n                df[col] = df[col].astype(\'Int64\')\n\n    # Détection des valeurs manquantes\n    self.missing_values = df.isnull().sum()\n\n    self._log("detect_schema", params={})\n    return self'
    },
    "detect_schema_drift": {
        "params": ['reference_df'],
        "docstring": 'Detecte les changements de schéma entre le DataFrame actuel et un DataFrame de référence.',
        "code": 'def detect_schema_drift(self, reference_df=None):\n    """\n    Detecte les changements de schéma entre le DataFrame actuel et un DataFrame de référence.\n    """\n    self._validate_df()\n    self._backup()\n\n    if reference_df is None:\n        raise ValueError("Un DataFrame de référence est requis pour détecter le drift du schéma.")\n\n    # Vérification des colonnes manquantes ou supplémentaires\n    current_columns = set(self.df.columns)\n    reference_columns = set(reference_df.columns)\n\n    missing_columns = reference_columns - current_columns\n    extra_columns = current_columns - reference_columns\n\n    # Vérification des types de données\n    dtype_drift = {}\n    for col in current_columns.intersection(reference_columns):\n        if str(self.df[col].dtype) != str(reference_df[col].dtype):\n            dtype_drift[col] = {\n                \'current\': str(self.df[col].dtype),\n                \'reference\': str(reference_df[col].dtype)\n            }\n\n    # Stockage des résultats\n    self.schema_drift = {\n        \'missing_columns\': list(missing_columns),\n        \'extra_columns\': list(extra_columns),\n        \'dtype_drift\': dtype_drift\n    }\n\n    self._log("detect_schema_drift", params={\n        \'missing_columns\': len(missing_columns),\n        \'extra_columns\': len(extra_columns),\n        \'dtype_drift_count\': len(dtype_drift)\n    })\n    return self'
    },
    "detect_sensitive_data": {
        "params": ['columns', 'patterns'],
        "docstring": 'Détecte et masque les données sensibles dans le DataFrame.\nLes colonnes à analyser peuvent être spécifiées, ainsi que des motifs de données sensibles.',
        "code": 'def detect_sensitive_data(self, columns=None, patterns=None):\n    """\n    Détecte et masque les données sensibles dans le DataFrame.\n    Les colonnes à analyser peuvent être spécifiées, ainsi que des motifs de données sensibles.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Définir les motifs par défaut si non spécifiés\n    default_patterns = {\n        \'email\': r\'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\',\n        \'phone\': r\'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\',\n        \'ssn\': r\'\\b\\d{3}-\\d{2}-\\d{4}\\b\'\n    }\n\n    # Utiliser les motifs fournis ou les motifs par défaut\n    search_patterns = patterns if patterns else default_patterns\n\n    # Si aucune colonne spécifiée, analyser toutes les colonnes de type string\n    cols_to_check = columns if columns else self.df.select_dtypes(include=[\'object\']).columns\n\n    # Détection et masquage des données sensibles\n    for col in cols_to_check:\n        for pattern_name, regex_pattern in search_patterns.items():\n            self.df[col] = self.df[col].apply(\n                lambda x: re.sub(regex_pattern, f\'[SENSITIVE_{pattern_name.upper()}]\', str(x))\n                if isinstance(x, str) else x\n            )\n\n    self._log("detect_sensitive_data", params={\'columns\': columns, \'patterns\': patterns})\n    return self'
    },
    "detect_sentiment": {
        "params": ['text_column', 'sentiment_column'],
        "docstring": "Détecte le sentiment des textes dans une colonne spécifiée et ajoute les résultats dans une nouvelle colonne.\nUtilise un modèle de sentiment simple basé sur des mots clés positifs et négatifs.\n\nParameters\n----------\ntext_column : str, optional\n    Nom de la colonne contenant les textes à analyser (default is 'text')\nsentiment_column : str, optional\n    Nom de la colonne où stocker les résultats du sentiment (default is 'sentiment')\n\nReturns\n-------\nDataCleaner\n    Retourne l'instance actuelle pour permettre le chaînage des méthodes.",
        "code": 'def detect_sentiment(self, text_column: str = \'text\', sentiment_column: str = \'sentiment\'):\n    """\n    Détecte le sentiment des textes dans une colonne spécifiée et ajoute les résultats dans une nouvelle colonne.\n    Utilise un modèle de sentiment simple basé sur des mots clés positifs et négatifs.\n\n    Parameters\n    ----------\n    text_column : str, optional\n        Nom de la colonne contenant les textes à analyser (default is \'text\')\n    sentiment_column : str, optional\n        Nom de la colonne où stocker les résultats du sentiment (default is \'sentiment\')\n\n    Returns\n    -------\n    DataCleaner\n        Retourne l\'instance actuelle pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Liste de mots positifs et négatifs\n    positive_words = [\'bon\', \'super\', \'excellent\', \'content\', \'heureux\']\n    negative_words = [\'mauvais\', \'nul\', \'horrible\', \'triste\', \'mécontent\']\n\n    # Fonction pour détecter le sentiment\n    def get_sentiment(text):\n        if pd.isna(text):\n            return None\n\n        text_lower = str(text).lower()\n        positive_count = sum(word in text_lower for word in positive_words)\n        negative_count = sum(word in text_lower for word in negative_words)\n\n        if positive_count > negative_count:\n            return \'positif\'\n        elif negative_count > positive_count:\n            return \'négatif\'\n        else:\n            return \'neutre\'\n\n    # Application de la détection de sentiment\n    self.df[sentiment_column] = self.df[text_column].apply(get_sentiment)\n\n    self._log("detect_sentiment", params={\'text_column\': text_column, \'sentiment_column\': sentiment_column})\n    return self'
    },
    "detect_skew": {
        "params": ['columns'],
        "docstring": 'Detects skewness in numerical columns of the DataFrame.\nFor each specified column, calculates and stores skewness values.',
        "code": 'def detect_skew(self, columns=None):\n    """\n    Detects skewness in numerical columns of the DataFrame.\n    For each specified column, calculates and stores skewness values.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numerical_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numerical_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    skewness = {}\n    for col in numerical_cols:\n        skewness[col] = self.df[col].skew()\n\n    self.skewness = skewness\n\n    self._log("detect_skew", params={\'columns\': columns})\n    return self'
    },
    "detect_text_outliers": {
        "params": ['text_columns'],
        "docstring": "Détecte et marque les valeurs aberrantes dans les colonnes de texte.\nLes valeurs aberrantes sont définies comme des chaînes dont la longueur est\nen dehors d'un intervalle interquartile (IQR) calculé sur les longueurs des chaînes.",
        "code": 'def detect_text_outliers(self, text_columns=None):\n    """\n    Détecte et marque les valeurs aberrantes dans les colonnes de texte.\n    Les valeurs aberrantes sont définies comme des chaînes dont la longueur est\n    en dehors d\'un intervalle interquartile (IQR) calculé sur les longueurs des chaînes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if text_columns is None:\n        text_columns = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in text_columns:\n        lengths = self.df[col].astype(str).apply(len)\n        q1, q3 = lengths.quantile(0.25), lengths.quantile(0.75)\n        iqr = q3 - q1\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        self.df[f\'{col}_outlier\'] = (\n            (lengths < lower_bound) |\n            (lengths > upper_bound)\n        ).astype(int)\n\n    self._log("detect_text_outliers", params={\'text_columns\': text_columns})\n    return self'
    },
    "detect_time_columns": {
        "params": ['threshold'],
        "docstring": 'Détecte et marque les colonnes potentiellement temporelles dans le DataFrame.\nUtilise un seuil pour identifier les colonnes contenant des valeurs numériques\nqui pourraient représenter des heures, minutes ou secondes.',
        "code": 'def detect_time_columns(self, threshold=0.5):\n    """\n    Détecte et marque les colonnes potentiellement temporelles dans le DataFrame.\n    Utilise un seuil pour identifier les colonnes contenant des valeurs numériques\n    qui pourraient représenter des heures, minutes ou secondes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Détection des colonnes temporelles potentielles\n    for col in self.df.columns:\n        if pd.api.types.is_numeric_dtype(self.df[col]):\n            # Vérification des valeurs entre 0 et 24 pour les heures\n            hour_mask = (self.df[col] >= 0) & (self.df[col] <= 24)\n            # Vérification des valeurs entre 0 et 60 pour les minutes/secondes\n            minute_mask = (self.df[col] >= 0) & (self.df[col] <= 60)\n\n            # Si plus de X% des valeurs correspondent à ces plages\n            if (hour_mask.mean() > threshold) or (minute_mask.mean() > threshold):\n                self.df[f\'is_{col}_time\'] = hour_mask | minute_mask\n\n    self._log("detect_time_columns", params={\'threshold\': threshold})\n    return self'
    },
    "detect_time_drift": {
        "params": ['datetime_columns'],
        "docstring": "Détecte les dérives temporelles dans les colonnes de type datetime.\nSi aucune colonne n'est spécifiée, toutes les colonnes de type datetime sont analysées.\n\nArgs:\n    datetime_columns (list): Liste des noms de colonnes à analyser. Par défaut, None.",
        "code": 'def detect_time_drift(self, datetime_columns=None):\n    """\n    Détecte les dérives temporelles dans les colonnes de type datetime.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type datetime sont analysées.\n\n    Args:\n        datetime_columns (list): Liste des noms de colonnes à analyser. Par défaut, None.\n    """\n    self._validate_df()\n    self._backup()\n\n    if datetime_columns is None:\n        datetime_columns = self.df.select_dtypes(include=[\'datetime64\']).columns.tolist()\n\n    for col in datetime_columns:\n        if col not in self.df.columns:\n            raise ValueError(f"Colonne {col} non trouvée dans le DataFrame")\n\n    self._log("detect_time_drift", params={\'datetime_columns\': datetime_columns})\n    return self'
    },
    "detect_typo_clusters": {
        "params": ['column_name', 'threshold'],
        "docstring": 'Détecte et regroupe les fautes de frappe potentielles dans une colonne en utilisant la similarité des chaînes.',
        "code": 'def detect_typo_clusters(self, column_name: str, threshold: float = 0.85):\n    """\n    Détecte et regroupe les fautes de frappe potentielles dans une colonne en utilisant la similarité des chaînes.\n    """\n    self._validate_df()\n    self._backup()\n\n    from fuzzywuzzy import fuzz\n    import pandas as pd\n\n    # Vérification de l\'existence de la colonne\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame.")\n\n    # Création d\'une copie de la colonne pour éviter les modifications directes\n    column_data = self.df[column_name].dropna().unique()\n\n    # Initialisation des clusters\n    clusters = []\n    used_indices = set()\n\n    for i, str1 in enumerate(column_data):\n        if i in used_indices:\n            continue\n\n        # Nouveau cluster pour la chaîne actuelle\n        current_cluster = [str1]\n        used_indices.add(i)\n\n        for j, str2 in enumerate(column_data):\n            if j in used_indices:\n                continue\n\n            # Calcul de la similarité\n            similarity = fuzz.ratio(str1.lower(), str2.lower()) / 100\n\n            if similarity >= threshold:\n                current_cluster.append(str2)\n                used_indices.add(j)\n\n        clusters.append(current_cluster)\n\n    # Ajout des clusters au DataFrame\n    self.df[\'typo_clusters\'] = None\n    for cluster in clusters:\n        if len(cluster) > 1:\n            for typo in cluster:\n                self.df.loc[self.df[column_name] == typo, \'typo_clusters\'] = \', \'.join(cluster)\n\n    self._log("detect_typo_clusters", params={\'column_name\': column_name, \'threshold\': threshold})\n    return self'
    },
    "distribution_summary": {
        "params": ['columns'],
        "docstring": "Génère un résumé des distributions statistiques pour les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont analysées.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    Liste des noms de colonnes à analyser. Si None, toutes les colonnes numériques sont utilisées.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance courante pour permettre le chaînage des méthodes.",
        "code": 'def distribution_summary(self, columns=None):\n    """\n    Génère un résumé des distributions statistiques pour les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont analysées.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        Liste des noms de colonnes à analyser. Si None, toutes les colonnes numériques sont utilisées.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance courante pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    summary = pd.DataFrame({\n        \'count\': self.df[numeric_cols].count(),\n        \'mean\': self.df[numeric_cols].mean(),\n        \'std\': self.df[numeric_cols].std(),\n        \'min\': self.df[numeric_cols].min(),\n        \'25%\': self.df[numeric_cols].quantile(0.25),\n        \'50%\': self.df[numeric_cols].quantile(0.5),\n        \'75%\': self.df[numeric_cols].quantile(0.75),\n        \'max\': self.df[numeric_cols].max()\n    })\n\n    self.distribution_summary = summary\n    self._log("distribution_summary", params={\'columns\': columns})\n    return self'
    },
    "drop_missing_columns": {
        "params": ['threshold'],
        "docstring": 'Supprime les colonnes contenant trop de valeurs manquantes selon un seuil donné.',
        "code": 'def drop_missing_columns(self, threshold=0.5):\n    """\n    Supprime les colonnes contenant trop de valeurs manquantes selon un seuil donné.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le pourcentage de valeurs manquantes par colonne\n    missing_percent = self.df.isnull().mean()\n\n    # Sélectionner les colonnes à conserver\n    cols_to_keep = missing_percent[missing_percent <= threshold].index\n\n    # Supprimer les colonnes avec trop de valeurs manquantes\n    self.df = self.df[cols_to_keep]\n\n    self._log("drop_missing_columns", params={\'threshold\': threshold})\n    return self'
    },
    "drop_missing_rows": {
        "params": ['subset', 'how', 'inplace'],
        "docstring": "Supprime les lignes contenant des valeurs manquantes selon les critères spécifiés.\n\nParameters:\n    subset (list, optional): Liste des colonnes à vérifier pour les valeurs manquantes.\n                            Si None, toutes les colonnes sont vérifiées.\n    how (str, optional): Méthode de suppression ('any' ou 'all'). 'any' supprime si au moins une valeur manquante,\n                        'all' supprime uniquement si toutes les valeurs sont manquantes.\n    inplace (bool, optional): Si True, modifie le DataFrame en place. Sinon, retourne une nouvelle instance.\n\nReturns:\n    DataCleaner: Instance de la classe pour permettre le chaînage des méthodes.",
        "code": 'def drop_missing_rows(self, subset=None, how=\'any\', inplace=False):\n    """\n    Supprime les lignes contenant des valeurs manquantes selon les critères spécifiés.\n\n    Parameters:\n        subset (list, optional): Liste des colonnes à vérifier pour les valeurs manquantes.\n                                Si None, toutes les colonnes sont vérifiées.\n        how (str, optional): Méthode de suppression (\'any\' ou \'all\'). \'any\' supprime si au moins une valeur manquante,\n                            \'all\' supprime uniquement si toutes les valeurs sont manquantes.\n        inplace (bool, optional): Si True, modifie le DataFrame en place. Sinon, retourne une nouvelle instance.\n\n    Returns:\n        DataCleaner: Instance de la classe pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if inplace:\n        self.df.dropna(subset=subset, how=how, inplace=True)\n    else:\n        self.df = self.df.dropna(subset=subset, how=how)\n\n    self._log("drop_missing_rows", params={\'subset\': subset, \'how\': how, \'inplace\': inplace})\n    return self'
    },
    "encode_ordinal": {
        "params": ['column_name', 'mapping'],
        "docstring": 'Encode une colonne avec des valeurs ordinales en utilisant un dictionnaire de mappage.\n\nArgs:\n    column_name (str): Nom de la colonne à encoder.\n    mapping (dict): Dictionnaire où les clés sont les valeurs originales\n                    et les valeurs sont les encodages numériques correspondants.',
        "code": 'def encode_ordinal(self, column_name: str, mapping: dict):\n    """\n    Encode une colonne avec des valeurs ordinales en utilisant un dictionnaire de mappage.\n\n    Args:\n        column_name (str): Nom de la colonne à encoder.\n        mapping (dict): Dictionnaire où les clés sont les valeurs originales\n                        et les valeurs sont les encodages numériques correspondants.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification que la colonne existe\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame.")\n\n    # Encodage des valeurs\n    self.df[column_name] = self.df[column_name].map(mapping)\n\n    # Gestion des valeurs NaN (si nécessaire)\n    if self.df[column_name].isna().any():\n        default_value = mapping.get(\'NaN\', None)\n        if default_value is not None:\n            self.df[column_name] = self.df[column_name].fillna(default_value)\n\n    self._log("encode_ordinal", params={\'column_name\': column_name, \'mapping\': mapping})\n    return self'
    },
    "expand_dataset_long_format": {
        "params": ['id_columns', 'value_columns'],
        "docstring": 'Transforme un DataFrame en format large vers un format long.\nLes colonnes spécifiées dans id_columns deviennent les identifiants,\net les valeurs des colonnes value_columns sont empilées.',
        "code": 'def expand_dataset_long_format(self, id_columns=None, value_columns=None):\n    """\n    Transforme un DataFrame en format large vers un format long.\n    Les colonnes spécifiées dans id_columns deviennent les identifiants,\n    et les valeurs des colonnes value_columns sont empilées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if id_columns is None:\n        id_columns = [col for col in self.df.columns if col not in value_columns]\n    elif isinstance(id_columns, str):\n        id_columns = [id_columns]\n\n    if value_columns is None:\n        value_columns = [col for col in self.df.columns if col not in id_columns]\n    elif isinstance(value_columns, str):\n        value_columns = [value_columns]\n\n    self.df = pd.melt(\n        self.df,\n        id_vars=id_columns,\n        value_vars=value_columns,\n        var_name=\'variable\',\n        value_name=\'value\'\n    )\n\n    self._log("expand_dataset_long_format", params={\n        \'id_columns\': id_columns,\n        \'value_columns\': value_columns\n    })\n    return self'
    },
    "export_cleaned_csv": {
        "params": ['file_path', 'index'],
        "docstring": "Exporte le DataFrame nettoyé au format CSV à l'emplacement spécifié.\n\nArgs:\n    file_path (str): Chemin du fichier de sortie CSV.\n    index (bool, optional): Indique si l'index doit être exporté. Par défaut False.",
        "code": 'def export_cleaned_csv(self, file_path: str, index: bool = False):\n    """\n    Exporte le DataFrame nettoyé au format CSV à l\'emplacement spécifié.\n\n    Args:\n        file_path (str): Chemin du fichier de sortie CSV.\n        index (bool, optional): Indique si l\'index doit être exporté. Par défaut False.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Sauvegarde du DataFrame nettoyé en CSV\n    self.df.to_csv(file_path, index=index)\n\n    self._log("export_cleaned_csv", params={"file_path": file_path, "index": index})\n    return self'
    },
    "export_cleaned_excel": {
        "params": ['file_path', 'sheet_name', 'index'],
        "docstring": "Exporte les données nettoyées vers un fichier Excel.\n\nArgs:\n    file_path (str): Chemin du fichier Excel de destination\n    sheet_name (str, optional): Nom de la feuille de calcul. Par défaut 'Cleaned Data'\n    index (bool, optional): Indique si l'index doit être exporté. Par défaut False",
        "code": 'def export_cleaned_excel(self, file_path: str, sheet_name: str = \'Cleaned Data\', index: bool = False):\n    """\n    Exporte les données nettoyées vers un fichier Excel.\n\n    Args:\n        file_path (str): Chemin du fichier Excel de destination\n        sheet_name (str, optional): Nom de la feuille de calcul. Par défaut \'Cleaned Data\'\n        index (bool, optional): Indique si l\'index doit être exporté. Par défaut False\n    """\n    self._validate_df()\n    self._backup()\n\n    self.df.to_excel(file_path, sheet_name=sheet_name, index=index)\n\n    self._log("export_cleaned_excel", params={\'file_path\': file_path, \'sheet_name\': sheet_name, \'index\': index})\n    return self'
    },
    "export_cleaned_json": {
        "params": ['file_path', 'orient'],
        "docstring": "Exporte les données nettoyées au format JSON.\n\nArgs:\n    file_path: Chemin du fichier de sortie\n    orient: Orientation du JSON (par défaut 'records')\n    **kwargs: Arguments supplémentaires pour to_json",
        "code": 'def export_cleaned_json(self, file_path: str, orient: str = \'records\', **kwargs):\n    """\n    Exporte les données nettoyées au format JSON.\n\n    Args:\n        file_path: Chemin du fichier de sortie\n        orient: Orientation du JSON (par défaut \'records\')\n        **kwargs: Arguments supplémentaires pour to_json\n    """\n    self._validate_df()\n    self._backup()\n\n    self.df.to_json(file_path, orient=orient, **kwargs)\n\n    self._log("export_cleaned_json", params={\'file_path\': file_path, \'orient\': orient})\n    return self'
    },
    "export_pipeline_script": {
        "params": ['output_path'],
        "docstring": 'Exporte le pipeline de nettoyage sous forme de script Python.',
        "code": 'def export_pipeline_script(self, output_path: str):\n    """\n    Exporte le pipeline de nettoyage sous forme de script Python.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Récupération des opérations effectuées\n    operations = []\n    for op in self._operations:\n        if hasattr(op, \'to_script\'):\n            operations.append(op.to_script())\n        else:\n            operations.append(str(op))\n\n    # Création du script\n    script = f"""# Pipeline de nettoyage généré automatiquement\\n\\nimport pandas as pd\\n\\ndef clean_data(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Nettoyage des données selon le pipeline enregistré\\n    \\"\\"\\"\\n    df = pd.DataFrame(df)\\n\\n"""\n    for op in operations:\n        script += f"    {op}\\n"\n    script += "\\n    return df\\n"\n\n    # Écriture du fichier\n    with open(output_path, \'w\', encoding=\'utf-8\') as f:\n        f.write(script)\n\n    self._log("export_pipeline_script", params={\'output_path\': output_path})\n    return self'
    },
    "extract_city": {
        "params": ['column_name'],
        "docstring": "Extrait la ville d'une colonne contenant des adresses ou des informations géographiques.\nSupprime les autres informations et conserve uniquement le nom de la ville.\n\nArgs:\n    column_name (str): Nom de la colonne contenant les informations géographiques.",
        "code": 'def extract_city(self, column_name: str):\n    """\n    Extrait la ville d\'une colonne contenant des adresses ou des informations géographiques.\n    Supprime les autres informations et conserve uniquement le nom de la ville.\n\n    Args:\n        column_name (str): Nom de la colonne contenant les informations géographiques.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification de l\'existence de la colonne\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame.")\n\n    # Nettoyage et extraction de la ville\n    self.df[column_name] = self.df[column_name].astype(str).str.extract(r\'([A-Z][a-z]+(?: [A-Z][a-z]+)*)\', expand=False)\n\n    self._log("extract_city", params={"column_name": column_name})\n    return self'
    },
    "extract_country": {
        "params": ['column_name', 'country_column'],
        "docstring": "Extrait et nettoie les informations de pays à partir d'une colonne spécifiée.\nLes valeurs sont standardisées et placées dans une nouvelle colonne 'country'.",
        "code": 'def extract_country(self, column_name: str, country_column: str = \'country\'):\n    """\n    Extrait et nettoie les informations de pays à partir d\'une colonne spécifiée.\n    Les valeurs sont standardisées et placées dans une nouvelle colonne \'country\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Nettoyage et extraction des pays\n    self.df[country_column] = self.df[column_name].str.extract(r\'([A-Z][a-z]+(?: [A-Z][a-z]+)*)\', expand=False)\n    self.df[country_column] = self.df[country_column].str.title()\n\n    # Suppression des doublons et standardisation\n    unique_countries = self.df[country_column].unique()\n    country_mapping = {c: c.strip() for c in unique_countries}\n    self.df[country_column] = self.df[country_column].map(country_mapping)\n\n    self._log("extract_country", params={\'column_name\': column_name, \'country_column\': country_column})\n    return self'
    },
    "extract_date_components": {
        "params": ['date_column', 'output_columns'],
        "docstring": "Extrait les composantes d'une colonne de dates (année, mois, jour) et les ajoute au DataFrame.\nSi output_columns est spécifié, utilise ces noms pour les nouvelles colonnes.",
        "code": 'def extract_date_components(self, date_column: str, output_columns: list = None):\n    """\n    Extrait les composantes d\'une colonne de dates (année, mois, jour) et les ajoute au DataFrame.\n    Si output_columns est spécifié, utilise ces noms pour les nouvelles colonnes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Définir les noms par défaut des colonnes de sortie\n    default_output_columns = [\'year\', \'month\', \'day\']\n    output_columns = output_columns or default_output_columns\n\n    # Convertir la colonne de dates en datetime si ce n\'est pas déjà fait\n    self.df[date_column] = pd.to_datetime(self.df[date_column], errors=\'coerce\')\n\n    # Extraire les composantes et les ajouter au DataFrame\n    self.df[output_columns[0]] = self.df[date_column].dt.year\n    self.df[output_columns[1]] = self.df[date_column].dt.month\n    self.df[output_columns[2]] = self.df[date_column].dt.day\n\n    self._log("extract_date_components", params={\'date_column\': date_column, \'output_columns\': output_columns})\n    return self'
    },
    "extract_domain": {
        "params": ['column_name'],
        "docstring": "Extrait le domaine d'un URL dans une colonne spécifiée.",
        "code": 'def extract_domain(self, column_name):\n    """\n    Extrait le domaine d\'un URL dans une colonne spécifiée.\n    """\n    self._validate_df()\n    self._backup()\n\n    import re\n    pattern = r\'(?:https?:\\/\\/)?(?:www\\.)?([^\\/]+)\'\n    self.df[column_name] = self.df[column_name].apply(lambda x: re.search(pattern, str(x)).group(1) if pd.notna(x) and re.search(pattern, str(x)) else None)\n\n    self._log("extract_domain", params={\'column_name\': column_name})\n    return self'
    },
    "extract_hostname": {
        "params": ['url_column'],
        "docstring": "Extrait le nom d'hôte des URLs dans la colonne spécifiée.",
        "code": 'def extract_hostname(self, url_column: str):\n    """\n    Extrait le nom d\'hôte des URLs dans la colonne spécifiée.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification que la colonne existe\n    if url_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{url_column}\' n\'existe pas dans le DataFrame")\n\n    # Extraction du nom d\'hôte\n    self.df[\'hostname\'] = self.df[url_column].str.extract(r\'(?P<hostname>[^/]+)\\.\', expand=False)\n    self.df[\'hostname\'] = self.df[\'hostname\'].str.replace(r\'www\\.\', \'\')\n\n    self._log("extract_hostname", params={\'url_column\': url_column})\n    return self'
    },
    "extract_keywords": {
        "params": ['column_name', 'keyword_list', 'new_column_name'],
        "docstring": 'Extracts keywords from a specified column and creates a new column with the found keywords.\nIf no keyword is found, the value in the new column will be NaN.\n\nParameters:\n    column_name (str): The name of the column to search for keywords.\n    keyword_list (list): List of keywords to search for in the specified column.\n    new_column_name (str, optional): Name for the new column. Defaults to f"{column_name}_keywords".\n\nReturns:\n    DataCleaner: The instance of the DataCleaner class for method chaining.',
        "code": 'def extract_keywords(self, column_name: str, keyword_list: list, new_column_name: str = None):\n    """\n    Extracts keywords from a specified column and creates a new column with the found keywords.\n    If no keyword is found, the value in the new column will be NaN.\n\n    Parameters:\n        column_name (str): The name of the column to search for keywords.\n        keyword_list (list): List of keywords to search for in the specified column.\n        new_column_name (str, optional): Name for the new column. Defaults to f"{column_name}_keywords".\n\n    Returns:\n        DataCleaner: The instance of the DataCleaner class for method chaining.\n    """\n    self._validate_df()\n    self._backup()\n\n    if new_column_name is None:\n        new_column_name = f"{column_name}_keywords"\n\n    self.df[new_column_name] = self.df[column_name].apply(\n        lambda x: [kw for kw in keyword_list if kw.lower() in str(x).lower()] if pd.notna(x) else None\n    )\n\n    self._log("extract_keywords", params={\n        "column_name": column_name,\n        "keyword_list": keyword_list,\n        "new_column_name": new_column_name\n    })\n    return self'
    },
    "extract_ngrams": {
        "params": ['text_column', 'n'],
        "docstring": 'Extracts n-grams from a specified text column and adds them as new columns.\n\nParameters:\n    text_column (str): Name of the column containing the text to process\n    n (int): Size of the n-grams to extract (default: 2 for bigrams)',
        "code": 'def extract_ngrams(self, text_column: str, n: int = 2):\n    """\n    Extracts n-grams from a specified text column and adds them as new columns.\n\n    Parameters:\n        text_column (str): Name of the column containing the text to process\n        n (int): Size of the n-grams to extract (default: 2 for bigrams)\n    """\n    self._validate_df()\n    self._backup()\n\n    # Create n-grams and add as new columns\n    for i in range(1, n + 1):\n        self.df[f\'ngram_{i}\'] = self.df[text_column].apply(\n            lambda x: \' \'.join(ngram for ngram in zip(*[x.split()[j:] for j in range(i)]))\n        )\n\n    self._log("extract_ngrams", params={\'text_column\': text_column, \'n\': n})\n    return self'
    },
    "extract_path": {
        "params": ['path_column', 'new_column_name'],
        "docstring": "Extrait le chemin d'accès à partir d'une colonne contenant des chemins de fichiers.\nSi new_column_name est fourni, crée une nouvelle colonne avec le chemin extrait,\nsinon remplace la colonne d'origine.",
        "code": 'def extract_path(self, path_column: str, new_column_name: str = None):\n    """\n    Extrait le chemin d\'accès à partir d\'une colonne contenant des chemins de fichiers.\n    Si new_column_name est fourni, crée une nouvelle colonne avec le chemin extrait,\n    sinon remplace la colonne d\'origine.\n    """\n    self._validate_df()\n    self._backup()\n\n    if new_column_name is None:\n        new_column_name = path_column\n\n    # Extraire le chemin d\'accès\n    self.df[new_column_name] = self.df[path_column].str.extract(r\'(.*/)(?=[^/]*$)\')\n\n    self._log("extract_path", params={"path_column": path_column, "new_column_name": new_column_name})\n    return self'
    },
    "extract_postal_code": {
        "params": ['column_name'],
        "docstring": "Extrait et nettoie les codes postaux d'une colonne donnée.\nSupprime les caractères non numériques, sauf le premier caractère qui peut être une lettre (pour les codes postaux comme '75001' ou 'H7N 3E2').",
        "code": 'def extract_postal_code(self, column_name: str):\n    """\n    Extrait et nettoie les codes postaux d\'une colonne donnée.\n    Supprime les caractères non numériques, sauf le premier caractère qui peut être une lettre (pour les codes postaux comme \'75001\' ou \'H7N 3E2\').\n    """\n    self._validate_df()\n    self._backup()\n\n    # Code principal\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne {column_name} n\'existe pas dans le DataFrame.")\n\n    def clean_postal_code(code):\n        if pd.isna(code):\n            return np.nan\n        # Conserver le premier caractère (lettre ou chiffre) et les chiffres suivants\n        cleaned = re.sub(r\'^[a-zA-Z0-9]\', \'\', str(code))\n        cleaned = re.sub(r\'[^a-zA-Z0-9]\', \'\', cleaned)\n        return cleaned\n\n    self.df[column_name] = self.df[column_name].apply(clean_postal_code)\n\n    self._log("extract_postal_code", params={\'column_name\': column_name})\n    return self'
    },
    "extract_seasonal_features": {
        "params": ['date_column', 'season_columns'],
        "docstring": 'Extracts seasonal features (month, quarter, day of week) from a date column.\nIf season_columns is provided, adds these columns to the dataframe.',
        "code": 'def extract_seasonal_features(self, date_column: str, season_columns: list = None):\n    """\n    Extracts seasonal features (month, quarter, day of week) from a date column.\n    If season_columns is provided, adds these columns to the dataframe.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Convert date column to datetime if not already\n    self.df[date_column] = pd.to_datetime(self.df[date_column])\n\n    # Extract seasonal features\n    self.df[\'month\'] = self.df[date_column].dt.month\n    self.df[\'quarter\'] = self.df[date_column].dt.quarter\n    self.df[\'day_of_week\'] = self.df[date_column].dt.dayofweek\n\n    # Add custom season columns if provided\n    if season_columns:\n        for col in season_columns:\n            self.df[col] = pd.to_datetime(self.df[date_column]).dt.strftime(col)\n\n    self._log("extract_seasonal_features", params={\n        \'date_column\': date_column,\n        \'season_columns\': season_columns\n    })\n    return self'
    },
    "fix_encoding_errors": {
        "params": ['df'],
        "docstring": "Corrige les erreurs d'encodage dans le DataFrame.\nRemplace les caractères spéciaux mal encodés par leurs équivalents corrects.",
        "code": 'def fix_encoding_errors(self, df=None):\n    """\n    Corrige les erreurs d\'encodage dans le DataFrame.\n    Remplace les caractères spéciaux mal encodés par leurs équivalents corrects.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    # Remplacer les caractères mal encodés par leurs équivalents corrects\n    df = df.applymap(lambda x: x.replace(\'\\x92\', "\'") if isinstance(x, str) else x)\n    df = df.applymap(lambda x: x.replace(\'\\x93\', \'"\') if isinstance(x, str) else x)\n    df = df.applymap(lambda x: x.replace(\'\\x94\', \'"\') if isinstance(x, str) else x)\n    df = df.applymap(lambda x: x.replace(\'\\x85\', \'...\') if isinstance(x, str) else x)\n    df = df.applymap(lambda x: x.replace(\'\\x96\', \'-\') if isinstance(x, str) else x)\n    df = df.applymap(lambda x: x.replace(\'\\x97\', \'-\') if isinstance(x, str) else x)\n\n    self.df = df\n\n    self._log("fix_encoding_errors", params={})\n    return self'
    },
    "fix_misplaced_decimal": {
        "params": ['columns'],
        "docstring": 'Corrige les décimales mal placées dans les colonnes spécifiées.\nLes valeurs sont multipliées par 10^facteur pour corriger leur position.',
        "code": 'def fix_misplaced_decimal(self, columns=None):\n    """\n    Corrige les décimales mal placées dans les colonnes spécifiées.\n    Les valeurs sont multipliées par 10^facteur pour corriger leur position.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        # Calculer le facteur de correction basé sur la distribution des valeurs\n        values = self.df[col].dropna()\n        if len(values) == 0:\n            continue\n\n        # Estimation du facteur (ex: si les valeurs sont entre 10 et 20, facteur probable = -1)\n        mean_val = values.mean()\n        order_magnitude = int(np.log10(mean_val)) if mean_val != 0 else 0\n        factor = -order_magnitude\n\n        # Application de la correction\n        self.df[col] = values * (10 ** factor)\n\n    self._log("fix_misplaced_decimal", params={\'columns\': columns})\n    return self'
    },
    "fix_thousand_separator": {
        "params": ['columns'],
        "docstring": "Remplace les séparateurs de milliers (espace ou point) par des virgules et convertit en float.\nSi aucun colonne n'est spécifiée, traite toutes les colonnes de type object contenant des séparateurs.",
        "code": 'def fix_thousand_separator(self, columns=None):\n    """\n    Remplace les séparateurs de milliers (espace ou point) par des virgules et convertit en float.\n    Si aucun colonne n\'est spécifiée, traite toutes les colonnes de type object contenant des séparateurs.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = [col for col in self.df.columns\n                  if self.df[col].dtype == \'object\' and any(\' \' in str(val) or \'.\' in str(val)\n                                                          for val in self.df[col].sample(min(10, len(self.df))) if pd.notna(val))]\n\n    for col in columns:\n        self.df[col] = (self.df[col].replace({r\'\\s+\': \'\', r\'\\.\': \'\'}, regex=True)\n                        .str.replace(\',\', \'.\')\n                        .astype(float))\n\n    self._log("fix_thousand_separator", params={\'columns\': columns})\n    return self'
    },
    "flag_imputed": {
        "params": ['columns'],
        "docstring": "Marque les valeurs imputées dans le DataFrame avec un suffixe '_imputed'.\nSi aucun colonne n'est spécifiée, toutes les colonnes sont traitées.",
        "code": 'def flag_imputed(self, columns=None):\n    """\n    Marque les valeurs imputées dans le DataFrame avec un suffixe \'_imputed\'.\n    Si aucun colonne n\'est spécifiée, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        imputed_col = f"{col}_imputed"\n        self.df[imputed_col] = self.df[col].isna().astype(int)\n\n    self._log("flag_imputed", params={"columns": columns})\n    return self'
    },
    "floor_datetime": {
        "params": ['column_name', 'freq'],
        "docstring": "Truncates datetime values in the specified column to a given frequency.\nFor example, with freq='D', it will keep only the date part (truncating time).",
        "code": 'def floor_datetime(self, column_name: str, freq: str = \'D\'):\n    """\n    Truncates datetime values in the specified column to a given frequency.\n    For example, with freq=\'D\', it will keep only the date part (truncating time).\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"Column \'{column_name}\' not found in DataFrame")\n\n    self.df[column_name] = self.df[column_name].dt.floor(freq)\n\n    self._log("floor_datetime", params={\'column_name\': column_name, \'freq\': freq})\n    return self'
    },
    "frequency_encode": {
        "params": ['column_name'],
        "docstring": "Applique un encodage par fréquence sur une colonne spécifiée.\nRemplace les valeurs de la colonne par leur fréquence d'apparition dans l'ensemble des données.",
        "code": 'def frequency_encode(self, column_name: str):\n    """\n    Applique un encodage par fréquence sur une colonne spécifiée.\n    Remplace les valeurs de la colonne par leur fréquence d\'apparition dans l\'ensemble des données.\n    """\n    self._validate_df()\n    self._backup()\n\n    frequency_map = self.df[column_name].value_counts(normalize=True).to_dict()\n    self.df[column_name] = self.df[column_name].map(frequency_map)\n\n    self._log("frequency_encode", params={"column_name": column_name})\n    return self'
    },
    "fuzzy_match_categories": {
        "params": ['df_target', 'category_column', 'mapping_dict', 'threshold'],
        "docstring": "Applique une correspondance floue entre les catégories d'un DataFrame et un dictionnaire de référence.\nUtilise la similarité de séquence (fuzzy matching) pour corriger les catégories mal orthographiées.\n\nParameters:\n-----------\ndf_target : pd.DataFrame\n    DataFrame contenant les données à nettoyer.\ncategory_column : str\n    Nom de la colonne contenant les catégories à corriger.\nmapping_dict : dict\n    Dictionnaire de correspondance {catégorie_correcte: [variantes_possibles]}.\nthreshold : float, optional\n    Seuil de similarité pour accepter une correspondance (0-1), par défaut 0.8.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance actuelle pour permettre le chaînage de méthodes.",
        "code": 'def fuzzy_match_categories(self, df_target, category_column, mapping_dict, threshold=0.8):\n    """\n    Applique une correspondance floue entre les catégories d\'un DataFrame et un dictionnaire de référence.\n    Utilise la similarité de séquence (fuzzy matching) pour corriger les catégories mal orthographiées.\n\n    Parameters:\n    -----------\n    df_target : pd.DataFrame\n        DataFrame contenant les données à nettoyer.\n    category_column : str\n        Nom de la colonne contenant les catégories à corriger.\n    mapping_dict : dict\n        Dictionnaire de correspondance {catégorie_correcte: [variantes_possibles]}.\n    threshold : float, optional\n        Seuil de similarité pour accepter une correspondance (0-1), par défaut 0.8.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance actuelle pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    from fuzzywuzzy import fuzz\n\n    def get_best_match(category, mapping_dict):\n        best_match = None\n        best_score = 0\n\n        for correct_cat, variants in mapping_dict.items():\n            if category in variants:\n                return correct_cat\n            for variant in variants + [correct_cat]:\n                score = fuzz.ratio(category.lower(), variant.lower())\n                if score > best_score and score >= threshold * 100:\n                    best_score = score\n                    best_match = correct_cat\n\n        return category if best_match is None else best_match\n\n    self.df[category_column] = self.df[category_column].apply(\n        lambda x: get_best_match(str(x), mapping_dict)\n    )\n\n    self._log("fuzzy_match_categories", params={\n        \'category_column\': category_column,\n        \'threshold\': threshold\n    })\n    return self'
    },
    "generate_anomaly_report": {
        "params": ['threshold'],
        "docstring": "Génère un rapport d'anomalies en identifiant les valeurs aberrantes dans le DataFrame.\nUtilise la méthode de l'écart-type pour détecter les anomalies.",
        "code": 'def generate_anomaly_report(self, threshold=3.0):\n    """\n    Génère un rapport d\'anomalies en identifiant les valeurs aberrantes dans le DataFrame.\n    Utilise la méthode de l\'écart-type pour détecter les anomalies.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calcul des statistiques pour chaque colonne numérique\n    numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    anomalies = {}\n\n    for col in numeric_cols:\n        mean = self.df[col].mean()\n        std = self.df[col].std()\n\n        if std == 0:\n            continue\n\n        lower_bound = mean - threshold * std\n        upper_bound = mean + threshold * std\n\n        anomalies[col] = {\n            \'mean\': mean,\n            \'std\': std,\n            \'lower_bound\': lower_bound,\n            \'upper_bound\': upper_bound,\n            \'anomalies_count\': ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum(),\n            \'anomalies_indices\': self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)].index.tolist()\n        }\n\n    self.anomaly_report = anomalies\n\n    self._log("generate_anomaly_report", params={\'threshold\': threshold})\n    return self'
    },
    "generate_cleaning_report": {
        "params": ['output_path'],
        "docstring": 'Génère un rapport de nettoyage des données avec les statistiques avant/après.',
        "code": 'def generate_cleaning_report(self, output_path=None):\n    """\n    Génère un rapport de nettoyage des données avec les statistiques avant/après.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Statistiques avant nettoyage\n    initial_stats = {\n        \'shape\': self.df.shape,\n        \'missing_values\': self.df.isnull().sum(),\n        \'duplicates\': self.df.duplicated().sum(),\n        \'dtypes\': self.df.dtypes\n    }\n\n    # Nettoyage des données (exemple de méthodes appelées)\n    self.remove_duplicates()\n    self.handle_missing_values()\n    self.convert_data_types()\n\n    # Statistiques après nettoyage\n    final_stats = {\n        \'shape\': self.df.shape,\n        \'missing_values\': self.df.isnull().sum(),\n        \'duplicates\': self.df.duplicated().sum(),\n        \'dtypes\': self.df.dtypes\n    }\n\n    # Création du rapport\n    report = {\n        \'initial_stats\': initial_stats,\n        \'final_stats\': final_stats,\n        \'operations_performed\': [\n            method.__name__ for method in [self.remove_duplicates,\n                                         self.handle_missing_values,\n                                         self.convert_data_types]\n        ]\n    }\n\n    # Sauvegarde du rapport\n    if output_path:\n        import json\n        with open(output_path, \'w\') as f:\n            json.dump(report, f)\n\n    self._log("generate_cleaning_report", params={\'output_path\': output_path})\n    return self'
    },
    "generate_data_profile": {
        "params": ['report_path'],
        "docstring": 'Génère un profil des données incluant des statistiques descriptives et des informations sur les types de données.',
        "code": 'def generate_data_profile(self, report_path=None):\n    """\n    Génère un profil des données incluant des statistiques descriptives et des informations sur les types de données.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Génération du profil des données\n    profile = {\n        \'shape\': self.df.shape,\n        \'dtypes\': self.df.dtypes.to_dict(),\n        \'describe\': self.df.describe(include=\'all\').to_dict(),\n        \'missing_values\': self.df.isnull().sum().to_dict(),\n        \'unique_values\': {col: self.df[col].nunique() for col in self.df.columns}\n    }\n\n    # Sauvegarde du profil si un chemin est fourni\n    if report_path:\n        import json\n        with open(report_path, \'w\') as f:\n            json.dump(profile, f)\n\n    self._log("generate_data_profile", params={\'report_path\': report_path})\n    return self'
    },
    "generate_missing_report": {
        "params": ['columns'],
        "docstring": 'Génère un rapport sur les valeurs manquantes dans le DataFrame.\nSi des colonnes sont spécifiées, le rapport se limite à ces colonnes.',
        "code": 'def generate_missing_report(self, columns=None):\n    """\n    Génère un rapport sur les valeurs manquantes dans le DataFrame.\n    Si des colonnes sont spécifiées, le rapport se limite à ces colonnes.\n    """\n    self._validate_df()\n    self._backup()\n\n    missing_data = {}\n    if columns is None:\n        columns = self.df.columns\n\n    for column in columns:\n        missing_count = self.df[column].isna().sum()\n        if missing_count > 0:\n            missing_data[column] = {\n                \'missing_count\': missing_count,\n                \'missing_percentage\': (missing_count / len(self.df)) * 100\n            }\n\n    self.missing_report = missing_data\n\n    self._log("generate_missing_report", params={\'columns\': columns})\n    return self'
    },
    "generate_schema_report": {
        "params": ['output_path'],
        "docstring": 'Génère un rapport sur le schéma des données (types de colonnes, valeurs manquantes, etc.).\nSi output_path est spécifié, sauvegarde le rapport dans ce fichier.',
        "code": 'def generate_schema_report(self, output_path=None):\n    """\n    Génère un rapport sur le schéma des données (types de colonnes, valeurs manquantes, etc.).\n    Si output_path est spécifié, sauvegarde le rapport dans ce fichier.\n    """\n    self._validate_df()\n    self._backup()\n\n    schema_report = {\n        \'column_names\': list(self.df.columns),\n        \'dtypes\': self.df.dtypes.to_dict(),\n        \'missing_values\': self.df.isnull().sum().to_dict(),\n        \'unique_counts\': {col: self.df[col].nunique() for col in self.df.columns},\n        \'sample_values\': {col: self.df[col].dropna().unique()[:5] for col in self.df.columns}\n    }\n\n    if output_path:\n        import json\n        with open(output_path, \'w\') as f:\n            json.dump(schema_report, f)\n\n    self._log("generate_schema_report", params={\'output_path\': output_path})\n    return self'
    },
    "geocode_addresses": {
        "params": ['address_column', 'lat_column', 'lng_column'],
        "docstring": "Géocode les adresses d'un DataFrame en utilisant l'API de géocodage.\nAjoute les colonnes latitude et longitude au DataFrame si elles n'existent pas déjà.\n\nParameters:\n    address_column (str): Nom de la colonne contenant les adresses à géocoder.\n    lat_column (str): Nom de la colonne où stocker les latitudes.\n    lng_column (str): Nom de la colonne où stocker les longitudes.\n\nReturns:\n    DataCleaner: Instance actuelle pour permettre le chaînage de méthodes.",
        "code": 'def geocode_addresses(self, address_column=\'address\', lat_column=\'latitude\', lng_column=\'longitude\'):\n    """\n    Géocode les adresses d\'un DataFrame en utilisant l\'API de géocodage.\n    Ajoute les colonnes latitude et longitude au DataFrame si elles n\'existent pas déjà.\n\n    Parameters:\n        address_column (str): Nom de la colonne contenant les adresses à géocoder.\n        lat_column (str): Nom de la colonne où stocker les latitudes.\n        lng_column (str): Nom de la colonne où stocker les longitudes.\n\n    Returns:\n        DataCleaner: Instance actuelle pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérifier si les colonnes d\'adresses existent\n    if address_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{address_column}\' n\'existe pas dans le DataFrame.")\n\n    # Initialiser les colonnes de géocodage si elles n\'existent pas\n    if lat_column not in self.df.columns:\n        self.df[lat_column] = None\n    if lng_column not in self.df.columns:\n        self.df[lng_column] = None\n\n    # Géocodage des adresses\n    for idx, row in self.df.iterrows():\n        address = row[address_column]\n        if pd.notna(address):\n            # Ici, vous devriez appeler votre service de géocodage\n            # Par exemple: lat, lng = geocode_service.geocode(address)\n            # Pour cet exemple, nous simulons des valeurs\n            lat, lng = self._simulate_geocode(address)\n            self.df.at[idx, lat_column] = lat\n            self.df.at[idx, lng_column] = lng\n\n    self._log("geocode_addresses", params={\n        \'address_column\': address_column,\n        \'lat_column\': lat_column,\n        \'lng_column\': lng_column\n    })\n    return self'
    },
    "harmonize_labels": {
        "params": ['column_name', 'mapping_dict'],
        "docstring": "Harmonise les étiquettes d'une colonne en utilisant un dictionnaire de correspondance.",
        "code": 'def harmonize_labels(self, column_name: str, mapping_dict: dict):\n    """\n    Harmonise les étiquettes d\'une colonne en utilisant un dictionnaire de correspondance.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Appliquer la correspondance des étiquettes\n    self.df[column_name] = self.df[column_name].map(mapping_dict)\n\n    # Remplacer les valeurs NaN par une valeur par défaut si nécessaire\n    self.df[column_name] = self.df[column_name].fillna(\'Unknown\')\n\n    self._log("harmonize_labels", params={\'column_name\': column_name, \'mapping_dict\': mapping_dict})\n    return self'
    },
    "harmonize_timezones": {
        "params": ['timezone', 'date_columns'],
        "docstring": "Harmonise les fuseaux horaires des colonnes de dates dans le DataFrame.\nConvertit toutes les colonnes de dates spécifiées au fuseau horaire donné.\n\nParameters:\n-----------\ntimezone : str, optional\n    Le fuseau horaire cible (par défaut 'UTC')\ndate_columns : list, optional\n    Liste des noms de colonnes contenant des dates à harmoniser.\n    Si None, toutes les colonnes de type datetime sont traitées.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance courante pour le chaînage de méthodes.",
        "code": 'def harmonize_timezones(self, timezone=\'UTC\', date_columns=None):\n    """\n    Harmonise les fuseaux horaires des colonnes de dates dans le DataFrame.\n    Convertit toutes les colonnes de dates spécifiées au fuseau horaire donné.\n\n    Parameters:\n    -----------\n    timezone : str, optional\n        Le fuseau horaire cible (par défaut \'UTC\')\n    date_columns : list, optional\n        Liste des noms de colonnes contenant des dates à harmoniser.\n        Si None, toutes les colonnes de type datetime sont traitées.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance courante pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        date_columns = self.df.select_dtypes(include=[\'datetime\']).columns.tolist()\n\n    for col in date_columns:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].dt.tz_convert(timezone)\n\n    self._log("harmonize_timezones", params={\'timezone\': timezone, \'date_columns\': date_columns})\n    return self'
    },
    "hash_encode": {
        "params": ['columns_to_hash'],
        "docstring": 'Applique un encodage par hachage sur les colonnes spécifiées.',
        "code": 'def hash_encode(self, columns_to_hash=None):\n    """\n    Applique un encodage par hachage sur les colonnes spécifiées.\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n\n    if columns_to_hash is None:\n        columns_to_hash = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n\n    for col in columns_to_hash:\n        if col in self.df.columns:\n            # Appliquer un encodage par hachage\n            self.df[col] = pd.util.hash_pandas_object(self.df[col]).astype(int)\n\n    self._log("hash_encode", params={\'columns_to_hash\': columns_to_hash})\n    return self'
    },
    "hash_sensitive_columns": {
        "params": ['columns'],
        "docstring": "Hash les colonnes sensibles du DataFrame pour protéger les données personnelles.\nSi aucune colonne n'est spécifiée, toutes les colonnes contenant 'email', 'phone' ou 'ssn'\ndans leur nom seront hachées par défaut.",
        "code": 'def hash_sensitive_columns(self, columns=None):\n    """\n    Hash les colonnes sensibles du DataFrame pour protéger les données personnelles.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes contenant \'email\', \'phone\' ou \'ssn\'\n    dans leur nom seront hachées par défaut.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        # Détection automatique des colonnes sensibles\n        sensitive_columns = [col for col in self.df.columns if any(keyword in col.lower() for keyword in [\'email\', \'phone\', \'ssn\'])]\n    else:\n        sensitive_columns = columns\n\n    for col in sensitive_columns:\n        if col in self.df.columns:\n            # Application du hachage (exemple avec SHA-256)\n            self.df[col] = self.df[col].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest() if pd.notna(x) else x)\n\n    self._log("hash_sensitive_columns", params={\'columns\': sensitive_columns})\n    return self'
    },
    "impute_categorical_by_prob": {
        "params": ['columns', 'strategy', 'threshold'],
        "docstring": "Impute missing values in categorical columns based on probability or most frequent value.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to impute. If None, all categorical columns are processed.\nstrategy : str, optional\n    Imputation strategy ('most_frequent' or 'probability'). Default is 'most_frequent'.\nthreshold : float, optional\n    Threshold for probability strategy (0-1). Default is 0.5.\n\nReturns:\n--------\nDataCleaner\n    The instance itself.",
        "code": 'def impute_categorical_by_prob(self, columns=None, strategy=\'most_frequent\', threshold=0.5):\n    """\n    Impute missing values in categorical columns based on probability or most frequent value.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to impute. If None, all categorical columns are processed.\n    strategy : str, optional\n        Imputation strategy (\'most_frequent\' or \'probability\'). Default is \'most_frequent\'.\n    threshold : float, optional\n        Threshold for probability strategy (0-1). Default is 0.5.\n\n    Returns:\n    --------\n    DataCleaner\n        The instance itself.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\', \'category\']).columns\n\n    for col in columns:\n        if strategy == \'most_frequent\':\n            mode_val = self.df[col].mode()[0]\n            self.df[col].fillna(mode_val, inplace=True)\n        elif strategy == \'probability\':\n            value_counts = self.df[col].value_counts(normalize=True)\n            mask = (self.df[col].isna()) & (value_counts[self.df[col]].ge(threshold))\n            self.df.loc[mask, col] = value_counts[self.df[col]].idxmax()\n        else:\n            raise ValueError("Invalid strategy. Choose \'most_frequent\' or \'probability\'.")\n\n    self._log("impute_categorical_by_prob", params={\'columns\': columns, \'strategy\': strategy, \'threshold\': threshold})\n    return self'
    },
    "impute_groupwise_mean": {
        "params": ['group_columns', 'value_column'],
        "docstring": "Impute les valeurs manquantes dans une colonne de valeurs en utilisant la moyenne des groupes définis par d'autres colonnes.\n\nArgs:\n    group_columns (list): Liste de noms de colonnes utilisées pour définir les groupes.\n    value_column (str): Nom de la colonne contenant les valeurs à imputer.\n\nReturns:\n    DataCleaner: Instance actuelle pour permettre le chaînage des méthodes.",
        "code": 'def impute_groupwise_mean(self, group_columns, value_column):\n    """\n    Impute les valeurs manquantes dans une colonne de valeurs en utilisant la moyenne des groupes définis par d\'autres colonnes.\n\n    Args:\n        group_columns (list): Liste de noms de colonnes utilisées pour définir les groupes.\n        value_column (str): Nom de la colonne contenant les valeurs à imputer.\n\n    Returns:\n        DataCleaner: Instance actuelle pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la moyenne par groupe\n    group_means = self.df.groupby(group_columns)[value_column].transform(\'mean\')\n\n    # Imputer les valeurs manquantes avec la moyenne du groupe correspondant\n    self.df[value_column] = self.df[value_column].fillna(group_means)\n\n    self._log("impute_groupwise_mean", params={\'group_columns\': group_columns, \'value_column\': value_column})\n    return self'
    },
    "impute_groupwise_median": {
        "params": ['group_cols', 'value_cols'],
        "docstring": 'Impute missing values with the median of each group.\n\nParameters:\n-----------\ngroup_cols : list, optional\n    List of column names to group by. If None, uses all columns.\nvalue_cols : list, optional\n    List of column names to impute. If None, uses all numeric columns.',
        "code": 'def impute_groupwise_median(self, group_cols=None, value_cols=None):\n    """\n    Impute missing values with the median of each group.\n\n    Parameters:\n    -----------\n    group_cols : list, optional\n        List of column names to group by. If None, uses all columns.\n    value_cols : list, optional\n        List of column names to impute. If None, uses all numeric columns.\n    """\n    self._validate_df()\n    self._backup()\n\n    if group_cols is None:\n        group_cols = list(self.df.columns)\n    if value_cols is None:\n        value_cols = self.df.select_dtypes(include=[\'number\']).columns\n\n    median_values = self.df.groupby(group_cols)[value_cols].transform(\'median\')\n    self.df[value_cols] = self.df[value_cols].fillna(median_values)\n\n    self._log("impute_groupwise_median", params={\'group_cols\': group_cols, \'value_cols\': value_cols})\n    return self'
    },
    "impute_iterative": {
        "params": ['columns', 'max_iter', 'tol'],
        "docstring": "Impute les valeurs manquantes de manière itérative en utilisant la régression multiple.\nPour chaque colonne avec des valeurs manquantes, utilise les autres colonnes comme prédicteurs.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à imputer. Si None, toutes les colonnes avec valeurs manquantes sont traitées.\nmax_iter : int, optional\n    Nombre maximal d'itérations (par défaut 10).\ntol : float, optional\n    Tolérance pour la convergence (par défaut 1e-4).\n\nReturns:\n--------\nself : DataCleaner\n    Retourne l'instance pour le chaînage de méthodes.",
        "code": 'def impute_iterative(self, columns=None, max_iter=10, tol=1e-4):\n    """\n    Impute les valeurs manquantes de manière itérative en utilisant la régression multiple.\n    Pour chaque colonne avec des valeurs manquantes, utilise les autres colonnes comme prédicteurs.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à imputer. Si None, toutes les colonnes avec valeurs manquantes sont traitées.\n    max_iter : int, optional\n        Nombre maximal d\'itérations (par défaut 10).\n    tol : float, optional\n        Tolérance pour la convergence (par défaut 1e-4).\n\n    Returns:\n    --------\n    self : DataCleaner\n        Retourne l\'instance pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = [col for col in self.df.columns if self.df[col].isna().any()]\n\n    for _ in range(max_iter):\n        prev_df = self.df.copy()\n        for col in columns:\n            if self.df[col].isna().any():\n                mask = self.df[col].isna()\n                X = self.df.drop(columns=[col])\n                y = self.df[col]\n\n                from sklearn.linear_model import LinearRegression\n                model = LinearRegression()\n                model.fit(X[~mask], y[~mask])\n                self.df.loc[mask, col] = model.predict(X[mask])\n\n        if (self.df - prev_df).abs().max().max() < tol:\n            break\n\n    self._log("impute_iterative", params={"columns": columns, "max_iter": max_iter, "tol": tol})\n    return self'
    },
    "impute_knn": {
        "params": ['n_neighbors', 'weights', 'columns'],
        "docstring": "Impute missing values using K-Nearest Neighbors (KNN).\n\nParameters:\n-----------\nn_neighbors : int, optional\n    Number of neighbors to use (default is 5).\nweights : str or callable, optional\n    Weight function used in prediction (default is 'uniform').\ncolumns : list of str, optional\n    List of column names to impute (default is all numeric columns).",
        "code": 'def impute_knn(self, n_neighbors=5, weights=\'uniform\', columns=None):\n    """\n    Impute missing values using K-Nearest Neighbors (KNN).\n\n    Parameters:\n    -----------\n    n_neighbors : int, optional\n        Number of neighbors to use (default is 5).\n    weights : str or callable, optional\n        Weight function used in prediction (default is \'uniform\').\n    columns : list of str, optional\n        List of column names to impute (default is all numeric columns).\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.impute import KNNImputer\n    import numpy as np\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n\n    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n    self.df[columns] = imputer.fit_transform(self.df[columns])\n\n    self._log("impute_knn", params={\'n_neighbors\': n_neighbors, \'weights\': weights, \'columns\': columns})\n    return self'
    },
    "impute_mean": {
        "params": ['columns'],
        "docstring": "Remplace les valeurs manquantes par la moyenne des colonnes spécifiées.\nSi aucune colonne n'est spécifiée, applique l'imputation sur toutes les colonnes numériques.",
        "code": 'def impute_mean(self, columns=None):\n    """\n    Remplace les valeurs manquantes par la moyenne des colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, applique l\'imputation sur toutes les colonnes numériques.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        if self.df[col].isna().any():\n            mean_val = self.df[col].mean()\n            self.df[col].fillna(mean_val, inplace=True)\n\n    self._log("impute_mean", params={\'columns\': columns})\n    return self'
    },
    "impute_median": {
        "params": ['columns'],
        "docstring": "Remplace les valeurs manquantes par la médiane des colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont traitées.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    Liste des noms de colonnes à traiter. Si None, toutes les colonnes numériques sont traitées.",
        "code": 'def impute_median(self, columns=None):\n    """\n    Remplace les valeurs manquantes par la médiane des colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont traitées.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        Liste des noms de colonnes à traiter. Si None, toutes les colonnes numériques sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col])]\n\n    for col in numeric_cols:\n        median_val = self.df[col].median()\n        self.df[col] = self.df[col].fillna(median_val)\n\n    self._log("impute_median", params={\'columns\': columns})\n    return self'
    },
    "impute_mode": {
        "params": ['columns'],
        "docstring": "Impute les valeurs manquantes par la mode des colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes sont traitées.\n\nParameters\n----------\ncolumns : list of str or None, optional\n    Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.",
        "code": 'def impute_mode(self, columns=None):\n    """\n    Impute les valeurs manquantes par la mode des colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes sont traitées.\n\n    Parameters\n    ----------\n    columns : list of str or None, optional\n        Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        mode_value = self.df[col].mode()\n        if not mode_value.empty:\n            self.df[col] = self.df[col].fillna(mode_value[0])\n\n    self._log("impute_mode", params={"columns": columns})\n    return self'
    },
    "impute_regression": {
        "params": ['target_column', 'feature_columns', 'model_type', 'n_neighbors'],
        "docstring": "Impute missing values in the target column using a regression model trained on feature columns.\n\nParameters:\n    target_column (str): Name of the column with missing values to impute\n    feature_columns (list): List of column names used as features for the regression model\n    model_type (str): Type of regression model to use ('linear', 'kNN')\n    n_neighbors (int): Number of neighbors for k-NN regression (only used when model_type='kNN')",
        "code": 'def impute_regression(self, target_column: str, feature_columns: list, model_type: str = \'linear\', n_neighbors: int = 5):\n    """\n    Impute missing values in the target column using a regression model trained on feature columns.\n\n    Parameters:\n        target_column (str): Name of the column with missing values to impute\n        feature_columns (list): List of column names used as features for the regression model\n        model_type (str): Type of regression model to use (\'linear\', \'kNN\')\n        n_neighbors (int): Number of neighbors for k-NN regression (only used when model_type=\'kNN\')\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.experimental import enable_iterative_imputer\n    from sklearn.impute import IterativeImputer, KNNImputer\n\n    # Prepare data\n    X = self.df[feature_columns]\n    y = self.df[target_column]\n\n    # Initialize imputer based on model type\n    if model_type == \'linear\':\n        imputer = IterativeImputer(random_state=0)\n    elif model_type == \'kNN\':\n        imputer = KNNImputer(n_neighbors=n_neighbors)\n    else:\n        raise ValueError("model_type must be either \'linear\' or \'kNN\'")\n\n    # Fit and transform\n    self.df[target_column] = imputer.fit_transform(X.join(y))\n\n    self._log("impute_regression", params={\n        \'target_column\': target_column,\n        \'feature_columns\': feature_columns,\n        \'model_type\': model_type,\n        \'n_neighbors\': n_neighbors\n    })\n    return self'
    },
    "impute_with_constant": {
        "params": ['value', 'columns'],
        "docstring": 'Remplace les valeurs manquantes par une constante spécifiée.\n\nParameters:\n-----------\nvalue : int, float or str\n    Valeur constante pour imputer les valeurs manquantes.\ncolumns : list of str, optional\n    Liste des colonnes à traiter. Si None, toutes les colonnes sont traitées.',
        "code": 'def impute_with_constant(self, value, columns=None):\n    """\n    Remplace les valeurs manquantes par une constante spécifiée.\n\n    Parameters:\n    -----------\n    value : int, float or str\n        Valeur constante pour imputer les valeurs manquantes.\n    columns : list of str, optional\n        Liste des colonnes à traiter. Si None, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        if col in self.df.columns:\n            self.df[col].fillna(value, inplace=True)\n\n    self._log("impute_with_constant", params={"value": value, "columns": columns})\n    return self'
    },
    "infer_dtypes": {
        "params": ['df'],
        "docstring": 'Infère et convertit les types de données des colonnes du DataFrame.\nLes types courants comme int, float, datetime et bool sont détectés automatiquement.',
        "code": 'def infer_dtypes(self, df=None):\n    """\n    Infère et convertit les types de données des colonnes du DataFrame.\n    Les types courants comme int, float, datetime et bool sont détectés automatiquement.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is not None:\n        self.df = df.copy()\n\n    for col in self.df.columns:\n        # Conversion des types numériques\n        if pd.api.types.is_numeric_dtype(self.df[col]):\n            self.df[col] = pd.to_numeric(self.df[col], errors=\'ignore\')\n\n        # Conversion des types booléens\n        elif pd.api.types.is_bool_dtype(self.df[col]):\n            self.df[col] = self.df[col].astype(\'bool\')\n\n        # Conversion des types datetime\n        elif pd.api.types.is_datetime64_any_dtype(self.df[col]):\n            self.df[col] = pd.to_datetime(self.df[col], errors=\'ignore\')\n\n        # Conversion des types catégorie\n        elif self.df[col].nunique() / len(self.df[col]) < 0.5:\n            self.df[col] = self.df[col].astype(\'category\')\n\n    self._log("infer_dtypes", params={})\n    return self'
    },
    "infer_semantic_types": {
        "params": ['df'],
        "docstring": 'Infère les types sémantiques des colonnes du DataFrame.',
        "code": 'def infer_semantic_types(self, df=None):\n    """\n    Infère les types sémantiques des colonnes du DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is not None:\n        self.df = df.copy()\n\n    # Inférence des types sémantiques\n    for col in self.df.columns:\n        if pd.api.types.is_numeric_dtype(self.df[col]):\n            # Vérification si la colonne est une date\n            if self.df[col].dtype == \'int64\' and len(self.df[col]) > 0:\n                unique_values = self.df[col].nunique()\n                total_values = len(self.df[col])\n                if unique_values / total_values < 0.1:\n                    self.df[col] = pd.to_datetime(self.df[col], errors=\'coerce\')\n            # Vérification si la colonne est une catégorie\n            elif self.df[col].nunique() / len(self.df[col]) < 0.5:\n                self.df[col] = self.df[col].astype(\'category\')\n\n        elif pd.api.types.is_datetime64_any_dtype(self.df[col]):\n            # Conversion en datetime si nécessaire\n            self.df[col] = pd.to_datetime(self.df[col])\n\n        elif pd.api.types.is_string_dtype(self.df[col]):\n            # Nettoyage des chaînes de caractères\n            self.df[col] = self.df[col].str.strip()\n            # Conversion en catégorie si peu de valeurs uniques\n            if self.df[col].nunique() / len(self.df[col]) < 0.5:\n                self.df[col] = self.df[col].astype(\'category\')\n\n    self._log("infer_semantic_types", params={})\n    return self'
    },
    "infer_time_granularity": {
        "params": ['datetime_column'],
        "docstring": "Infère la granularité temporelle d'une colonne de dates et ajoute une nouvelle colonne avec cette information.",
        "code": 'def infer_time_granularity(self, datetime_column: str):\n    """\n    Infère la granularité temporelle d\'une colonne de dates et ajoute une nouvelle colonne avec cette information.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérifier que la colonne existe\n    if datetime_column not in self.df.columns:\n        raise ValueError(f"La colonne {datetime_column} n\'existe pas dans le DataFrame")\n\n    # Convertir la colonne en datetime si ce n\'est pas déjà fait\n    self.df[datetime_column] = pd.to_datetime(self.df[datetime_column])\n\n    # Calculer les différences entre dates consécutives\n    time_diffs = self.df[datetime_column].diff().dt.total_seconds()\n\n    # Supprimer les NaN (première ligne)\n    time_diffs = time_diffs.dropna()\n\n    # Calculer les statistiques des différences\n    mean_diff = time_diffs.mean()\n    std_diff = time_diffs.std()\n\n    # Déterminer la granularité en fonction des statistiques\n    if mean_diff < 60:\n        granularity = \'second\'\n    elif mean_diff < 3600:  # 1 heure\n        granularity = \'minute\'\n    elif mean_diff < 86400:  # 1 jour\n        granularity = \'hour\'\n    elif mean_diff < 2592000:  # ~30 jours\n        granularity = \'day\'\n    elif mean_diff < 2629743:  # ~1 mois\n        granularity = \'month\'\n    else:\n        granularity = \'year\'\n\n    # Ajouter la colonne de granularité\n    self.df[\'time_granularity\'] = granularity\n\n    self._log("infer_time_granularity", params={\'datetime_column\': datetime_column})\n    return self'
    },
    "interpolate_numeric": {
        "params": ['columns'],
        "docstring": "Interpole les valeurs manquantes dans les colonnes numériques spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont traitées.",
        "code": 'def interpolate_numeric(self, columns=None):\n    """\n    Interpole les valeurs manquantes dans les colonnes numériques spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Sélectionner les colonnes numériques si aucune n\'est spécifiée\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = columns\n\n    # Interpolation linéaire pour chaque colonne numérique\n    for col in numeric_cols:\n        if self.df[col].isna().any():\n            self.df[col] = self.df[col].interpolate(method=\'linear\')\n\n    self._log("interpolate_numeric", params={\'columns\': columns})\n    return self'
    },
    "interpolate_polynomial": {
        "params": ['column_name', 'degree'],
        "docstring": "Interpole les valeurs manquantes d'une colonne avec un polynôme de degré spécifié.",
        "code": 'def interpolate_polynomial(self, column_name: str, degree: int = 2):\n    """\n    Interpole les valeurs manquantes d\'une colonne avec un polynôme de degré spécifié.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification de l\'existence de la colonne\n    if column_name not in self.df.columns:\n        raise ValueError(f"La colonne \'{column_name}\' n\'existe pas dans le DataFrame.")\n\n    # Création d\'un index pour l\'interpolation\n    x = np.arange(len(self.df))\n    mask = ~self.df[column_name].isna()\n\n    # Interpolation polynomiale\n    if sum(mask) > degree + 1:  # Besoin d\'au moins degree+1 points pour l\'interpolation\n        coeffs = np.polyfit(x[mask], self.df.loc[mask, column_name], degree)\n        poly = np.poly1d(coeffs)\n        self.df[column_name] = poly(x)\n\n    self._log("interpolate_polynomial", params={"column_name": column_name, "degree": degree})\n    return self'
    },
    "interpolate_time_series": {
        "params": ['time_column', 'value_columns', 'method'],
        "docstring": "Interpole les valeurs manquantes dans une série temporelle.\n\nArgs:\n    time_column (str): Nom de la colonne contenant les timestamps.\n    value_columns (list, optional): Liste des colonnes à interpoler. Si None, toutes les colonnes numériques sont interpolées.\n    method (str): Méthode d'interpolation ('linear', 'time', 'index', etc.).",
        "code": 'def interpolate_time_series(self, time_column=\'time\', value_columns=None, method=\'linear\'):\n    """\n    Interpole les valeurs manquantes dans une série temporelle.\n\n    Args:\n        time_column (str): Nom de la colonne contenant les timestamps.\n        value_columns (list, optional): Liste des colonnes à interpoler. Si None, toutes les colonnes numériques sont interpolées.\n        method (str): Méthode d\'interpolation (\'linear\', \'time\', \'index\', etc.).\n    """\n    self._validate_df()\n    self._backup()\n\n    if value_columns is None:\n        value_columns = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n        value_columns.remove(time_column)\n\n    for col in value_columns:\n        self.df[col] = self.df.set_index(time_column)[col].interpolate(method=method).values\n\n    self._log("interpolate_time_series", params={\'time_column\': time_column, \'value_columns\': value_columns, \'method\': method})\n    return self'
    },
    "label_encode": {
        "params": ['columns'],
        "docstring": "Applique un encodage de labels aux colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes de type object sont encodées.\n\nParameters\n----------\ncolumns : list, optional\n    Liste des noms de colonnes à encoder (par défaut None)",
        "code": 'def label_encode(self, columns=None):\n    """\n    Applique un encodage de labels aux colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type object sont encodées.\n\n    Parameters\n    ----------\n    columns : list, optional\n        Liste des noms de colonnes à encoder (par défaut None)\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.preprocessing import LabelEncoder\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n\n    for col in columns:\n        if col in self.df.columns:\n            le = LabelEncoder()\n            self.df[col] = le.fit_transform(self.df[col].astype(str))\n\n    self._log("label_encode", params={\'columns\': columns})\n    return self'
    },
    "lag_features": {
        "params": ['columns', 'periods'],
        "docstring": "Crée des caractéristiques de décalage (lag) pour les colonnes spécifiées.\nPour chaque colonne, crée une nouvelle colonne avec le suffixe '_lag_{period}'.",
        "code": 'def lag_features(self, columns=None, periods=1):\n    """\n    Crée des caractéristiques de décalage (lag) pour les colonnes spécifiées.\n    Pour chaque colonne, crée une nouvelle colonne avec le suffixe \'_lag_{period}\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns.tolist()\n\n    for col in columns:\n        if col in self.df.columns:\n            for period in periods if isinstance(periods, list) else [periods]:\n                new_col = f"{col}_lag_{period}"\n                self.df[new_col] = self.df[col].shift(period)\n\n    self._log("lag_features", params={"columns": columns, "periods": periods})\n    return self'
    },
    "lead_features": {
        "params": ['target_col'],
        "docstring": 'Nettoie et prépare les features de lead (clients potentiels) dans le DataFrame.\nSupprime les doublons, remplit les valeurs manquantes et encode les variables catégorielles.',
        "code": 'def lead_features(self, target_col: str = None):\n    """\n    Nettoie et prépare les features de lead (clients potentiels) dans le DataFrame.\n    Supprime les doublons, remplit les valeurs manquantes et encode les variables catégorielles.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des doublons\n    self.df = self.df.drop_duplicates()\n\n    # Gestion des valeurs manquantes\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\':\n            self.df[col] = self.df[col].fillna(\'Unknown\')\n        else:\n            self.df[col] = self.df[col].fillna(self.df[col].median())\n\n    # Encodage des variables catégorielles\n    categorical_cols = self.df.select_dtypes(include=[\'object\']).columns\n    for col in categorical_cols:\n        if col != target_col:  # On ne modifie pas la colonne cible\n            self.df = pd.get_dummies(self.df, columns=[col], prefix=[col])\n\n    self._log("lead_features", params={})\n    return self'
    },
    "lemmatize_text_column": {
        "params": ['text_column', 'language'],
        "docstring": "Lemmatize the specified text column in the DataFrame.\n\nArgs:\n    text_column (str): Name of the column containing text to lemmatize.\n    language (str, optional): Language for lemmatization. Defaults to 'english'.\n\nReturns:\n    DataCleaner: The instance itself for method chaining.",
        "code": 'def lemmatize_text_column(self, text_column: str, language: str = \'english\'):\n    """\n    Lemmatize the specified text column in the DataFrame.\n\n    Args:\n        text_column (str): Name of the column containing text to lemmatize.\n        language (str, optional): Language for lemmatization. Defaults to \'english\'.\n\n    Returns:\n        DataCleaner: The instance itself for method chaining.\n    """\n    self._validate_df()\n    self._backup()\n\n    import spacy\n    from spacy.lang.en import English\n\n    # Load the appropriate language model\n    if language == \'english\':\n        nlp = English()\n    else:\n        try:\n            nlp = spacy.load(language)\n        except OSError:\n            raise ValueError(f"Language model for \'{language}\' not found. Please install it first.")\n\n    # Process each text in the column\n    self.df[text_column] = self.df[text_column].apply(\n        lambda text: \' \'.join([token.lemma_ for token in nlp(text)]) if isinstance(text, str) else text\n    )\n\n    self._log("lemmatize_text_column", params={\'text_column\': text_column, \'language\': language})\n    return self'
    },
    "load_transform_history": {
        "params": ['history_df'],
        "docstring": "Charge et transforme un DataFrame d'historique des transformations.\nSupprime les colonnes inutiles, convertit les types de données et nettoie les valeurs.",
        "code": 'def load_transform_history(self, history_df):\n    """\n    Charge et transforme un DataFrame d\'historique des transformations.\n    Supprime les colonnes inutiles, convertit les types de données et nettoie les valeurs.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des colonnes inutiles\n    cols_to_drop = [\'id\', \'timestamp\']\n    history_df.drop(columns=cols_to_drop, errors=\'ignore\', inplace=True)\n\n    # Conversion des types de données\n    date_cols = [\'start_date\', \'end_date\']\n    for col in date_cols:\n        history_df[col] = pd.to_datetime(history_df[col], errors=\'coerce\')\n\n    # Nettoyage des valeurs\n    history_df.fillna({\'status\': \'unknown\', \'notes\': \'\'}, inplace=True)\n    history_df[\'notes\'] = history_df[\'notes\'].str.strip()\n\n    # Transformation des valeurs catégorielles\n    status_mapping = {\'completed\': \'done\', \'failed\': \'error\'}\n    history_df[\'status\'] = history_df[\'status\'].map(status_mapping).fillna(history_df[\'status\'])\n\n    self._log("load_transform_history", params={\'columns_dropped\': cols_to_drop})\n    return self'
    },
    "log_transform": {
        "params": ['columns'],
        "docstring": "Applique une transformation logarithmique aux colonnes spécifiées.\nSi aucune colonne n'est spécifiée, applique la transformation à toutes les colonnes numériques.",
        "code": 'def log_transform(self, columns=None):\n    """\n    Applique une transformation logarithmique aux colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, applique la transformation à toutes les colonnes numériques.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        if (self.df[col] <= 0).any():\n            raise ValueError(f"La colonne {col} contient des valeurs non positives. Impossible d\'appliquer une transformation logarithmique.")\n\n        self.df[col] = np.log(self.df[col])\n\n    self._log("log_transform", params={\'columns\': columns})\n    return self'
    },
    "map_categories": {
        "params": ['category_mapping'],
        "docstring": 'Applique un mappage de catégories à une colonne spécifiée du DataFrame.\n\nArgs:\n    category_mapping (dict): Dictionnaire contenant les anciennes valeurs\n                            comme clés et les nouvelles valeurs comme valeurs.',
        "code": 'def map_categories(self, category_mapping):\n    """\n    Applique un mappage de catégories à une colonne spécifiée du DataFrame.\n\n    Args:\n        category_mapping (dict): Dictionnaire contenant les anciennes valeurs\n                                comme clés et les nouvelles valeurs comme valeurs.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérifier si category_mapping est un dictionnaire non vide\n    if not isinstance(category_mapping, dict) or len(category_mapping) == 0:\n        raise ValueError("category_mapping doit être un dictionnaire non vide")\n\n    # Appliquer le mappage aux colonnes appropriées\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\' and any(item in self.df[col].unique() for item in category_mapping.keys()):\n            self.df[col] = self.df[col].map(category_mapping).fillna(self.df[col])\n\n    self._log("map_categories", params={\'category_mapping\': category_mapping})\n    return self'
    },
    "mark_missing_cells": {
        "params": ['columns'],
        "docstring": "Marque les cellules manquantes dans le DataFrame avec une valeur spécifique.\nSi aucun colonne n'est spécifiée, toutes les colonnes sont traitées.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.",
        "code": 'def mark_missing_cells(self, columns=None):\n    """\n    Marque les cellules manquantes dans le DataFrame avec une valeur spécifique.\n    Si aucun colonne n\'est spécifiée, toutes les colonnes sont traitées.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        self.df[col] = self.df[col].apply(lambda x: \'MISSING\' if pd.isna(x) else x)\n\n    self._log("mark_missing_cells", params={\'columns\': columns})\n    return self'
    },
    "mask_values": {
        "params": ['columns', 'value_to_mask', 'mask_value'],
        "docstring": 'Masque les valeurs spécifiées dans les colonnes données en remplaçant par une valeur de masque.\n\nArgs:\n    columns (list, optional): Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.\n    value_to_mask (scalar or list): Valeur(s) à masquer. Peut être un seul valeur ou une liste de valeurs.\n    mask_value (scalar, optional): Valeur à utiliser pour le masquage. Par défaut "masked".\n\nReturns:\n    DataCleaner: Retourne l\'instance courante pour permettre le chaînage.',
        "code": 'def mask_values(self, columns=None, value_to_mask=None, mask_value="masked"):\n    """\n    Masque les valeurs spécifiées dans les colonnes données en remplaçant par une valeur de masque.\n\n    Args:\n        columns (list, optional): Liste des noms de colonnes à traiter. Si None, toutes les colonnes sont traitées.\n        value_to_mask (scalar or list): Valeur(s) à masquer. Peut être un seul valeur ou une liste de valeurs.\n        mask_value (scalar, optional): Valeur à utiliser pour le masquage. Par défaut "masked".\n\n    Returns:\n        DataCleaner: Retourne l\'instance courante pour permettre le chaînage.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Convertir value_to_mask en liste si ce n\'est pas déjà le cas\n    if not isinstance(value_to_mask, (list, tuple)):\n        value_to_mask = [value_to_mask]\n\n    # Déterminer les colonnes à traiter\n    cols_to_process = self.df.columns if columns is None else columns\n\n    # Appliquer le masquage\n    for col in cols_to_process:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].where(\n                ~self.df[col].isin(value_to_mask),\n                other=mask_value\n            )\n\n    self._log("mask_values", params={\n        "columns": columns,\n        "value_to_mask": value_to_mask,\n        "mask_value": mask_value\n    })\n    return self'
    },
    "merge_duplicate_records": {
        "params": ['id_columns', 'value_columns', 'agg_funcs'],
        "docstring": "Fusionne les enregistrements dupliqués en utilisant des colonnes d'identification et des fonctions d'agrégation.",
        "code": 'def merge_duplicate_records(self, id_columns=None, value_columns=None, agg_funcs=None):\n    """\n    Fusionne les enregistrements dupliqués en utilisant des colonnes d\'identification et des fonctions d\'agrégation.\n    """\n    self._validate_df()\n    self._backup()\n\n    if id_columns is None:\n        id_columns = []\n    if value_columns is None:\n        value_columns = [col for col in self.df.columns if col not in id_columns]\n    if agg_funcs is None:\n        agg_funcs = {col: \'first\' for col in value_columns}\n\n    # Vérification des colonnes\n    all_columns = id_columns + list(agg_funcs.keys())\n    missing_cols = [col for col in all_columns if col not in self.df.columns]\n    if missing_cols:\n        raise ValueError(f"Les colonnes suivantes sont manquantes: {missing_cols}")\n\n    # Fusion des enregistrements dupliqués\n    self.df = (self.df.groupby(id_columns, as_index=False)\n               .agg(agg_funcs))\n\n    self._log("merge_duplicate_records", params={\n        \'id_columns\': id_columns,\n        \'value_columns\': value_columns,\n        \'agg_funcs\': agg_funcs\n    })\n    return self'
    },
    "merge_rare_categories": {
        "params": ['column', 'threshold', 'replace_with'],
        "docstring": 'Fusionne les catégories rares dans une colonne en une seule catégorie.\n\nArgs:\n    column (str): Nom de la colonne à traiter\n    threshold (int, optional): Seuil pour considérer une catégorie comme rare. Defaults to 5.\n    replace_with (str, optional): Valeur de remplacement pour les catégories rares. Defaults to "Other".',
        "code": 'def merge_rare_categories(self, column: str, threshold: int = 5, replace_with: str = "Other"):\n    """\n    Fusionne les catégories rares dans une colonne en une seule catégorie.\n\n    Args:\n        column (str): Nom de la colonne à traiter\n        threshold (int, optional): Seuil pour considérer une catégorie comme rare. Defaults to 5.\n        replace_with (str, optional): Valeur de remplacement pour les catégories rares. Defaults to "Other".\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification de l\'existence de la colonne\n    if column not in self.df.columns:\n        raise ValueError(f"La colonne \'{column}\' n\'existe pas dans le DataFrame")\n\n    # Calcul des comptages par catégorie\n    value_counts = self.df[column].value_counts()\n\n    # Identification des catégories rares\n    rare_categories = value_counts[value_counts <= threshold].index\n\n    # Remplacement des catégories rares\n    if len(rare_categories) > 0:\n        self.df[column] = self.df[column].replace(rare_categories, replace_with)\n\n    self._log("merge_rare_categories", params={"column": column, "threshold": threshold, "replace_with": replace_with})\n    return self'
    },
    "merge_similar_columns": {
        "params": ['threshold'],
        "docstring": "Fusionne les colonnes similaires dans le DataFrame en fonction d'un seuil de similarité.\nLes colonnes fusionnées sont supprimées et une nouvelle colonne est créée avec les valeurs combinées.",
        "code": 'def merge_similar_columns(self, threshold=0.85):\n    """\n    Fusionne les colonnes similaires dans le DataFrame en fonction d\'un seuil de similarité.\n    Les colonnes fusionnées sont supprimées et une nouvelle colonne est créée avec les valeurs combinées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Trouver les colonnes similaires\n    similar_columns = {}\n    columns = self.df.columns\n\n    for i in range(len(columns)):\n        if columns[i] not in similar_columns:\n            similar_columns[columns[i]] = []\n            for j in range(i + 1, len(columns)):\n                if columns[j] not in similar_columns:\n                    # Calculer la similarité entre les deux colonnes\n                    similarity = self._calculate_similarity(self.df[columns[i]], self.df[columns[j]])\n                    if similarity >= threshold:\n                        similar_columns[columns[i]].append(columns[j])\n\n    # Fusionner les colonnes similaires\n    for base_col, similar_cols in similar_columns.items():\n        if similar_cols:\n            # Créer une nouvelle colonne avec les valeurs combinées\n            self.df[base_col] = self.df[base_col].combine_first(self.df[similar_cols].fillna(\'\'))\n\n            # Supprimer les colonnes similaires\n            self.df.drop(columns=similar_cols, inplace=True)\n\n    self._log("merge_similar_columns", params={"threshold": threshold})\n    return self'
    },
    "merge_tables": {
        "params": ['df_list', 'on', 'how', 'suffixes'],
        "docstring": 'Fusionne une liste de DataFrames en un seul DataFrame sur des colonnes communes.',
        "code": 'def merge_tables(self, df_list, on=None, how=\'inner\', suffixes=(\'_x\', \'_y\')):\n    """\n    Fusionne une liste de DataFrames en un seul DataFrame sur des colonnes communes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if not isinstance(df_list, list) or len(df_list) < 2:\n        raise ValueError("df_list doit être une liste contenant au moins deux DataFrames")\n\n    merged_df = df_list[0]\n    for df in df_list[1:]:\n        merged_df = pd.merge(merged_df, df, on=on, how=how, suffixes=suffixes)\n\n    self.df = merged_df\n\n    self._log("merge_tables", params={\'on\': on, \'how\': how})\n    return self'
    },
    "minmax_scale": {
        "params": ['columns'],
        "docstring": "Applique une mise à l'échelle Min-Max aux colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont traitées.\n\nParameters\n----------\ncolumns : list of str, optional\n    Liste des noms de colonnes à mettre à l'échelle (par défaut None)",
        "code": 'def minmax_scale(self, columns=None):\n    """\n    Applique une mise à l\'échelle Min-Max aux colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont traitées.\n\n    Parameters\n    ----------\n    columns : list of str, optional\n        Liste des noms de colonnes à mettre à l\'échelle (par défaut None)\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col])]\n\n    for col in numeric_cols:\n        min_val = self.df[col].min()\n        max_val = self.df[col].max()\n\n        if min_val != max_val:  # Évite la division par zéro\n            self.df[col] = (self.df[col] - min_val) / (max_val - min_val)\n\n    self._log("minmax_scale", params={\'columns\': columns})\n    return self'
    },
    "normalize_booleans": {
        "params": ['columns'],
        "docstring": "Normalize boolean values in specified columns to True/False.\nConverts common string representations of booleans (e.g., 'yes', 'no', 'true', 'false') to actual boolean values.\nIf no columns are specified, processes all object-type columns in the DataFrame.\n\nParameters:\n    columns (list, optional): List of column names to normalize. Defaults to None.",
        "code": 'def normalize_booleans(self, columns=None):\n    """\n    Normalize boolean values in specified columns to True/False.\n    Converts common string representations of booleans (e.g., \'yes\', \'no\', \'true\', \'false\') to actual boolean values.\n    If no columns are specified, processes all object-type columns in the DataFrame.\n\n    Parameters:\n        columns (list, optional): List of column names to normalize. Defaults to None.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in columns:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].str.lower()\n            self.df[col] = self.df[col].replace({\n                \'true\': True, \'false\': False,\n                \'yes\': True, \'no\': False,\n                \'y\': True, \'n\': False,\n                \'1\': True, \'0\': False\n            })\n\n    self._log("normalize_booleans", params={\'columns\': columns})\n    return self'
    },
    "normalize_categories": {
        "params": ['column_name', 'mapping_dict'],
        "docstring": 'Normalize categories in a specified column using a provided mapping dictionary.\nIf no mapping is provided, the function will automatically normalize categories\nby converting them to lowercase and stripping whitespace.',
        "code": 'def normalize_categories(self, column_name: str, mapping_dict: dict = None):\n    """\n    Normalize categories in a specified column using a provided mapping dictionary.\n    If no mapping is provided, the function will automatically normalize categories\n    by converting them to lowercase and stripping whitespace.\n    """\n    self._validate_df()\n    self._backup()\n\n    if mapping_dict is not None:\n        self.df[column_name] = self.df[column_name].map(mapping_dict)\n    else:\n        self.df[column_name] = self.df[column_name].str.lower().str.strip()\n\n    self._log("normalize_categories", params={\'column_name\': column_name, \'mapping_dict\': mapping_dict is not None})\n    return self'
    },
    "normalize_coordinates": {
        "params": ['columns'],
        "docstring": "Normalize coordinate values in specified columns to be within [0, 1] range.\nFor each column, scales values using min-max normalization: (x - min) / (max - min).\nIf no columns specified, processes all numeric columns containing 'x' or 'y' in their names.",
        "code": 'def normalize_coordinates(self, columns=None):\n    """\n    Normalize coordinate values in specified columns to be within [0, 1] range.\n    For each column, scales values using min-max normalization: (x - min) / (max - min).\n    If no columns specified, processes all numeric columns containing \'x\' or \'y\' in their names.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n        columns = [col for col in numeric_cols if \'x\' in col.lower() or \'y\' in col.lower()]\n\n    for col in columns:\n        if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col]):\n            min_val = self.df[col].min()\n            max_val = self.df[col].max()\n            if min_val != max_val:  # Avoid division by zero\n                self.df[col] = (self.df[col] - min_val) / (max_val - min_val)\n\n    self._log("normalize_coordinates", params={\'columns\': columns})\n    return self'
    },
    "normalize_currency": {
        "params": ['column_name'],
        "docstring": 'Normalize currency values in a specified column by removing non-numeric characters and converting to float.',
        "code": 'def normalize_currency(self, column_name):\n    """\n    Normalize currency values in a specified column by removing non-numeric characters and converting to float.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Remove non-numeric characters and convert to float\n    self.df[column_name] = self.df[column_name].replace(\'[^\\d\\-\\.]\', \'\', regex=True).astype(float)\n\n    self._log("normalize_currency", params={\'column_name\': column_name})\n    return self'
    },
    "normalize_decimals": {
        "params": ['columns'],
        "docstring": 'Normalize decimal values in specified columns by rounding to a reasonable number of decimal places.\nIf no columns are specified, all numeric columns in the DataFrame will be processed.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to normalize. If None, all numeric columns are processed.',
        "code": 'def normalize_decimals(self, columns=None):\n    """\n    Normalize decimal values in specified columns by rounding to a reasonable number of decimal places.\n    If no columns are specified, all numeric columns in the DataFrame will be processed.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to normalize. If None, all numeric columns are processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        if self.df[col].dtype.kind in \'biufc\':  # Check for numeric types\n            self.df[col] = self.df[col].round(2)\n\n    self._log("normalize_decimals", params={\'columns\': columns})\n    return self'
    },
    "normalize_emails": {
        "params": ['column_name', 'domain'],
        "docstring": 'Normalize email addresses by converting to lowercase and optionally setting a specific domain.',
        "code": 'def normalize_emails(self, column_name=\'email\', domain=None):\n    """\n    Normalize email addresses by converting to lowercase and optionally setting a specific domain.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"Column \'{column_name}\' not found in DataFrame")\n\n    self.df[column_name] = self.df[column_name].str.lower()\n\n    if domain:\n        domain_part = f"@{domain}"\n        self.df[column_name] = self.df[column_name].apply(\n            lambda x: f"{x.split(\'@\')[0]}{domain_part}" if \'@\' in x else x\n        )\n\n    self._log("normalize_emails", params={\'column_name\': column_name, \'domain\': domain})\n    return self'
    },
    "normalize_measurement_system": {
        "params": ['metric_columns'],
        "docstring": 'Normalise les colonnes de mesure en utilisant le système métrique.\nConvertit les unités impériales (pouces, livres) en unités métriques (centimètres, kilogrammes).',
        "code": 'def normalize_measurement_system(self, metric_columns=None):\n    """\n    Normalise les colonnes de mesure en utilisant le système métrique.\n    Convertit les unités impériales (pouces, livres) en unités métriques (centimètres, kilogrammes).\n    """\n    self._validate_df()\n    self._backup()\n\n    if metric_columns is None:\n        metric_columns = []\n\n    for column in metric_columns:\n        if column in self.df.columns:\n            # Conversion des pouces en centimètres\n            if \'inch\' in column.lower() or \'inches\' in column.lower():\n                self.df[column] = self.df[column] * 2.54\n            # Conversion des livres en kilogrammes\n            elif \'pound\' in column.lower() or \'lbs\' in column.lower():\n                self.df[column] = self.df[column] * 0.453592\n\n    self._log("normalize_measurement_system", params={\'metric_columns\': metric_columns})\n    return self'
    },
    "normalize_phone_numbers": {
        "params": ['column_name'],
        "docstring": 'Normalize phone numbers in the specified column by removing all non-digit characters\nand ensuring a consistent length (10 digits for US numbers).',
        "code": 'def normalize_phone_numbers(self, column_name):\n    """\n    Normalize phone numbers in the specified column by removing all non-digit characters\n    and ensuring a consistent length (10 digits for US numbers).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Remove all non-digit characters\n    self.df[column_name] = self.df[column_name].astype(str).str.replace(r\'\\D+\', \'\', regex=True)\n\n    # Ensure consistent length (10 digits for US numbers)\n    self.df[column_name] = self.df[column_name].str.pad(width=10, side=\'left\', fillchar=\'0\')\n\n    self._log("normalize_phone_numbers", params={\'column_name\': column_name})\n    return self'
    },
    "normalize_sentences": {
        "params": ['text_column', 'language', 'lowercase'],
        "docstring": 'Normalize sentences in a specified text column by removing special characters,\nconverting to lowercase if specified, and ensuring consistent spacing.',
        "code": 'def normalize_sentences(self, text_column: str, language: str = \'fr\', lowercase: bool = True):\n    """\n    Normalize sentences in a specified text column by removing special characters,\n    converting to lowercase if specified, and ensuring consistent spacing.\n    """\n    self._validate_df()\n    self._backup()\n\n    import re\n    from unidecode import unidecode\n\n    # Define translation table for special characters\n    char_map = str.maketrans({\n        \'“\': \'"\', \'”\': \'"\',\n        \'‘\': "\'", \'’\': "\'",\n        \'—\': \'-\', \'–\': \'-\',\n        \'…\': \'...\'\n    })\n\n    # Normalize the specified text column\n    self.df[text_column] = (\n        self.df[text_column]\n        .astype(str)\n        .str.translate(char_map)  # Replace special characters\n        .str.normalize(\'NFKD\')    # Normalize unicode characters\n        .str.encode(\'ascii\', errors=\'ignore\').str.decode(\'utf-8\')  # Remove non-ASCII\n        .apply(lambda x: unidecode(x) if isinstance(x, str) else x)  # Remove accents\n        .str.replace(r\'[^\\w\\s\\-.,!?;]\', \'\', regex=True)  # Remove special characters\n        .str.replace(r\'\\s+\', \' \', regex=True)  # Replace multiple spaces with single space\n        .str.strip()  # Remove leading/trailing whitespace\n    )\n\n    if lowercase:\n        self.df[text_column] = self.df[text_column].str.lower()\n\n    self._log("normalize_sentences", params={\'text_column\': text_column, \'language\': language, \'lowercase\': lowercase})\n    return self'
    },
    "normalize_urls": {
        "params": ['url_column', 'protocol', 'remove_query_params'],
        "docstring": 'Normalize URLs in the specified column by ensuring consistent protocol and removing query parameters.\n\nParameters:\n    url_column (str): Name of the column containing URLs to normalize.\n    protocol (str, optional): Protocol to use for normalization. Defaults to "https://".\n    remove_query_params (bool, optional): Whether to remove query parameters. Defaults to True.',
        "code": 'def normalize_urls(self, url_column: str, protocol: str = "https://", remove_query_params: bool = True):\n    """\n    Normalize URLs in the specified column by ensuring consistent protocol and removing query parameters.\n\n    Parameters:\n        url_column (str): Name of the column containing URLs to normalize.\n        protocol (str, optional): Protocol to use for normalization. Defaults to "https://".\n        remove_query_params (bool, optional): Whether to remove query parameters. Defaults to True.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Normalize URLs\n    if url_column in self.df.columns:\n        self.df[url_column] = self.df[url_column].astype(str)\n        self.df[url_column] = self.df[url_column].str.lower()\n\n        # Add protocol if missing\n        self.df[url_column] = self.df[url_column].str.replace(r\'^(http://|https://)\', \'\', regex=True)\n        self.df[url_column] = protocol + self.df[url_column]\n\n        # Remove query parameters if requested\n        if remove_query_params:\n            self.df[url_column] = self.df[url_column].str.split(\'?\').str[0]\n\n    self._log("normalize_urls", params={\n        "url_column": url_column,\n        "protocol": protocol,\n        "remove_query_params": remove_query_params\n    })\n    return self'
    },
    "normalize_zscore": {
        "params": ['columns'],
        "docstring": "Normalise les colonnes spécifiées en utilisant la méthode Z-score.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont normalisées.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    Liste des noms de colonnes à normaliser. Si None, toutes les colonnes numériques sont traitées.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance courante pour permettre le chaînage de méthodes.",
        "code": 'def normalize_zscore(self, columns=None):\n    """\n    Normalise les colonnes spécifiées en utilisant la méthode Z-score.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont normalisées.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        Liste des noms de colonnes à normaliser. Si None, toutes les colonnes numériques sont traitées.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance courante pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    for col in numeric_cols:\n        mean = self.df[col].mean()\n        std = self.df[col].std()\n        if std != 0:\n            self.df[col] = (self.df[col] - mean) / std\n        else:\n            self.df[col] = 0\n\n    self._log("normalize_zscore", params={\'columns\': columns})\n    return self'
    },
    "one_hot_encode": {
        "params": ['columns'],
        "docstring": "Applique un encodage one-hot sur les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes de type object sont encodées.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à encoder (par défaut None)",
        "code": 'def one_hot_encode(self, columns=None):\n    """\n    Applique un encodage one-hot sur les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type object sont encodées.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à encoder (par défaut None)\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n\n    for col in columns:\n        if col in self.df.columns and len(self.df[col].unique()) > 2:\n            dummies = pd.get_dummies(self.df[col], prefix=col, dtype=int)\n            self.df = pd.concat([self.df.drop(col, axis=1), dummies], axis=1)\n\n    self._log("one_hot_encode", params={\'columns\': columns})\n    return self'
    },
    "optimize_float_precision": {
        "params": ['columns'],
        "docstring": 'Optimize the precision of float columns by converting them to appropriate numeric types.\nIf no columns are specified, all float columns in the DataFrame will be processed.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to optimize. If None, all float columns are processed.',
        "code": 'def optimize_float_precision(self, columns=None):\n    """\n    Optimize the precision of float columns by converting them to appropriate numeric types.\n    If no columns are specified, all float columns in the DataFrame will be processed.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to optimize. If None, all float columns are processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'float\']).columns\n\n    for col in columns:\n        if self.df[col].dtype == \'float64\':\n            # Convert to float32 if the column can be represented with less precision\n            self.df[col] = pd.to_numeric(self.df[col], downcast=\'float\')\n\n    self._log("optimize_float_precision", params={\'columns\': columns})\n    return self'
    },
    "optimize_int_precision": {
        "params": ['columns'],
        "docstring": 'Optimize the precision of integer columns by converting them to the smallest possible dtype.\nIf no columns are specified, all integer columns in the DataFrame will be processed.\n\nParameters:\n-----------\ncolumns : list, optional\n    List of column names to optimize. If None, all integer columns are processed.',
        "code": 'def optimize_int_precision(self, columns=None):\n    """\n    Optimize the precision of integer columns by converting them to the smallest possible dtype.\n    If no columns are specified, all integer columns in the DataFrame will be processed.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        List of column names to optimize. If None, all integer columns are processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        # Select all integer columns\n        int_cols = self.df.select_dtypes(include=[\'int\']).columns.tolist()\n    else:\n        # Validate specified columns\n        int_cols = [col for col in columns if col in self.df.columns and pd.api.types.is_integer_dtype(self.df[col])]\n\n    for col in int_cols:\n        current_min = self.df[col].min()\n        current_max = self.df[col].max()\n\n        # Determine the optimal dtype\n        if pd.api.types.is_integer_dtype(self.df[col]):\n            # Find the smallest dtype that can hold the values\n            if current_min >= 0:\n                if current_max < 256:\n                    new_dtype = \'uint8\'\n                elif current_max < 65536:\n                    new_dtype = \'uint16\'\n                elif current_max < 4294967296:\n                    new_dtype = \'uint32\'\n                else:\n                    new_dtype = \'uint64\'\n            else:\n                if current_min >= -128 and current_max < 127:\n                    new_dtype = \'int8\'\n                elif current_min >= -32768 and current_max < 32767:\n                    new_dtype = \'int16\'\n                elif current_min >= -2147483648 and current_max < 2147483647:\n                    new_dtype = \'int32\'\n                else:\n                    new_dtype = \'int64\'\n\n            # Convert to the optimal dtype\n            self.df[col] = self.df[col].astype(new_dtype)\n\n    self._log("optimize_int_precision", params={\'columns\': columns})\n    return self'
    },
    "parse_dates": {
        "params": ['date_columns', 'format', 'errors'],
        "docstring": "Parse and standardize date columns in the DataFrame.\n\nParameters:\n-----------\ndate_columns : list, optional\n    List of column names to parse as dates. If None, attempts to infer date columns.\nformat : str or list, optional\n    Date format(s) to use for parsing. 'mixed' tries multiple common formats.\nerrors : str, optional\n    How to handle parsing errors: 'raise', 'coerce' or 'ignore'.",
        "code": 'def parse_dates(self, date_columns=None, format=\'mixed\', errors=\'raise\'):\n    """\n    Parse and standardize date columns in the DataFrame.\n\n    Parameters:\n    -----------\n    date_columns : list, optional\n        List of column names to parse as dates. If None, attempts to infer date columns.\n    format : str or list, optional\n        Date format(s) to use for parsing. \'mixed\' tries multiple common formats.\n    errors : str, optional\n        How to handle parsing errors: \'raise\', \'coerce\' or \'ignore\'.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        # Infer potential date columns (object dtype and common date patterns)\n        date_columns = [col for col in self.df.columns\n                       if self.df[col].dtype == \'object\' and\n                       any(self.df[col].str.contains(pattern).any()\n                           for pattern in [\'-\', \'/\', \' \', \':\'])]\n\n    if isinstance(format, str) and format == \'mixed\':\n        formats = [\'%Y-%m-%d\', \'%d/%m/%Y\', \'%m/%d/%Y\',\n                  \'%Y%m%d\', \'%d-%b-%Y\', \'%b %d, %Y\']\n    else:\n        formats = [format]\n\n    for col in date_columns:\n        if col in self.df.columns:\n            try:\n                # Try parsing with the specified format(s)\n                if isinstance(formats, list):\n                    for fmt in formats:\n                        try:\n                            self.df[col] = pd.to_datetime(self.df[col], format=fmt, errors=\'coerce\')\n                            if not self.df[col].isna().all():\n                                break\n                        except:\n                            continue\n\n                # Fallback to pandas\' automatic detection if mixed formats were specified\n                if isinstance(formats, list) and self.df[col].isna().all():\n                    self.df[col] = pd.to_datetime(self.df[col], errors=errors)\n            except Exception as e:\n                if errors == \'raise\':\n                    raise ValueError(f"Failed to parse dates in column {col}: {str(e)}")\n\n    self._log("parse_dates", params={\'date_columns\': date_columns, \'format\': format, \'errors\': errors})\n    return self'
    },
    "parse_dates_mixed_formats": {
        "params": ['date_columns'],
        "docstring": 'Parse columns with mixed date formats into datetime objects.\nHandles various common date formats and standardizes them.\n\nParameters:\n-----------\ndate_columns : list, optional\n    List of column names to parse as dates. If None, all object columns are attempted.',
        "code": 'def parse_dates_mixed_formats(self, date_columns=None):\n    """\n    Parse columns with mixed date formats into datetime objects.\n    Handles various common date formats and standardizes them.\n\n    Parameters:\n    -----------\n    date_columns : list, optional\n        List of column names to parse as dates. If None, all object columns are attempted.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        date_columns = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n\n    for col in date_columns:\n        self.df[col] = pd.to_datetime(self.df[col], errors=\'coerce\', dayfirst=True, yearfirst=False)\n\n    self._log("parse_dates_mixed_formats", params={\'date_columns\': date_columns})\n    return self'
    },
    "parse_datetime_tz": {
        "params": ['date_columns', 'timezone', 'format'],
        "docstring": "Convertit les colonnes de dates en objets datetime avec gestion des fuseaux horaires.\n\nArgs:\n    date_columns (list, optional): Liste des noms de colonnes à convertir.\n                                  Si None, toutes les colonnes sont analysées.\n    timezone (str, optional): Fuseau horaire cible. Par défaut 'UTC'.\n    format (str, optional): Format des dates si connu. Ex: '%Y-%m-%d %H:%M:%S'.\n\nReturns:\n    DataCleaner: Instance actuelle pour chaînage de méthodes.",
        "code": 'def parse_datetime_tz(self, date_columns=None, timezone=\'UTC\', format=None):\n    """\n    Convertit les colonnes de dates en objets datetime avec gestion des fuseaux horaires.\n\n    Args:\n        date_columns (list, optional): Liste des noms de colonnes à convertir.\n                                      Si None, toutes les colonnes sont analysées.\n        timezone (str, optional): Fuseau horaire cible. Par défaut \'UTC\'.\n        format (str, optional): Format des dates si connu. Ex: \'%Y-%m-%d %H:%M:%S\'.\n\n    Returns:\n        DataCleaner: Instance actuelle pour chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        date_columns = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in date_columns:\n        try:\n            self.df[col] = pd.to_datetime(self.df[col], format=format, utc=True)\n            self.df[col] = self.df[col].dt.tz_convert(timezone)\n        except (ValueError, TypeError):\n            continue\n\n    self._log("parse_datetime_tz", params={\'date_columns\': date_columns, \'timezone\': timezone, \'format\': format})\n    return self'
    },
    "pipeline_add_step": {
        "params": ['step_name', 'func'],
        "docstring": 'Ajoute une étape de nettoyage ou de préparation à la pipeline.',
        "code": 'def pipeline_add_step(self, step_name: str, func: callable, **kwargs):\n    """\n    Ajoute une étape de nettoyage ou de préparation à la pipeline.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Exécution de la fonction avec les arguments fournis\n    func(self.df, **kwargs)\n\n    # Mise à jour du nom de l\'étape dans la pipeline\n    if hasattr(self, \'_pipeline_steps\'):\n        self._pipeline_steps.append(step_name)\n    else:\n        self._pipeline_steps = [step_name]\n\n    self._log("pipeline_add_step", params={\'step_name\': step_name, \'kwargs\': kwargs})\n    return self'
    },
    "pipeline_export": {
        "params": ['output_format', 'output_path'],
        "docstring": "Nettoie et prépare les données pour l'export dans le format spécifié.",
        "code": 'def pipeline_export(self, output_format="csv", output_path=None):\n    """\n    Nettoie et prépare les données pour l\'export dans le format spécifié.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des doublons\n    self.df = self.df.drop_duplicates()\n\n    # Conversion des types de données\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\':\n            try:\n                self.df[col] = pd.to_datetime(self.df[col])\n            except (ValueError, TypeError):\n                pass\n\n    # Gestion des valeurs manquantes\n    self.df = self.df.fillna({\n        \'numeric\': 0,\n        \'object\': \'Unknown\',\n        \'datetime\': pd.NaT\n    })\n\n    # Export des données\n    if output_format == "csv":\n        self.df.to_csv(output_path, index=False)\n    elif output_format == "parquet":\n        self.df.to_parquet(output_path, index=False)\n    elif output_format == "excel":\n        self.df.to_excel(output_path, index=False)\n\n    self._log("pipeline_export", params={"output_format": output_format, "output_path": output_path})\n    return self'
    },
    "pipeline_import": {
        "params": ['df'],
        "docstring": "Nettoie et prépare les données d'un DataFrame en appliquant une série de transformations standard.",
        "code": 'def pipeline_import(self, df):\n    """\n    Nettoie et prépare les données d\'un DataFrame en appliquant une série de transformations standard.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des doublons\n    df = df.drop_duplicates()\n\n    # Conversion des colonnes de date si nécessaire\n    for col in df.select_dtypes(include=[\'object\']).columns:\n        try:\n            df[col] = pd.to_datetime(df[col])\n        except (ValueError, TypeError):\n            pass\n\n    # Suppression des colonnes vides\n    df = df.dropna(axis=1, how=\'all\')\n\n    # Remplissage des valeurs manquantes par la médiane pour les colonnes numériques\n    for col in df.select_dtypes(include=[\'number\']).columns:\n        if df[col].isna().any():\n            median_val = df[col].median()\n            df[col] = df[col].fillna(median_val)\n\n    # Standardisation des noms de colonnes\n    df.columns = df.columns.str.lower().str.replace(\' \', \'_\')\n\n    self.df = df\n    self._log("pipeline_import", params={})\n    return self'
    },
    "pipeline_preview": {
        "params": ['n_rows'],
        "docstring": 'Affiche un aperçu des données nettoyées avec un nombre limité de lignes.',
        "code": 'def pipeline_preview(self, n_rows=5):\n    """\n    Affiche un aperçu des données nettoyées avec un nombre limité de lignes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Afficher un aperçu des données\n    preview = self.df.head(n_rows)\n\n    # Loguer l\'action\n    self._log("pipeline_preview", params={"n_rows": n_rows})\n    return preview'
    },
    "pipeline_remove_step": {
        "params": ['columns_to_remove'],
        "docstring": "Supprime les colonnes spécifiées du DataFrame.\nSi aucune colonne n'est spécifiée, ne fait rien.",
        "code": 'def pipeline_remove_step(self, columns_to_remove=None):\n    """\n    Supprime les colonnes spécifiées du DataFrame.\n    Si aucune colonne n\'est spécifiée, ne fait rien.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns_to_remove:\n        for column in columns_to_remove:\n            if column in self.df.columns:\n                self.df.drop(column, axis=1, inplace=True)\n\n    self._log("pipeline_remove_step", params={"columns_to_remove": columns_to_remove})\n    return self'
    },
    "pipeline_run": {
        "params": ['df'],
        "docstring": 'Nettoie et prépare les données en appliquant une série de transformations.\nSi df est fourni, utilise ce dataframe sinon utilise self.df.',
        "code": 'def pipeline_run(self, df=None):\n    """\n    Nettoie et prépare les données en appliquant une série de transformations.\n    Si df est fourni, utilise ce dataframe sinon utilise self.df.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is not None:\n        self.df = df.copy()\n\n    # Suppression des doublons\n    self.df.drop_duplicates(inplace=True)\n\n    # Conversion des types de données\n    for col in self.df.columns:\n        if self.df[col].dtype == \'object\':\n            self.df[col] = self.df[col].astype(\'string\')\n        elif np.issubdtype(self.df[col].dtype, np.number):\n            self.df[col] = pd.to_numeric(self.df[col], errors=\'coerce\')\n\n    # Gestion des valeurs manquantes\n    for col in self.df.columns:\n        if self.df[col].dtype == \'string\':\n            self.df[col].fillna(\'missing\', inplace=True)\n        else:\n            self.df[col].fillna(self.df[col].median(), inplace=True)\n\n    # Suppression des colonnes inutiles\n    if hasattr(self, \'columns_to_drop\'):\n        self.df.drop(columns=self.columns_to_drop, inplace=True)\n\n    # Application des transformations personnalisées\n    if hasattr(self, \'transformations\'):\n        for transform in self.transformations:\n            self.df = transform(self.df)\n\n    self._log("pipeline_run", params={})\n    return self'
    },
    "pivot_columns": {
        "params": ['index', 'columns', 'values'],
        "docstring": 'Pivote les colonnes du DataFrame selon les paramètres spécifiés.',
        "code": 'def pivot_columns(self, index=None, columns=None, values=None):\n    """\n    Pivote les colonnes du DataFrame selon les paramètres spécifiés.\n    """\n    self._validate_df()\n    self._backup()\n\n    if index is not None and columns is not None:\n        self.df = self.df.pivot(index=index, columns=columns, values=values)\n\n    self._log("pivot_columns", params={"index": index, "columns": columns, "values": values})\n    return self'
    },
    "project_coordinates": {
        "params": ['df_columns', 'projection_method'],
        "docstring": 'Projette les coordonnées géographiques des colonnes spécifiées selon la méthode de projection choisie.',
        "code": 'def project_coordinates(self, df_columns: list = None, projection_method: str = \'wgs84\'):\n    """\n    Projette les coordonnées géographiques des colonnes spécifiées selon la méthode de projection choisie.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des colonnes\n    if df_columns is None:\n        df_columns = [col for col in self.df.columns if \'lat\' in col.lower() or \'lon\' in col.lower()]\n    else:\n        df_columns = [col for col in df_columns if col in self.df.columns]\n\n    # Projection des coordonnées\n    for col in df_columns:\n        if \'lat\' in col.lower():\n            self.df[col] = self._project_latitude(self.df[col], projection_method)\n        elif \'lon\' in col.lower():\n            self.df[col] = self._project_longitude(self.df[col], projection_method)\n\n    self._log("project_coordinates", params={\'df_columns\': df_columns, \'projection_method\': projection_method})\n    return self'
    },
    "pseudonymize_ids": {
        "params": ['id_columns'],
        "docstring": "Pseudonymizes specified ID columns by replacing their values with hashed versions.\nIf no columns are specified, all columns containing 'id' in their name will be processed.\n\nParameters:\n-----------\nid_columns : list of str, optional\n    List of column names to pseudonymize. If None, all columns with 'id' in their name will be processed.",
        "code": 'def pseudonymize_ids(self, id_columns=None):\n    """\n    Pseudonymizes specified ID columns by replacing their values with hashed versions.\n    If no columns are specified, all columns containing \'id\' in their name will be processed.\n\n    Parameters:\n    -----------\n    id_columns : list of str, optional\n        List of column names to pseudonymize. If None, all columns with \'id\' in their name will be processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if id_columns is None:\n        id_columns = [col for col in self.df.columns if \'id\' in col.lower()]\n\n    for column in id_columns:\n        if column in self.df.columns:\n            # Apply SHA-256 hashing to each value\n            self.df[column] = self.df[column].astype(str).apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n\n    self._log("pseudonymize_ids", params={"id_columns": id_columns})\n    return self'
    },
    "quality_score": {
        "params": ['threshold'],
        "docstring": 'Calcule un score de qualité pour chaque colonne du DataFrame et supprime celles dont le score est inférieur au seuil.\nLe score de qualité est basé sur la proportion de valeurs non nulles et non vides.',
        "code": 'def quality_score(self, threshold=0.7):\n    """\n    Calcule un score de qualité pour chaque colonne du DataFrame et supprime celles dont le score est inférieur au seuil.\n    Le score de qualité est basé sur la proportion de valeurs non nulles et non vides.\n    """\n    self._validate_df()\n    self._backup()\n\n    quality_scores = {}\n    for column in self.df.columns:\n        valid_values = self.df[column].dropna().apply(lambda x: str(x).strip() != \'\')\n        quality_scores[column] = valid_values.mean()\n\n    columns_to_drop = [col for col, score in quality_scores.items() if score < threshold]\n    self.df.drop(columns=columns_to_drop, inplace=True)\n\n    self._log("quality_score", params={"threshold": threshold})\n    return self'
    },
    "quantile_transform": {
        "params": ['columns', 'n_quantiles', 'output_distribution', 'ignore_imputed'],
        "docstring": "Applique une transformation quantile aux colonnes spécifiées pour normaliser la distribution.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.\nn_quantiles : int, default=100\n    Nombre de quantiles à utiliser pour la transformation.\noutput_distribution : str, default='normal'\n    Distribution de sortie ('normal' ou 'uniform').\nignore_imputed : bool, default=False\n    Si True, ignore les colonnes marquées comme imputées.\n\nReturns:\n--------\nself : DataCleaner\n    Retourne l'instance actuelle pour le chaînage de méthodes.",
        "code": 'def quantile_transform(self, columns=None, n_quantiles=100, output_distribution=\'normal\', ignore_imputed=False):\n    """\n    Applique une transformation quantile aux colonnes spécifiées pour normaliser la distribution.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.\n    n_quantiles : int, default=100\n        Nombre de quantiles à utiliser pour la transformation.\n    output_distribution : str, default=\'normal\'\n        Distribution de sortie (\'normal\' ou \'uniform\').\n    ignore_imputed : bool, default=False\n        Si True, ignore les colonnes marquées comme imputées.\n\n    Returns:\n    --------\n    self : DataCleaner\n        Retourne l\'instance actuelle pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.preprocessing import QuantileTransformer\n    import pandas as pd\n\n    # Sélection des colonnes à transformer\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = columns\n\n    # Filtrer les colonnes imputées si nécessaire\n    if ignore_imputed and hasattr(self, \'imputed_columns\'):\n        numeric_cols = [col for col in numeric_cols if col not in self.imputed_columns]\n\n    # Transformation quantile\n    transformer = QuantileTransformer(n_quantiles=n_quantiles,\n                                     output_distribution=output_distribution)\n    self.df[numeric_cols] = transformer.fit_transform(self.df[numeric_cols])\n\n    self._log("quantile_transform", params={\n        \'columns\': numeric_cols,\n        \'n_quantiles\': n_quantiles,\n        \'output_distribution\': output_distribution,\n        \'ignore_imputed\': ignore_imputed\n    })\n    return self'
    },
    "rank_transform": {
        "params": ['columns', 'method', 'pct'],
        "docstring": "Applique une transformation de rang aux colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont transformées.\n\nParameters\n----------\ncolumns : list of str, optional\n    Liste des colonnes à transformer. Par défaut None (toutes les colonnes numériques).\nmethod : str, optional\n    Méthode de traitement des valeurs égales ('average', 'min', 'max', 'first', 'dense').\n    Par défaut 'average'.\npct : bool, optional\n    Si True, transforme en pourcentage (0-100). Par défaut False.\n\nReturns\n-------\nself : DataCleaner\n    Retourne l'instance courante pour le chaînage de méthodes.",
        "code": 'def rank_transform(self, columns=None, method=\'average\', pct=False):\n    """\n    Applique une transformation de rang aux colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont transformées.\n\n    Parameters\n    ----------\n    columns : list of str, optional\n        Liste des colonnes à transformer. Par défaut None (toutes les colonnes numériques).\n    method : str, optional\n        Méthode de traitement des valeurs égales (\'average\', \'min\', \'max\', \'first\', \'dense\').\n        Par défaut \'average\'.\n    pct : bool, optional\n        Si True, transforme en pourcentage (0-100). Par défaut False.\n\n    Returns\n    -------\n    self : DataCleaner\n        Retourne l\'instance courante pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n\n    for col in columns:\n        if pct:\n            self.df[col] = self.df[col].rank(method=method, pct=True) * 100\n        else:\n            self.df[col] = self.df[col].rank(method=method)\n\n    self._log("rank_transform", params={\'columns\': columns, \'method\': method, \'pct\': pct})\n    return self'
    },
    "reduce_memory_usage": {
        "params": ['df'],
        "docstring": "Réduit l'utilisation mémoire du DataFrame en convertissant les types de données aux types les plus appropriés.",
        "code": 'def reduce_memory_usage(self, df=None):\n    """\n    Réduit l\'utilisation mémoire du DataFrame en convertissant les types de données aux types les plus appropriés.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    start_mem = df.memory_usage().sum() / 1024**2\n    self._log(f"Mémoire initiale: {start_mem:.2f} Mo")\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \'int\':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    self._log(f"Mémoire finale: {end_mem:.2f} Mo")\n    self._log(f"Réduction mémoire: {(start_mem - end_mem):.2f} Mo ({100*(start_mem-end_mem)/start_mem:.1f}%)")\n\n    if df is not self.df:\n        self.df = df\n\n    self._log("reduce_memory_usage", params={})\n    return self'
    },
    "regex_clean": {
        "params": ['column', 'pattern', 'replacement', 'inplace'],
        "docstring": 'Nettoie une colonne en utilisant des expressions régulières.\n\nApplique un motif regex pour remplacer ou supprimer des motifs dans une colonne spécifiée.\nSi \'replacement\' est vide, les motifs correspondants sont supprimés.\n\nParameters\n----------\ncolumn : str\n    Nom de la colonne à nettoyer.\npattern : str\n    Motif regex à rechercher.\nreplacement : str, optional\n    Chaîne de remplacement (par défaut "").\ninplace : bool, optional\n    Si True, modifie le DataFrame en place (par défaut True).\n\nReturns\n-------\nself : DataCleaner\n    Instance de la classe pour le chaînage des méthodes.',
        "code": 'def regex_clean(self, column: str, pattern: str, replacement: str = "", inplace: bool = True):\n    """\n    Nettoie une colonne en utilisant des expressions régulières.\n\n    Applique un motif regex pour remplacer ou supprimer des motifs dans une colonne spécifiée.\n    Si \'replacement\' est vide, les motifs correspondants sont supprimés.\n\n    Parameters\n    ----------\n    column : str\n        Nom de la colonne à nettoyer.\n    pattern : str\n        Motif regex à rechercher.\n    replacement : str, optional\n        Chaîne de remplacement (par défaut "").\n    inplace : bool, optional\n        Si True, modifie le DataFrame en place (par défaut True).\n\n    Returns\n    -------\n    self : DataCleaner\n        Instance de la classe pour le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if inplace:\n        self.df[column] = self.df[column].replace(to_replace=pattern, value=replacement, regex=True)\n    else:\n        self.df = self.df.copy()\n        self.df[column] = self.df[column].replace(to_replace=pattern, value=replacement, regex=True)\n\n    self._log("regex_clean", params={"column": column, "pattern": pattern, "replacement": replacement})\n    return self'
    },
    "regex_extract": {
        "params": ['column_name', 'pattern', 'new_column_name'],
        "docstring": "Applique une extraction par expression régulière sur une colonne et stocke le résultat dans une nouvelle colonne.\n\nArgs:\n    column_name: Nom de la colonne source à traiter.\n    pattern: Expression régulière à appliquer pour l'extraction.\n    new_column_name: Nom de la nouvelle colonne (par défaut, le nom original avec suffixe '_extracted').",
        "code": 'def regex_extract(self, column_name: str, pattern: str, new_column_name: str = None):\n    """\n    Applique une extraction par expression régulière sur une colonne et stocke le résultat dans une nouvelle colonne.\n\n    Args:\n        column_name: Nom de la colonne source à traiter.\n        pattern: Expression régulière à appliquer pour l\'extraction.\n        new_column_name: Nom de la nouvelle colonne (par défaut, le nom original avec suffixe \'_extracted\').\n    """\n    self._validate_df()\n    self._backup()\n\n    if new_column_name is None:\n        new_column_name = f"{column_name}_extracted"\n\n    self.df[new_column_name] = self.df[column_name].str.extract(pattern, expand=False)\n\n    self._log("regex_extract", params={\n        "column_name": column_name,\n        "pattern": pattern,\n        "new_column_name": new_column_name\n    })\n    return self'
    },
    "regex_replace": {
        "params": ['column', 'pattern', 'replacement'],
        "docstring": "Remplace les occurrences d'un motif regex dans une colonne spécifiée.\n\nArgs:\n    column (str): Nom de la colonne à traiter.\n    pattern (str): Motif regex à rechercher.\n    replacement (str): Chaîne de remplacement.\n\nRaises:\n    KeyError: Si la colonne n'existe pas dans le DataFrame.",
        "code": 'def regex_replace(self, column: str, pattern: str, replacement: str):\n    """\n    Remplace les occurrences d\'un motif regex dans une colonne spécifiée.\n\n    Args:\n        column (str): Nom de la colonne à traiter.\n        pattern (str): Motif regex à rechercher.\n        replacement (str): Chaîne de remplacement.\n\n    Raises:\n        KeyError: Si la colonne n\'existe pas dans le DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column not in self.df.columns:\n        raise KeyError(f"Column \'{column}\' does not exist in DataFrame")\n\n    self.df[column] = self.df[column].replace(to_replace=pattern, value=replacement, regex=True)\n\n    self._log("regex_replace", params={"column": column, "pattern": pattern, "replacement": replacement})\n    return self'
    },
    "remove_accents": {
        "params": ['column_names'],
        "docstring": "Supprime les accents des caractères dans les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, traite toutes les colonnes de type string.",
        "code": 'def remove_accents(self, column_names=None):\n    """\n    Supprime les accents des caractères dans les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, traite toutes les colonnes de type string.\n    """\n    self._validate_df()\n    self._backup()\n\n    import unicodedata\n\n    if column_names is None:\n        column_names = self.df.select_dtypes(include=[\'object\']).columns\n\n    for col in column_names:\n        self.df[col] = self.df[col].apply(\n            lambda x: \'\'.join(\n                c for c in unicodedata.normalize(\'NFD\', str(x))\n                if unicodedata.category(c) != \'Mn\'\n            ) if isinstance(x, str) else x\n        )\n\n    self._log("remove_accents", params={"column_names": column_names})\n    return self'
    },
    "remove_constant_columns": {
        "params": ['threshold'],
        "docstring": 'Supprime les colonnes constantes ou quasi-constantes du DataFrame.\nUne colonne est considérée comme constante si la proportion de valeurs identiques\ndépasse le seuil spécifié (par défaut 1.0 pour les colonnes parfaitement constantes).',
        "code": 'def remove_constant_columns(self, threshold=1.0):\n    """\n    Supprime les colonnes constantes ou quasi-constantes du DataFrame.\n    Une colonne est considérée comme constante si la proportion de valeurs identiques\n    dépasse le seuil spécifié (par défaut 1.0 pour les colonnes parfaitement constantes).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la proportion de valeurs identiques pour chaque colonne\n    constant_mask = self.df.nunique(numeric_only=False) / len(self.df) <= threshold\n\n    # Supprimer les colonnes constantes\n    self.df = self.df.loc[:, ~constant_mask]\n\n    self._log("remove_constant_columns", params={"threshold": threshold})\n    return self'
    },
    "remove_empty_columns": {
        "params": ['threshold'],
        "docstring": 'Supprime les colonnes vides ou contenant principalement des valeurs manquantes.\nUne colonne est considérée comme vide si le pourcentage de valeurs manquantes dépasse le seuil spécifié.\n\nArgs:\n    threshold (float): Seuil pour déterminer si une colonne est considérée comme vide.\n                      Doit être entre 0 et 1 (par défaut: 0.7).',
        "code": 'def remove_empty_columns(self, threshold=0.7):\n    """\n    Supprime les colonnes vides ou contenant principalement des valeurs manquantes.\n    Une colonne est considérée comme vide si le pourcentage de valeurs manquantes dépasse le seuil spécifié.\n\n    Args:\n        threshold (float): Seuil pour déterminer si une colonne est considérée comme vide.\n                          Doit être entre 0 et 1 (par défaut: 0.7).\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer le pourcentage de valeurs manquantes pour chaque colonne\n    missing_percent = self.df.isnull().mean()\n\n    # Identifier les colonnes à supprimer\n    cols_to_drop = missing_percent[missing_percent >= threshold].index\n\n    # Supprimer les colonnes\n    self.df.drop(columns=cols_to_drop, inplace=True)\n\n    self._log("remove_empty_columns", params={"threshold": threshold})\n    return self'
    },
    "remove_numeric_negatives": {
        "params": ['columns'],
        "docstring": "Remplace les valeurs négatives par NaN dans les colonnes numériques spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont traitées.",
        "code": 'def remove_numeric_negatives(self, columns=None):\n    """\n    Remplace les valeurs négatives par NaN dans les colonnes numériques spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    for col in numeric_cols:\n        self.df.loc[self.df[col] < 0, col] = np.nan\n\n    self._log("remove_numeric_negatives", params={\'columns\': columns})\n    return self'
    },
    "remove_outliers_iqr": {
        "params": ['columns', 'threshold'],
        "docstring": "Supprime les valeurs aberrantes en utilisant la méthode de l'IQR (Interquartile Range).\nLes valeurs aberrantes sont définies comme celles en dehors de [Q1 - threshold*IQR, Q3 + threshold*IQR].",
        "code": 'def remove_outliers_iqr(self, columns=None, threshold=1.5):\n    """\n    Supprime les valeurs aberrantes en utilisant la méthode de l\'IQR (Interquartile Range).\n    Les valeurs aberrantes sont définies comme celles en dehors de [Q1 - threshold*IQR, Q3 + threshold*IQR].\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for column in columns:\n        if pd.api.types.is_numeric_dtype(self.df[column]):\n            Q1 = self.df[column].quantile(0.25)\n            Q3 = self.df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - threshold * IQR\n            upper_bound = Q3 + threshold * IQR\n\n            self.df[column] = self.df[column].where(\n                (self.df[column] >= lower_bound) & (self.df[column] <= upper_bound),\n                np.nan\n            )\n\n    self._log("remove_outliers_iqr", params={"columns": columns, "threshold": threshold})\n    return self'
    },
    "remove_outliers_mad": {
        "params": ['df', 'columns', 'threshold'],
        "docstring": 'Supprime les valeurs aberrantes en utilisant la méthode du Median Absolute Deviation (MAD).',
        "code": 'def remove_outliers_mad(self, df=None, columns=None, threshold=3.5):\n    """\n    Supprime les valeurs aberrantes en utilisant la méthode du Median Absolute Deviation (MAD).\n    """\n    self._validate_df()\n    self._backup()\n\n    df = df if df is not None else self.df\n    columns = columns if columns is not None else df.columns\n\n    median = df[columns].median()\n    mad = (df[columns] - median).abs().median()\n\n    modified_mask = ((df[columns] - median).abs() > threshold * mad)\n\n    df.loc[modified_mask, columns] = np.nan\n\n    self._log("remove_outliers_mad", params={"columns": columns, "threshold": threshold})\n    return self'
    },
    "remove_stopwords": {
        "params": ['text_column', 'language', 'custom_stopwords'],
        "docstring": "Supprime les stopwords d'une colonne de texte spécifiée.\n\nParameters:\n    text_column (str): Nom de la colonne contenant le texte à nettoyer.\n    language (str, optional): Langue pour les stopwords par défaut. Defaults to 'english'.\n    custom_stopwords (list, optional): Liste personnalisée de stopwords à ajouter. Defaults to None.",
        "code": 'def remove_stopwords(self, text_column: str, language: str = \'english\', custom_stopwords: list = None):\n    """\n    Supprime les stopwords d\'une colonne de texte spécifiée.\n\n    Parameters:\n        text_column (str): Nom de la colonne contenant le texte à nettoyer.\n        language (str, optional): Langue pour les stopwords par défaut. Defaults to \'english\'.\n        custom_stopwords (list, optional): Liste personnalisée de stopwords à ajouter. Defaults to None.\n    """\n    self._validate_df()\n    self._backup()\n\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n\n    # Charger les stopwords par défaut\n    try:\n        default_stopwords = set(stopwords.words(language))\n    except:\n        raise ValueError(f"Langue non supportée: {language}")\n\n    # Ajouter les stopwords personnalisés si spécifiés\n    if custom_stopwords:\n        default_stopwords.update(custom_stopwords)\n\n    # Fonction pour nettoyer le texte\n    def clean_text(text):\n        if isinstance(text, str):\n            words = word_tokenize(text)\n            filtered_words = [word for word in words if word.lower() not in default_stopwords]\n            return \' \'.join(filtered_words)\n        return text\n\n    # Appliquer le nettoyage\n    self.df[text_column] = self.df[text_column].apply(clean_text)\n\n    self._log("remove_stopwords", params={\'text_column\': text_column, \'language\': language})\n    return self'
    },
    "rename_columns_regex": {
        "params": ['pattern_dict'],
        "docstring": 'Renomme les colonnes du DataFrame en utilisant des expressions régulières.\nLe dictionnaire pattern_dict contient des paires {pattern: replacement}.',
        "code": 'def rename_columns_regex(self, pattern_dict):\n    """\n    Renomme les colonnes du DataFrame en utilisant des expressions régulières.\n    Le dictionnaire pattern_dict contient des paires {pattern: replacement}.\n    """\n    self._validate_df()\n    self._backup()\n\n    import re\n    for old_name, new_name in pattern_dict.items():\n        self.df.columns = self.df.columns.str.replace(old_name, new_name)\n\n    self._log("rename_columns_regex", params={"pattern_dict": pattern_dict})\n    return self'
    },
    "rename_columns_titlecase": {
        "params": ['df'],
        "docstring": 'Renomme les colonnes du DataFrame en Title Case.',
        "code": 'def rename_columns_titlecase(self, df=None):\n    """\n    Renomme les colonnes du DataFrame en Title Case.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    df.columns = [col.title() for col in df.columns]\n\n    self._log("rename_columns_titlecase", params={})\n    return self'
    },
    "reorder_categories": {
        "params": ['column', 'order'],
        "docstring": "Réordonne les catégories d'une colonne selon un ordre spécifié.\n\nArgs:\n    column (str): Nom de la colonne à réordonner.\n    order (list): Liste des catégories dans l'ordre souhaité.\n\nRaises:\n    ValueError: Si la colonne n'existe pas ou si l'ordre contient des catégories non présentes.",
        "code": 'def reorder_categories(self, column: str, order: list):\n    """\n    Réordonne les catégories d\'une colonne selon un ordre spécifié.\n\n    Args:\n        column (str): Nom de la colonne à réordonner.\n        order (list): Liste des catégories dans l\'ordre souhaité.\n\n    Raises:\n        ValueError: Si la colonne n\'existe pas ou si l\'ordre contient des catégories non présentes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column not in self.df.columns:\n        raise ValueError(f"La colonne \'{column}\' n\'existe pas dans le DataFrame.")\n\n    if not set(order).issubset(self.df[column].cat.categories):\n        raise ValueError("L\'ordre contient des catégories non présentes dans la colonne.")\n\n    self.df[column] = pd.Categorical(self.df[column], categories=order, ordered=True)\n\n    self._log("reorder_categories", params={"column": column, "order": order})\n    return self'
    },
    "repair_corrupted_cells": {
        "params": ['columns', 'strategy', 'threshold'],
        "docstring": 'Remplace les valeurs corrompues ou manquantes dans les colonnes spécifiées.\nLes valeurs sont remplacées selon la stratégie choisie (moyenne, médiane ou constante).',
        "code": 'def repair_corrupted_cells(self, columns=None, strategy=\'mean\', threshold=0.5):\n    """\n    Remplace les valeurs corrompues ou manquantes dans les colonnes spécifiées.\n    Les valeurs sont remplacées selon la stratégie choisie (moyenne, médiane ou constante).\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if self.df[col].isna().mean() > threshold:\n            if strategy == \'mean\':\n                fill_value = self.df[col].mean()\n            elif strategy == \'median\':\n                fill_value = self.df[col].median()\n            elif isinstance(strategy, (int, float)):\n                fill_value = strategy\n            else:\n                raise ValueError(f"Stratégie non reconnue: {strategy}")\n\n            self.df[col] = self.df[col].fillna(fill_value)\n\n    self._log("repair_corrupted_cells", params={\'columns\': columns, \'strategy\': strategy, \'threshold\': threshold})\n    return self'
    },
    "repair_outlier_rows": {
        "params": ['columns', 'method', 'threshold'],
        "docstring": "Répare les lignes avec des valeurs aberrantes dans le DataFrame.\n\nArgs:\n    columns (list, optional): Liste des colonnes à vérifier pour les valeurs aberrantes.\n                             Si None, toutes les colonnes numériques sont traitées.\n    method (str): Méthode de réparation des valeurs aberrantes. 'clip' pour élaguer,\n                 'remove' pour supprimer les lignes, ou 'replace' pour remplacer par la médiane.\n    threshold (float): Seuil Z-score pour détecter les valeurs aberrantes.\n\nRaises:\n    ValueError: Si la méthode spécifiée n'est pas valide.",
        "code": 'def repair_outlier_rows(self, columns=None, method=\'clip\', threshold=3.0):\n    """\n    Répare les lignes avec des valeurs aberrantes dans le DataFrame.\n\n    Args:\n        columns (list, optional): Liste des colonnes à vérifier pour les valeurs aberrantes.\n                                 Si None, toutes les colonnes numériques sont traitées.\n        method (str): Méthode de réparation des valeurs aberrantes. \'clip\' pour élaguer,\n                     \'remove\' pour supprimer les lignes, ou \'replace\' pour remplacer par la médiane.\n        threshold (float): Seuil Z-score pour détecter les valeurs aberrantes.\n\n    Raises:\n        ValueError: Si la méthode spécifiée n\'est pas valide.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = columns\n\n    if method not in [\'clip\', \'remove\', \'replace\']:\n        raise ValueError("Méthode non valide. Utilisez \'clip\', \'remove\' ou \'replace\'.")\n\n    for col in numeric_cols:\n        if method == \'clip\':\n            q1 = self.df[col].quantile(0.25)\n            q3 = self.df[col].quantile(0.75)\n            iqr = q3 - q1\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n            self.df[col] = self.df[col].clip(lower_bound, upper_bound)\n        elif method == \'remove\':\n            z_scores = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n            self.df = self.df[abs(z_scores) < threshold]\n        elif method == \'replace\':\n            median = self.df[col].median()\n            z_scores = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n            self.df.loc[abs(z_scores) >= threshold, col] = median\n\n    self._log("repair_outlier_rows", params={\'columns\': columns, \'method\': method, \'threshold\': threshold})\n    return self'
    },
    "repair_schema": {
        "params": ['df'],
        "docstring": 'Répare le schéma des données en standardisant les noms de colonnes et les types de données.',
        "code": 'def repair_schema(self, df=None):\n    """\n    Répare le schéma des données en standardisant les noms de colonnes et les types de données.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Si un DataFrame est fourni, l\'utiliser sinon utiliser celui de la classe\n    df_to_clean = df if df is not None else self.df\n\n    # Standardiser les noms de colonnes (minuscules, sans espaces)\n    df_to_clean.columns = df_to_clean.columns.str.lower().str.replace(\' \', \'_\')\n\n    # Convertir les types de données\n    for col in df_to_clean.columns:\n        if df_to_clean[col].dtype == \'object\':\n            # Essayer de convertir en datetime si possible\n            try:\n                df_to_clean[col] = pd.to_datetime(df_to_clean[col])\n            except (ValueError, TypeError):\n                pass\n        elif df_to_clean[col].dtype == \'float64\':\n            # Convertir en int si possible\n            if df_to_clean[col].dropna().mod(1).eq(0).all():\n                df_to_clean[col] = df_to_clean[col].astype(\'Int64\')\n\n    # Si un DataFrame a été fourni, le retourner\n    if df is not None:\n        return df_to_clean\n\n    self.df = df_to_clean\n    self._log("repair_schema", params={})\n    return self'
    },
    "replace_missing_with_nan": {
        "params": ['df'],
        "docstring": "Remplace les valeurs manquantes spécifiques par NaN dans le DataFrame.\nLes valeurs manquantes peuvent être des chaînes comme 'NA', 'N/A', 'null', etc.",
        "code": 'def replace_missing_with_nan(self, df=None):\n    """\n    Remplace les valeurs manquantes spécifiques par NaN dans le DataFrame.\n    Les valeurs manquantes peuvent être des chaînes comme \'NA\', \'N/A\', \'null\', etc.\n    """\n    self._validate_df()\n    self._backup()\n\n    if df is None:\n        df = self.df\n\n    missing_values = [\'NA\', \'N/A\', \'null\', \'\', \'#N/A\', \'-\', \'--\']\n    df.replace(missing_values, np.nan, inplace=True)\n\n    self._log("replace_missing_with_nan", params={})\n    return self'
    },
    "resolve_inconsistent_rows": {
        "params": ['threshold'],
        "docstring": "Identifie et supprime les lignes incohérentes dans le DataFrame en fonction d'un seuil de similarité.",
        "code": 'def resolve_inconsistent_rows(self, threshold=0.7):\n    """\n    Identifie et supprime les lignes incohérentes dans le DataFrame en fonction d\'un seuil de similarité.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la similarité entre chaque ligne et la moyenne du DataFrame\n    from sklearn.metrics.pairwise import cosine_similarity\n    import numpy as np\n\n    # Convertir les données en valeurs numériques si nécessaire\n    numeric_df = self.df.select_dtypes(include=[np.number])\n\n    # Calculer la moyenne des lignes\n    mean_row = numeric_df.mean(axis=0)\n\n    # Calculer la similarité cosinus entre chaque ligne et la moyenne\n    similarities = cosine_similarity(numeric_df, [mean_row])[0]\n\n    # Identifier les lignes à supprimer (similarité inférieure au seuil)\n    inconsistent_rows = similarities < threshold\n\n    # Supprimer les lignes incohérentes\n    self.df = self.df[~inconsistent_rows]\n\n    self._log("resolve_inconsistent_rows", params={"threshold": threshold})\n    return self'
    },
    "resolve_partial_duplicates": {
        "params": ['subset', 'threshold'],
        "docstring": "Identifie et résout les doublons partiels dans le DataFrame en fonction d'un seuil de similarité.\nLes colonnes spécifiées dans 'subset' sont utilisées pour détecter les doublons partiels.\nSi aucun subset n'est fourni, toutes les colonnes sont utilisées.\n\nParameters:\n-----------\nsubset : list of str, optional\n    Liste des colonnes à considérer pour la détection de doublons partiels.\nthreshold : float, optional\n    Seuil de similarité (0 à 1) pour considérer deux lignes comme des doublons partiels.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance actuelle pour permettre le chaînage de méthodes.",
        "code": 'def resolve_partial_duplicates(self, subset=None, threshold=0.8):\n    """\n    Identifie et résout les doublons partiels dans le DataFrame en fonction d\'un seuil de similarité.\n    Les colonnes spécifiées dans \'subset\' sont utilisées pour détecter les doublons partiels.\n    Si aucun subset n\'est fourni, toutes les colonnes sont utilisées.\n\n    Parameters:\n    -----------\n    subset : list of str, optional\n        Liste des colonnes à considérer pour la détection de doublons partiels.\n    threshold : float, optional\n        Seuil de similarité (0 à 1) pour considérer deux lignes comme des doublons partiels.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance actuelle pour permettre le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    if subset is None:\n        subset = self.df.columns.tolist()\n\n    # Création d\'une copie du DataFrame pour éviter les modifications directes\n    df = self.df.copy()\n\n    # Identification des doublons partiels\n    duplicates_mask = pd.DataFrame(False, index=df.index, columns=[\'is_duplicate\'])\n\n    for i in range(len(df)):\n        for j in range(i + 1, len(df)):\n            # Comparaison des valeurs dans le subset\n            similarity = sum(df.iloc[i][col] == df.iloc[j][col] for col in subset) / len(subset)\n            if similarity >= threshold:\n                duplicates_mask.iloc[j] = True\n\n    # Résolution des doublons partiels en gardant la première occurrence\n    self.df = df[~duplicates_mask[\'is_duplicate\']].copy()\n\n    self._log("resolve_partial_duplicates", params={\'subset\': subset, \'threshold\': threshold})\n    return self'
    },
    "reverse_geocode": {
        "params": ['latitude_col', 'longitude_col', 'output_cols', 'api_key'],
        "docstring": 'Ajoute des informations de géocodage inverse aux données en utilisant les colonnes latitude et longitude.',
        "code": 'def reverse_geocode(self, latitude_col: str = \'latitude\', longitude_col: str = \'longitude\',\n                    output_cols: tuple = (\'city\', \'country\'), api_key: str = None):\n    """\n    Ajoute des informations de géocodage inverse aux données en utilisant les colonnes latitude et longitude.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des colonnes nécessaires\n    if latitude_col not in self.df.columns or longitude_col not in self.df.columns:\n        raise ValueError(f"Les colonnes {latitude_col} ou {longitude_col} sont manquantes dans le DataFrame")\n\n    # Implémentation du géocodage inverse (exemple avec une API fictive)\n    # Remplacer par l\'appel réel à votre service de géocodage\n    for idx, row in self.df.iterrows():\n        lat = row[latitude_col]\n        lng = row[longitude_col]\n\n        # Appel à l\'API de géocodage inverse (exemple)\n        if api_key:\n            # Exemple d\'appel API (à adapter selon votre service)\n            response = requests.get(\n                f"https://api.geocoding.example.com/reverse?lat={lat}&lon={lng}",\n                headers={"Authorization": f"Bearer {api_key}"}\n            )\n            data = response.json()\n        else:\n            # Logique de fallback si pas d\'API key\n            data = {"city": "Unknown", "country": "Unknown"}\n\n        # Mise à jour des colonnes de sortie\n        for col, value in zip(output_cols, [data.get(col) for col in output_cols]):\n            self.df.at[idx, col] = value\n\n    self._log("reverse_geocode", params={\n        \'latitude_col\': latitude_col,\n        \'longitude_col\': longitude_col,\n        \'output_cols\': output_cols\n    })\n    return self'
    },
    "robust_scale": {
        "params": ['columns'],
        "docstring": "Applique une mise à l'échelle robuste aux colonnes spécifiées en utilisant le RobustScaler.\nLes valeurs sont centrées sur la médiane et mises à l'échelle en fonction de l'écart interquartile (IQR).",
        "code": 'def robust_scale(self, columns=None):\n    """\n    Applique une mise à l\'échelle robuste aux colonnes spécifiées en utilisant le RobustScaler.\n    Les valeurs sont centrées sur la médiane et mises à l\'échelle en fonction de l\'écart interquartile (IQR).\n    """\n    self._validate_df()\n    self._backup()\n\n    from sklearn.preprocessing import RobustScaler\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    scaler = RobustScaler()\n    self.df[columns] = scaler.fit_transform(self.df[columns])\n\n    self._log("robust_scale", params={"columns": columns})\n    return self'
    },
    "rollback_last_operation": {
        "params": [],
        "docstring": 'Annule la dernière opération de nettoyage en restaurant les données depuis le dernier backup.',
        "code": 'def rollback_last_operation(self):\n    """\n    Annule la dernière opération de nettoyage en restaurant les données depuis le dernier backup.\n    """\n    self._validate_df()\n    self._backup()\n\n    if hasattr(self, \'_backups\') and len(self._backups) > 1:\n        self.df = self._backups[-2].copy()\n        self._backups.pop()\n\n    self._log("rollback_last_operation", params={})\n    return self'
    },
    "rollback_to_checkpoint": {
        "params": ['checkpoint_name'],
        "docstring": "Restaure les données à partir d'un point de contrôle sauvegardé.\nSi aucun checkpoint n'est trouvé, lève une exception.",
        "code": 'def rollback_to_checkpoint(self, checkpoint_name):\n    """\n    Restaure les données à partir d\'un point de contrôle sauvegardé.\n    Si aucun checkpoint n\'est trouvé, lève une exception.\n    """\n    self._validate_df()\n    self._backup()\n\n    if checkpoint_name not in self._checkpoints:\n        raise ValueError(f"Checkpoint \'{checkpoint_name}\' not found")\n\n    self._df = self._checkpoints[checkpoint_name].copy()\n\n    self._log("rollback_to_checkpoint", params={"checkpoint_name": checkpoint_name})\n    return self'
    },
    "rolling_time_features": {
        "params": ['window_size', 'min_periods', 'center'],
        "docstring": 'Ajoute des caractéristiques temporelles glissantes aux données.\nCalcul les moyennes et écarts-types mobiles sur une fenêtre temporelle donnée.',
        "code": 'def rolling_time_features(self, window_size=3, min_periods=1, center=False):\n    """\n    Ajoute des caractéristiques temporelles glissantes aux données.\n    Calcul les moyennes et écarts-types mobiles sur une fenêtre temporelle donnée.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification de la colonne \'timestamp\'\n    if \'timestamp\' not in self.df.columns:\n        raise ValueError("La colonne \'timestamp\' est requise pour le calcul des caractéristiques temporelles")\n\n    # Tri du DataFrame par timestamp\n    self.df = self.df.sort_values(\'timestamp\')\n\n    # Calcul des caractéristiques glissantes\n    numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    numeric_cols.remove(\'timestamp\') if \'timestamp\' in numeric_cols else None\n\n    for col in numeric_cols:\n        self.df[f\'{col}_rolling_mean\'] = (\n            self.df[col]\n            .rolling(window=window_size, min_periods=min_periods, center=center)\n            .mean()\n        )\n        self.df[f\'{col}_rolling_std\'] = (\n            self.df[col]\n            .rolling(window=window_size, min_periods=min_periods, center=center)\n            .std()\n        )\n\n    self._log("rolling_time_features", params={\n        \'window_size\': window_size,\n        \'min_periods\': min_periods,\n        \'center\': center\n    })\n    return self'
    },
    "round_datetime": {
        "params": ['column', 'freq', 'inplace'],
        "docstring": "Arrondit les valeurs datetime d'une colonne à la fréquence spécifiée.",
        "code": 'def round_datetime(self, column: str, freq: str = \'H\', inplace: bool = True):\n    """\n    Arrondit les valeurs datetime d\'une colonne à la fréquence spécifiée.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column not in self.df.columns:\n        raise ValueError(f"La colonne \'{column}\' n\'existe pas dans le DataFrame.")\n\n    if not pd.api.types.is_datetime64_any_dtype(self.df[column]):\n        raise ValueError(f"La colonne \'{column}\' n\'est pas de type datetime.")\n\n    self.df[column] = self.df[column].dt.round(freq)\n\n    self._log("round_datetime", params={\'column\': column, \'freq\': freq})\n    return self'
    },
    "sanitize_column_names": {
        "params": ['df'],
        "docstring": 'Nettoie et standardise les noms des colonnes du DataFrame.\nRemplace les espaces par des underscores, convertit en minuscules,\net supprime les caractères spéciaux non autorisés.',
        "code": 'def sanitize_column_names(self, df=None):\n    """\n    Nettoie et standardise les noms des colonnes du DataFrame.\n    Remplace les espaces par des underscores, convertit en minuscules,\n    et supprime les caractères spéciaux non autorisés.\n    """\n    self._validate_df()\n    self._backup()\n\n    df = df if df is not None else self.df\n    df.columns = (\n        df.columns\n        .str.lower()\n        .str.replace(r\'\\s+\', \'_\', regex=True)\n        .str.replace(r\'[^a-z0-9_]+\', \'\', regex=True)\n    )\n\n    self._log("sanitize_column_names", params={})\n    return self'
    },
    "sanitize_filename": {
        "params": ['column_name'],
        "docstring": 'Nettoie et prépare les noms de fichiers dans une colonne donnée.\nSupprime les caractères spéciaux, espaces et normalise le format.',
        "code": 'def sanitize_filename(self, column_name):\n    """\n    Nettoie et prépare les noms de fichiers dans une colonne donnée.\n    Supprime les caractères spéciaux, espaces et normalise le format.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Remplacer les caractères spéciaux et espaces par des underscores\n    self.df[column_name] = self.df[column_name].str.replace(r\'[^\\w\\s-]\', \'\', regex=True)\n    self.df[column_name] = self.df[column_name].str.replace(r\'\\s+\', \'_\', regex=True)\n    self.df[column_name] = self.df[column_name].str.replace(r\'^-+|-+$\', \'\', regex=True)\n    self.df[column_name] = self.df[column_name].str.lower()\n\n    # Supprimer les doublons de underscores\n    self.df[column_name] = self.df[column_name].str.replace(r\'_+\', \'_\', regex=True)\n\n    # Supprimer les underscores en début et fin de chaîne\n    self.df[column_name] = self.df[column_name].str.strip(\'_\')\n\n    # Remplacer les espaces restants par des underscores\n    self.df[column_name] = self.df[column_name].str.replace(\' \', \'_\')\n\n    self._log("sanitize_filename", params={\'column_name\': column_name})\n    return self'
    },
    "save_transform_history": {
        "params": ['history_path'],
        "docstring": "Sauvegarde l'historique des transformations appliquées au DataFrame.\nSi aucun chemin n'est spécifié, l'historique est sauvegardé dans un fichier\nnommé 'transform_history.json' dans le répertoire courant.",
        "code": 'def save_transform_history(self, history_path=None):\n    """\n    Sauvegarde l\'historique des transformations appliquées au DataFrame.\n    Si aucun chemin n\'est spécifié, l\'historique est sauvegardé dans un fichier\n    nommé \'transform_history.json\' dans le répertoire courant.\n    """\n    self._validate_df()\n    self._backup()\n\n    import json\n    from pathlib import Path\n\n    if history_path is None:\n        history_path = \'transform_history.json\'\n\n    # Sauvegarde de l\'historique\n    with open(history_path, \'w\') as f:\n        json.dump(self._transform_history, f, indent=4)\n\n    self._log("save_transform_history", params={\'history_path\': history_path})\n    return self'
    },
    "segment_paragraphs": {
        "params": ['text_column', 'separator', 'new_columns'],
        "docstring": "Segmente un texte en paragraphes et les stocke dans de nouvelles colonnes.\n\n    Args:\n        text_column: Nom de la colonne contenant le texte à segmenter.\n        separator: Séparateur utilisé pour diviser les paragraphes (par défaut '\n\n').\n        new_columns: Liste des noms de colonnes pour stocker les paragraphes.\n                    Si None, utilise 'paragraph_1', 'paragraph_2', etc.\n\n    Returns:\n        self: L'instance actuelle de DataCleaner.",
        "code": 'def segment_paragraphs(self, text_column: str, separator: str = \'\\n\\n\', new_columns: list[str] | None = None):\n    """\n    Segmente un texte en paragraphes et les stocke dans de nouvelles colonnes.\n\n    Args:\n        text_column: Nom de la colonne contenant le texte à segmenter.\n        separator: Séparateur utilisé pour diviser les paragraphes (par défaut \'\\n\\n\').\n        new_columns: Liste des noms de colonnes pour stocker les paragraphes.\n                    Si None, utilise \'paragraph_1\', \'paragraph_2\', etc.\n\n    Returns:\n        self: L\'instance actuelle de DataCleaner.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des arguments\n    if text_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{text_column}\' n\'existe pas dans le DataFrame.")\n\n    if new_columns is None:\n        new_columns = [f\'paragraph_{i+1}\' for i in range(self.df[text_column].str.count(separator).max() + 1)]\n\n    # Création des nouvelles colonnes\n    for i, col in enumerate(new_columns):\n        self.df[col] = self.df[text_column].str.split(separator).str[i]\n\n    # Suppression des colonnes vides\n    self.df = self.df.dropna(how=\'all\', axis=1)\n\n    self._log("segment_paragraphs", params={\n        \'text_column\': text_column,\n        \'separator\': separator,\n        \'new_columns\': new_columns\n    })\n    return self'
    },
    "segment_sentences": {
        "params": ['text_column', 'sentence_delimiter', 'new_column_name'],
        "docstring": "Segmente les phrases d'une colonne de texte en utilisant un délimiteur spécifié.\nChaque phrase est stockée dans une nouvelle colonne sous forme de liste.\n\nArgs:\n    text_column (str): Nom de la colonne contenant le texte à segmenter.\n    sentence_delimiter (str, optional): Délimiteur utilisé pour séparer les phrases. Par défaut '.'\n    new_column_name (str, optional): Nom de la nouvelle colonne pour stocker les phrases segmentées. Par défaut 'sentences'",
        "code": 'def segment_sentences(self, text_column: str, sentence_delimiter: str = \'.\', new_column_name: str = \'sentences\'):\n    """\n    Segmente les phrases d\'une colonne de texte en utilisant un délimiteur spécifié.\n    Chaque phrase est stockée dans une nouvelle colonne sous forme de liste.\n\n    Args:\n        text_column (str): Nom de la colonne contenant le texte à segmenter.\n        sentence_delimiter (str, optional): Délimiteur utilisé pour séparer les phrases. Par défaut \'.\'\n        new_column_name (str, optional): Nom de la nouvelle colonne pour stocker les phrases segmentées. Par défaut \'sentences\'\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification de l\'existence des colonnes\n    if text_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{text_column}\' n\'existe pas dans le DataFrame.")\n\n    # Segmentation des phrases\n    self.df[new_column_name] = self.df[text_column].apply(\n        lambda x: [s.strip() for s in str(x).split(sentence_delimiter) if s.strip()]\n    )\n\n    self._log("segment_sentences", params={\n        \'text_column\': text_column,\n        \'sentence_delimiter\': sentence_delimiter,\n        \'new_column_name\': new_column_name\n    })\n    return self'
    },
    "smooth_numeric_noise": {
        "params": ['columns', 'window_size'],
        "docstring": "Lisse les valeurs numériques en appliquant une moyenne mobile sur les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont traitées.",
        "code": 'def smooth_numeric_noise(self, columns=None, window_size=3):\n    """\n    Lisse les valeurs numériques en appliquant une moyenne mobile sur les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    for col in numeric_cols:\n        self.df[col] = self.df[col].rolling(window=window_size, min_periods=1).mean()\n\n    self._log("smooth_numeric_noise", params={\'columns\': columns, \'window_size\': window_size})\n    return self'
    },
    "smooth_outliers": {
        "params": ['columns', 'method', 'clip_range'],
        "docstring": 'Lisse les valeurs aberrantes dans les colonnes spécifiées.',
        "code": 'def smooth_outliers(self, columns=None, method=\'clip\', clip_range=3):\n    """\n    Lisse les valeurs aberrantes dans les colonnes spécifiées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if method == \'clip\':\n            mean = self.df[col].mean()\n            std = self.df[col].std()\n            lower_bound = mean - clip_range * std\n            upper_bound = mean + clip_range * std\n            self.df[col] = self.df[col].clip(lower_bound, upper_bound)\n        elif method == \'replace\':\n            mean = self.df[col].mean()\n            std = self.df[col].std()\n            lower_bound = mean - clip_range * std\n            upper_bound = mean + clip_range * std\n            self.df[col] = np.where(\n                (self.df[col] < lower_bound) | (self.df[col] > upper_bound),\n                mean,\n                self.df[col]\n            )\n\n    self._log("smooth_outliers", params={\'columns\': columns, \'method\': method, \'clip_range\': clip_range})\n    return self'
    },
    "sort_columns_alphabetically": {
        "params": ['df'],
        "docstring": "Trie les colonnes du DataFrame par ordre alphabétique.\nSi aucun DataFrame n'est fourni, utilise le DataFrame interne de la classe.",
        "code": 'def sort_columns_alphabetically(self, df=None):\n    """\n    Trie les colonnes du DataFrame par ordre alphabétique.\n    Si aucun DataFrame n\'est fourni, utilise le DataFrame interne de la classe.\n    """\n    self._validate_df()\n    self._backup()\n\n    df_to_sort = df if df is not None else self.df\n    sorted_df = df_to_sort.reindex(sorted(df_to_sort.columns), axis=1)\n\n    if df is not None:\n        return sorted_df\n    else:\n        self.df = sorted_df\n\n    self._log("sort_columns_alphabetically", params={})\n    return self'
    },
    "spatial_join": {
        "params": ['gdf_points', 'gdf_polygons', 'join_key', 'output_key'],
        "docstring": 'Effectue une jointure spatiale entre un GeoDataFrame de points et un GeoDataFrame de polygones.\nLes points sont joints aux polygones dont ils sont contenus, et les propriétés des polygones\nsont ajoutées au GeoDataFrame de points.',
        "code": 'def spatial_join(self, gdf_points, gdf_polygons, join_key=\'geometry\', output_key=\'properties\'):\n    """\n    Effectue une jointure spatiale entre un GeoDataFrame de points et un GeoDataFrame de polygones.\n    Les points sont joints aux polygones dont ils sont contenus, et les propriétés des polygones\n    sont ajoutées au GeoDataFrame de points.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des types d\'entrée\n    if not hasattr(gdf_points, \'geometry\') or not hasattr(gdf_polygons, \'geometry\'):\n        raise ValueError("Les entrées doivent être des GeoDataFrames avec une colonne \'geometry\'")\n\n    # Réalisation de la jointure spatiale\n    joined_gdf = gpd.sjoin(gdf_points, gdf_polygons, how=\'left\', op=\'within\')\n\n    # Fusion des propriétés\n    if output_key:\n        joined_gdf = pd.concat([joined_gdf, joined_gdf[join_key].apply(pd.Series)], axis=1)\n        joined_gdf = joined_gdf.drop(columns=[join_key])\n\n    # Mise à jour du DataFrame\n    self.df = joined_gdf\n\n    self._log("spatial_join", params={\n        \'gdf_points\': str(gdf_points.shape),\n        \'gdf_polygons\': str(gdf_polygons.shape)\n    })\n    return self'
    },
    "sqrt_transform": {
        "params": ['columns'],
        "docstring": "Applique une transformation racine carrée aux colonnes spécifiées.\nSi aucune colonne n'est spécifiée, applique la transformation à toutes les colonnes numériques.",
        "code": 'def sqrt_transform(self, columns=None):\n    """\n    Applique une transformation racine carrée aux colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, applique la transformation à toutes les colonnes numériques.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns\n    else:\n        numeric_cols = columns\n\n    for col in numeric_cols:\n        self.df[col] = np.sqrt(self.df[col])\n\n    self._log("sqrt_transform", params={\'columns\': columns})\n    return self'
    },
    "stack_unstack": {
        "params": ['index_cols', 'value_cols'],
        "docstring": 'Empile et déplie les colonnes spécifiées pour restructurer le DataFrame.\nSi index_cols est fourni, les colonnes spécifiées sont utilisées comme index lors du dépliage.\nSi value_cols est fourni, seules ces colonnes seront empilées.\n\nParameters:\n    index_cols (list): Liste des noms de colonnes à utiliser comme index lors du dépliage.\n    value_cols (list): Liste des noms de colonnes à empiler. Si None, toutes les colonnes sont empilées.',
        "code": 'def stack_unstack(self, index_cols=None, value_cols=None):\n    """\n    Empile et déplie les colonnes spécifiées pour restructurer le DataFrame.\n    Si index_cols est fourni, les colonnes spécifiées sont utilisées comme index lors du dépliage.\n    Si value_cols est fourni, seules ces colonnes seront empilées.\n\n    Parameters:\n        index_cols (list): Liste des noms de colonnes à utiliser comme index lors du dépliage.\n        value_cols (list): Liste des noms de colonnes à empiler. Si None, toutes les colonnes sont empilées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if index_cols is not None:\n        self.df = self.df.set_index(index_cols)\n\n    if value_cols is not None:\n        cols_to_stack = [col for col in self.df.columns if col in value_cols]\n    else:\n        cols_to_stack = [col for col in self.df.columns if col not in index_cols]\n\n    stacked_df = self.df[cols_to_stack].stack()\n\n    if value_cols is not None:\n        new_columns = pd.MultiIndex.from_product([index_cols, [col for col in value_cols if col in self.df.columns]])\n    else:\n        new_columns = pd.MultiIndex.from_product([index_cols, [col for col in self.df.columns if col not in index_cols]])\n\n    unstacked_df = stacked_df.unstack()\n\n    self.df = pd.concat([self.df.drop(columns=cols_to_stack), unstacked_df], axis=1)\n\n    self._log("stack_unstack", params={"index_cols": index_cols, "value_cols": value_cols})\n    return self'
    },
    "standardize": {
        "params": ['columns'],
        "docstring": "Standardise les colonnes numériques en utilisant la méthode z-score.\nSi aucune colonne n'est spécifiée, toutes les colonnes numériques sont standardisées.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    Liste des noms de colonnes à standardiser. Si None, toutes les colonnes numériques sont traitées.",
        "code": 'def standardize(self, columns=None):\n    """\n    Standardise les colonnes numériques en utilisant la méthode z-score.\n    Si aucune colonne n\'est spécifiée, toutes les colonnes numériques sont standardisées.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        Liste des noms de colonnes à standardiser. Si None, toutes les colonnes numériques sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        numeric_cols = self.df.select_dtypes(include=[\'number\']).columns.tolist()\n    else:\n        numeric_cols = [col for col in columns if col in self.df.select_dtypes(include=[\'number\']).columns]\n\n    for col in numeric_cols:\n        mean = self.df[col].mean()\n        std = self.df[col].std()\n        if std != 0:\n            self.df[col] = (self.df[col] - mean) / std\n\n    self._log("standardize", params={\'columns\': columns})\n    return self'
    },
    "standardize_categories": {
        "params": ['columns'],
        "docstring": 'Standardize category values across specified columns by converting them to lowercase and stripping whitespace.\nIf no columns are specified, all object-type columns will be processed.\n\nParameters:\n-----------\ncolumns : list of str, optional\n    List of column names to standardize. If None, all object-type columns are processed.',
        "code": 'def standardize_categories(self, columns=None):\n    """\n    Standardize category values across specified columns by converting them to lowercase and stripping whitespace.\n    If no columns are specified, all object-type columns will be processed.\n\n    Parameters:\n    -----------\n    columns : list of str, optional\n        List of column names to standardize. If None, all object-type columns are processed.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n\n    for col in columns:\n        if col in self.df.columns:\n            self.df[col] = self.df[col].astype(str).str.lower().str.strip()\n\n    self._log("standardize_categories", params={\'columns\': columns})\n    return self'
    },
    "standardize_units": {
        "params": ['unit_mapping'],
        "docstring": 'Standardizes units in the DataFrame according to a provided mapping.\nConverts values to consistent units based on the unit_mapping dictionary.',
        "code": 'def standardize_units(self, unit_mapping=None):\n    """\n    Standardizes units in the DataFrame according to a provided mapping.\n    Converts values to consistent units based on the unit_mapping dictionary.\n    """\n    self._validate_df()\n    self._backup()\n\n    if unit_mapping is None:\n        unit_mapping = {}\n\n    for column, target_unit in unit_mapping.items():\n        if column in self.df.columns:\n            # Example conversion logic (customize based on your needs)\n            if target_unit == \'metric\':\n                # Convert to metric units (example: miles to km)\n                self.df[column] = self.df[column] * 1.60934\n            elif target_unit == \'imperial\':\n                # Convert to imperial units (example: km to miles)\n                self.df[column] = self.df[column] / 1.60934\n            # Add more conversion cases as needed\n\n    self._log("standardize_units", params={\'unit_mapping\': unit_mapping})\n    return self'
    },
    "stem_text_column": {
        "params": ['column_name', 'language', 'custom_stopwords'],
        "docstring": 'Applique le stemming sur une colonne de texte et nettoie les données.',
        "code": 'def stem_text_column(self, column_name: str, language: str = \'english\', custom_stopwords: list = None):\n    """\n    Applique le stemming sur une colonne de texte et nettoie les données.\n    """\n    self._validate_df()\n    self._backup()\n\n    from nltk.stem import PorterStemmer\n    from nltk.corpus import stopwords\n    import pandas as pd\n\n    # Initialisation du stemmer et des stopwords\n    stemmer = PorterStemmer()\n    if custom_stopwords is None:\n        stop_words = set(stopwords.words(language))\n    else:\n        stop_words = set(custom_stopwords)\n\n    # Fonction de nettoyage et stemming\n    def clean_and_stem(text):\n        if pd.isna(text):\n            return text\n        # Convertir en minuscule et supprimer les ponctuations\n        text = \'\'.join([char.lower() for char in text if char.isalpha() or char.isspace()])\n        # Tokenization et suppression des stopwords\n        words = text.split()\n        filtered_words = [word for word in words if word not in stop_words]\n        # Appliquer le stemming\n        stemmed_words = [stemmer.stem(word) for word in filtered_words]\n        return \' \'.join(stemmed_words)\n\n    # Appliquer la fonction à la colonne spécifiée\n    self.df[column_name] = self.df[column_name].apply(clean_and_stem)\n\n    self._log("stem_text_column", params={\'column_name\': column_name, \'language\': language})\n    return self'
    },
    "strip_bom": {
        "params": ['columns'],
        "docstring": "Supprime les caractères BOM (Byte Order Mark) des colonnes spécifiées.\nSi aucune colonne n'est spécifiée, traite toutes les colonnes du DataFrame.",
        "code": 'def strip_bom(self, columns=None):\n    """\n    Supprime les caractères BOM (Byte Order Mark) des colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, traite toutes les colonnes du DataFrame.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        if self.df[col].dtype == \'object\':\n            self.df[col] = self.df[col].str.replace(\'\\ufeff\', \'\', regex=False)\n\n    self._log("strip_bom", params={\'columns\': columns})\n    return self'
    },
    "strip_non_numeric": {
        "params": ['columns'],
        "docstring": "Supprime les caractères non numériques des colonnes spécifiées.\nSi aucune colonne n'est spécifiée, traite toutes les colonnes du DataFrame.\n\nParameters:\n    columns (list, optional): Liste des noms de colonnes à traiter. Par défaut None.",
        "code": 'def strip_non_numeric(self, columns=None):\n    """\n    Supprime les caractères non numériques des colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, traite toutes les colonnes du DataFrame.\n\n    Parameters:\n        columns (list, optional): Liste des noms de colonnes à traiter. Par défaut None.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        if self.df[col].dtype == object:  # Vérifie si la colonne est de type objet (texte)\n            self.df[col] = self.df[col].str.replace(r\'[^0-9.-]\', \'\', regex=True)\n\n    self._log("strip_non_numeric", params={"columns": columns})\n    return self'
    },
    "strip_whitespace_columns": {
        "params": ['columns'],
        "docstring": "Supprime les espaces en début et fin de chaîne pour les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, applique l'opération à toutes les colonnes de type string.",
        "code": 'def strip_whitespace_columns(self, columns=None):\n    """\n    Supprime les espaces en début et fin de chaîne pour les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, applique l\'opération à toutes les colonnes de type string.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        string_columns = self.df.select_dtypes(include=[\'object\']).columns\n    else:\n        string_columns = columns\n\n    for col in string_columns:\n        self.df[col] = self.df[col].str.strip()\n\n    self._log("strip_whitespace_columns", params={\'columns\': columns})\n    return self'
    },
    "target_encode": {
        "params": ['target_column', 'categorical_columns', 'alpha', 'prior'],
        "docstring": "Applique le target encoding sur les colonnes catégorielles spécifiées.\nLe target encoding remplace les catégories par la probabilité moyenne\nde l'événement cible pour chaque catégorie.\n\nArgs:\n    target_column (str): Nom de la colonne cible binaire.\n    categorical_columns (list): Liste des noms des colonnes catégorielles à encoder.\n    alpha (float, optional): Paramètre de régularisation pour éviter le surapprentissage. Default 1.0.\n    prior (float, optional): A priori global sur la probabilité de l'événement cible. Default None.\n\nReturns:\n    DataCleaner: Instance actuelle pour le chaînage de méthodes.",
        "code": 'def target_encode(self, target_column: str, categorical_columns: list, alpha: float = 1.0, prior: float = None):\n    """\n    Applique le target encoding sur les colonnes catégorielles spécifiées.\n    Le target encoding remplace les catégories par la probabilité moyenne\n    de l\'événement cible pour chaque catégorie.\n\n    Args:\n        target_column (str): Nom de la colonne cible binaire.\n        categorical_columns (list): Liste des noms des colonnes catégorielles à encoder.\n        alpha (float, optional): Paramètre de régularisation pour éviter le surapprentissage. Default 1.0.\n        prior (float, optional): A priori global sur la probabilité de l\'événement cible. Default None.\n\n    Returns:\n        DataCleaner: Instance actuelle pour le chaînage de méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Calculer la probabilité globale si prior n\'est pas fourni\n    if prior is None:\n        global_mean = self.df[target_column].mean()\n    else:\n        global_mean = prior\n\n    # Appliquer le target encoding pour chaque colonne catégorielle\n    for col in categorical_columns:\n        # Calculer la probabilité moyenne pour chaque catégorie\n        category_means = self.df.groupby(col)[target_column].mean()\n\n        # Appliquer la régularisation\n        category_means = (category_means * len(self.df[col]) + alpha * global_mean) / (len(self.df[col]) + alpha)\n\n        # Remplacer les valeurs par leur probabilité moyenne\n        self.df[col] = self.df[col].map(category_means)\n\n    self._log("target_encode", params={\n        "target_column": target_column,\n        "categorical_columns": categorical_columns,\n        "alpha": alpha,\n        "prior": prior\n    })\n    return self'
    },
    "time_difference": {
        "params": ['date_column', 'reference_date', 'output_column'],
        "docstring": 'Calcule la différence de temps entre une colonne de dates et une date de référence.\nLa différence est calculée en jours, heures, minutes et secondes.\n\nParameters:\n    date_column (str): Nom de la colonne contenant les dates à comparer\n    reference_date (Union[str, datetime]): Date de référence pour le calcul\n    output_column (str): Nom de la colonne de sortie. Si None, utilise date_column + "_diff"',
        "code": 'def time_difference(self, date_column: str, reference_date: Union[str, datetime], output_column: str = None):\n    """\n    Calcule la différence de temps entre une colonne de dates et une date de référence.\n    La différence est calculée en jours, heures, minutes et secondes.\n\n    Parameters:\n        date_column (str): Nom de la colonne contenant les dates à comparer\n        reference_date (Union[str, datetime]): Date de référence pour le calcul\n        output_column (str): Nom de la colonne de sortie. Si None, utilise date_column + "_diff"\n    """\n    self._validate_df()\n    self._backup()\n\n    # Convertir la date de référence en datetime si c\'est une chaîne\n    if isinstance(reference_date, str):\n        reference_date = pd.to_datetime(reference_date)\n\n    # Créer le nom de la colonne de sortie si non spécifié\n    if output_column is None:\n        output_column = f"{date_column}_diff"\n\n    # Calculer la différence de temps\n    self.df[output_column] = (pd.to_datetime(self.df[date_column]) - reference_date).dt.total_seconds()\n\n    self._log("time_difference", params={\n        "date_column": date_column,\n        "reference_date": reference_date,\n        "output_column": output_column\n    })\n    return self'
    },
    "tokenize_columns": {
        "params": ['columns'],
        "docstring": 'Tokenize les noms des colonnes du DataFrame en remplaçant les espaces par des underscores\net en convertissant tout en minuscules.\n\nArgs:\n    columns (list, optional): Liste des colonnes à tokeniser. Si None, toutes les colonnes sont traitées.',
        "code": 'def tokenize_columns(self, columns=None):\n    """\n    Tokenize les noms des colonnes du DataFrame en remplaçant les espaces par des underscores\n    et en convertissant tout en minuscules.\n\n    Args:\n        columns (list, optional): Liste des colonnes à tokeniser. Si None, toutes les colonnes sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        new_col_name = col.replace(\' \', \'_\').lower()\n        self.df.rename(columns={col: new_col_name}, inplace=True)\n\n    self._log("tokenize_columns", params={\'columns\': columns})\n    return self'
    },
    "tokenize_sensitive_data": {
        "params": ['columns'],
        "docstring": 'Tokenizes sensitive data in specified columns to protect privacy.\nIf no columns are specified, attempts to auto-detect sensitive columns.',
        "code": 'def tokenize_sensitive_data(self, columns=None):\n    """\n    Tokenizes sensitive data in specified columns to protect privacy.\n    If no columns are specified, attempts to auto-detect sensitive columns.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        # Auto-detect potentially sensitive columns (e.g., containing email, SSN patterns)\n        columns = self.df.select_dtypes(include=[\'object\']).columns.tolist()\n        columns = [col for col in columns if self.df[col].str.contains(r\'@|-|\\d{3}-\\d{2}-\\d{4}\', na=False).any()]\n\n    for col in columns:\n        if col not in self.df.columns:\n            continue\n\n        # Generate tokens (in a real implementation, use proper tokenization)\n        self.df[col] = \'TOKEN_\' + self.df[col].astype(str).apply(lambda x: hash(x) % 10**8)\n\n    self._log("tokenize_sensitive_data", params={\'columns\': columns})\n    return self'
    },
    "transpose_table": {
        "params": ['columns_to_transpose'],
        "docstring": "Transpose les colonnes spécifiées en lignes et vice versa.\nSi aucune colonne n'est spécifiée, transpose l'ensemble du DataFrame.\n\nParameters:\n    columns_to_transpose (list): Liste des noms de colonnes à transposer.\n                                 Si None, transpose l'ensemble du DataFrame.\n\nReturns:\n    self: Retourne l'instance courante pour permettre le chaînage.",
        "code": 'def transpose_table(self, columns_to_transpose=None):\n    """\n    Transpose les colonnes spécifiées en lignes et vice versa.\n    Si aucune colonne n\'est spécifiée, transpose l\'ensemble du DataFrame.\n\n    Parameters:\n        columns_to_transpose (list): Liste des noms de colonnes à transposer.\n                                     Si None, transpose l\'ensemble du DataFrame.\n\n    Returns:\n        self: Retourne l\'instance courante pour permettre le chaînage.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns_to_transpose is None:\n        self.df = self.df.transpose()\n    else:\n        # Sélectionner les colonnes à transposer\n        cols_to_transpose = self.df[columns_to_transpose]\n        # Transposer ces colonnes\n        transposed_cols = cols_to_transpose.transpose()\n        # Remplacer les colonnes originales par la version transposée\n        self.df = pd.concat([self.df.drop(columns=columns_to_transpose, axis=1),\n                            transposed_cols], axis=1)\n\n    self._log("transpose_table", params={"columns_to_transpose": columns_to_transpose})\n    return self'
    },
    "trim_strings": {
        "params": ['columns'],
        "docstring": "Supprime les espaces en début et fin de chaîne pour les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, applique l'opération à toutes les colonnes de type string.",
        "code": 'def trim_strings(self, columns=None):\n    """\n    Supprime les espaces en début et fin de chaîne pour les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, applique l\'opération à toutes les colonnes de type string.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        string_columns = self.df.select_dtypes(include=[\'object\']).columns\n    else:\n        string_columns = columns\n\n    for col in string_columns:\n        self.df[col] = self.df[col].astype(str).str.strip()\n\n    self._log("trim_strings", params={\'columns\': columns})\n    return self'
    },
    "unify_date_format": {
        "params": ['date_columns'],
        "docstring": "Unifie le format des colonnes de dates dans le DataFrame en utilisant le format ISO 8601 (YYYY-MM-DD).\nSi aucune colonne n'est spécifiée, toutes les colonnes de type datetime sont traitées.",
        "code": 'def unify_date_format(self, date_columns=None):\n    """\n    Unifie le format des colonnes de dates dans le DataFrame en utilisant le format ISO 8601 (YYYY-MM-DD).\n    Si aucune colonne n\'est spécifiée, toutes les colonnes de type datetime sont traitées.\n    """\n    self._validate_df()\n    self._backup()\n\n    if date_columns is None:\n        date_columns = self.df.select_dtypes(include=[\'datetime\']).columns.tolist()\n\n    for col in date_columns:\n        if col in self.df.columns:\n            self.df[col] = pd.to_datetime(self.df[col]).dt.strftime(\'%Y-%m-%d\')\n\n    self._log("unify_date_format", params={"date_columns": date_columns})\n    return self'
    },
    "unpivot_columns": {
        "params": ['id_vars', 'var_name', 'value_name'],
        "docstring": 'Transforme un DataFrame en format large vers un format long en utilisant les colonnes spécifiées.',
        "code": 'def unpivot_columns(self, id_vars=None, var_name=\'variable\', value_name=\'value\'):\n    """\n    Transforme un DataFrame en format large vers un format long en utilisant les colonnes spécifiées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des paramètres\n    if id_vars is None:\n        raise ValueError("Les colonnes d\'identification (id_vars) doivent être spécifiées.")\n\n    # Melt du DataFrame\n    self.df = pd.melt(self.df, id_vars=id_vars, var_name=var_name, value_name=value_name)\n\n    self._log("unpivot_columns", params={\'id_vars\': id_vars, \'var_name\': var_name, \'value_name\': value_name})\n    return self'
    },
    "validate_category_membership": {
        "params": ['category_column', 'valid_categories'],
        "docstring": "Valide que les valeurs d'une colonne de catégorie appartiennent à une liste de catégories valides.\nLes valeurs non valides sont remplacées par NaN.",
        "code": 'def validate_category_membership(self, category_column: str, valid_categories: list):\n    """\n    Valide que les valeurs d\'une colonne de catégorie appartiennent à une liste de catégories valides.\n    Les valeurs non valides sont remplacées par NaN.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des paramètres\n    if category_column not in self.df.columns:\n        raise ValueError(f"La colonne \'{category_column}\' n\'existe pas dans le DataFrame.")\n    if not isinstance(valid_categories, list):\n        raise TypeError("valid_categories doit être une liste.")\n\n    # Remplacement des valeurs non valides par NaN\n    self.df[category_column] = self.df[category_column].where(\n        self.df[category_column].isin(valid_categories), np.nan\n    )\n\n    self._log("validate_category_membership", params={\n        "category_column": category_column,\n        "valid_categories": valid_categories\n    })\n    return self'
    },
    "validate_coordinates": {
        "params": ['latitude_col', 'longitude_col'],
        "docstring": 'Valide et corrige les coordonnées géographiques dans le DataFrame.\nVérifie que les valeurs sont dans les plages valides (-90 à 90 pour latitude,\n-180 à 180 pour longitude) et remplace les valeurs invalides par NaN.',
        "code": 'def validate_coordinates(self, latitude_col=\'latitude\', longitude_col=\'longitude\'):\n    """\n    Valide et corrige les coordonnées géographiques dans le DataFrame.\n    Vérifie que les valeurs sont dans les plages valides (-90 à 90 pour latitude,\n    -180 à 180 pour longitude) et remplace les valeurs invalides par NaN.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Validation des colonnes\n    if latitude_col not in self.df.columns or longitude_col not in self.df.columns:\n        raise ValueError(f"Les colonnes \'{latitude_col}\' ou \'{longitude_col}\' n\'existent pas dans le DataFrame")\n\n    # Validation des coordonnées\n    self.df.loc[(self.df[latitude_col] < -90) | (self.df[latitude_col] > 90), latitude_col] = np.nan\n    self.df.loc[(self.df[longitude_col] < -180) | (self.df[longitude_col] > 180), longitude_col] = np.nan\n\n    self._log("validate_coordinates", params={\'latitude_col\': latitude_col, \'longitude_col\': longitude_col})\n    return self'
    },
    "validate_email": {
        "params": ['column_name'],
        "docstring": 'Valide les adresses email dans une colonne spécifiée.\nSupprime les lignes avec des emails invalides et standardise le format.',
        "code": 'def validate_email(self, column_name):\n    """\n    Valide les adresses email dans une colonne spécifiée.\n    Supprime les lignes avec des emails invalides et standardise le format.\n    """\n    self._validate_df()\n    self._backup()\n\n    import re\n    email_pattern = r\'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\'\n\n    # Supprimer les lignes avec des emails invalides\n    self.df = self.df[self.df[column_name].str.match(email_pattern, na=False)]\n\n    # Standardiser le format des emails (minuscules)\n    self.df[column_name] = self.df[column_name].str.lower()\n\n    self._log("validate_email", params={\'column_name\': column_name})\n    return self'
    },
    "validate_ip": {
        "params": ['column_name'],
        "docstring": 'Valide les adresses IP dans une colonne spécifiée.\nLes adresses IP doivent être au format IPv4 valide (quatre octets entre 0 et 255).',
        "code": 'def validate_ip(self, column_name):\n    """\n    Valide les adresses IP dans une colonne spécifiée.\n    Les adresses IP doivent être au format IPv4 valide (quatre octets entre 0 et 255).\n    """\n    self._validate_df()\n    self._backup()\n\n    import re\n\n    ip_pattern = r\'^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\'\n\n    def is_valid_ip(ip):\n        if pd.isna(ip):\n            return False\n        return bool(re.match(ip_pattern, str(ip)))\n\n    self.df[column_name] = self.df[column_name].apply(is_valid_ip)\n\n    self._log("validate_ip", params={\'column_name\': column_name})\n    return self'
    },
    "validate_monotonicity": {
        "params": ['column_name', 'increasing'],
        "docstring": "Valide la monotonicité d'une colonne (croissante ou décroissante).\n\nArgs:\n    column_name: Nom de la colonne à vérifier.\n    increasing: Si True, vérifie si la colonne est monotone croissante,\n               sinon vérifie si elle est monotone décroissante.",
        "code": 'def validate_monotonicity(self, column_name: str, increasing: bool = True):\n    """\n    Valide la monotonicité d\'une colonne (croissante ou décroissante).\n\n    Args:\n        column_name: Nom de la colonne à vérifier.\n        increasing: Si True, vérifie si la colonne est monotone croissante,\n                   sinon vérifie si elle est monotone décroissante.\n    """\n    self._validate_df()\n    self._backup()\n\n    if column_name not in self.df.columns:\n        raise ValueError(f"Column \'{column_name}\' not found in DataFrame")\n\n    if increasing:\n        monotonic = self.df[column_name].is_monotonic_increasing\n    else:\n        monotonic = self.df[column_name].is_monotonic_decreasing\n\n    if not monotonic:\n        raise ValueError(f"Column \'{column_name}\' is not monotonic {\'increasing\' if increasing else \'decreasing\'}")\n\n    self._log("validate_monotonicity", params={"column_name": column_name, "increasing": increasing})\n    return self'
    },
    "validate_no_duplicates": {
        "params": ['subset'],
        "docstring": "Supprime les doublons des données en fonction d'un sous-ensemble de colonnes spécifié.\nSi aucun sous-ensemble n'est fourni, vérifie l'ensemble du DataFrame.\n\nParameters:\n-----------\nsubset : list of str, optional\n    Liste des noms de colonnes à prendre en compte pour détecter les doublons.\n    Si None, toutes les colonnes sont utilisées.\n\nReturns:\n--------\nDataCleaner\n    Retourne l'instance courante pour permettre le chaînage des méthodes.",
        "code": 'def validate_no_duplicates(self, subset=None):\n    """\n    Supprime les doublons des données en fonction d\'un sous-ensemble de colonnes spécifié.\n    Si aucun sous-ensemble n\'est fourni, vérifie l\'ensemble du DataFrame.\n\n    Parameters:\n    -----------\n    subset : list of str, optional\n        Liste des noms de colonnes à prendre en compte pour détecter les doublons.\n        Si None, toutes les colonnes sont utilisées.\n\n    Returns:\n    --------\n    DataCleaner\n        Retourne l\'instance courante pour permettre le chaînage des méthodes.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des doublons\n    if subset:\n        self.df = self.df.drop_duplicates(subset=subset)\n    else:\n        self.df = self.df.drop_duplicates()\n\n    self._log("validate_no_duplicates", params={"subset": subset})\n    return self'
    },
    "validate_no_missing": {
        "params": ['columns'],
        "docstring": "Vérifie qu'il n'y a pas de valeurs manquantes dans les colonnes spécifiées.\nSi des valeurs manquantes sont trouvées, elles sont remplacées par la valeur par défaut\nde leur type (0 pour les entiers, 0.0 pour les flottants, '' pour les chaînes).",
        "code": 'def validate_no_missing(self, columns=None):\n    """\n    Vérifie qu\'il n\'y a pas de valeurs manquantes dans les colonnes spécifiées.\n    Si des valeurs manquantes sont trouvées, elles sont remplacées par la valeur par défaut\n    de leur type (0 pour les entiers, 0.0 pour les flottants, \'\' pour les chaînes).\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns\n\n    for col in columns:\n        if self.df[col].isnull().any():\n            dtype = self.df[col].dtype\n            if np.issubdtype(dtype, np.integer):\n                self.df[col].fillna(0, inplace=True)\n            elif np.issubdtype(dtype, np.floating):\n                self.df[col].fillna(0.0, inplace=True)\n            else:\n                self.df[col].fillna(\'\', inplace=True)\n\n    self._log("validate_no_missing", params={\'columns\': columns})\n    return self'
    },
    "validate_numeric_range": {
        "params": ['column', 'min_val', 'max_val'],
        "docstring": "Valide que les valeurs d'une colonne numérique sont dans une plage spécifiée.\nLes valeurs hors plage sont remplacées par NaN.",
        "code": 'def validate_numeric_range(self, column: str, min_val: float = None, max_val: float = None):\n    """\n    Valide que les valeurs d\'une colonne numérique sont dans une plage spécifiée.\n    Les valeurs hors plage sont remplacées par NaN.\n    """\n    self._validate_df()\n    self._backup()\n\n    if min_val is not None or max_val is not None:\n        mask = pd.Series(True, index=self.df.index)\n        if min_val is not None:\n            mask &= self.df[column] >= min_val\n        if max_val is not None:\n            mask &= self.df[column] <= max_val\n        self.df.loc[~mask, column] = np.nan\n\n    self._log("validate_numeric_range", params={"column": column, "min_val": min_val, "max_val": max_val})\n    return self'
    },
    "validate_phone": {
        "params": ['column_name', 'country_code'],
        "docstring": 'Valide et nettoie les numéros de téléphone dans la colonne spécifiée.\nSupprime les caractères non numériques, ajoute le code pays si fourni,\net vérifie la longueur minimale du numéro.',
        "code": 'def validate_phone(self, column_name: str, country_code: str = None):\n    """\n    Valide et nettoie les numéros de téléphone dans la colonne spécifiée.\n    Supprime les caractères non numériques, ajoute le code pays si fourni,\n    et vérifie la longueur minimale du numéro.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des caractères non numériques et espaces\n    self.df[column_name] = self.df[column_name].astype(str).str.replace(r\'[^\\d]\', \'\', regex=True)\n\n    # Ajout du code pays si fourni\n    if country_code:\n        self.df[column_name] = country_code + self.df[column_name]\n\n    # Vérification de la longueur minimale (10 chiffres pour un numéro international standard)\n    self.df = self.df[self.df[column_name].str.len() >= 10]\n\n    self._log("validate_phone", params={\'column_name\': column_name, \'country_code\': country_code})\n    return self'
    },
    "validate_regex": {
        "params": ['column_name', 'regex_pattern'],
        "docstring": "Valide les valeurs d'une colonne en fonction d'un motif regex.\nLes lignes ne correspondant pas au motif sont supprimées.",
        "code": 'def validate_regex(self, column_name: str, regex_pattern: str):\n    """\n    Valide les valeurs d\'une colonne en fonction d\'un motif regex.\n    Les lignes ne correspondant pas au motif sont supprimées.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Suppression des lignes ne correspondant pas au motif regex\n    mask = self.df[column_name].str.contains(regex_pattern, na=False)\n    self.df = self.df[mask]\n\n    self._log("validate_regex", params={"column_name": column_name, "regex_pattern": regex_pattern})\n    return self'
    },
    "validate_schema": {
        "params": ['schema'],
        "docstring": "Valide le schéma des données en fonction du dictionnaire de schéma fourni.\nVérifie les types de colonnes, les valeurs uniques et les contraintes spécifiques.\n\nArgs:\n    schema (dict): Dictionnaire spécifiant le schéma attendu pour les données.\n                  Exemple: {'col1': 'int', 'col2': ['val1', 'val2']}",
        "code": 'def validate_schema(self, schema):\n    """\n    Valide le schéma des données en fonction du dictionnaire de schéma fourni.\n    Vérifie les types de colonnes, les valeurs uniques et les contraintes spécifiques.\n\n    Args:\n        schema (dict): Dictionnaire spécifiant le schéma attendu pour les données.\n                      Exemple: {\'col1\': \'int\', \'col2\': [\'val1\', \'val2\']}\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des colonnes présentes dans le DataFrame\n    missing_columns = set(schema.keys()) - set(self.df.columns)\n    if missing_columns:\n        raise ValueError(f"Les colonnes suivantes sont manquantes: {missing_columns}")\n\n    # Validation des types et valeurs pour chaque colonne\n    for column, expected_type in schema.items():\n        if isinstance(expected_type, type):\n            # Vérification du type de colonne\n            if not np.issubdtype(self.df[column].dtype, expected_type):\n                raise ValueError(f"La colonne \'{column}\' doit être de type {expected_type.__name__}")\n        elif isinstance(expected_type, list):\n            # Vérification des valeurs uniques\n            invalid_values = set(self.df[column]) - set(expected_type)\n            if invalid_values:\n                raise ValueError(f"Valeurs invalides dans la colonne \'{column}\': {invalid_values}")\n        else:\n            raise ValueError(f"Type de schéma non supporté pour la colonne \'{column}\'")\n\n    self._log("validate_schema", params={\'schema\': schema})\n    return self'
    },
    "validate_schema_constraints": {
        "params": ['schema'],
        "docstring": 'Valide les contraintes de schéma sur le DataFrame en cours.\nVérifie que les colonnes, types et valeurs respectent le schéma fourni.',
        "code": 'def validate_schema_constraints(self, schema):\n    """\n    Valide les contraintes de schéma sur le DataFrame en cours.\n    Vérifie que les colonnes, types et valeurs respectent le schéma fourni.\n    """\n    self._validate_df()\n    self._backup()\n\n    # Vérification des colonnes présentes\n    missing_columns = set(schema.keys()) - set(self.df.columns)\n    if missing_columns:\n        raise ValueError(f"Colonnes manquantes dans le DataFrame: {missing_columns}")\n\n    # Vérification des types de données\n    for column, expected_type in schema.items():\n        if not pd.api.types.is_dtype_equal(self.df[column].dtype, expected_type):\n            raise TypeError(f"Type de données incorrect pour la colonne {column}. Attendu: {expected_type}, trouvé: {self.df[column].dtype}")\n\n    # Vérification des valeurs uniques (si spécifiées dans le schéma)\n    for column, constraints in schema.items():\n        if isinstance(constraints, dict) and \'unique\' in constraints:\n            if not self.df[column].is_unique:\n                raise ValueError(f"La colonne {column} doit contenir des valeurs uniques")\n\n    self._log("validate_schema_constraints", params={"schema": schema})\n    return self'
    },
    "validate_unique": {
        "params": ['columns'],
        "docstring": "Vérifie et garantit l'unicité des valeurs dans les colonnes spécifiées.\nSi aucune colonne n'est spécifiée, vérifie l'unicité de toutes les colonnes.\n\nParameters:\n    columns (list, optional): Liste des noms de colonnes à vérifier. Par défaut None.",
        "code": 'def validate_unique(self, columns=None):\n    """\n    Vérifie et garantit l\'unicité des valeurs dans les colonnes spécifiées.\n    Si aucune colonne n\'est spécifiée, vérifie l\'unicité de toutes les colonnes.\n\n    Parameters:\n        columns (list, optional): Liste des noms de colonnes à vérifier. Par défaut None.\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.columns.tolist()\n    elif not isinstance(columns, list):\n        raise ValueError("Le paramètre \'columns\' doit être une liste de noms de colonnes.")\n\n    for col in columns:\n        if col not in self.df.columns:\n            raise ValueError(f"La colonne \'{col}\' n\'existe pas dans le DataFrame.")\n\n        duplicates = self.df.duplicated(subset=[col], keep=False)\n        if duplicates.any():\n            self._log(f"Duplicates found in column \'{col}\'. Dropping them.")\n            self.df = self.df[~duplicates]\n\n    self._log("validate_unique", params={"columns": columns})\n    return self'
    },
    "validate_url": {
        "params": ['url_column'],
        "docstring": 'Valide les URLs dans une colonne spécifiée et les formate correctement.\nSupprime les lignes avec des URLs invalides ou vides.\n\nParameters:\n    url_column (str): Nom de la colonne contenant les URLs à valider',
        "code": 'def validate_url(self, url_column: str):\n    """\n    Valide les URLs dans une colonne spécifiée et les formate correctement.\n    Supprime les lignes avec des URLs invalides ou vides.\n\n    Parameters:\n        url_column (str): Nom de la colonne contenant les URLs à valider\n    """\n    self._validate_df()\n    self._backup()\n\n    import pandas as pd\n    from urllib.parse import urlparse\n\n    def is_valid_url(url):\n        try:\n            result = urlparse(url)\n            return all([result.scheme, result.netloc])\n        except:\n            return False\n\n    # Convertir les URLs en minuscules et valider\n    self.df[url_column] = self.df[url_column].str.lower()\n    valid_mask = self.df[url_column].apply(is_valid_url)\n\n    # Supprimer les lignes avec des URLs invalides\n    self.df = self.df[valid_mask]\n\n    # Ajouter un préfixe http:// si manquant\n    self.df[url_column] = self.df[url_column].apply(\n        lambda x: f"http://{x}" if not x.startswith((\'http://\', \'https://\')) else x\n    )\n\n    self._log("validate_url", params={\'url_column\': url_column})\n    return self'
    },
    "winsorize": {
        "params": ['columns', 'limits'],
        "docstring": 'Applique la méthode de Winsorization pour limiter les valeurs aberrantes dans les colonnes spécifiées.\nLa Winsorization remplace les valeurs extrêmes par la valeur la plus proche non extrême.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à traiter. Si None, toutes les colonnes numériques sont traitées.\nlimits : tuple of float, optional\n    Pourcentage de valeurs à limiter en bas et en haut (par défaut 5% des deux côtés).',
        "code": 'def winsorize(self, columns=None, limits=(0.05, 0.05)):\n    """\n    Applique la méthode de Winsorization pour limiter les valeurs aberrantes dans les colonnes spécifiées.\n    La Winsorization remplace les valeurs extrêmes par la valeur la plus proche non extrême.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à traiter. Si None, toutes les colonnes numériques sont traitées.\n    limits : tuple of float, optional\n        Pourcentage de valeurs à limiter en bas et en haut (par défaut 5% des deux côtés).\n    """\n    self._validate_df()\n    self._backup()\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[\'number\']).columns\n\n    for col in columns:\n        if col not in self.df.columns or not np.issubdtype(self.df[col].dtype, np.number):\n            continue\n\n        lower_limit = self.df[col].quantile(limits[0])\n        upper_limit = self.df[col].quantile(1 - limits[1])\n\n        self.df.loc[self.df[col] < lower_limit, col] = lower_limit\n        self.df.loc[self.df[col] > upper_limit, col] = upper_limit\n\n    self._log("winsorize", params={\'columns\': columns, \'limits\': limits})\n    return self'
    },
    "yeojohnson_transform": {
        "params": ['columns'],
        "docstring": 'Applique une transformation de Yeo-Johnson aux colonnes spécifiées.\nCette transformation est utile pour normaliser des données non gaussiennes\ntout en conservant les valeurs négatives.\n\nParameters:\n-----------\ncolumns : list, optional\n    Liste des colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.',
        "code": 'def yeojohnson_transform(self, columns=None):\n    """\n    Applique une transformation de Yeo-Johnson aux colonnes spécifiées.\n    Cette transformation est utile pour normaliser des données non gaussiennes\n    tout en conservant les valeurs négatives.\n\n    Parameters:\n    -----------\n    columns : list, optional\n        Liste des colonnes à transformer. Si None, toutes les colonnes numériques sont transformées.\n    """\n    self._validate_df()\n    self._backup()\n\n    from scipy.stats import yeojohnson\n    import numpy as np\n\n    if columns is None:\n        columns = self.df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for col in columns:\n        if col in self.df.columns:\n            # Appliquer la transformation Yeo-Johnson\n            transformed_data, _ = yeojohnson(self.df[col].values)\n            self.df[col] = transformed_data\n\n    self._log("yeojohnson_transform", params={"columns": columns})\n    return self'
    }
}


# FONCTIONS UTILITAIRES
def get_function_names():
    """Retourne la liste de tous les noms de fonctions"""
    return sorted(ALL_FUNCTIONS.keys())

def get_function_info(name):
    """Retourne les infos d'une fonction"""
    return ALL_FUNCTIONS.get(name)

def search_functions(keyword):
    """Recherche des fonctions par mot-clé"""
    keyword = keyword.lower()
    results = []
    for name, data in ALL_FUNCTIONS.items():
        if keyword in name.lower() or keyword in data["docstring"].lower():
            results.append(name)
    return sorted(results)

def get_functions_by_category():
    """Retourne les fonctions regroupées par catégorie"""
    categories = {
        "Detection": [f for f in ALL_FUNCTIONS if f.startswith("detect_")],
        "Imputation": [f for f in ALL_FUNCTIONS if f.startswith("impute_")],
        "Validation": [f for f in ALL_FUNCTIONS if f.startswith("validate_")],
        "Conversion": [f for f in ALL_FUNCTIONS if f.startswith("convert_")],
        "Normalization": [f for f in ALL_FUNCTIONS if f.startswith("normalize_")],
        "Extraction": [f for f in ALL_FUNCTIONS if f.startswith("extract_")],
        "Anonymization": [f for f in ALL_FUNCTIONS if f.startswith("anonymize_")],
        "Pipeline": [f for f in ALL_FUNCTIONS if f.startswith("pipeline_")],
        "Export": [f for f in ALL_FUNCTIONS if f.startswith("export_")],
        "Auto": [f for f in ALL_FUNCTIONS if f.startswith("auto_")],
    }
    return {k: sorted(v) for k, v in categories.items() if v}

# Statistiques
TOTAL_FUNCTIONS = 257
FUNCTION_NAMES = get_function_names()
